{
  "name": "paged_attention_v2",
  "kernel": {
    "kernel_type": "cuda",
    "source_code": "/*\n * Adapted from\n * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp\n * Copyright (c) 2023, The vLLM team.\n * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n// Begin include attention_kernels.cuh\n/*\n * Adapted from\n * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp\n * Copyright (c) 2023, The vLLM team.\n * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <torch/all.h>\n#include <ATen/cuda/CUDAContext.h>\n#include <c10/cuda/CUDAGuard.h>\n#include <algorithm>\n\n// Begin include attention_dtypes.h\n#pragma once\n\n// Begin include attention_generic.cuh\n/*\n * Adapted from\n * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h\n * Copyright (c) 2023, The vLLM team.\n * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n#pragma once\n\n#include <stdint.h>\n\nnamespace vllm {\n\n// A vector type to store Q, K, V elements.\ntemplate <typename T, int VEC_SIZE>\nstruct Vec {};\n\n// A vector type to store FP32 accumulators.\ntemplate <typename T>\nstruct FloatVec {};\n\n// Template vector operations.\ntemplate <typename Acc, typename A, typename B>\ninline __device__ Acc mul(A a, B b);\n\ntemplate <typename T>\ninline __device__ float sum(T v);\n\ntemplate <typename T>\ninline __device__ float dot(T a, T b) {\n  return sum(mul<T, T, T>(a, b));\n}\n\ntemplate <typename A, typename T>\ninline __device__ float dot(T a, T b) {\n  return sum(mul<A, T, T>(a, b));\n}\n\ntemplate <typename T>\ninline __device__ void zero(T& dst) {\n  constexpr int WORDS = sizeof(T) / 4;\n  union {\n    T raw;\n    uint32_t words[WORDS];\n  } tmp;\n\n#pragma unroll\n  for (int ii = 0; ii < WORDS; ++ii) {\n    tmp.words[ii] = 0u;\n  }\n  dst = tmp.raw;\n}\n\n}  // namespace vllm\n// End include attention_generic.cuh\n// Begin include dtype_float16.cuh\n/*\n * Adapted from\n * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp\n * and\n * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h\n * Copyright (c) 2023, The vLLM team.\n * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n#pragma once\n\n// Begin include dtype_float32.cuh\n/*\n * Adapted from\n * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp\n * and\n * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h\n * Copyright (c) 2023, The vLLM team.\n * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n#pragma once\n\n\n#include <stdint.h>\n\nnamespace vllm {\n\n// Define custom FP32 vector data types.\nstruct Float4_ {\n  float2 x;\n  float2 y;\n};\n\nstruct Float8_ {\n  float2 x;\n  float2 y;\n  float2 z;\n  float2 w;\n};\n\n// FP32 vector types for Q, K, V.\ntemplate <>\nstruct Vec<float, 1> {\n  using Type = float;\n};\ntemplate <>\nstruct Vec<float, 2> {\n  using Type = float2;\n};\ntemplate <>\nstruct Vec<float, 4> {\n  using Type = float4;\n};\n\n// FP32 accumulator vector types corresponding to Vec.\ntemplate <>\nstruct FloatVec<float> {\n  using Type = float;\n};\ntemplate <>\nstruct FloatVec<float2> {\n  using Type = float2;\n};\ntemplate <>\nstruct FloatVec<float4> {\n  using Type = float4;\n};\n\n// Vector addition.\ninline __device__ float add(float a, float b) { return a + b; }\n\ninline __device__ float2 add(float2 a, float2 b) {\n  float2 c;\n  c.x = add(a.x, b.x);\n  c.y = add(a.y, b.y);\n  return c;\n}\n\ninline __device__ float4 add(float4 a, float4 b) {\n  float4 c;\n  c.x = add(a.x, b.x);\n  c.y = add(a.y, b.y);\n  c.z = add(a.z, b.z);\n  c.w = add(a.w, b.w);\n  return c;\n}\n\n// Vector multiplication.\ntemplate <>\ninline __device__ float mul<float, float>(float a, float b) {\n  return a * b;\n}\n\ntemplate <>\ninline __device__ float2 mul(float2 a, float2 b) {\n  float2 c;\n  c.x = a.x * b.x;\n  c.y = a.y * b.y;\n  return c;\n}\n\ntemplate <>\ninline __device__ float2 mul(float a, float2 b) {\n  float2 c;\n  c.x = a * b.x;\n  c.y = a * b.y;\n  return c;\n}\n\ntemplate <>\ninline __device__ float4 mul(float4 a, float4 b) {\n  float4 c;\n  c.x = a.x * b.x;\n  c.y = a.y * b.y;\n  c.z = a.z * b.z;\n  c.w = a.w * b.w;\n  return c;\n}\n\ntemplate <>\ninline __device__ float4 mul(float a, float4 b) {\n  float4 c;\n  c.x = a * b.x;\n  c.y = a * b.y;\n  c.z = a * b.z;\n  c.w = a * b.w;\n  return c;\n}\n\n// Vector fused multiply-add.\ninline __device__ float fma(float a, float b, float c) { return a * b + c; }\n\ninline __device__ float2 fma(float2 a, float2 b, float2 c) {\n  float2 d;\n  d.x = fma(a.x, b.x, c.x);\n  d.y = fma(a.y, b.y, c.y);\n  return d;\n}\n\ninline __device__ float2 fma(float a, float2 b, float2 c) {\n  float2 d;\n  d.x = fma(a, b.x, c.x);\n  d.y = fma(a, b.y, c.y);\n  return d;\n}\n\ninline __device__ float4 fma(float4 a, float4 b, float4 c) {\n  float4 d;\n  d.x = fma(a.x, b.x, c.x);\n  d.y = fma(a.y, b.y, c.y);\n  d.z = fma(a.z, b.z, c.z);\n  d.w = fma(a.w, b.w, c.w);\n  return d;\n}\n\ninline __device__ float4 fma(float a, float4 b, float4 c) {\n  float4 d;\n  d.x = fma(a, b.x, c.x);\n  d.y = fma(a, b.y, c.y);\n  d.z = fma(a, b.z, c.z);\n  d.w = fma(a, b.w, c.w);\n  return d;\n}\n\ninline __device__ Float4_ fma(float a, Float4_ b, Float4_ c) {\n  Float4_ d;\n  d.x = fma(a, b.x, c.x);\n  d.y = fma(a, b.y, c.y);\n  return d;\n}\n\ninline __device__ Float8_ fma(float a, Float8_ b, Float8_ c) {\n  Float8_ d;\n  d.x = fma(a, b.x, c.x);\n  d.y = fma(a, b.y, c.y);\n  d.z = fma(a, b.z, c.z);\n  d.w = fma(a, b.w, c.w);\n  return d;\n}\n\n// Vector sum.\ntemplate <>\ninline __device__ float sum(float v) {\n  return v;\n}\n\ntemplate <>\ninline __device__ float sum(float2 v) {\n  return v.x + v.y;\n}\n\ntemplate <>\ninline __device__ float sum(float4 v) {\n  return v.x + v.y + v.z + v.w;\n}\n\ntemplate <>\ninline __device__ float sum(Float4_ v) {\n  return v.x.x + v.x.y + v.y.x + v.y.y;\n}\n\ntemplate <>\ninline __device__ float sum(Float8_ v) {\n  return v.x.x + v.x.y + v.y.x + v.y.y + v.z.x + v.z.y + v.w.x + v.w.y;\n}\n\n// Vector dot product.\ninline __device__ float dot(float a, float b) { return a * b; }\n\ninline __device__ float dot(float2 a, float2 b) {\n  float2 c = mul<float2, float2, float2>(a, b);\n  return c.x + c.y;\n}\n\ninline __device__ float dot(Float4_ a, Float4_ b) {\n  float2 acc = mul<float2, float2, float2>(a.x, b.x);\n  acc = fma(a.y, b.y, acc);\n  return acc.x + acc.y;\n}\n\ninline __device__ float dot(Float8_ a, Float8_ b) {\n  float2 acc = mul<float2, float2, float2>(a.x, b.x);\n  acc = fma(a.y, b.y, acc);\n  acc = fma(a.z, b.z, acc);\n  acc = fma(a.w, b.w, acc);\n  return acc.x + acc.y;\n}\n\n// From float to float.\ninline __device__ void from_float(float& dst, float src) { dst = src; }\n\ninline __device__ void from_float(float2& dst, float2 src) { dst = src; }\n\ninline __device__ void from_float(float4& dst, float4 src) { dst = src; }\n\n// From float to float.\ninline __device__ float to_float(float u) { return u; }\n\ninline __device__ float2 to_float(float2 u) { return u; }\n\ninline __device__ float4 to_float(float4 u) { return u; }\n\ninline __device__ Float4_ to_float(Float4_ u) { return u; }\n\ninline __device__ Float8_ to_float(Float8_ u) { return u; }\n\n// Zero-out a variable.\ninline __device__ void zero(float& dst) { dst = 0.f; }\n\n}  // namespace vllm\n// End include dtype_float32.cuh\n\n#ifdef USE_ROCM\n  #include <hip/hip_fp16.h>\n#endif\n\n#include <stdint.h>\n\nnamespace vllm {\n\n// FP16 vector types for Q, K, V.\ntemplate <>\nstruct Vec<uint16_t, 1> {\n  using Type = uint16_t;\n};\ntemplate <>\nstruct Vec<uint16_t, 2> {\n  using Type = uint32_t;\n};\ntemplate <>\nstruct Vec<uint16_t, 4> {\n  using Type = uint2;\n};\ntemplate <>\nstruct Vec<uint16_t, 8> {\n  using Type = uint4;\n};\n\n// FP32 accumulator vector types corresponding to Vec.\ntemplate <>\nstruct FloatVec<uint16_t> {\n  using Type = float;\n};\ntemplate <>\nstruct FloatVec<uint32_t> {\n  using Type = float2;\n};\ntemplate <>\nstruct FloatVec<uint2> {\n  using Type = Float4_;\n};\ntemplate <>\nstruct FloatVec<uint4> {\n  using Type = Float8_;\n};\n\n// Utility functions for type conversions.\ninline __device__ uint32_t h0_h0(uint16_t a) {\n#ifndef USE_ROCM\n  uint32_t b;\n  asm volatile(\"mov.b32 %0, {%1, %1};\" : \"=r\"(b) : \"h\"(a));\n  return b;\n#else\n  union {\n    uint32_t u32;\n    uint16_t u16[2];\n  } tmp;\n  tmp.u16[0] = a;\n  tmp.u16[1] = a;\n  return tmp.u32;\n#endif\n}\n\ninline __device__ float half_to_float(uint16_t h) {\n  float f;\n#ifndef USE_ROCM\n  asm volatile(\"cvt.f32.f16 %0, %1;\\n\" : \"=f\"(f) : \"h\"(h));\n#else\n  asm volatile(\"v_cvt_f32_f16 %0, %1;\" : \"=v\"(f) : \"v\"(h));\n#endif\n  return f;\n}\n\ninline __device__ float2 half2_to_float2(uint32_t v) {\n#ifndef USE_ROCM\n  uint16_t lo, hi;\n  asm volatile(\"mov.b32 {%0, %1}, %2;\\n\" : \"=h\"(lo), \"=h\"(hi) : \"r\"(v));\n  return make_float2(half_to_float(lo), half_to_float(hi));\n#else\n  union {\n    uint32_t u32;\n    uint16_t u16[2];\n  } tmp;\n  tmp.u32 = v;\n  float2 ret;\n  ret.x = half_to_float(tmp.u16[0]);\n  ret.y = half_to_float(tmp.u16[1]);\n  return ret;\n#endif\n}\n\ninline __device__ uint16_t float_to_half(float f) {\n  union {\n    uint32_t u32;\n    uint16_t u16[2];\n  } tmp;\n#ifndef USE_ROCM\n  asm volatile(\"cvt.rn.f16.f32 %0, %1;\\n\" : \"=h\"(tmp.u16[0]) : \"f\"(f));\n#else\n  asm volatile(\"v_cvt_f16_f32 %0, %1;\\n\" : \"=v\"(tmp.u32) : \"v\"(f));\n#endif\n  return tmp.u16[0];\n}\n\ninline __device__ uint32_t float2_to_half2(float2 f) {\n  union {\n    uint32_t u32;\n    uint16_t u16[2];\n  } tmp;\n#ifndef USE_ROCM\n  #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800\n  asm volatile(\"cvt.rn.f16x2.f32 %0, %1, %2;\\n\"\n               : \"=r\"(tmp.u32)\n               : \"f\"(f.y), \"f\"(f.x));\n  #else\n  asm volatile(\"cvt.rn.f16.f32 %0, %1;\\n\" : \"=h\"(tmp.u16[0]) : \"f\"(f.x));\n  asm volatile(\"cvt.rn.f16.f32 %0, %1;\\n\" : \"=h\"(tmp.u16[1]) : \"f\"(f.y));\n  #endif\n#else\n  tmp.u16[0] = float_to_half(f.x);\n  tmp.u16[1] = float_to_half(f.y);\n#endif\n  return tmp.u32;\n}\n\n// Vector addition.\ninline __device__ uint16_t add(uint16_t a, uint16_t b) {\n  uint16_t c;\n#ifndef USE_ROCM\n  asm volatile(\"add.f16 %0, %1, %2;\\n\" : \"=h\"(c) : \"h\"(a), \"h\"(b));\n#else\n  asm volatile(\"v_add_f16 %0, %1, %2;\\n\" : \"=v\"(c) : \"v\"(a), \"v\"(b));\n#endif\n  return c;\n}\n\ninline __device__ uint32_t add(uint32_t a, uint32_t b) {\n  uint32_t c;\n#ifndef USE_ROCM\n  asm volatile(\"add.f16x2 %0, %1, %2;\\n\" : \"=r\"(c) : \"r\"(a), \"r\"(b));\n#else\n  asm volatile(\"v_pk_add_f16 %0, %1, %2;\\n\" : \"=v\"(c) : \"v\"(a), \"v\"(b));\n#endif\n  return c;\n}\n\ninline __device__ uint2 add(uint2 a, uint2 b) {\n  uint2 c;\n  c.x = add(a.x, b.x);\n  c.y = add(a.y, b.y);\n  return c;\n}\n\ninline __device__ uint4 add(uint4 a, uint4 b) {\n  uint4 c;\n  c.x = add(a.x, b.x);\n  c.y = add(a.y, b.y);\n  c.z = add(a.z, b.z);\n  c.w = add(a.w, b.w);\n  return c;\n}\n\ninline __device__ float2 add(uint32_t a, float2 fb) {\n  float2 fa = half2_to_float2(a);\n  return add(fa, fb);\n}\n\ninline __device__ Float4_ add(uint2 a, Float4_ fb) {\n  Float4_ fc;\n  fc.x = add(a.x, fb.x);\n  fc.y = add(a.y, fb.y);\n  return fc;\n}\n\ninline __device__ Float8_ add(uint4 a, Float8_ fb) {\n  Float8_ fc;\n  fc.x = add(a.x, fb.x);\n  fc.y = add(a.y, fb.y);\n  fc.z = add(a.z, fb.z);\n  fc.w = add(a.w, fb.w);\n  return fc;\n}\n\n// Vector multiplication.\ntemplate <>\ninline __device__ uint16_t mul(uint16_t a, uint16_t b) {\n  uint16_t c;\n#ifndef USE_ROCM\n  asm volatile(\"mul.f16 %0, %1, %2;\\n\" : \"=h\"(c) : \"h\"(a), \"h\"(b));\n#else\n  asm volatile(\"v_mul_f16 %0, %1, %2;\\n\" : \"=v\"(c) : \"v\"(a), \"v\"(b));\n#endif\n  return c;\n}\n\ntemplate <>\ninline __device__ uint32_t mul(uint32_t a, uint32_t b) {\n  uint32_t c;\n#ifndef USE_ROCM\n  asm volatile(\"mul.f16x2 %0, %1, %2;\\n\" : \"=r\"(c) : \"r\"(a), \"r\"(b));\n#else\n  asm volatile(\"v_pk_mul_f16 %0, %1, %2;\\n\" : \"=v\"(c) : \"v\"(a), \"v\"(b));\n#endif\n  return c;\n}\n\ntemplate <>\ninline __device__ uint32_t mul(uint16_t a, uint32_t b) {\n  return mul<uint32_t, uint32_t, uint32_t>(h0_h0(a), b);\n}\n\ntemplate <>\ninline __device__ uint2 mul(uint2 a, uint2 b) {\n  uint2 c;\n  c.x = mul<uint32_t, uint32_t, uint32_t>(a.x, b.x);\n  c.y = mul<uint32_t, uint32_t, uint32_t>(a.y, b.y);\n  return c;\n}\n\ntemplate <>\ninline __device__ uint2 mul(uint16_t a, uint2 b) {\n  uint32_t s = h0_h0(a);\n  uint2 c;\n  c.x = mul<uint32_t, uint32_t, uint32_t>(s, b.x);\n  c.y = mul<uint32_t, uint32_t, uint32_t>(s, b.y);\n  return c;\n}\n\ntemplate <>\ninline __device__ uint4 mul(uint4 a, uint4 b) {\n  uint4 c;\n  c.x = mul<uint32_t, uint32_t, uint32_t>(a.x, b.x);\n  c.y = mul<uint32_t, uint32_t, uint32_t>(a.y, b.y);\n  c.z = mul<uint32_t, uint32_t, uint32_t>(a.z, b.z);\n  c.w = mul<uint32_t, uint32_t, uint32_t>(a.w, b.w);\n  return c;\n}\n\ntemplate <>\ninline __device__ uint4 mul(uint16_t a, uint4 b) {\n  uint32_t s = h0_h0(a);\n  uint4 c;\n  c.x = mul<uint32_t, uint32_t, uint32_t>(s, b.x);\n  c.y = mul<uint32_t, uint32_t, uint32_t>(s, b.y);\n  c.z = mul<uint32_t, uint32_t, uint32_t>(s, b.z);\n  c.w = mul<uint32_t, uint32_t, uint32_t>(s, b.w);\n  return c;\n}\n\ntemplate <>\ninline __device__ float mul(uint16_t a, uint16_t b) {\n  float fa = half_to_float(a);\n  float fb = half_to_float(b);\n  return fa * fb;\n}\n\ntemplate <>\ninline __device__ float2 mul(uint32_t a, uint32_t b) {\n  float2 fa = half2_to_float2(a);\n  float2 fb = half2_to_float2(b);\n  return mul<float2, float2, float2>(fa, fb);\n}\n\ntemplate <>\ninline __device__ float2 mul(uint16_t a, uint32_t b) {\n  return mul<float2, uint32_t, uint32_t>(h0_h0(a), b);\n}\n\ntemplate <>\ninline __device__ Float4_ mul(uint2 a, uint2 b) {\n  Float4_ fc;\n  fc.x = mul<float2, uint32_t, uint32_t>(a.x, b.x);\n  fc.y = mul<float2, uint32_t, uint32_t>(a.y, b.y);\n  return fc;\n}\n\ntemplate <>\ninline __device__ Float4_ mul(uint16_t a, uint2 b) {\n  uint32_t s = h0_h0(a);\n  Float4_ fc;\n  fc.x = mul<float2, uint32_t, uint32_t>(s, b.x);\n  fc.y = mul<float2, uint32_t, uint32_t>(s, b.y);\n  return fc;\n}\n\ntemplate <>\ninline __device__ Float8_ mul(uint4 a, uint4 b) {\n  Float8_ fc;\n  fc.x = mul<float2, uint32_t, uint32_t>(a.x, b.x);\n  fc.y = mul<float2, uint32_t, uint32_t>(a.y, b.y);\n  fc.z = mul<float2, uint32_t, uint32_t>(a.z, b.z);\n  fc.w = mul<float2, uint32_t, uint32_t>(a.w, b.w);\n  return fc;\n}\n\ntemplate <>\ninline __device__ Float8_ mul(uint16_t a, uint4 b) {\n  uint32_t s = h0_h0(a);\n  Float8_ fc;\n  fc.x = mul<float2, uint32_t, uint32_t>(s, b.x);\n  fc.y = mul<float2, uint32_t, uint32_t>(s, b.y);\n  fc.z = mul<float2, uint32_t, uint32_t>(s, b.z);\n  fc.w = mul<float2, uint32_t, uint32_t>(s, b.w);\n  return fc;\n}\n\n// Vector fused multiply-add.\ninline __device__ uint32_t fma(uint32_t a, uint32_t b, uint32_t c) {\n  uint32_t d;\n#ifndef USE_ROCM\n  asm volatile(\"fma.rn.f16x2 %0, %1, %2, %3;\\n\"\n               : \"=r\"(d)\n               : \"r\"(a), \"r\"(b), \"r\"(c));\n#else\n  asm volatile(\"v_pk_fma_f16 %0, %1, %2, %3;\\n\"\n               : \"=v\"(d)\n               : \"v\"(a), \"v\"(b), \"v\"(c));\n#endif\n  return d;\n}\n\ninline __device__ uint32_t fma(uint16_t a, uint32_t b, uint32_t c) {\n  return fma(h0_h0(a), b, c);\n}\n\ninline __device__ uint2 fma(uint2 a, uint2 b, uint2 c) {\n  uint2 d;\n  d.x = fma(a.x, b.x, c.x);\n  d.y = fma(a.y, b.y, c.y);\n  return d;\n}\n\ninline __device__ uint2 fma(uint16_t a, uint2 b, uint2 c) {\n  uint32_t s = h0_h0(a);\n  uint2 d;\n  d.x = fma(s, b.x, c.x);\n  d.y = fma(s, b.y, c.y);\n  return d;\n}\n\ninline __device__ uint4 fma(uint4 a, uint4 b, uint4 c) {\n  uint4 d;\n  d.x = fma(a.x, b.x, c.x);\n  d.y = fma(a.y, b.y, c.y);\n  d.z = fma(a.z, b.z, c.z);\n  d.w = fma(a.w, b.w, c.w);\n  return d;\n}\n\ninline __device__ uint4 fma(uint16_t a, uint4 b, uint4 c) {\n  uint32_t s = h0_h0(a);\n  uint4 d;\n  d.x = fma(s, b.x, c.x);\n  d.y = fma(s, b.y, c.y);\n  d.z = fma(s, b.z, c.z);\n  d.w = fma(s, b.w, c.w);\n  return d;\n}\n\ninline __device__ float fma(uint16_t a, uint16_t b, float fc) {\n  float fa = half_to_float(a);\n  float fb = half_to_float(b);\n  return fa * fb + fc;\n}\n\ninline __device__ float2 fma(uint32_t a, uint32_t b, float2 fc) {\n  float2 fa = half2_to_float2(a);\n  float2 fb = half2_to_float2(b);\n  return fma(fa, fb, fc);\n}\n\ninline __device__ float2 fma(uint16_t a, uint32_t b, float2 fc) {\n  return fma(h0_h0(a), b, fc);\n}\n\ninline __device__ Float4_ fma(uint2 a, uint2 b, Float4_ fc) {\n  Float4_ fd;\n  fd.x = fma(a.x, b.x, fc.x);\n  fd.y = fma(a.y, b.y, fc.y);\n  return fd;\n}\n\ninline __device__ Float4_ fma(uint16_t a, uint2 b, Float4_ fc) {\n  uint32_t s = h0_h0(a);\n  Float4_ fd;\n  fd.x = fma(s, b.x, fc.x);\n  fd.y = fma(s, b.y, fc.y);\n  return fd;\n}\n\ninline __device__ Float8_ fma(uint4 a, uint4 b, Float8_ fc) {\n  Float8_ fd;\n  fd.x = fma(a.x, b.x, fc.x);\n  fd.y = fma(a.y, b.y, fc.y);\n  fd.z = fma(a.z, b.z, fc.z);\n  fd.w = fma(a.w, b.w, fc.w);\n  return fd;\n}\n\ninline __device__ Float8_ fma(uint16_t a, uint4 b, Float8_ fc) {\n  uint32_t s = h0_h0(a);\n  Float8_ fd;\n  fd.x = fma(s, b.x, fc.x);\n  fd.y = fma(s, b.y, fc.y);\n  fd.z = fma(s, b.z, fc.z);\n  fd.w = fma(s, b.w, fc.w);\n  return fd;\n}\n\n// Vector sum.\ntemplate <>\ninline __device__ float sum(uint16_t v) {\n  return half_to_float(v);\n}\n\ntemplate <>\ninline __device__ float sum(uint32_t v) {\n  float2 tmp = half2_to_float2(v);\n  return tmp.x + tmp.y;\n}\n\ntemplate <>\ninline __device__ float sum(uint2 v) {\n  uint32_t c = add(v.x, v.y);\n  return sum(c);\n}\n\ntemplate <>\ninline __device__ float sum(uint4 v) {\n  uint32_t c = add(v.x, v.y);\n  c = add(c, v.z);\n  c = add(c, v.w);\n  return sum(c);\n}\n\n// From float32 to float16.\ninline __device__ void from_float(uint16_t& dst, float src) {\n  dst = float_to_half(src);\n}\n\ninline __device__ void from_float(uint32_t& dst, float2 src) {\n  dst = float2_to_half2(src);\n}\n\ninline __device__ void from_float(uint2& dst, Float4_ src) {\n  dst.x = float2_to_half2(src.x);\n  dst.y = float2_to_half2(src.y);\n}\n\ninline __device__ void from_float(uint4& dst, Float8_ src) {\n  dst.x = float2_to_half2(src.x);\n  dst.y = float2_to_half2(src.y);\n  dst.z = float2_to_half2(src.z);\n  dst.w = float2_to_half2(src.w);\n}\n\n// From float16 to float32.\ninline __device__ float to_float(uint16_t u) { return half_to_float(u); }\n\ninline __device__ float2 to_float(uint32_t u) { return half2_to_float2(u); }\n\ninline __device__ Float4_ to_float(uint2 u) {\n  Float4_ tmp;\n  tmp.x = half2_to_float2(u.x);\n  tmp.y = half2_to_float2(u.y);\n  return tmp;\n}\n\ninline __device__ Float8_ to_float(uint4 u) {\n  Float8_ tmp;\n  tmp.x = half2_to_float2(u.x);\n  tmp.y = half2_to_float2(u.y);\n  tmp.z = half2_to_float2(u.z);\n  tmp.w = half2_to_float2(u.w);\n  return tmp;\n}\n\n// Zero-out a variable.\ninline __device__ void zero(uint16_t& dst) { dst = uint16_t(0); }\n\n}  // namespace vllm\n// End include dtype_float16.cuh\n// Begin include dtype_bfloat16.cuh\n/*\n * Adapted from\n * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp\n * and\n * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h\n * Copyright (c) 2023, The vLLM team.\n * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n#pragma once\n\n\n#ifndef USE_ROCM\n  #include <cuda_bf16.h>\n  #include <cuda_fp16.h>\n#else\n  #include <hip/hip_bf16.h>\n  #include <hip/hip_fp16.h>\n\ntypedef __hip_bfloat162 __nv_bfloat162;\ntypedef __hip_bfloat16 __nv_bfloat16;\n#endif\n\n#include <stdint.h>\n\nnamespace vllm {\n\n// Define custom BF16 vector data types.\nstruct bf16_4_t {\n  __nv_bfloat162 x;\n  __nv_bfloat162 y;\n};\n\nstruct bf16_8_t {\n  __nv_bfloat162 x;\n  __nv_bfloat162 y;\n  __nv_bfloat162 z;\n  __nv_bfloat162 w;\n};\n\n// BF16 vector types for Q, K, V.\ntemplate <>\nstruct Vec<__nv_bfloat16, 1> {\n  using Type = __nv_bfloat16;\n};\ntemplate <>\nstruct Vec<__nv_bfloat16, 2> {\n  using Type = __nv_bfloat162;\n};\ntemplate <>\nstruct Vec<__nv_bfloat16, 4> {\n  using Type = bf16_4_t;\n};\ntemplate <>\nstruct Vec<__nv_bfloat16, 8> {\n  using Type = bf16_8_t;\n};\n\n// FP32 accumulator vector types corresponding to Vec.\ntemplate <>\nstruct FloatVec<__nv_bfloat16> {\n  using Type = float;\n};\ntemplate <>\nstruct FloatVec<__nv_bfloat162> {\n  using Type = float2;\n};\ntemplate <>\nstruct FloatVec<bf16_4_t> {\n  using Type = Float4_;\n};\ntemplate <>\nstruct FloatVec<bf16_8_t> {\n  using Type = Float8_;\n};\n\n// Utility functions for type conversions.\ninline __device__ float2 bf1622float2(const __nv_bfloat162 val) {\n#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800\n  assert(false);\n#else\n  return __bfloat1622float2(val);\n#endif\n  __builtin_unreachable();  // Suppress missing return statement warning\n}\n\ninline __device__ __nv_bfloat162 bf162bf162(const __nv_bfloat16 val) {\n#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800\n  assert(false);\n#else\n  return __bfloat162bfloat162(val);\n#endif\n  __builtin_unreachable();  // Suppress missing return statement warning\n}\n\n// Vector addition.\ninline __device__ __nv_bfloat16 add(__nv_bfloat16 a, __nv_bfloat16 b) {\n#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800\n  assert(false);\n#else\n  #ifndef USE_ROCM\n  return a + b;\n  #else\n  return __hadd(a, b);\n  #endif\n#endif\n  __builtin_unreachable();  // Suppress missing return statement warning\n}\n\ninline __device__ __nv_bfloat162 add(__nv_bfloat162 a, __nv_bfloat162 b) {\n#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800\n  assert(false);\n#else\n  return __hadd2(a, b);\n#endif\n  __builtin_unreachable();  // Suppress missing return statement warning\n}\n\ninline __device__ bf16_4_t add(bf16_4_t a, bf16_4_t b) {\n  bf16_4_t c;\n  c.x = add(a.x, b.x);\n  c.y = add(a.y, b.y);\n  return c;\n}\n\ninline __device__ bf16_8_t add(bf16_8_t a, bf16_8_t b) {\n  bf16_8_t c;\n  c.x = add(a.x, b.x);\n  c.y = add(a.y, b.y);\n  c.z = add(a.z, b.z);\n  c.w = add(a.w, b.w);\n  return c;\n}\n\ninline __device__ float2 add(__nv_bfloat162 a, float2 fb) {\n  float2 fa = bf1622float2(a);\n  return add(fa, fb);\n}\n\ninline __device__ Float4_ add(bf16_4_t a, Float4_ fb) {\n  Float4_ fc;\n  fc.x = add(a.x, fb.x);\n  fc.y = add(a.y, fb.y);\n  return fc;\n}\n\ninline __device__ Float8_ add(bf16_8_t a, Float8_ fb) {\n  Float8_ fc;\n  fc.x = add(a.x, fb.x);\n  fc.y = add(a.y, fb.y);\n  fc.z = add(a.z, fb.z);\n  fc.w = add(a.w, fb.w);\n  return fc;\n}\n\n// Vector multiplication.\ntemplate <>\ninline __device__ __nv_bfloat16 mul(__nv_bfloat16 a, __nv_bfloat16 b) {\n#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800\n  assert(false);\n#else\n  return __hmul(a, b);\n#endif\n  __builtin_unreachable();  // Suppress missing return statement warning\n}\n\ntemplate <>\ninline __device__ __nv_bfloat162 mul(__nv_bfloat162 a, __nv_bfloat162 b) {\n#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800\n  assert(false);\n#else\n  return __hmul2(a, b);\n#endif\n  __builtin_unreachable();  // Suppress missing return statement warning\n}\n\ntemplate <>\ninline __device__ __nv_bfloat162 mul(__nv_bfloat16 a, __nv_bfloat162 b) {\n  return mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(bf162bf162(a), b);\n}\n\ntemplate <>\ninline __device__ bf16_4_t mul(bf16_4_t a, bf16_4_t b) {\n  bf16_4_t c;\n  c.x = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(a.x, b.x);\n  c.y = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(a.y, b.y);\n  return c;\n}\n\ntemplate <>\ninline __device__ bf16_4_t mul(__nv_bfloat16 a, bf16_4_t b) {\n  __nv_bfloat162 s = bf162bf162(a);\n  bf16_4_t c;\n  c.x = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(s, b.x);\n  c.y = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(s, b.y);\n  return c;\n}\n\ntemplate <>\ninline __device__ bf16_8_t mul(bf16_8_t a, bf16_8_t b) {\n  bf16_8_t c;\n  c.x = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(a.x, b.x);\n  c.y = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(a.y, b.y);\n  c.z = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(a.z, b.z);\n  c.w = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(a.w, b.w);\n  return c;\n}\n\ntemplate <>\ninline __device__ bf16_8_t mul(__nv_bfloat16 a, bf16_8_t b) {\n  __nv_bfloat162 s = bf162bf162(a);\n  bf16_8_t c;\n  c.x = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(s, b.x);\n  c.y = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(s, b.y);\n  c.z = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(s, b.z);\n  c.w = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(s, b.w);\n  return c;\n}\n\ntemplate <>\ninline __device__ float mul(__nv_bfloat16 a, __nv_bfloat16 b) {\n  float fa = __bfloat162float(a);\n  float fb = __bfloat162float(b);\n  return fa * fb;\n}\n\ntemplate <>\ninline __device__ float2 mul(__nv_bfloat162 a, __nv_bfloat162 b) {\n  float2 fa = bf1622float2(a);\n  float2 fb = bf1622float2(b);\n  return mul<float2, float2, float2>(fa, fb);\n}\n\ntemplate <>\ninline __device__ float2 mul(__nv_bfloat16 a, __nv_bfloat162 b) {\n  return mul<float2, __nv_bfloat162, __nv_bfloat162>(bf162bf162(a), b);\n}\n\ntemplate <>\ninline __device__ Float4_ mul(bf16_4_t a, bf16_4_t b) {\n  Float4_ fc;\n  fc.x = mul<float2, __nv_bfloat162, __nv_bfloat162>(a.x, b.x);\n  fc.y = mul<float2, __nv_bfloat162, __nv_bfloat162>(a.y, b.y);\n  return fc;\n}\n\ntemplate <>\ninline __device__ Float4_ mul(__nv_bfloat16 a, bf16_4_t b) {\n  __nv_bfloat162 s = bf162bf162(a);\n  Float4_ fc;\n  fc.x = mul<float2, __nv_bfloat162, __nv_bfloat162>(s, b.x);\n  fc.y = mul<float2, __nv_bfloat162, __nv_bfloat162>(s, b.y);\n  return fc;\n}\n\ntemplate <>\ninline __device__ Float8_ mul(bf16_8_t a, bf16_8_t b) {\n  Float8_ fc;\n  fc.x = mul<float2, __nv_bfloat162, __nv_bfloat162>(a.x, b.x);\n  fc.y = mul<float2, __nv_bfloat162, __nv_bfloat162>(a.y, b.y);\n  fc.z = mul<float2, __nv_bfloat162, __nv_bfloat162>(a.z, b.z);\n  fc.w = mul<float2, __nv_bfloat162, __nv_bfloat162>(a.w, b.w);\n  return fc;\n}\n\ntemplate <>\ninline __device__ Float8_ mul(__nv_bfloat16 a, bf16_8_t b) {\n  __nv_bfloat162 s = bf162bf162(a);\n  Float8_ fc;\n  fc.x = mul<float2, __nv_bfloat162, __nv_bfloat162>(s, b.x);\n  fc.y = mul<float2, __nv_bfloat162, __nv_bfloat162>(s, b.y);\n  fc.z = mul<float2, __nv_bfloat162, __nv_bfloat162>(s, b.z);\n  fc.w = mul<float2, __nv_bfloat162, __nv_bfloat162>(s, b.w);\n  return fc;\n}\n\n// Vector fused multiply-add.\ninline __device__ __nv_bfloat162 fma(__nv_bfloat162 a, __nv_bfloat162 b,\n                                     __nv_bfloat162 c) {\n#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800\n  assert(false);\n#else\n  return __hfma2(a, b, c);\n#endif\n  __builtin_unreachable();  // Suppress missing return statement warning\n}\n\ninline __device__ __nv_bfloat162 fma(__nv_bfloat16 a, __nv_bfloat162 b,\n                                     __nv_bfloat162 c) {\n#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800\n  assert(false);\n#else\n  return __hfma2(bf162bf162(a), b, c);\n#endif\n  __builtin_unreachable();  // Suppress missing return statement warning\n}\n\ninline __device__ bf16_4_t fma(bf16_4_t a, bf16_4_t b, bf16_4_t c) {\n  bf16_4_t d;\n  d.x = fma(a.x, b.x, c.x);\n  d.y = fma(a.y, b.y, c.y);\n  return d;\n}\n\ninline __device__ bf16_4_t fma(__nv_bfloat16 a, bf16_4_t b, bf16_4_t c) {\n  __nv_bfloat162 s = bf162bf162(a);\n  bf16_4_t d;\n  d.x = fma(s, b.x, c.x);\n  d.y = fma(s, b.y, c.y);\n  return d;\n}\n\ninline __device__ bf16_8_t fma(bf16_8_t a, bf16_8_t b, bf16_8_t c) {\n  bf16_8_t d;\n  d.x = fma(a.x, b.x, c.x);\n  d.y = fma(a.y, b.y, c.y);\n  d.z = fma(a.z, b.z, c.z);\n  d.w = fma(a.w, b.w, c.w);\n  return d;\n}\n\ninline __device__ bf16_8_t fma(__nv_bfloat16 a, bf16_8_t b, bf16_8_t c) {\n  __nv_bfloat162 s = bf162bf162(a);\n  bf16_8_t d;\n  d.x = fma(s, b.x, c.x);\n  d.y = fma(s, b.y, c.y);\n  d.z = fma(s, b.z, c.z);\n  d.w = fma(s, b.w, c.w);\n  return d;\n}\n\ninline __device__ float fma(__nv_bfloat16 a, __nv_bfloat16 b, float fc) {\n  return __bfloat162float(a) * __bfloat162float(b) + fc;\n}\n\ninline __device__ float2 fma(__nv_bfloat162 a, __nv_bfloat162 b, float2 fc) {\n  float2 fa = bf1622float2(a);\n  float2 fb = bf1622float2(b);\n  return fma(fa, fb, fc);\n}\n\ninline __device__ float2 fma(__nv_bfloat16 a, __nv_bfloat162 b, float2 fc) {\n  return fma(bf162bf162(a), b, fc);\n}\n\ninline __device__ Float4_ fma(bf16_4_t a, bf16_4_t b, Float4_ fc) {\n  Float4_ fd;\n  fd.x = fma(a.x, b.x, fc.x);\n  fd.y = fma(a.y, b.y, fc.y);\n  return fd;\n}\n\ninline __device__ Float4_ fma(__nv_bfloat16 a, bf16_4_t b, Float4_ fc) {\n  __nv_bfloat162 s = bf162bf162(a);\n  Float4_ fd;\n  fd.x = fma(s, b.x, fc.x);\n  fd.y = fma(s, b.y, fc.y);\n  return fd;\n}\n\ninline __device__ Float8_ fma(bf16_8_t a, bf16_8_t b, Float8_ fc) {\n  Float8_ fd;\n  fd.x = fma(a.x, b.x, fc.x);\n  fd.y = fma(a.y, b.y, fc.y);\n  fd.z = fma(a.z, b.z, fc.z);\n  fd.w = fma(a.w, b.w, fc.w);\n  return fd;\n}\n\ninline __device__ Float8_ fma(__nv_bfloat16 a, bf16_8_t b, Float8_ fc) {\n  __nv_bfloat162 s = bf162bf162(a);\n  Float8_ fd;\n  fd.x = fma(s, b.x, fc.x);\n  fd.y = fma(s, b.y, fc.y);\n  fd.z = fma(s, b.z, fc.z);\n  fd.w = fma(s, b.w, fc.w);\n  return fd;\n}\n\n// Vector sum.\ntemplate <>\ninline __device__ float sum(__nv_bfloat16 v) {\n  return __bfloat162float(v);\n}\n\ntemplate <>\ninline __device__ float sum(__nv_bfloat162 v) {\n  float2 vf = bf1622float2(v);\n  return vf.x + vf.y;\n}\n\ntemplate <>\ninline __device__ float sum(bf16_4_t v) {\n  return sum(v.x) + sum(v.y);\n}\n\ntemplate <>\ninline __device__ float sum(bf16_8_t v) {\n  return sum(v.x) + sum(v.y) + sum(v.z) + sum(v.w);\n}\n\n// From float32 to bfloat16.\ninline __device__ void from_float(__nv_bfloat16& dst, float src) {\n  dst = __float2bfloat16(src);\n}\n\ninline __device__ void from_float(__nv_bfloat162& dst, float2 src) {\n#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800\n  assert(false);\n#else\n  dst = __float22bfloat162_rn(src);\n#endif\n}\n\ninline __device__ void from_float(bf16_4_t& dst, Float4_ src) {\n#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800\n  assert(false);\n#else\n  dst.x = __float22bfloat162_rn(src.x);\n  dst.y = __float22bfloat162_rn(src.y);\n#endif\n}\n\ninline __device__ void from_float(bf16_8_t& dst, Float8_ src) {\n#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800\n  assert(false);\n#else\n  dst.x = __float22bfloat162_rn(src.x);\n  dst.y = __float22bfloat162_rn(src.y);\n  dst.z = __float22bfloat162_rn(src.z);\n  dst.w = __float22bfloat162_rn(src.w);\n#endif\n}\n\n// From bfloat16 to float32.\ninline __device__ float to_float(__nv_bfloat16 u) {\n  return __bfloat162float(u);\n}\n\n// Zero-out a variable.\ninline __device__ void zero(__nv_bfloat16& dst) {\n#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800\n  assert(false);\n#else\n  // Same as CUDART_ZERO_BF16 introduced in CUDA 12.2.\n  dst = __ushort_as_bfloat16((unsigned short)0x0000U);\n#endif\n}\n\n}  // namespace vllm\n// End include dtype_bfloat16.cuh\n// Begin include dtype_fp8.cuh\n#pragma once\n\n\n#include <stdint.h>\n#ifdef ENABLE_FP8\n  #ifndef USE_ROCM\n    #include <cuda_fp8.h>\n  #endif  // USE_ROCM\n#endif    // ENABLE_FP8\n\nnamespace vllm {\n\nenum class Fp8KVCacheDataType {\n  kAuto = 0,\n  kFp8E4M3 = 1,\n  kFp8E5M2 = 2,\n};\n\n// fp8 vector types for quantization of kv cache\ntemplate <>\nstruct Vec<uint8_t, 1> {\n  using Type = uint8_t;\n};\n\ntemplate <>\nstruct Vec<uint8_t, 2> {\n  using Type = uint16_t;\n};\n\ntemplate <>\nstruct Vec<uint8_t, 4> {\n  using Type = uint32_t;\n};\n\ntemplate <>\nstruct Vec<uint8_t, 8> {\n  using Type = uint2;\n};\n\n}  // namespace vllm\n// End include dtype_fp8.cuh\n// End include attention_dtypes.h\n// Begin include attention_utils.cuh\n/*\n * Adapted from\n * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp\n * Copyright (c) 2023, The vLLM team.\n * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n#pragma once\n\n// Begin include ../cuda_compat.h\n#pragma once\n\n#ifdef USE_ROCM\n  #include <hip/hip_runtime.h>\n#endif\n\n#ifdef USE_ROCM\nstruct Utils {\n  static __host__ int get_warp_size() {\n    static bool is_cached = false;\n    static int result;\n\n    if (!is_cached) {\n      int device_id;\n      cudaDeviceProp deviceProp;\n      cudaGetDevice(&device_id);\n      cudaGetDeviceProperties(&deviceProp, device_id);\n\n      result = deviceProp.warpSize;\n      is_cached = true;\n    }\n\n    return result;\n  }\n\n  static __device__ constexpr int get_warp_size() {\n  #ifdef __GFX9__\n    return 64;\n  #else\n    return 32;\n  #endif\n  }\n};\n\n  #define WARP_SIZE Utils::get_warp_size()\n#else\n  #define WARP_SIZE 32\n#endif\n\n#ifndef USE_ROCM\n  #define VLLM_LDG(arg) __ldg(arg)\n#else\n  #define VLLM_LDG(arg) *(arg)\n#endif\n\n#ifndef USE_ROCM\n  #define VLLM_SHFL_XOR_SYNC(var, lane_mask) \\\n    __shfl_xor_sync(uint32_t(-1), var, lane_mask)\n  #define VLLM_SHFL_XOR_SYNC_WIDTH(var, lane_mask, width) \\\n    __shfl_xor_sync(uint32_t(-1), var, lane_mask, width)\n#else\n  #define VLLM_SHFL_XOR_SYNC(var, lane_mask) __shfl_xor(var, lane_mask)\n  #define VLLM_SHFL_XOR_SYNC_WIDTH(var, lane_mask, width) \\\n    __shfl_xor(var, lane_mask, width)\n#endif\n\n#ifndef USE_ROCM\n  #define VLLM_SHFL_SYNC(var, src_lane) __shfl_sync(uint32_t(-1), var, src_lane)\n#else\n  #define VLLM_SHFL_SYNC(var, src_lane) __shfl(var, src_lane)\n#endif\n\n#ifndef USE_ROCM\n  #define VLLM_SHFL_DOWN_SYNC(var, lane_delta) \\\n    __shfl_down_sync(uint32_t(-1), var, lane_delta)\n#else\n  #define VLLM_SHFL_DOWN_SYNC(var, lane_delta) __shfl_down(var, lane_delta)\n#endif\n\n#ifndef USE_ROCM\n  #define VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(FUNC, VAL) \\\n    cudaFuncSetAttribute(FUNC, cudaFuncAttributeMaxDynamicSharedMemorySize, VAL)\n#else\n  #define VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(FUNC, VAL) \\\n    hipFuncSetAttribute(FUNC, hipFuncAttributeMaxDynamicSharedMemorySize, VAL)\n#endif\n// End include ../cuda_compat.h\n\n#include <float.h>\n#include <type_traits>\n\nnamespace vllm {\n\n// Q*K^T operation.\ntemplate <int THREAD_GROUP_SIZE, typename Vec, int N>\ninline __device__ float qk_dot_(const Vec (&q)[N], const Vec (&k)[N]) {\n  using A_vec = typename FloatVec<Vec>::Type;\n  // Compute the parallel products for Q*K^T (treat vector lanes separately).\n  A_vec qk_vec = mul<A_vec, Vec, Vec>(q[0], k[0]);\n#pragma unroll\n  for (int ii = 1; ii < N; ++ii) {\n    qk_vec = vllm::fma(q[ii], k[ii], qk_vec);\n  }\n\n  // Finalize the reduction across lanes.\n  float qk = sum(qk_vec);\n#pragma unroll\n  for (int mask = THREAD_GROUP_SIZE / 2; mask >= 1; mask /= 2) {\n    qk += VLLM_SHFL_XOR_SYNC(qk, mask);\n  }\n  return qk;\n}\n\ntemplate <typename T, int THREAD_GROUP_SIZE>\nstruct Qk_dot {\n  template <typename Vec, int N>\n  static inline __device__ float dot(const Vec (&q)[N], const Vec (&k)[N]) {\n    return qk_dot_<THREAD_GROUP_SIZE>(q, k);\n  }\n};\n\n}  // namespace vllm\n// End include attention_utils.cuh\n\n#ifdef USE_ROCM\n  #include <hip/hip_bf16.h>\n// Begin include ../quantization/fp8/amd/quant_utils.cuh\n#pragma once\n#include <hip/hip_fp8.h>\n\n#include <hip/hip_fp16.h>\n#include <hip/hip_bf16.h>\n#include <hip/hip_bfloat16.h>\n\n\nnamespace vllm {\n#ifdef USE_ROCM\n\nnamespace fp8 {\n  #ifdef ENABLE_FP8\n\n// Use hardware cvt instruction for fp8 on rocm\ntemplate <typename fp8_type>\n__device__ __forceinline__ fp8_type cvt_c10(float const r) {\n  return {};\n}\n\n// __hip_fp8_e4m3 only exists starting in ROCm 6.3. The macro\n// HIP_FP8_TYPE_OCP comes from the hip_fp8.h header and also makes\n// its first appearance in ROCm 6.3. Since VLLM_DISPATCH_FP8_TYPES\n// on ROCm instantiates both OCP and FNUZ kernels, we need to replace\n// the new HW cvt with something reasonable that doesn't rely on the\n// ROCm 6.3 feature. This allows compiling on ROCm 6.2 or newer.\ntemplate <>\n__device__ __forceinline__ c10::Float8_e4m3fn cvt_c10(float const r) {\n    #if HIP_FP8_TYPE_OCP\n  return c10::Float8_e4m3fn(\n      __hip_cvt_float_to_fp8(r, __hip_fp8_e4m3::__default_saturation,\n                             __hip_fp8_e4m3::__default_interpret),\n      c10::Float8_e4m3fn::from_bits());\n    #else\n  // Cast implemented by pytorch. Uses bit manipulation instead of HW cvt.\n  // HW cvt above is faster when it is available (ROCm 6.3 or newer).\n  return static_cast<c10::Float8_e4m3fn>(r);\n    #endif\n}\n\ntemplate <>\n__device__ __forceinline__ c10::Float8_e4m3fnuz cvt_c10(float const r) {\n  return c10::Float8_e4m3fnuz(\n      __hip_cvt_float_to_fp8(r, __hip_fp8_e4m3_fnuz::__default_saturation,\n                             __hip_fp8_e4m3_fnuz::__default_interpret),\n      c10::Float8_e4m3fnuz::from_bits());\n}\n\ntemplate <typename Tout, typename Tin>\n__inline__ __device__ Tout vec_conversion(const Tin& x) {\n  return x;\n}\n\ntemplate <typename Tout, typename Tin>\n__inline__ __device__ Tout scaled_vec_conversion(const Tin& x,\n                                                 const float scale) {\n  return x;\n}\n\n    #if HIP_FP8_TYPE_OCP\nusing fp8_type = __hip_fp8_e4m3;\nusing fp8x2_type = __hip_fp8x2_e4m3;\n    #else\nusing fp8_type = __hip_fp8_e4m3_fnuz;\nusing fp8x2_type = __hip_fp8x2_e4m3_fnuz;\n    #endif\n\n// fp8 -> half\ntemplate <>\n__inline__ __device__ uint16_t\nvec_conversion<uint16_t, uint8_t>(const uint8_t& a) {\n  return __hip_cvt_fp8_to_halfraw(a, fp8_type::__default_interpret).x;\n}\n\n// fp8x2 -> half2\ntemplate <>\n__inline__ __device__ uint32_t\nvec_conversion<uint32_t, uint16_t>(const uint16_t& a) {\n  union {\n    __half2_raw h2r;\n    uint32_t ui32;\n  } tmp;\n  tmp.h2r = __hip_cvt_fp8x2_to_halfraw2(a, fp8_type::__default_interpret);\n  return tmp.ui32;\n}\n\n// fp8x4 -> half2x2\ntemplate <>\n__inline__ __device__ uint2 vec_conversion<uint2, uint32_t>(const uint32_t& a) {\n  union {\n    uint2 u32x2;\n    uint32_t u32[2];\n  } tmp;\n  tmp.u32[0] = vec_conversion<uint32_t, uint16_t>((uint16_t)a);\n  tmp.u32[1] = vec_conversion<uint32_t, uint16_t>((uint16_t)(a >> 16U));\n  return tmp.u32x2;\n}\n\n// fp8x8 -> half2x4\ntemplate <>\n__inline__ __device__ uint4 vec_conversion<uint4, uint2>(const uint2& a) {\n  union {\n    uint4 u64x2;\n    uint2 u64[2];\n  } tmp;\n  tmp.u64[0] = vec_conversion<uint2, uint32_t>(a.x);\n  tmp.u64[1] = vec_conversion<uint2, uint32_t>(a.y);\n  return tmp.u64x2;\n}\n\nusing __nv_bfloat16 = __hip_bfloat16;\n\n// fp8 -> __nv_bfloat16\ntemplate <>\n__inline__ __device__ __nv_bfloat16\nvec_conversion<__nv_bfloat16, uint8_t>(const uint8_t& a) {\n  fp8_type f8;\n  f8.__x = a;\n  return __float2bfloat16(static_cast<float>(f8));\n}\n\nusing __nv_bfloat162 = __hip_bfloat162;\n\n// fp8x2 -> __nv_bfloat162\ntemplate <>\n__inline__ __device__ __nv_bfloat162\nvec_conversion<__nv_bfloat162, uint16_t>(const uint16_t& a) {\n  __nv_bfloat162 res;\n  res.x = vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)a);\n  res.y = vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)(a >> 8U));\n  return res;\n}\n\n// fp8x4 -> bf16_4_t\ntemplate <>\n__inline__ __device__ bf16_4_t\nvec_conversion<bf16_4_t, uint32_t>(const uint32_t& a) {\n  bf16_4_t res;\n  res.x = vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)a);\n  res.y = vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)(a >> 16U));\n  return res;\n}\n\n// fp8x8 -> bf16_8_t\ntemplate <>\n__inline__ __device__ bf16_8_t vec_conversion<bf16_8_t, uint2>(const uint2& a) {\n  bf16_4_t tmp1, tmp2;\n  tmp1 = vec_conversion<bf16_4_t, uint32_t>(a.x);\n  tmp2 = vec_conversion<bf16_4_t, uint32_t>(a.y);\n  bf16_8_t res;\n  res.x = tmp1.x;\n  res.y = tmp1.y;\n  res.z = tmp2.x;\n  res.w = tmp2.y;\n  return res;\n}\n\n// fp8 -> float\ntemplate <>\n__inline__ __device__ float vec_conversion<float, uint8_t>(const uint8_t& a) {\n  fp8_type f8;\n  f8.__x = a;\n  return static_cast<float>(f8);\n}\n\n// fp8x2 -> float2\ntemplate <>\n__inline__ __device__ float2\nvec_conversion<float2, uint16_t>(const uint16_t& a) {\n  fp8x2_type f8x2;\n  f8x2.__x = a;\n  return static_cast<float2>(f8x2);\n}\n\n// fp8x4 -> float4\ntemplate <>\n__inline__ __device__ Float4_\nvec_conversion<Float4_, uint32_t>(const uint32_t& a) {\n  Float4_ res;\n  res.x = vec_conversion<float2, uint16_t>((uint16_t)a);\n  res.y = vec_conversion<float2, uint16_t>((uint16_t)(a >> 16U));\n  return res;\n}\n\n// fp8x4 -> float4\ntemplate <>\n__inline__ __device__ float4\nvec_conversion<float4, uint32_t>(const uint32_t& a) {\n  Float4_ tmp = vec_conversion<Float4_, uint32_t>(a);\n  float4 res = make_float4(tmp.x.x, tmp.x.y, tmp.y.x, tmp.y.y);\n  return res;\n}\n\n// fp8x8 -> float8\ntemplate <>\n__inline__ __device__ Float8_ vec_conversion<Float8_, uint2>(const uint2& a) {\n  Float4_ tmp1, tmp2;\n  tmp1 = vec_conversion<Float4_, uint32_t>(a.x);\n  tmp2 = vec_conversion<Float4_, uint32_t>(a.y);\n  Float8_ res;\n  res.x = tmp1.x;\n  res.y = tmp1.y;\n  res.z = tmp2.x;\n  res.w = tmp2.y;\n  return res;\n}\n\n// half -> fp8\ntemplate <>\n__inline__ __device__ uint8_t\nvec_conversion<uint8_t, uint16_t>(const uint16_t& a) {\n  __half_raw tmp;\n  tmp.x = a;\n  return __hip_cvt_halfraw_to_fp8(tmp, fp8_type::__default_saturation,\n                                  fp8_type::__default_interpret);\n}\n\ntemplate <>\n__inline__ __device__ uint16_t\nvec_conversion<uint16_t, uint32_t>(const uint32_t& a) {\n  union {\n    uint32_t ui32;\n    __half2_raw h2r;\n  } tmp;\n  tmp.ui32 = a;\n  return __hip_cvt_halfraw2_to_fp8x2(tmp.h2r, fp8_type::__default_saturation,\n                                     fp8_type::__default_interpret);\n}\n\n// bf16 -> fp8\ntemplate <>\n__inline__ __device__ uint8_t\nvec_conversion<uint8_t, __nv_bfloat16>(const __nv_bfloat16& a) {\n  return __hip_cvt_float_to_fp8(__bfloat162float(a),\n                                fp8_type::__default_saturation,\n                                fp8_type::__default_interpret);\n}\n\n// float -> fp8\ntemplate <>\n__inline__ __device__ uint8_t vec_conversion<uint8_t, float>(const float& a) {\n  return __hip_cvt_float_to_fp8(a, fp8_type::__default_saturation,\n                                fp8_type::__default_interpret);\n}\n\n// float2 -> half2\ntemplate <>\n__inline__ __device__ uint32_t\nvec_conversion<uint32_t, float2>(const float2& a) {\n  union {\n    half2 float16;\n    uint32_t uint32;\n  };\n\n  float16 = __float22half2_rn(a);\n  return uint32;\n}\n\n// Float4 -> half2x2\ntemplate <>\n__inline__ __device__ uint2 vec_conversion<uint2, Float4_>(const Float4_& a) {\n  uint2 b;\n  float2 val;\n  val.x = a.x.x;\n  val.y = a.x.y;\n  b.x = vec_conversion<uint32_t, float2>(val);\n\n  val.x = a.y.x;\n  val.y = a.y.y;\n  b.y = vec_conversion<uint32_t, float2>(val);\n  return b;\n}\n\n// Float4 -> float4\ntemplate <>\n__inline__ __device__ float4 vec_conversion<float4, Float4_>(const Float4_& a) {\n  float4 b;\n  b.x = a.x.x;\n  b.y = a.x.y;\n  b.z = a.y.x;\n  b.w = a.y.y;\n  return b;\n}\n\n// Float8 -> half2x4\ntemplate <>\n__inline__ __device__ uint4 vec_conversion<uint4, Float8_>(const Float8_& a) {\n  uint4 b;\n  b.x = vec_conversion<uint32_t, float2>(a.x);\n  b.y = vec_conversion<uint32_t, float2>(a.y);\n  b.z = vec_conversion<uint32_t, float2>(a.z);\n  b.w = vec_conversion<uint32_t, float2>(a.w);\n  return b;\n}\n\n// float2 -> bfloat162\ntemplate <>\n__inline__ __device__ __nv_bfloat162\nvec_conversion<__nv_bfloat162, float2>(const float2& a) {\n  __nv_bfloat162 b = __float22bfloat162_rn(a);\n  return b;\n}\n\n// Float4 -> bfloat162x2\ntemplate <>\n__inline__ __device__ bf16_4_t\nvec_conversion<bf16_4_t, Float4_>(const Float4_& a) {\n  bf16_4_t b;\n  b.x = __float22bfloat162_rn(a.x);\n  b.y = __float22bfloat162_rn(a.y);\n  return b;\n}\n\n// Float8 -> bfloat162x4\ntemplate <>\n__inline__ __device__ bf16_8_t\nvec_conversion<bf16_8_t, Float8_>(const Float8_& a) {\n  bf16_8_t b;\n  b.x = __float22bfloat162_rn(a.x);\n  b.y = __float22bfloat162_rn(a.y);\n  b.z = __float22bfloat162_rn(a.z);\n  b.w = __float22bfloat162_rn(a.w);\n  return b;\n}\n\n/* Scaled and vectorized conversions, for data exchange between high and low\n   precision domains\n\n   Convention of the scale in API, e.g: FP8_data = Quantization(\n   High_Precision_data / scale ) s.t. Quantize(HP / scale) => FP8 Dequant(FP8) *\n   scale =>  HP\n\n */\n\nusing __nv_bfloat16 = __hip_bfloat16;\n\n// fp8 -> __nv_bfloat16\ntemplate <>\n__inline__ __device__ __nv_bfloat16\nscaled_vec_conversion<__nv_bfloat16, uint8_t>(const uint8_t& a, float scale) {\n  fp8_type f8;\n  f8.__x = a;\n  return __float2bfloat16(static_cast<float>(f8) * scale);\n}\n\n// fp8x2 -> __nv_bfloat162\ntemplate <>\n__inline__ __device__ __nv_bfloat162\nscaled_vec_conversion<__nv_bfloat162, uint16_t>(const uint16_t& a,\n                                                float scale) {\n  __nv_bfloat162 res;\n  res.x = scaled_vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)a, scale);\n  res.y =\n      scaled_vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)(a >> 8U), scale);\n  return res;\n}\n\n// fp8x4 -> bf16_4_t\ntemplate <>\n__inline__ __device__ bf16_4_t\nscaled_vec_conversion<bf16_4_t, uint32_t>(const uint32_t& a, float scale) {\n  bf16_4_t res;\n  res.x = scaled_vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)a, scale);\n  res.y = scaled_vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)(a >> 16U),\n                                                          scale);\n  return res;\n}\n\n// fp8x8 -> bf16_8_t\ntemplate <>\n__inline__ __device__ bf16_8_t\nscaled_vec_conversion<bf16_8_t, uint2>(const uint2& a, float scale) {\n  bf16_4_t tmp1, tmp2;\n  tmp1 = scaled_vec_conversion<bf16_4_t, uint32_t>(a.x, scale);\n  tmp2 = scaled_vec_conversion<bf16_4_t, uint32_t>(a.y, scale);\n  bf16_8_t res;\n  res.x = tmp1.x;\n  res.y = tmp1.y;\n  res.z = tmp2.x;\n  res.w = tmp2.y;\n  return res;\n}\n\n// fp8 -> float\ntemplate <>\n__inline__ __device__ float scaled_vec_conversion<float, uint8_t>(\n    const uint8_t& a, float scale) {\n  fp8_type f8;\n  f8.__x = a;\n  return static_cast<float>(f8) * scale;\n}\n\n// fp8x2 -> float2\ntemplate <>\n__inline__ __device__ float2\nscaled_vec_conversion<float2, uint16_t>(const uint16_t& a, float scale) {\n  fp8x2_type f8x2;\n  f8x2.__x = a;\n  return static_cast<float2>(f8x2) * scale;\n}\n\n// fp8x4 -> float4\ntemplate <>\n__inline__ __device__ Float4_\nscaled_vec_conversion<Float4_, uint32_t>(const uint32_t& a, const float scale) {\n  Float4_ res;\n  res.x = scaled_vec_conversion<float2, uint16_t>((uint16_t)a, scale);\n  res.y = scaled_vec_conversion<float2, uint16_t>((uint16_t)(a >> 16U), scale);\n  return res;\n}\n\n// fp8x4 -> float4\ntemplate <>\n__inline__ __device__ float4\nscaled_vec_conversion<float4, uint32_t>(const uint32_t& a, float scale) {\n  Float4_ res = scaled_vec_conversion<Float4_, uint32_t>(a, scale);\n  return {res.x.x, res.x.y, res.y.x, res.y.y};\n}\n\n// fp8x8 -> float8\ntemplate <>\n__inline__ __device__ Float8_\nscaled_vec_conversion<Float8_, uint2>(const uint2& a, float scale) {\n  Float4_ tmp1, tmp2;\n  tmp1 = scaled_vec_conversion<Float4_, uint32_t>(a.x, scale);\n  tmp2 = scaled_vec_conversion<Float4_, uint32_t>(a.y, scale);\n  Float8_ res;\n  res.x = tmp1.x;\n  res.y = tmp1.y;\n  res.z = tmp2.x;\n  res.w = tmp2.y;\n  return res;\n}\n\n// fp8 -> half\ntemplate <>\n__inline__ __device__ uint16_t\nscaled_vec_conversion<uint16_t, uint8_t>(const uint8_t& a, float scale) {\n  __half_raw res;\n  res.data = scaled_vec_conversion<float, uint8_t>(a, scale);\n  return res.x;\n}\n\n// fp8x2 -> half2\ntemplate <>\n__inline__ __device__ uint32_t\nscaled_vec_conversion<uint32_t, uint16_t>(const uint16_t& a, float scale) {\n  union {\n    __half2_raw h2r;\n    uint32_t ui32;\n  } tmp;\n  tmp.h2r = __hip_cvt_fp8x2_to_halfraw2(a, fp8_type::__default_interpret);\n  tmp.h2r.x.data *= scale;\n  tmp.h2r.y.data *= scale;\n  return tmp.ui32;\n}\n\n// fp8x4 -> half2x2\ntemplate <>\n__inline__ __device__ uint2\nscaled_vec_conversion<uint2, uint32_t>(const uint32_t& a, float scale) {\n  union {\n    uint2 u32x2;\n    uint32_t u32[2];\n  } tmp;\n  tmp.u32[0] = scaled_vec_conversion<uint32_t, uint16_t>((uint16_t)a, scale);\n  tmp.u32[1] =\n      scaled_vec_conversion<uint32_t, uint16_t>((uint16_t)(a >> 16U), scale);\n  return tmp.u32x2;\n}\n\n// fp8x8 -> half2x4\ntemplate <>\n__inline__ __device__ uint4 scaled_vec_conversion<uint4, uint2>(const uint2& a,\n                                                                float scale) {\n  union {\n    uint4 u64x2;\n    uint2 u64[2];\n  } tmp;\n  tmp.u64[0] = scaled_vec_conversion<uint2, uint32_t>(a.x, scale);\n  tmp.u64[1] = scaled_vec_conversion<uint2, uint32_t>(a.y, scale);\n  return tmp.u64x2;\n}\n\n// half -> fp8\ntemplate <>\n__inline__ __device__ uint8_t\nscaled_vec_conversion<uint8_t, uint16_t>(const uint16_t& a, float scale) {\n  __half_raw tmp;\n  tmp.x = a;\n  tmp.data /= scale;\n  return __hip_cvt_halfraw_to_fp8(tmp, fp8_type::__default_saturation,\n                                  fp8_type::__default_interpret);\n}\n\n// halfx2 -> fp8x2\ntemplate <>\n__inline__ __device__ uint16_t\nscaled_vec_conversion<uint16_t, uint32_t>(const uint32_t& a, float scale) {\n  union {\n    uint32_t ui32;\n    __half2_raw h2r;\n  } tmp;\n  tmp.ui32 = a;\n  tmp.h2r.x.data /= scale;\n  tmp.h2r.y.data /= scale;\n  return __hip_cvt_halfraw2_to_fp8x2(tmp.h2r, fp8_type::__default_saturation,\n                                     fp8_type::__default_interpret);\n}\n\n// half2x2 -> fp8x4\ntemplate <>\n__inline__ __device__ uint32_t\nscaled_vec_conversion<uint32_t, uint2>(const uint2& a, float scale) {\n  union {\n    uint16_t ui16[2];\n    uint32_t ui32;\n  } tmp;\n  tmp.ui16[0] = scaled_vec_conversion<uint16_t, uint32_t>(a.x, scale);\n  tmp.ui16[1] = scaled_vec_conversion<uint16_t, uint32_t>(a.y, scale);\n  return tmp.ui32;\n}\n\n// half2x4 -> fp8x8\ntemplate <>\n__inline__ __device__ uint2 scaled_vec_conversion<uint2, uint4>(const uint4& a,\n                                                                float scale) {\n  union {\n    uint2 ui2[2];\n    uint4 ui4;\n  } tmp;\n  tmp.ui4 = a;\n  uint2 res;\n  res.x = scaled_vec_conversion<uint32_t, uint2>(tmp.ui2[0], scale);\n  res.y = scaled_vec_conversion<uint32_t, uint2>(tmp.ui2[1], scale);\n  return res;\n}\n\n// bf16 -> fp8\ntemplate <>\n__inline__ __device__ uint8_t scaled_vec_conversion<uint8_t, __nv_bfloat16>(\n    const __nv_bfloat16& a, float scale) {\n  return __hip_cvt_float_to_fp8(__bfloat162float(a) / scale,\n                                fp8_type::__default_saturation,\n                                fp8_type::__default_interpret);\n}\n\n// bf16x2 -> fp8x2\ntemplate <>\n__inline__ __device__ uint16_t scaled_vec_conversion<uint16_t, __nv_bfloat162>(\n    const __nv_bfloat162& a, float scale) {\n  union {\n    uint8_t ui8[2];\n    uint16_t ui16;\n  } tmp;\n  tmp.ui8[0] = scaled_vec_conversion<uint8_t, __nv_bfloat16>(a.x, scale);\n  tmp.ui8[1] = scaled_vec_conversion<uint8_t, __nv_bfloat16>(a.y, scale);\n  return tmp.ui16;\n}\n\n// bf16x4 -> fp8x4\ntemplate <>\n__inline__ __device__ uint32_t\nscaled_vec_conversion<uint32_t, bf16_4_t>(const bf16_4_t& a, float scale) {\n  union {\n    uint16_t ui16[2];\n    uint32_t ui32;\n  } tmp;\n  tmp.ui16[0] = scaled_vec_conversion<uint16_t, __nv_bfloat162>(a.x, scale);\n  tmp.ui16[1] = scaled_vec_conversion<uint16_t, __nv_bfloat162>(a.y, scale);\n  return tmp.ui32;\n}\n\n// bf16x8 -> fp8x8\ntemplate <>\n__inline__ __device__ uint2\nscaled_vec_conversion<uint2, bf16_8_t>(const bf16_8_t& a, float scale) {\n  uint2 res;\n  res.x = scaled_vec_conversion<uint32_t, bf16_4_t>({a.x, a.y}, scale);\n  res.y = scaled_vec_conversion<uint32_t, bf16_4_t>({a.z, a.w}, scale);\n  return res;\n}\n\n// float -> fp8\ntemplate <>\n__inline__ __device__ uint8_t\nscaled_vec_conversion<uint8_t, float>(const float& a, float scale) {\n  return __hip_cvt_float_to_fp8(a / scale, fp8_type::__default_saturation,\n                                fp8_type::__default_interpret);\n}\n\n// floatx2 -> fp8x2\ntemplate <>\n__inline__ __device__ uint16_t\nscaled_vec_conversion<uint16_t, float2>(const float2& a, float scale) {\n  return __hip_cvt_float2_to_fp8x2(a / scale, fp8_type::__default_saturation,\n                                   fp8_type::__default_interpret);\n}\n\n// floatx4 -> fp8x4\ntemplate <>\n__inline__ __device__ uint32_t\nscaled_vec_conversion<uint32_t, float4>(const float4& a, float scale) {\n  union {\n    uint16_t ui16[2];\n    uint32_t ui32;\n  } tmp;\n  tmp.ui16[0] = scaled_vec_conversion<uint16_t, float2>({a.x, a.y}, scale);\n  tmp.ui16[1] = scaled_vec_conversion<uint16_t, float2>({a.z, a.w}, scale);\n  return tmp.ui32;\n}\n  #endif  // ENABLE_FP8\n\ntemplate <typename Tout, typename Tin, Fp8KVCacheDataType kv_dt>\n__inline__ __device__ Tout convert(const Tin& x) {\n  #ifdef ENABLE_FP8\n  if constexpr (kv_dt == Fp8KVCacheDataType::kFp8E4M3) {\n    return vec_conversion<Tout, Tin>(x);\n  }\n  #endif\n  assert(false);\n  return {};  // Squash missing return statement warning\n}\n\ntemplate <typename Tout, typename Tin, Fp8KVCacheDataType kv_dt>\n__inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {\n  #ifdef ENABLE_FP8\n  if constexpr (kv_dt == Fp8KVCacheDataType::kFp8E4M3) {\n    return scaled_vec_conversion<Tout, Tin>(x, scale);\n  }\n  #endif\n  assert(false);\n  return {};  // Squash missing return statement warning\n}\n\n  // The following macro is used to dispatch the conversion function based on\n  // the data type of the key and value cache. The FN is a macro that calls a\n  // function with template<typename scalar_t, typename cache_t,\n  // Fp8KVCacheDataType kv_dt>.\n  #define DISPATCH_BY_KV_CACHE_DTYPE(SRC_DTYPE, KV_DTYPE, FN)                  \\\n    if (KV_DTYPE == \"auto\") {                                                  \\\n      if (SRC_DTYPE == at::ScalarType::Float) {                                \\\n        FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\\n      } else if (SRC_DTYPE == at::ScalarType::Half) {                          \\\n        FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\\n      } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                      \\\n        FN(__nv_bfloat16, __nv_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\\n      } else {                                                                 \\\n        TORCH_CHECK(false, \"Unsupported input type of kv cache: \", SRC_DTYPE); \\\n      }                                                                        \\\n    } else {                                                                   \\\n      if (KV_DTYPE == \"fp8\" || KV_DTYPE == \"fp8_e4m3\") {                       \\\n        if (SRC_DTYPE == at::ScalarType::Float) {                              \\\n          FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\\n        } else if (SRC_DTYPE == at::ScalarType::Half) {                        \\\n          FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\\n        } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                    \\\n          FN(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\\n        } else {                                                               \\\n          TORCH_CHECK(false,                                                   \\\n                      \"Unsupported input type of kv cache: \", SRC_DTYPE);      \\\n        }                                                                      \\\n      } else {                                                                 \\\n        TORCH_CHECK(false, \"Unsupported data type of kv cache: \", KV_DTYPE);   \\\n      }                                                                        \\\n    }\n\n}  // namespace fp8\n#endif  // USE_ROCM\n}  // namespace vllm\n// End include ../quantization/fp8/amd/quant_utils.cuh\ntypedef __hip_bfloat16 __nv_bfloat16;\n#else\n// Begin include ../quantization/fp8/nvidia/quant_utils.cuh\n#pragma once\n\n#include <assert.h>\n#include <float.h>\n#include <stdint.h>\n#include <type_traits>\n\nnamespace vllm {\n#ifndef USE_ROCM\n\nnamespace fp8 {\n  #ifdef ENABLE_FP8\n\n    #if 0  // Disable the following code to reduce the binary size.\ntemplate <typename Tout, typename Tin>\n__inline__ __device__ Tout\nvec_conversion(const Tin &x, const __nv_fp8_interpretation_t fp8_type) {\n  return x;\n}\n\n// fp8 -> half\ntemplate <>\n__inline__ __device__ uint16_t vec_conversion<uint16_t, uint8_t>(\n    const uint8_t &a, const __nv_fp8_interpretation_t fp8_type) {\n  __half_raw res = __nv_cvt_fp8_to_halfraw(a, fp8_type);\n  return res.x;\n}\n\n// fp8x2 -> half2\ntemplate <>\n__inline__ __device__ uint32_t vec_conversion<uint32_t, uint16_t>(\n    const uint16_t &a, const __nv_fp8_interpretation_t fp8_type) {\n  union {\n    uint16_t u16[2];\n    uint32_t u32;\n  } tmp;\n  __half2_raw res = __nv_cvt_fp8x2_to_halfraw2(a, fp8_type);\n  tmp.u16[0] = res.x;\n  tmp.u16[1] = res.y;\n  return tmp.u32;\n}\n\n// fp8x4 -> half2x2\ntemplate <>\n__inline__ __device__ uint2 vec_conversion<uint2, uint32_t>(\n    const uint32_t &a, const __nv_fp8_interpretation_t fp8_type) {\n  union {\n    uint2 u32x2;\n    uint32_t u32[2];\n  } tmp;\n  tmp.u32[0] = vec_conversion<uint32_t, uint16_t>((uint16_t)a, fp8_type);\n  tmp.u32[1] =\n      vec_conversion<uint32_t, uint16_t>((uint16_t)(a >> 16U), fp8_type);\n  return tmp.u32x2;\n}\n\n// fp8x8 -> half2x4\ntemplate <>\n__inline__ __device__ uint4 vec_conversion<uint4, uint2>(\n    const uint2 &a, const __nv_fp8_interpretation_t fp8_type) {\n  union {\n    uint4 u64x2;\n    uint2 u64[2];\n  } tmp;\n  tmp.u64[0] = vec_conversion<uint2, uint32_t>(a.x, fp8_type);\n  tmp.u64[1] = vec_conversion<uint2, uint32_t>(a.y, fp8_type);\n  return tmp.u64x2;\n}\n\n// fp8 -> __nv_bfloat16\ntemplate <>\n__inline__ __device__ __nv_bfloat16 vec_conversion<__nv_bfloat16, uint8_t>(\n    const uint8_t &a, const __nv_fp8_interpretation_t fp8_type) {\n  // Note there is no direct convert function from fp8 to bf16.\n  // fp8 -> half\n  __half_raw res = __nv_cvt_fp8_to_halfraw(a, fp8_type);\n  // half -> float -> bf16\n  float tmp = half_to_float(res.x);\n  return __float2bfloat16(tmp);\n}\n\n// fp8x2 -> __nv_bfloat162\ntemplate <>\n__inline__ __device__ __nv_bfloat162 vec_conversion<__nv_bfloat162, uint16_t>(\n    const uint16_t &a, const __nv_fp8_interpretation_t fp8_type) {\n  __nv_bfloat162 res;\n  res.x = vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)a, fp8_type);\n  res.y = vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)(a >> 8U), fp8_type);\n  return res;\n}\n\n// fp8x4 -> bf16_4_t\ntemplate <>\n__inline__ __device__ bf16_4_t vec_conversion<bf16_4_t, uint32_t>(\n    const uint32_t &a, const __nv_fp8_interpretation_t fp8_type) {\n  bf16_4_t res;\n  res.x = vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)a, fp8_type);\n  res.y =\n      vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)(a >> 16U), fp8_type);\n  return res;\n}\n\n// fp8x8 -> bf16_8_t\ntemplate <>\n__inline__ __device__ bf16_8_t vec_conversion<bf16_8_t, uint2>(\n    const uint2 &a, const __nv_fp8_interpretation_t fp8_type) {\n  bf16_4_t tmp1, tmp2;\n  tmp1 = vec_conversion<bf16_4_t, uint32_t>(a.x, fp8_type);\n  tmp2 = vec_conversion<bf16_4_t, uint32_t>(a.y, fp8_type);\n  bf16_8_t res;\n  res.x = tmp1.x;\n  res.y = tmp1.y;\n  res.z = tmp2.x;\n  res.w = tmp2.y;\n  return res;\n}\n\n// fp8 -> float\ntemplate <>\n__inline__ __device__ float\nvec_conversion<float, uint8_t>(const uint8_t &a,\n                               const __nv_fp8_interpretation_t fp8_type) {\n  // fp8 -> half\n  uint16_t tmp = vec_conversion<uint16_t, uint8_t>(a, fp8_type);\n  // half -> float\n  return half_to_float(tmp);\n}\n\n// fp8x2 -> float2\ntemplate <>\n__inline__ __device__ float2 vec_conversion<float2, uint16_t>(\n    const uint16_t &a, const __nv_fp8_interpretation_t fp8_type) {\n  // fp8x2 -> half2\n  uint32_t tmp = vec_conversion<uint32_t, uint16_t>(a, fp8_type);\n  // half2 -> float2\n  return half2_to_float2(tmp);\n}\n\n// fp8x4 -> float4\ntemplate <>\n__inline__ __device__ Float4_ vec_conversion<Float4_, uint32_t>(\n    const uint32_t &a, const __nv_fp8_interpretation_t fp8_type) {\n  Float4_ res;\n  res.x = vec_conversion<float2, uint16_t>((uint16_t)a, fp8_type);\n  res.y = vec_conversion<float2, uint16_t>((uint16_t)(a >> 16U), fp8_type);\n  return res;\n}\n\n// fp8x8 -> float8\ntemplate <>\n__inline__ __device__ Float8_ vec_conversion<Float8_, uint2>(\n    const uint2 &a, const __nv_fp8_interpretation_t fp8_type) {\n  Float4_ tmp1, tmp2;\n  tmp1 = vec_conversion<Float4_, uint32_t>(a.x, fp8_type);\n  tmp2 = vec_conversion<Float4_, uint32_t>(a.y, fp8_type);\n  Float8_ res;\n  res.x = tmp1.x;\n  res.y = tmp1.y;\n  res.z = tmp2.x;\n  res.w = tmp2.y;\n  return res;\n}\n\n// half -> fp8\ntemplate <>\n__inline__ __device__ uint8_t vec_conversion<uint8_t, uint16_t>(\n    const uint16_t &a, const __nv_fp8_interpretation_t fp8_type) {\n  __half_raw tmp;\n  tmp.x = a;\n  __nv_fp8_storage_t res =\n      __nv_cvt_halfraw_to_fp8(tmp, __NV_SATFINITE, fp8_type);\n  return (uint8_t)res;\n}\n\n// bf16 -> fp8\ntemplate <>\n__inline__ __device__ uint8_t vec_conversion<uint8_t, __nv_bfloat16>(\n    const __nv_bfloat16 &a, const __nv_fp8_interpretation_t fp8_type) {\n      #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800\n  assert(false);\n      #else\n  __nv_fp8_storage_t res = __nv_cvt_bfloat16raw_to_fp8(\n      __nv_bfloat16_raw(a), __NV_SATFINITE, fp8_type);\n  return (uint8_t)res;\n      #endif\n}\n\n// float -> fp8\ntemplate <>\n__inline__ __device__ uint8_t vec_conversion<uint8_t, float>(\n    const float &a, const __nv_fp8_interpretation_t fp8_type) {\n  __nv_fp8_storage_t res = __nv_cvt_float_to_fp8(a, __NV_SATFINITE, fp8_type);\n  return (uint8_t)res;\n}\n\n// fp8x4 -> float4\ntemplate <>\n__inline__ __device__ float4 vec_conversion<float4, uint32_t>(\n    const uint32_t &a, const __nv_fp8_interpretation_t fp8_type) {\n  Float4_ tmp = vec_conversion<Float4_, uint32_t>(a, fp8_type);\n  float4 res = make_float4(tmp.x.x, tmp.x.y, tmp.y.x, tmp.y.y);\n  return res;\n}\n\ntemplate <>\n__inline__ __device__ uint32_t vec_conversion<uint32_t, float2>(\n    const float2 &a, const __nv_fp8_interpretation_t fp8_type) {\n  union {\n    half2 float16;\n    uint32_t uint32;\n  };\n\n  float16 = __float22half2_rn(a);\n  return uint32;\n}\n\ntemplate <>\n__inline__ __device__ uint2 vec_conversion<uint2, Float4_>(\n    const Float4_ &a, const __nv_fp8_interpretation_t fp8_type) {\n  uint2 b;\n  float2 val;\n  val.x = a.x.x;\n  val.y = a.x.y;\n  b.x = vec_conversion<uint32_t, float2>(val, fp8_type);\n\n  val.x = a.y.x;\n  val.y = a.y.y;\n  b.y = vec_conversion<uint32_t, float2>(val, fp8_type);\n\n  return b;\n}\n\ntemplate <>\n__inline__ __device__ float4 vec_conversion<float4, Float4_>(\n    const Float4_ &a, const __nv_fp8_interpretation_t fp8_type) {\n  float4 b;\n  b.x = a.x.x;\n  b.y = a.x.y;\n  b.z = a.y.x;\n  b.w = a.y.y;\n  return b;\n}\n\ntemplate <>\n__inline__ __device__ uint4 vec_conversion<uint4, Float8_>(\n    const Float8_ &a, const __nv_fp8_interpretation_t fp8_type) {\n  uint4 b;\n  b.x = vec_conversion<uint32_t, float2>(a.x, fp8_type);\n  b.y = vec_conversion<uint32_t, float2>(a.y, fp8_type);\n  b.z = vec_conversion<uint32_t, float2>(a.z, fp8_type);\n  b.w = vec_conversion<uint32_t, float2>(a.w, fp8_type);\n  return b;\n}\n\ntemplate <>\n__inline__ __device__ __nv_bfloat162 vec_conversion<__nv_bfloat162, float2>(\n    const float2 &a, const __nv_fp8_interpretation_t fp8_type) {\n  __nv_bfloat162 b;\n  from_float(b, a);\n  return b;\n}\n\ntemplate <>\n__inline__ __device__ bf16_4_t vec_conversion<bf16_4_t, Float4_>(\n    const Float4_ &a, const __nv_fp8_interpretation_t fp8_type) {\n  bf16_4_t b;\n  from_float(b, a);\n  return b;\n}\n\ntemplate <>\n__inline__ __device__ bf16_8_t vec_conversion<bf16_8_t, Float8_>(\n    const Float8_ &a, const __nv_fp8_interpretation_t fp8_type) {\n  bf16_8_t b;\n  from_float(b, a);\n  return b;\n}\n    #endif\n\n/* Scaled and vectorized conversions, for data exchange between high and low\n   precision domains Convention of the scale in API, e.g: FP8_data =\n   Quantization( High_Precision_data / scale ) s.t. Quantize(HP / scale) => FP8\n     Dequant(FP8) * scale =>  HP\n */\n\ntemplate <typename Tout, typename Tin>\n__inline__ __device__ Tout scaled_vec_conversion(\n    const Tin& x, const float scale, const __nv_fp8_interpretation_t fp8_type) {\n  return x;\n}\n\n// fp8 -> half\ntemplate <>\n__inline__ __device__ uint16_t scaled_vec_conversion<uint16_t, uint8_t>(\n    const uint8_t& a, const float scale,\n    const __nv_fp8_interpretation_t fp8_type) {\n  __half_raw tmp = __nv_cvt_fp8_to_halfraw(a, fp8_type);\n  return float_to_half(half_to_float(tmp.x) * scale);\n}\n\n// fp8x2 -> half2\ntemplate <>\n__inline__ __device__ uint32_t scaled_vec_conversion<uint32_t, uint16_t>(\n    const uint16_t& a, const float scale,\n    const __nv_fp8_interpretation_t fp8_type) {\n  union {\n    uint16_t u16[2];\n    uint32_t u32;\n  } tmp;\n  __half2_raw res = __nv_cvt_fp8x2_to_halfraw2(a, fp8_type);\n  tmp.u16[0] = float_to_half(half_to_float(res.x) * scale);\n  tmp.u16[1] = float_to_half(half_to_float(res.y) * scale);\n  return tmp.u32;\n}\n\n// fp8x4 -> half2x2\ntemplate <>\n__inline__ __device__ uint2 scaled_vec_conversion<uint2, uint32_t>(\n    const uint32_t& a, const float scale,\n    const __nv_fp8_interpretation_t fp8_type) {\n  union {\n    uint2 u32x2;\n    uint32_t u32[2];\n  } tmp;\n  tmp.u32[0] =\n      scaled_vec_conversion<uint32_t, uint16_t>((uint16_t)a, scale, fp8_type);\n  tmp.u32[1] = scaled_vec_conversion<uint32_t, uint16_t>((uint16_t)(a >> 16U),\n                                                         scale, fp8_type);\n  return tmp.u32x2;\n}\n\n// fp8x8 -> half2x4\ntemplate <>\n__inline__ __device__ uint4\nscaled_vec_conversion<uint4, uint2>(const uint2& a, const float scale,\n                                    const __nv_fp8_interpretation_t fp8_type) {\n  union {\n    uint4 u64x2;\n    uint2 u64[2];\n  } tmp;\n  tmp.u64[0] = scaled_vec_conversion<uint2, uint32_t>(a.x, scale, fp8_type);\n  tmp.u64[1] = scaled_vec_conversion<uint2, uint32_t>(a.y, scale, fp8_type);\n  return tmp.u64x2;\n}\n\n// fp8 -> __nv_bfloat16\ntemplate <>\n__inline__ __device__ __nv_bfloat16\nscaled_vec_conversion<__nv_bfloat16, uint8_t>(\n    const uint8_t& a, const float scale,\n    const __nv_fp8_interpretation_t fp8_type) {\n  // Note there is no direct convert function from fp8 to bf16.\n  // fp8 -> half\n  __half_raw res = __nv_cvt_fp8_to_halfraw(a, fp8_type);\n  // half -> float -> bf16\n  float tmp = half_to_float(res.x);\n  return __float2bfloat16(tmp * scale);\n}\n\n// fp8x2 -> __nv_bfloat162\ntemplate <>\n__inline__ __device__ __nv_bfloat162\nscaled_vec_conversion<__nv_bfloat162, uint16_t>(\n    const uint16_t& a, const float scale,\n    const __nv_fp8_interpretation_t fp8_type) {\n  __nv_bfloat162 res;\n  res.x = scaled_vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)a, scale,\n                                                        fp8_type);\n  res.y = scaled_vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)(a >> 8U),\n                                                        scale, fp8_type);\n  return res;\n}\n\n// fp8x4 -> bf16_4_t\ntemplate <>\n__inline__ __device__ bf16_4_t scaled_vec_conversion<bf16_4_t, uint32_t>(\n    const uint32_t& a, const float scale,\n    const __nv_fp8_interpretation_t fp8_type) {\n  bf16_4_t res;\n  res.x = scaled_vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)a, scale,\n                                                          fp8_type);\n  res.y = scaled_vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)(a >> 16U),\n                                                          scale, fp8_type);\n  return res;\n}\n\n// fp8x8 -> bf16_8_t\ntemplate <>\n__inline__ __device__ bf16_8_t scaled_vec_conversion<bf16_8_t, uint2>(\n    const uint2& a, const float scale,\n    const __nv_fp8_interpretation_t fp8_type) {\n  bf16_4_t tmp1, tmp2;\n  tmp1 = scaled_vec_conversion<bf16_4_t, uint32_t>(a.x, scale, fp8_type);\n  tmp2 = scaled_vec_conversion<bf16_4_t, uint32_t>(a.y, scale, fp8_type);\n  bf16_8_t res;\n  res.x = tmp1.x;\n  res.y = tmp1.y;\n  res.z = tmp2.x;\n  res.w = tmp2.y;\n  return res;\n}\n\n// fp8 -> float\ntemplate <>\n__inline__ __device__ float scaled_vec_conversion<float, uint8_t>(\n    const uint8_t& a, const float scale,\n    const __nv_fp8_interpretation_t fp8_type) {\n  // fp8 -> half\n  __half_raw res = __nv_cvt_fp8_to_halfraw(a, fp8_type);\n  uint16_t tmp = res.x;\n\n  // half -> float\n  return half_to_float(tmp) * scale;\n}\n\n// fp8x2 -> float2\ntemplate <>\n__inline__ __device__ float2 scaled_vec_conversion<float2, uint16_t>(\n    const uint16_t& a, const float scale,\n    const __nv_fp8_interpretation_t fp8_type) {\n  // fp8x2 -> half2\n  uint32_t tmp = scaled_vec_conversion<uint32_t, uint16_t>(a, scale, fp8_type);\n  // half2 -> float2\n  return half2_to_float2(tmp);\n}\n\n// fp8x4 -> float4\ntemplate <>\n__inline__ __device__ Float4_ scaled_vec_conversion<Float4_, uint32_t>(\n    const uint32_t& a, const float scale,\n    const __nv_fp8_interpretation_t fp8_type) {\n  Float4_ res;\n  res.x = scaled_vec_conversion<float2, uint16_t>((uint16_t)a, scale, fp8_type);\n  res.y = scaled_vec_conversion<float2, uint16_t>((uint16_t)(a >> 16U), scale,\n                                                  fp8_type);\n  return res;\n}\n\n// fp8x8 -> float8\ntemplate <>\n__inline__ __device__ Float8_ scaled_vec_conversion<Float8_, uint2>(\n    const uint2& a, const float scale,\n    const __nv_fp8_interpretation_t fp8_type) {\n  Float4_ tmp1, tmp2;\n  tmp1 = scaled_vec_conversion<Float4_, uint32_t>(a.x, scale, fp8_type);\n  tmp2 = scaled_vec_conversion<Float4_, uint32_t>(a.y, scale, fp8_type);\n  Float8_ res;\n  res.x = tmp1.x;\n  res.y = tmp1.y;\n  res.z = tmp2.x;\n  res.w = tmp2.y;\n  return res;\n}\n\n// half -> fp8\ntemplate <>\n__inline__ __device__ uint8_t scaled_vec_conversion<uint8_t, uint16_t>(\n    const uint16_t& a, const float scale,\n    const __nv_fp8_interpretation_t fp8_type) {\n  __nv_fp8_storage_t res =\n      __nv_cvt_float_to_fp8(half_to_float(a) / scale, __NV_SATFINITE, fp8_type);\n  return (uint8_t)res;\n}\n\n// bf16 -> fp8\ntemplate <>\n__inline__ __device__ uint8_t scaled_vec_conversion<uint8_t, __nv_bfloat16>(\n    const __nv_bfloat16& a, const float scale,\n    const __nv_fp8_interpretation_t fp8_type) {\n    #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800\n  assert(false);\n    #else\n  __nv_fp8_storage_t res = __nv_cvt_float_to_fp8(__bfloat162float(a) / scale,\n                                                 __NV_SATFINITE, fp8_type);\n  return (uint8_t)res;\n    #endif\n  __builtin_unreachable();  // Suppress missing return statement warning\n}\n\n// float -> fp8\ntemplate <>\n__inline__ __device__ uint8_t scaled_vec_conversion<uint8_t, float>(\n    const float& a, const float scale,\n    const __nv_fp8_interpretation_t fp8_type) {\n  __nv_fp8_storage_t res =\n      __nv_cvt_float_to_fp8(a / scale, __NV_SATFINITE, fp8_type);\n  return (uint8_t)res;\n}\n\n// fp8x4 -> float4\ntemplate <>\n__inline__ __device__ float4 scaled_vec_conversion<float4, uint32_t>(\n    const uint32_t& a, const float scale,\n    const __nv_fp8_interpretation_t fp8_type) {\n  Float4_ tmp = scaled_vec_conversion<Float4_, uint32_t>(a, scale, fp8_type);\n  float4 res = make_float4(tmp.x.x, tmp.x.y, tmp.y.x, tmp.y.y);\n  return res;\n}\n  #endif  // ENABLE_FP8\n\ntemplate <typename Tout, typename Tin, Fp8KVCacheDataType kv_dt>\n__inline__ __device__ Tout convert(const Tin& x) {\n  #if 0  // Disable the following code to reduce the binary size.\n  if constexpr (kv_dt == Fp8KVCacheDataType::kFp8E4M3) {\n    return vec_conversion<Tout, Tin>(x, __NV_E4M3);\n  } else if constexpr (kv_dt == Fp8KVCacheDataType::kFp8E5M2) {\n    return vec_conversion<Tout, Tin>(x, __NV_E5M2);\n  }\n  #endif\n  assert(false);\n  __builtin_unreachable();  // Suppress missing return statement warning\n}\n\ntemplate <typename Tout, typename Tin, Fp8KVCacheDataType kv_dt>\n__inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {\n  #ifdef ENABLE_FP8\n  if constexpr (kv_dt == Fp8KVCacheDataType::kFp8E4M3) {\n    return scaled_vec_conversion<Tout, Tin>(x, scale, __NV_E4M3);\n  } else if constexpr (kv_dt == Fp8KVCacheDataType::kFp8E5M2) {\n    return scaled_vec_conversion<Tout, Tin>(x, scale, __NV_E5M2);\n  }\n  #endif\n  assert(false);\n  __builtin_unreachable();  // Suppress missing return statement warning\n}\n\n  // The following macro is used to dispatch the conversion function based on\n  // the data type of the key and value cache. The FN is a macro that calls a\n  // function with template<typename scalar_t, typename cache_t,\n  // Fp8KVCacheDataType kv_dt>.\n  #define DISPATCH_BY_KV_CACHE_DTYPE(SRC_DTYPE, KV_DTYPE, FN)                  \\\n    if (KV_DTYPE == \"auto\") {                                                  \\\n      if (SRC_DTYPE == at::ScalarType::Float) {                                \\\n        FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\\n      } else if (SRC_DTYPE == at::ScalarType::Half) {                          \\\n        FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\\n      } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                      \\\n        FN(__nv_bfloat16, __nv_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\\n      } else {                                                                 \\\n        TORCH_CHECK(false, \"Unsupported input type of kv cache: \", SRC_DTYPE); \\\n      }                                                                        \\\n    } else {                                                                   \\\n      if (KV_DTYPE == \"fp8\" || KV_DTYPE == \"fp8_e4m3\") {                       \\\n        if (SRC_DTYPE == at::ScalarType::Float) {                              \\\n          FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\\n        } else if (SRC_DTYPE == at::ScalarType::Half) {                        \\\n          FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\\n        } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                    \\\n          FN(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\\n        } else {                                                               \\\n          TORCH_CHECK(false,                                                   \\\n                      \"Unsupported input type of kv cache: \", SRC_DTYPE);      \\\n        }                                                                      \\\n      } else if (KV_DTYPE == \"fp8_e5m2\") {                                     \\\n        if (SRC_DTYPE == at::ScalarType::Float) {                              \\\n          FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E5M2);              \\\n        } else if (SRC_DTYPE == at::ScalarType::Half) {                        \\\n          FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E5M2);           \\\n        } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                    \\\n          FN(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E5M2);      \\\n        } else {                                                               \\\n          TORCH_CHECK(false,                                                   \\\n                      \"Unsupported input type of kv cache: \", SRC_DTYPE);      \\\n        }                                                                      \\\n      } else {                                                                 \\\n        TORCH_CHECK(false, \"Unsupported data type of kv cache: \", KV_DTYPE);   \\\n      }                                                                        \\\n    }\n\n}  // namespace fp8\n#endif  // not USE_ROCM\n}  // namespace vllm\n// End include ../quantization/fp8/nvidia/quant_utils.cuh\n#endif\n\n#define MAX(a, b) ((a) > (b) ? (a) : (b))\n#define MIN(a, b) ((a) < (b) ? (a) : (b))\n#define DIVIDE_ROUND_UP(a, b) (((a) + (b) - 1) / (b))\n\nnamespace vllm {\n\n// Utility function for attention softmax.\ntemplate <int NUM_WARPS>\ninline __device__ float block_sum(float* red_smem, float sum) {\n  // Decompose the thread index into warp / lane.\n  int warp = threadIdx.x / WARP_SIZE;\n  int lane = threadIdx.x % WARP_SIZE;\n\n  // Compute the sum per warp.\n#pragma unroll\n  for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {\n    sum += VLLM_SHFL_XOR_SYNC(sum, mask);\n  }\n\n  // Warp leaders store the data to shared memory.\n  if (lane == 0) {\n    red_smem[warp] = sum;\n  }\n\n  // Make sure the data is in shared memory.\n  __syncthreads();\n\n  // The warps compute the final sums.\n  if (lane < NUM_WARPS) {\n    sum = red_smem[lane];\n  }\n\n  // Parallel reduction inside the warp.\n#pragma unroll\n  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {\n    sum += VLLM_SHFL_XOR_SYNC(sum, mask);\n  }\n\n  // Broadcast to other threads.\n  return VLLM_SHFL_SYNC(sum, 0);\n}\n\n// TODO(woosuk): Merge the last two dimensions of the grid.\n// Grid: (num_heads, num_seqs, max_num_partitions).\ntemplate <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,\n          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,\n          bool IS_BLOCK_SPARSE,\n          int PARTITION_SIZE = 0>  // Zero means no partitioning.\n__device__ void paged_attention_kernel(\n    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]\n    float* __restrict__ max_logits,  // [num_seqs, num_heads,\n                                     // max_num_partitions]\n    scalar_t* __restrict__ out,  // [num_seqs, num_heads, max_num_partitions,\n                                 // head_size]\n    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]\n    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,\n                                          // head_size/x, block_size, x]\n    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,\n                                          // head_size, block_size]\n    const int num_kv_heads,               // [num_heads]\n    const float scale,\n    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]\n    const int* __restrict__ seq_lens,      // [num_seqs]\n    const int max_num_blocks_per_seq,\n    const float* __restrict__ alibi_slopes,  // [num_heads]\n    const int q_stride, const int kv_block_stride, const int kv_head_stride,\n    const float* k_scale, const float* v_scale, const int tp_rank,\n    const int blocksparse_local_blocks, const int blocksparse_vert_stride,\n    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {\n  const int seq_idx = blockIdx.y;\n  const int partition_idx = blockIdx.z;\n  const int max_num_partitions = gridDim.z;\n  constexpr bool USE_PARTITIONING = PARTITION_SIZE > 0;\n  const int seq_len = seq_lens[seq_idx];\n  if (USE_PARTITIONING && partition_idx * PARTITION_SIZE >= seq_len) {\n    // No work to do. Terminate the thread block.\n    return;\n  }\n\n  const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);\n  const int num_blocks_per_partition =\n      USE_PARTITIONING ? PARTITION_SIZE / BLOCK_SIZE : num_seq_blocks;\n\n  // [start_block_idx, end_block_idx) is the range of blocks to process.\n  const int start_block_idx =\n      USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;\n  const int end_block_idx =\n      MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);\n  const int num_blocks = end_block_idx - start_block_idx;\n\n  // [start_token_idx, end_token_idx) is the range of tokens to process.\n  const int start_token_idx = start_block_idx * BLOCK_SIZE;\n  const int end_token_idx =\n      MIN(start_token_idx + num_blocks * BLOCK_SIZE, seq_len);\n  const int num_tokens = end_token_idx - start_token_idx;\n\n  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);\n  constexpr int NUM_THREAD_GROUPS =\n      NUM_THREADS / THREAD_GROUP_SIZE;  // Note: This assumes THREAD_GROUP_SIZE\n                                        // divides NUM_THREADS\n  assert(NUM_THREADS % THREAD_GROUP_SIZE == 0);\n  constexpr int NUM_TOKENS_PER_THREAD_GROUP =\n      DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);\n  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;\n  const int thread_idx = threadIdx.x;\n  const int warp_idx = thread_idx / WARP_SIZE;\n  const int lane = thread_idx % WARP_SIZE;\n\n  const int head_idx = blockIdx.x;\n  const int num_heads = gridDim.x;\n  const int num_queries_per_kv = num_heads / num_kv_heads;\n  const int kv_head_idx = head_idx / num_queries_per_kv;\n  const float alibi_slope =\n      alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];\n\n  // A vector type to store a part of a key or a query.\n  // The vector size is configured in such a way that the threads in a thread\n  // group fetch or compute 16 bytes at a time. For example, if the size of a\n  // thread group is 4 and the data type is half, then the vector size is 16 /\n  // (4 * sizeof(half)) == 2.\n  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1);\n  using K_vec = typename Vec<scalar_t, VEC_SIZE>::Type;\n  using Q_vec = typename Vec<scalar_t, VEC_SIZE>::Type;\n  using Quant_vec = typename Vec<cache_t, VEC_SIZE>::Type;\n\n  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;\n  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;\n\n  const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;\n  const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;\n\n  // Load the query to registers.\n  // Each thread in a thread group has a different part of the query.\n  // For example, if the thread group size is 4, then the first thread in\n  // the group has 0, 4, 8, ... th vectors of the query, and the second thread\n  // has 1, 5, 9, ... th vectors of the query, and so on. NOTE(woosuk): Because\n  // q is split from a qkv tensor, it may not be contiguous.\n  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;\n  __shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];\n#pragma unroll\n  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;\n       i += NUM_THREAD_GROUPS) {\n    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;\n    q_vecs[thread_group_offset][i] =\n        *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);\n  }\n  __syncthreads();  // TODO(naed90): possible speedup if this is replaced with a\n                    // memory wall right before we use q_vecs\n\n  // Memory planning.\n  extern __shared__ char shared_mem[];\n  // NOTE(woosuk): We use FP32 for the softmax logits for better accuracy.\n  float* logits = reinterpret_cast<float*>(shared_mem);\n  // Workspace for reduction.\n  __shared__ float red_smem[2 * NUM_WARPS];\n\n  // x == THREAD_GROUP_SIZE * VEC_SIZE\n  // Each thread group fetches x elements from the key at a time.\n  constexpr int x = 16 / sizeof(cache_t);\n  float qk_max = -FLT_MAX;\n\n  // Iterate over the key blocks.\n  // Each warp fetches a block of keys for each iteration.\n  // Each thread group in a warp fetches a key from the block, and computes\n  // dot product with the query.\n  const int* block_table = block_tables + seq_idx * max_num_blocks_per_seq;\n\n  // blocksparse specific vars\n  int bs_block_offset;\n  int q_bs_block_id;\n  if constexpr (IS_BLOCK_SPARSE) {\n    // const int num_blocksparse_blocks = DIVIDE_ROUND_UP(seq_len,\n    // blocksparse_block_size);\n    q_bs_block_id = (seq_len - 1) / blocksparse_block_size;\n    if (blocksparse_head_sliding_step >= 0)\n      // sliding on q heads\n      bs_block_offset =\n          (tp_rank * num_heads + head_idx) * blocksparse_head_sliding_step + 1;\n    else\n      // sliding on kv heads\n      bs_block_offset = (tp_rank * num_kv_heads + kv_head_idx) *\n                            (-blocksparse_head_sliding_step) +\n                        1;\n  }\n\n  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;\n       block_idx += NUM_WARPS) {\n    // NOTE(woosuk): The block number is stored in int32. However, we cast it to\n    // int64 because int32 can lead to overflow when this variable is multiplied\n    // by large numbers (e.g., kv_block_stride).\n    // For blocksparse attention: skip computation on blocks that are not\n    // attended\n    if constexpr (IS_BLOCK_SPARSE) {\n      const int k_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;\n      const bool is_remote =\n          ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);\n      const bool is_local =\n          (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);\n      if (!is_remote && !is_local) {\n        for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {\n          const int physical_block_offset =\n              (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;\n          const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;\n\n          if (thread_group_offset == 0) {\n            // NOTE(linxihui): assign very large number to skipped tokens to\n            // avoid contribution to the sumexp softmax normalizer. This will\n            // not be used at computing sum(softmax*v) as the blocks will be\n            // skipped.\n            logits[token_idx - start_token_idx] = -FLT_MAX;\n          }\n        }\n        continue;\n      }\n    }\n    const int64_t physical_block_number =\n        static_cast<int64_t>(block_table[block_idx]);\n\n    // Load a key to registers.\n    // Each thread in a thread group has a different part of the key.\n    // For example, if the thread group size is 4, then the first thread in\n    // the group has 0, 4, 8, ... th vectors of the key, and the second thread\n    // has 1, 5, 9, ... th vectors of the key, and so on.\n    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {\n      const int physical_block_offset =\n          (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;\n      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;\n      K_vec k_vecs[NUM_VECS_PER_THREAD];\n\n#pragma unroll\n      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {\n        const cache_t* k_ptr =\n            k_cache + physical_block_number * kv_block_stride +\n            kv_head_idx * kv_head_stride + physical_block_offset * x;\n        const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;\n        const int offset1 = (vec_idx * VEC_SIZE) / x;\n        const int offset2 = (vec_idx * VEC_SIZE) % x;\n\n        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {\n          k_vecs[j] = *reinterpret_cast<const K_vec*>(\n              k_ptr + offset1 * BLOCK_SIZE * x + offset2);\n        } else {\n          // Vector conversion from Quant_vec to K_vec.\n          Quant_vec k_vec_quant = *reinterpret_cast<const Quant_vec*>(\n              k_ptr + offset1 * BLOCK_SIZE * x + offset2);\n          k_vecs[j] = fp8::scaled_convert<K_vec, Quant_vec, KV_DTYPE>(\n              k_vec_quant, *k_scale);\n        }\n      }\n\n      // Compute dot product.\n      // This includes a reduction across the threads in the same thread group.\n      float qk = scale * Qk_dot<scalar_t, THREAD_GROUP_SIZE>::dot(\n                             q_vecs[thread_group_offset], k_vecs);\n      // Add the ALiBi bias if slopes are given.\n      qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;\n\n      if (thread_group_offset == 0) {\n        // Store the partial reductions to shared memory.\n        // NOTE(woosuk): It is required to zero out the masked logits.\n        const bool mask = token_idx >= seq_len;\n        logits[token_idx - start_token_idx] = mask ? 0.f : qk;\n        // Update the max value.\n        qk_max = mask ? qk_max : fmaxf(qk_max, qk);\n      }\n    }\n  }\n\n  // Perform reduction across the threads in the same warp to get the\n  // max qk value for each \"warp\" (not across the thread block yet).\n  // The 0-th thread of each thread group already has its max qk value.\n#pragma unroll\n  for (int mask = WARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {\n    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));\n  }\n  if (lane == 0) {\n    red_smem[warp_idx] = qk_max;\n  }\n  __syncthreads();\n\n  // TODO(woosuk): Refactor this part.\n  // Get the max qk value for the sequence.\n  qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;\n#pragma unroll\n  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {\n    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));\n  }\n  // Broadcast the max qk value to all threads.\n  qk_max = VLLM_SHFL_SYNC(qk_max, 0);\n\n  // Get the sum of the exp values.\n  float exp_sum = 0.f;\n  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {\n    float val = __expf(logits[i] - qk_max);\n    logits[i] = val;\n    exp_sum += val;\n  }\n  exp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);\n\n  // Compute softmax.\n  const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f);\n  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {\n    logits[i] *= inv_sum;\n  }\n  __syncthreads();\n\n  // If partitioning is enabled, store the max logit and exp_sum.\n  if (USE_PARTITIONING && thread_idx == 0) {\n    float* max_logits_ptr = max_logits +\n                            seq_idx * num_heads * max_num_partitions +\n                            head_idx * max_num_partitions + partition_idx;\n    *max_logits_ptr = qk_max;\n    float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions +\n                          head_idx * max_num_partitions + partition_idx;\n    *exp_sums_ptr = exp_sum;\n  }\n\n  // Each thread will fetch 16 bytes from the value cache at a time.\n  constexpr int V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE);\n  using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;\n  using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;\n  using V_quant_vec = typename Vec<cache_t, V_VEC_SIZE>::Type;\n  using Float_L_vec = typename FloatVec<L_vec>::Type;\n\n  constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;\n  constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;\n  constexpr int NUM_ROWS_PER_THREAD =\n      DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER);\n\n  // NOTE(woosuk): We use FP32 for the accumulator for better accuracy.\n  float accs[NUM_ROWS_PER_THREAD];\n#pragma unroll\n  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n    accs[i] = 0.f;\n  }\n\n  scalar_t zero_value;\n  zero(zero_value);\n  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;\n       block_idx += NUM_WARPS) {\n    // NOTE(woosuk): The block number is stored in int32. However, we cast it to\n    // int64 because int32 can lead to overflow when this variable is multiplied\n    // by large numbers (e.g., kv_block_stride).\n    // For blocksparse attention: skip computation on blocks that are not\n    // attended\n    if constexpr (IS_BLOCK_SPARSE) {\n      int v_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;\n      if (!((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) &&\n          !((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {\n        continue;\n      }\n    }\n    const int64_t physical_block_number =\n        static_cast<int64_t>(block_table[block_idx]);\n    const int physical_block_offset = (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;\n    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;\n    L_vec logits_vec;\n    from_float(logits_vec, *reinterpret_cast<Float_L_vec*>(logits + token_idx -\n                                                           start_token_idx));\n\n    const cache_t* v_ptr = v_cache + physical_block_number * kv_block_stride +\n                           kv_head_idx * kv_head_stride;\n#pragma unroll\n    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;\n      if (row_idx < HEAD_SIZE) {\n        const int offset = row_idx * BLOCK_SIZE + physical_block_offset;\n        V_vec v_vec;\n\n        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {\n          v_vec = *reinterpret_cast<const V_vec*>(v_ptr + offset);\n        } else {\n          V_quant_vec v_quant_vec =\n              *reinterpret_cast<const V_quant_vec*>(v_ptr + offset);\n          // Vector conversion from V_quant_vec to V_vec.\n          v_vec = fp8::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec,\n                                                                    *v_scale);\n        }\n        if (block_idx == num_seq_blocks - 1) {\n          // NOTE(woosuk): When v_vec contains the tokens that are out of the\n          // context, we should explicitly zero out the values since they may\n          // contain NaNs. See\n          // https://github.com/vllm-project/vllm/issues/641#issuecomment-1682544472\n          scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vec);\n#pragma unroll\n          for (int j = 0; j < V_VEC_SIZE; j++) {\n            v_vec_ptr[j] = token_idx + j < seq_len ? v_vec_ptr[j] : zero_value;\n          }\n        }\n        accs[i] += dot(logits_vec, v_vec);\n      }\n    }\n  }\n\n  // Perform reduction within each warp.\n#pragma unroll\n  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n    float acc = accs[i];\n#pragma unroll\n    for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {\n      acc += VLLM_SHFL_XOR_SYNC(acc, mask);\n    }\n    accs[i] = acc;\n  }\n\n  // NOTE(woosuk): A barrier is required because the shared memory space for\n  // logits is reused for the output.\n  __syncthreads();\n\n  // Perform reduction across warps.\n  float* out_smem = reinterpret_cast<float*>(shared_mem);\n#pragma unroll\n  for (int i = NUM_WARPS; i > 1; i /= 2) {\n    int mid = i / 2;\n    // Upper warps write to shared memory.\n    if (warp_idx >= mid && warp_idx < i) {\n      float* dst = &out_smem[(warp_idx - mid) * HEAD_SIZE];\n#pragma unroll\n      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;\n        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {\n          dst[row_idx] = accs[i];\n        }\n      }\n    }\n    __syncthreads();\n\n    // Lower warps update the output.\n    if (warp_idx < mid) {\n      const float* src = &out_smem[warp_idx * HEAD_SIZE];\n#pragma unroll\n      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;\n        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {\n          accs[i] += src[row_idx];\n        }\n      }\n    }\n    __syncthreads();\n  }\n\n  // Write the final output.\n  if (warp_idx == 0) {\n    scalar_t* out_ptr =\n        out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +\n        head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE;\n#pragma unroll\n    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;\n      if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {\n        from_float(*(out_ptr + row_idx), accs[i]);\n      }\n    }\n  }\n}\n\n// Grid: (num_heads, num_seqs, 1).\ntemplate <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,\n          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,\n          bool IS_BLOCK_SPARSE>\n__global__ void paged_attention_v1_kernel(\n    scalar_t* __restrict__ out,           // [num_seqs, num_heads, head_size]\n    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]\n    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,\n                                          // head_size/x, block_size, x]\n    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,\n                                          // head_size, block_size]\n    const int num_kv_heads,               // [num_heads]\n    const float scale,\n    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]\n    const int* __restrict__ seq_lens,      // [num_seqs]\n    const int max_num_blocks_per_seq,\n    const float* __restrict__ alibi_slopes,  // [num_heads]\n    const int q_stride, const int kv_block_stride, const int kv_head_stride,\n    const float* k_scale, const float* v_scale, const int tp_rank,\n    const int blocksparse_local_blocks, const int blocksparse_vert_stride,\n    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {\n  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,\n                         KV_DTYPE, IS_BLOCK_SPARSE>(\n      /* exp_sums */ nullptr, /* max_logits */ nullptr, out, q, k_cache,\n      v_cache, num_kv_heads, scale, block_tables, seq_lens,\n      max_num_blocks_per_seq, alibi_slopes, q_stride, kv_block_stride,\n      kv_head_stride, k_scale, v_scale, tp_rank, blocksparse_local_blocks,\n      blocksparse_vert_stride, blocksparse_block_size,\n      blocksparse_head_sliding_step);\n}\n\n// Grid: (num_heads, num_seqs, max_num_partitions).\ntemplate <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,\n          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,\n          bool IS_BLOCK_SPARSE,\n          int PARTITION_SIZE>\n__global__ void paged_attention_v2_kernel(\n    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]\n    float* __restrict__ max_logits,       // [num_seqs, num_heads,\n                                          // max_num_partitions]\n    scalar_t* __restrict__ tmp_out,       // [num_seqs, num_heads,\n                                          // max_num_partitions, head_size]\n    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]\n    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,\n                                          // head_size/x, block_size, x]\n    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,\n                                          // head_size, block_size]\n    const int num_kv_heads,               // [num_heads]\n    const float scale,\n    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]\n    const int* __restrict__ seq_lens,      // [num_seqs]\n    const int max_num_blocks_per_seq,\n    const float* __restrict__ alibi_slopes,  // [num_heads]\n    const int q_stride, const int kv_block_stride, const int kv_head_stride,\n    const float* k_scale, const float* v_scale, const int tp_rank,\n    const int blocksparse_local_blocks, const int blocksparse_vert_stride,\n    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {\n  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,\n                         KV_DTYPE, IS_BLOCK_SPARSE, PARTITION_SIZE>(\n      exp_sums, max_logits, tmp_out, q, k_cache, v_cache, num_kv_heads, scale,\n      block_tables, seq_lens, max_num_blocks_per_seq, alibi_slopes, q_stride,\n      kv_block_stride, kv_head_stride, k_scale, v_scale, tp_rank,\n      blocksparse_local_blocks, blocksparse_vert_stride, blocksparse_block_size,\n      blocksparse_head_sliding_step);\n}\n\n// Grid: (num_heads, num_seqs).\ntemplate <typename scalar_t, int HEAD_SIZE, int NUM_THREADS,\n          int PARTITION_SIZE>\n__global__ void paged_attention_v2_reduce_kernel(\n    scalar_t* __restrict__ out,            // [num_seqs, num_heads, head_size]\n    const float* __restrict__ exp_sums,    // [num_seqs, num_heads,\n                                           // max_num_partitions]\n    const float* __restrict__ max_logits,  // [num_seqs, num_heads,\n                                           // max_num_partitions]\n    const scalar_t* __restrict__ tmp_out,  // [num_seqs, num_heads,\n                                           // max_num_partitions, head_size]\n    const int* __restrict__ seq_lens,      // [num_seqs]\n    const int max_num_partitions) {\n  const int num_heads = gridDim.x;\n  const int head_idx = blockIdx.x;\n  const int seq_idx = blockIdx.y;\n  const int seq_len = seq_lens[seq_idx];\n  const int num_partitions = DIVIDE_ROUND_UP(seq_len, PARTITION_SIZE);\n  if (num_partitions == 1) {\n    // No need to reduce. Only copy tmp_out to out.\n    scalar_t* out_ptr =\n        out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;\n    const scalar_t* tmp_out_ptr =\n        tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +\n        head_idx * max_num_partitions * HEAD_SIZE;\n    for (int i = threadIdx.x; i < HEAD_SIZE; i += blockDim.x) {\n      out_ptr[i] = tmp_out_ptr[i];\n    }\n    // Terminate the thread block.\n    return;\n  }\n\n  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;\n  const int warp_idx = threadIdx.x / WARP_SIZE;\n  const int lane = threadIdx.x % WARP_SIZE;\n\n  // Size: 2 * num_partitions.\n  extern __shared__ char shared_mem[];\n  // Workspace for reduction.\n  __shared__ float red_smem[2 * NUM_WARPS];\n\n  // Load max logits to shared memory.\n  float* shared_max_logits = reinterpret_cast<float*>(shared_mem);\n  const float* max_logits_ptr = max_logits +\n                                seq_idx * num_heads * max_num_partitions +\n                                head_idx * max_num_partitions;\n  float max_logit = -FLT_MAX;\n  for (int i = threadIdx.x; i < num_partitions; i += blockDim.x) {\n    const float l = max_logits_ptr[i];\n    shared_max_logits[i] = l;\n    max_logit = fmaxf(max_logit, l);\n  }\n  __syncthreads();\n\n  // Get the global max logit.\n  // Reduce within the warp.\n#pragma unroll\n  for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {\n    max_logit = fmaxf(max_logit, VLLM_SHFL_XOR_SYNC(max_logit, mask));\n  }\n  if (lane == 0) {\n    red_smem[warp_idx] = max_logit;\n  }\n  __syncthreads();\n  // Reduce across warps.\n  max_logit = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;\n#pragma unroll\n  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {\n    max_logit = fmaxf(max_logit, VLLM_SHFL_XOR_SYNC(max_logit, mask));\n  }\n  // Broadcast the max value to all threads.\n  max_logit = VLLM_SHFL_SYNC(max_logit, 0);\n\n  // Load rescaled exp sums to shared memory.\n  float* shared_exp_sums =\n      reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions);\n  const float* exp_sums_ptr = exp_sums +\n                              seq_idx * num_heads * max_num_partitions +\n                              head_idx * max_num_partitions;\n  float global_exp_sum = 0.0f;\n  for (int i = threadIdx.x; i < num_partitions; i += blockDim.x) {\n    float l = shared_max_logits[i];\n    float rescaled_exp_sum = exp_sums_ptr[i] * expf(l - max_logit);\n    global_exp_sum += rescaled_exp_sum;\n    shared_exp_sums[i] = rescaled_exp_sum;\n  }\n  __syncthreads();\n  global_exp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], global_exp_sum);\n  const float inv_global_exp_sum = __fdividef(1.0f, global_exp_sum + 1e-6f);\n\n  // Aggregate tmp_out to out.\n  const scalar_t* tmp_out_ptr =\n      tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +\n      head_idx * max_num_partitions * HEAD_SIZE;\n  scalar_t* out_ptr =\n      out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;\n#pragma unroll\n  for (int i = threadIdx.x; i < HEAD_SIZE; i += NUM_THREADS) {\n    float acc = 0.0f;\n    for (int j = 0; j < num_partitions; ++j) {\n      acc += to_float(tmp_out_ptr[j * HEAD_SIZE + i]) * shared_exp_sums[j] *\n             inv_global_exp_sum;\n    }\n    from_float(out_ptr[i], acc);\n  }\n}\n\n}  // namespace vllm\n\n#undef MAX\n#undef MIN\n#undef DIVIDE_ROUND_UP\n// End include attention_kernels.cuh\n\n#define MAX(a, b) ((a) > (b) ? (a) : (b))\n#define MIN(a, b) ((a) < (b) ? (a) : (b))\n#define DIVIDE_ROUND_UP(a, b) (((a) + (b) - 1) / (b))\n\n#define LAUNCH_PAGED_ATTENTION_V2(HEAD_SIZE)                                   \\\n  vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \\\n                                  NUM_THREADS, KV_DTYPE, IS_BLOCK_SPARSE,      \\\n                                  PARTITION_SIZE>                              \\\n      <<<grid, block, shared_mem_size, stream>>>(                              \\\n          exp_sums_ptr, max_logits_ptr, tmp_out_ptr, query_ptr, key_cache_ptr, \\\n          value_cache_ptr, num_kv_heads, scale, block_tables_ptr,              \\\n          seq_lens_ptr, max_num_blocks_per_seq, alibi_slopes_ptr, q_stride,    \\\n          kv_block_stride, kv_head_stride, k_scale_ptr, v_scale_ptr, tp_rank,  \\\n          blocksparse_local_blocks, blocksparse_vert_stride,                   \\\n          blocksparse_block_size, blocksparse_head_sliding_step);              \\\n  vllm::paged_attention_v2_reduce_kernel<T, HEAD_SIZE, NUM_THREADS,            \\\n                                         PARTITION_SIZE>                       \\\n      <<<reduce_grid, block, reduce_shared_mem_size, stream>>>(                \\\n          out_ptr, exp_sums_ptr, max_logits_ptr, tmp_out_ptr, seq_lens_ptr,    \\\n          max_num_partitions);\n\ntemplate <typename T, typename CACHE_T, int BLOCK_SIZE,\n          vllm::Fp8KVCacheDataType KV_DTYPE, bool IS_BLOCK_SPARSE,\n          int NUM_THREADS = 128, int PARTITION_SIZE = 512>\nvoid paged_attention_v2_launcher(\n    torch::Tensor& out, torch::Tensor& exp_sums, torch::Tensor& max_logits,\n    torch::Tensor& tmp_out, torch::Tensor& query, torch::Tensor& key_cache,\n    torch::Tensor& value_cache, int num_kv_heads, float scale,\n    torch::Tensor& block_tables, torch::Tensor& seq_lens, int max_seq_len,\n    const std::optional<torch::Tensor>& alibi_slopes, torch::Tensor& k_scale,\n    torch::Tensor& v_scale, const int tp_rank,\n    const int blocksparse_local_blocks, const int blocksparse_vert_stride,\n    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {\n  int num_seqs = query.size(0);\n  int num_heads = query.size(1);\n  int head_size = query.size(2);\n  int max_num_blocks_per_seq = block_tables.size(1);\n  int q_stride = query.stride(0);\n  int kv_block_stride = key_cache.stride(0);\n  int kv_head_stride = key_cache.stride(1);\n\n  // NOTE: alibi_slopes is optional.\n  const float* alibi_slopes_ptr =\n      alibi_slopes\n          ? reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())\n          : nullptr;\n\n  T* out_ptr = reinterpret_cast<T*>(out.data_ptr());\n  float* exp_sums_ptr = reinterpret_cast<float*>(exp_sums.data_ptr());\n  float* max_logits_ptr = reinterpret_cast<float*>(max_logits.data_ptr());\n  T* tmp_out_ptr = reinterpret_cast<T*>(tmp_out.data_ptr());\n  T* query_ptr = reinterpret_cast<T*>(query.data_ptr());\n  CACHE_T* key_cache_ptr = reinterpret_cast<CACHE_T*>(key_cache.data_ptr());\n  CACHE_T* value_cache_ptr = reinterpret_cast<CACHE_T*>(value_cache.data_ptr());\n  int* block_tables_ptr = block_tables.data_ptr<int>();\n  int* seq_lens_ptr = seq_lens.data_ptr<int>();\n  const float* k_scale_ptr = reinterpret_cast<const float*>(k_scale.data_ptr());\n  const float* v_scale_ptr = reinterpret_cast<const float*>(v_scale.data_ptr());\n\n  const int NUM_WARPS = NUM_THREADS / WARP_SIZE;\n  int max_num_partitions = DIVIDE_ROUND_UP(max_seq_len, PARTITION_SIZE);\n  int logits_size = PARTITION_SIZE * sizeof(float);\n  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);\n\n  // For paged attention v2 kernel.\n  dim3 grid(num_heads, num_seqs, max_num_partitions);\n  int shared_mem_size = std::max(logits_size, outputs_size);\n  // For paged attention v2 reduce kernel.\n  dim3 reduce_grid(num_heads, num_seqs);\n  int reduce_shared_mem_size = 2 * max_num_partitions * sizeof(float);\n\n  dim3 block(NUM_THREADS);\n  const at::cuda::OptionalCUDAGuard device_guard(device_of(query));\n  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n  switch (head_size) {\n    // NOTE(woosuk): To reduce the compilation time, we only compile for the\n    // head sizes that we use in the model. However, we can easily extend this\n    // to support any head size which is a multiple of 16.\n    case 32:\n      LAUNCH_PAGED_ATTENTION_V2(32);\n      break;\n    case 64:\n      LAUNCH_PAGED_ATTENTION_V2(64);\n      break;\n    case 80:\n      LAUNCH_PAGED_ATTENTION_V2(80);\n      break;\n    case 96:\n      LAUNCH_PAGED_ATTENTION_V2(96);\n      break;\n    case 112:\n      LAUNCH_PAGED_ATTENTION_V2(112);\n      break;\n    case 120:\n      LAUNCH_PAGED_ATTENTION_V2(120);\n      break;\n    case 128:\n      LAUNCH_PAGED_ATTENTION_V2(128);\n      break;\n    case 192:\n      LAUNCH_PAGED_ATTENTION_V2(192);\n      break;\n    case 256:\n      LAUNCH_PAGED_ATTENTION_V2(256);\n      break;\n    default:\n      TORCH_CHECK(false, \"Unsupported head size: \", head_size);\n      break;\n  }\n}\n\n#define CALL_V2_LAUNCHER(T, CACHE_T, BLOCK_SIZE, KV_DTYPE, IS_BLOCK_SPARSE)   \\\n  paged_attention_v2_launcher<T, CACHE_T, BLOCK_SIZE, KV_DTYPE,               \\\n                              IS_BLOCK_SPARSE>(                               \\\n      out, exp_sums, max_logits, tmp_out, query, key_cache, value_cache,      \\\n      num_kv_heads, scale, block_tables, seq_lens, max_seq_len, alibi_slopes, \\\n      k_scale, v_scale, tp_rank, blocksparse_local_blocks,                    \\\n      blocksparse_vert_stride, blocksparse_block_size,                        \\\n      blocksparse_head_sliding_step);\n\n#define CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, BLOCK_SIZE, IS_FP8_KV_CACHE) \\\n  if (is_block_sparse) {                                                   \\\n    CALL_V2_LAUNCHER(T, CACHE_T, BLOCK_SIZE, IS_FP8_KV_CACHE, true);       \\\n  } else {                                                                 \\\n    CALL_V2_LAUNCHER(T, CACHE_T, BLOCK_SIZE, IS_FP8_KV_CACHE, false);      \\\n  }\n\n// NOTE(woosuk): To reduce the compilation time, we omitted block sizes\n// 1, 2, 4, 64, 128, 256.\n#define CALL_V2_LAUNCHER_BLOCK_SIZE(T, CACHE_T, KV_DTYPE)         \\\n  switch (block_size) {                                           \\\n    case 8:                                                       \\\n      CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\\n      break;                                                      \\\n    case 16:                                                      \\\n      CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\\n      break;                                                      \\\n    case 32:                                                      \\\n      CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\\n      break;                                                      \\\n    default:                                                      \\\n      TORCH_CHECK(false, \"Unsupported block size: \", block_size); \\\n      break;                                                      \\\n  }\n\nvoid paged_attention_v2(\n    torch::Tensor& out,         // [num_seqs, num_heads, head_size]\n    torch::Tensor& exp_sums,    // [num_seqs, num_heads, max_num_partitions]\n    torch::Tensor& max_logits,  // [num_seqs, num_heads, max_num_partitions]\n    torch::Tensor&\n        tmp_out,  // [num_seqs, num_heads, max_num_partitions, head_size]\n    torch::Tensor& query,  // [num_seqs, num_heads, head_size]\n    torch::Tensor&\n        key_cache,  // [num_blocks, num_heads, head_size/x, block_size, x]\n    torch::Tensor&\n        value_cache,       // [num_blocks, num_heads, head_size, block_size]\n    int64_t num_kv_heads,  // [num_heads]\n    double scale,\n    torch::Tensor& block_tables,  // [num_seqs, max_num_blocks_per_seq]\n    torch::Tensor& seq_lens,      // [num_seqs]\n    int64_t block_size, int64_t max_seq_len,\n    const std::optional<torch::Tensor>& alibi_slopes,\n    const std::string& kv_cache_dtype, torch::Tensor& k_scale,\n    torch::Tensor& v_scale, const int64_t tp_rank,\n    const int64_t blocksparse_local_blocks,\n    const int64_t blocksparse_vert_stride, const int64_t blocksparse_block_size,\n    const int64_t blocksparse_head_sliding_step) {\n  const bool is_block_sparse = (blocksparse_vert_stride > 1);\n  DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,\n                             CALL_V2_LAUNCHER_BLOCK_SIZE)\n}\n\n#undef MAX\n#undef MIN\n#undef DIVIDE_ROUND_UP\n"
  }
}