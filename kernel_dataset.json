[
  {
    "type": "cuda",
    "path": "csrc/cuda_utils_kernels.cu",
    "source": [
      "#include \"cuda_utils.h\"",
      "#ifdef USE_ROCM",
      "  #include <hip/hip_runtime.h>",
      "  #include <hip/hip_runtime_api.h>",
      "#endif",
      "",
      "int64_t get_device_attribute(int64_t attribute, int64_t device_id) {",
      "  // Return the cached value on subsequent calls",
      "  static int value = [=]() {",
      "    int device = static_cast<int>(device_id);",
      "    if (device < 0) {",
      "      CUDA_CHECK(cudaGetDevice(&device));",
      "    }",
      "    int value;",
      "    CUDA_CHECK(cudaDeviceGetAttribute(",
      "        &value, static_cast<cudaDeviceAttr>(attribute), device));",
      "    return static_cast<int>(value);",
      "  }();",
      "",
      "  return value;",
      "}",
      "",
      "int64_t get_max_shared_memory_per_block_device_attribute(int64_t device_id) {",
      "  int64_t attribute;",
      "  // https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html",
      "  // cudaDevAttrMaxSharedMemoryPerBlockOptin = 97 if not is_hip() else 74",
      "",
      "#ifdef USE_ROCM",
      "  attribute = hipDeviceAttributeMaxSharedMemoryPerBlock;",
      "#else",
      "  attribute = cudaDevAttrMaxSharedMemoryPerBlockOptin;",
      "#endif",
      "",
      "  return get_device_attribute(attribute, device_id);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/custom_all_reduce.cuh",
    "source": [
      "#pragma once",
      "",
      "#include <cuda.h>",
      "#include <cuda_bf16.h>",
      "#include <cuda_fp16.h>",
      "#include <cuda_runtime.h>",
      "",
      "#if defined(USE_ROCM)",
      "typedef __hip_bfloat16 nv_bfloat16;",
      "#endif",
      "",
      "#include <iostream>",
      "#include <array>",
      "#include <limits>",
      "#include <map>",
      "#include <unordered_map>",
      "#include <vector>",
      "",
      "namespace vllm {",
      "#define CUDACHECK(cmd)                                              \\",
      "  do {                                                              \\",
      "    cudaError_t e = cmd;                                            \\",
      "    if (e != cudaSuccess) {                                         \\",
      "      printf(\"Failed: Cuda error %s:%d '%s'\\n\", __FILE__, __LINE__, \\",
      "             cudaGetErrorString(e));                                \\",
      "      exit(EXIT_FAILURE);                                           \\",
      "    }                                                               \\",
      "  } while (0)",
      "",
      "// Maximal number of blocks in allreduce kernel.",
      "constexpr int kMaxBlocks = 36;",
      "",
      "// Default number of blocks in allreduce kernel.",
      "#ifndef USE_ROCM",
      "const int defaultBlockLimit = 36;",
      "CUpointer_attribute rangeStartAddrAttr = CU_POINTER_ATTRIBUTE_RANGE_START_ADDR;",
      "#else",
      "const int defaultBlockLimit = 16;",
      "hipPointer_attribute rangeStartAddrAttr =",
      "    HIP_POINTER_ATTRIBUTE_RANGE_START_ADDR;",
      "#endif",
      "",
      "// Counter may overflow, but it's fine since unsigned int overflow is",
      "// well-defined behavior.",
      "using FlagType = uint32_t;",
      "",
      "// Two sets of peer counters are needed for two syncs: starting and ending an",
      "// operation. The reason is that it's possible for peer GPU block to arrive at",
      "// the second sync point while the current GPU block haven't passed the first",
      "// sync point. Thus, peer GPU may write counter+1 while current GPU is busy",
      "// waiting for counter. We use alternating counter array to avoid this",
      "// possibility.",
      "struct Signal {",
      "  alignas(128) FlagType start[kMaxBlocks][8];",
      "  alignas(128) FlagType end[kMaxBlocks][8];",
      "  alignas(128) FlagType _flag[kMaxBlocks];  // incremental flags for each rank",
      "};",
      "",
      "struct __align__(16) RankData {",
      "  const void* ptrs[8];",
      "};",
      "",
      "struct __align__(16) RankSignals {",
      "  Signal* signals[8];",
      "};",
      "",
      "// like std::array, but aligned",
      "template <typename T, int sz>",
      "struct __align__(alignof(T) * sz) array_t {",
      "  T data[sz];",
      "  using type = T;",
      "  static constexpr int size = sz;",
      "};",
      "",
      "// use packed type to maximize memory efficiency",
      "// goal: generate ld.128 and st.128 instructions",
      "template <typename T>",
      "struct packed_t {",
      "  // the (P)acked type for load/store",
      "  using P = array_t<T, 16 / sizeof(T)>;",
      "  // the (A)ccumulator type for reduction",
      "  using A = array_t<float, 16 / sizeof(T)>;",
      "};",
      "",
      "#define DINLINE __device__ __forceinline__",
      "",
      "// scalar cast functions",
      "DINLINE float upcast_s(half val) { return __half2float(val); }",
      "",
      "template <typename T>",
      "DINLINE T downcast_s(float val);",
      "template <>",
      "DINLINE half downcast_s(float val) {",
      "  return __float2half(val);",
      "}",
      "",
      "// scalar add functions",
      "// for some reason when compiling with Pytorch, the + operator for half and",
      "// bfloat is disabled so we call the intrinsics directly",
      "DINLINE half& assign_add(half& a, half b) {",
      "  a = __hadd(a, b);",
      "  return a;",
      "}",
      "DINLINE float& assign_add(float& a, float b) { return a += b; }",
      "",
      "#if (__CUDA_ARCH__ >= 800 || !defined(__CUDA_ARCH__))",
      "DINLINE float upcast_s(nv_bfloat16 val) { return __bfloat162float(val); }",
      "template <>",
      "DINLINE nv_bfloat16 downcast_s(float val) {",
      "  return __float2bfloat16(val);",
      "}",
      "DINLINE nv_bfloat16& assign_add(nv_bfloat16& a, nv_bfloat16 b) {",
      "  a = __hadd(a, b);",
      "  return a;",
      "}",
      "#endif",
      "",
      "template <typename T, int N>",
      "DINLINE array_t<T, N>& packed_assign_add(array_t<T, N>& a, array_t<T, N> b) {",
      "#pragma unroll",
      "  for (int i = 0; i < N; i++) {",
      "    assign_add(a.data[i], b.data[i]);",
      "  }",
      "  return a;",
      "}",
      "",
      "template <typename T, int N>",
      "DINLINE array_t<float, N> upcast(array_t<T, N> val) {",
      "  if constexpr (std::is_same<T, float>::value) {",
      "    return val;",
      "  } else {",
      "    array_t<float, N> out;",
      "#pragma unroll",
      "    for (int i = 0; i < N; i++) {",
      "      out.data[i] = upcast_s(val.data[i]);",
      "    }",
      "    return out;",
      "  }",
      "}",
      "",
      "template <typename O>",
      "DINLINE O downcast(array_t<float, O::size> val) {",
      "  if constexpr (std::is_same<typename O::type, float>::value) {",
      "    return val;",
      "  } else {",
      "    O out;",
      "#pragma unroll",
      "    for (int i = 0; i < O::size; i++) {",
      "      out.data[i] = downcast_s<typename O::type>(val.data[i]);",
      "    }",
      "    return out;",
      "  }",
      "}",
      "",
      "#if !defined(USE_ROCM)",
      "",
      "static DINLINE void st_flag_release(FlagType* flag_addr, FlagType flag) {",
      "  #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 700",
      "  asm volatile(\"st.release.sys.global.u32 [%1], %0;\" ::\"r\"(flag),",
      "               \"l\"(flag_addr));",
      "  #else",
      "  asm volatile(\"membar.sys; st.volatile.global.u32 [%1], %0;\" ::\"r\"(flag),",
      "               \"l\"(flag_addr));",
      "  #endif",
      "}",
      "",
      "static DINLINE FlagType ld_flag_acquire(FlagType* flag_addr) {",
      "  FlagType flag;",
      "  #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 700",
      "  asm volatile(\"ld.acquire.sys.global.u32 %0, [%1];\"",
      "               : \"=r\"(flag)",
      "               : \"l\"(flag_addr));",
      "  #else",
      "  asm volatile(\"ld.volatile.global.u32 %0, [%1]; membar.gl;\"",
      "               : \"=r\"(flag)",
      "               : \"l\"(flag_addr));",
      "  #endif",
      "  return flag;",
      "}",
      "",
      "static DINLINE void st_flag_volatile(FlagType* flag_addr, FlagType flag) {",
      "  asm volatile(\"st.volatile.global.u32 [%1], %0;\" ::\"r\"(flag), \"l\"(flag_addr));",
      "}",
      "",
      "static DINLINE FlagType ld_flag_volatile(FlagType* flag_addr) {",
      "  FlagType flag;",
      "  asm volatile(\"ld.volatile.global.u32 %0, [%1];\"",
      "               : \"=r\"(flag)",
      "               : \"l\"(flag_addr));",
      "  return flag;",
      "}",
      "",
      "// This function is meant to be used as the first synchronization in the all",
      "// reduce kernel. Thus, it doesn't need to make any visibility guarantees for",
      "// prior memory accesses. Note: volatile writes will not be reordered against",
      "// other volatile writes.",
      "template <int ngpus>",
      "DINLINE void barrier_at_start(const RankSignals& sg, Signal* self_sg,",
      "                              int rank) {",
      "  uint32_t flag = self_sg->_flag[blockIdx.x] + 1;",
      "  if (threadIdx.x < ngpus) {",
      "    auto peer_counter_ptr = &sg.signals[threadIdx.x]->start[blockIdx.x][rank];",
      "    auto self_counter_ptr = &self_sg->start[blockIdx.x][threadIdx.x];",
      "    // Write the expected counter value to peer and wait for correct value",
      "    // from peer.",
      "    st_flag_volatile(peer_counter_ptr, flag);",
      "    while (ld_flag_volatile(self_counter_ptr) != flag);",
      "  }",
      "  __syncthreads();",
      "  // use one thread to update flag",
      "  if (threadIdx.x == 0) self_sg->_flag[blockIdx.x] = flag;",
      "}",
      "",
      "// This function is meant to be used as the second or the final",
      "// synchronization barrier in the all reduce kernel. If it's the final",
      "// synchronization barrier, we don't need to make any visibility guarantees",
      "// for prior memory accesses.",
      "template <int ngpus, bool final_sync = false>",
      "DINLINE void barrier_at_end(const RankSignals& sg, Signal* self_sg, int rank) {",
      "  __syncthreads();",
      "  uint32_t flag = self_sg->_flag[blockIdx.x] + 1;",
      "  if (threadIdx.x < ngpus) {",
      "    auto peer_counter_ptr = &sg.signals[threadIdx.x]->end[blockIdx.x][rank];",
      "    auto self_counter_ptr = &self_sg->end[blockIdx.x][threadIdx.x];",
      "    // Write the expected counter value to peer and wait for correct value from",
      "    // peer.",
      "    if constexpr (!final_sync) {",
      "      st_flag_release(peer_counter_ptr, flag);",
      "      while (ld_flag_acquire(self_counter_ptr) != flag);",
      "    } else {",
      "      st_flag_volatile(peer_counter_ptr, flag);",
      "      while (ld_flag_volatile(self_counter_ptr) != flag);",
      "    }",
      "  }",
      "  if constexpr (!final_sync) __syncthreads();",
      "",
      "  // use one thread to update flag",
      "  if (threadIdx.x == 0) self_sg->_flag[blockIdx.x] = flag;",
      "}",
      "",
      "#else",
      "",
      "template <int ngpus>",
      "DINLINE void barrier_at_start(const RankSignals& sg, Signal* self_sg,",
      "                              int rank) {",
      "  uint32_t flag = self_sg->_flag[blockIdx.x] + 1;",
      "  if (threadIdx.x < ngpus) {",
      "    // simultaneously write to the corresponding flag of all ranks.",
      "    // Latency = 1 p2p write",
      "    __scoped_atomic_store_n(&sg.signals[threadIdx.x]->start[blockIdx.x][rank],",
      "                            flag, __ATOMIC_RELAXED, __MEMORY_SCOPE_SYSTEM);",
      "    // wait until we got true from all ranks",
      "    while (__scoped_atomic_load_n(&self_sg->start[blockIdx.x][threadIdx.x],",
      "                                  __ATOMIC_RELAXED,",
      "                                  __MEMORY_SCOPE_DEVICE) < flag);",
      "  }",
      "  __syncthreads();",
      "  // use one thread to update flag",
      "  if (threadIdx.x == 0) self_sg->_flag[blockIdx.x] = flag;",
      "}",
      "",
      "template <int ngpus, bool final_sync = false>",
      "DINLINE void barrier_at_end(const RankSignals& sg, Signal* self_sg, int rank) {",
      "  __syncthreads();",
      "  uint32_t flag = self_sg->_flag[blockIdx.x] + 1;",
      "  if (threadIdx.x < ngpus) {",
      "    // simultaneously write to the corresponding flag of all ranks.",
      "    // Latency = 1 p2p write",
      "    __scoped_atomic_store_n(&sg.signals[threadIdx.x]->end[blockIdx.x][rank],",
      "                            flag,",
      "                            final_sync ? __ATOMIC_RELAXED : __ATOMIC_RELEASE,",
      "                            __MEMORY_SCOPE_SYSTEM);",
      "    // wait until we got true from all ranks",
      "    while (",
      "        __scoped_atomic_load_n(&self_sg->end[blockIdx.x][threadIdx.x],",
      "                               final_sync ? __ATOMIC_RELAXED : __ATOMIC_ACQUIRE,",
      "                               __MEMORY_SCOPE_DEVICE) < flag);",
      "  }",
      "  if constexpr (!final_sync) __syncthreads();",
      "  // use one thread to update flag",
      "  if (threadIdx.x == 0) self_sg->_flag[blockIdx.x] = flag;",
      "}",
      "",
      "#endif",
      "",
      "template <typename P, int ngpus, typename A>",
      "DINLINE P packed_reduce(const P* ptrs[], int idx) {",
      "  A tmp = upcast(ptrs[0][idx]);",
      "#pragma unroll",
      "  for (int i = 1; i < ngpus; i++) {",
      "    packed_assign_add(tmp, upcast(ptrs[i][idx]));",
      "  }",
      "  return downcast<P>(tmp);",
      "}",
      "",
      "template <typename T, int ngpus>",
      "__global__ void __launch_bounds__(512, 1)",
      "    cross_device_reduce_1stage(RankData* _dp, RankSignals sg, Signal* self_sg,",
      "                               T* __restrict__ result, int rank, int size) {",
      "  using P = typename packed_t<T>::P;",
      "  using A = typename packed_t<T>::A;",
      "  // note: we don't reorder the address so the accumulation order is the same",
      "  // for all ranks, ensuring bitwise identical results",
      "  auto dp = *_dp;",
      "  barrier_at_start<ngpus>(sg, self_sg, rank);",
      "  // do the actual reduction",
      "  for (int idx = blockIdx.x * blockDim.x + threadIdx.x; idx < size;",
      "       idx += gridDim.x * blockDim.x) {",
      "    ((P*)result)[idx] = packed_reduce<P, ngpus, A>((const P**)&dp.ptrs[0], idx);",
      "  }",
      "  barrier_at_end<ngpus, true>(sg, self_sg, rank);",
      "}",
      "",
      "template <typename P>",
      "DINLINE P* get_tmp_buf(Signal* sg) {",
      "  return (P*)(((Signal*)sg) + 1);",
      "}",
      "",
      "template <typename T, int ngpus>",
      "__global__ void __launch_bounds__(512, 1)",
      "    cross_device_reduce_2stage(RankData* _dp, RankSignals sg, Signal* self_sg,",
      "                               T* __restrict__ result, int rank, int size) {",
      "  int tid = blockIdx.x * blockDim.x + threadIdx.x;",
      "  int stride = gridDim.x * blockDim.x;",
      "  using P = typename packed_t<T>::P;",
      "  using A = typename packed_t<T>::A;",
      "  int part = size / ngpus;",
      "  int start = rank * part;",
      "  int end = rank == ngpus - 1 ? size : start + part;",
      "  int largest_part = part + size % ngpus;",
      "  const P* ptrs[ngpus];",
      "  P* tmps[ngpus];",
      "#pragma unroll",
      "  for (int i = 0; i < ngpus; i++) {",
      "    int target = (rank + i) % ngpus;",
      "    ptrs[i] = (const P*)_dp->ptrs[target];",
      "    tmps[i] = get_tmp_buf<P>(sg.signals[target]);",
      "  }",
      "  auto tmp_out = tmps[0];",
      "  barrier_at_start<ngpus>(sg, self_sg, rank);",
      "",
      "  // stage 1: reduce scatter",
      "  for (int idx = start + tid; idx < end; idx += stride) {",
      "    tmp_out[idx - start] = packed_reduce<P, ngpus, A>(ptrs, idx);",
      "  }",
      "  barrier_at_end<ngpus>(sg, self_sg, rank);",
      "",
      "  // stage 2: allgather. Note: it's important to match the tid between",
      "  // the two stages, because visibility across devices is only guaranteed",
      "  // between threads that have the same tid. If thread i computes the sum of",
      "  // start + i in the first stage, then thread i also gathers start + i from",
      "  // all ranks.",
      "",
      "  for (int idx = tid; idx < largest_part; idx += stride) {",
      "#pragma unroll",
      "    for (int i = 0; i < ngpus; i++) {",
      "      int gather_from_rank = ((rank + i) % ngpus);",
      "      if (gather_from_rank == ngpus - 1 || idx < part) {",
      "        int dst_idx = gather_from_rank * part + idx;",
      "        ((P*)result)[dst_idx] = tmps[i][idx];",
      "      }",
      "    }",
      "  }",
      "}",
      "",
      "using IPC_KEY = std::array<uint8_t, sizeof(cudaIpcMemHandle_t)>;",
      "static_assert(sizeof(IPC_KEY) == sizeof(cudaIpcMemHandle_t));",
      "static_assert(alignof(IPC_KEY) == alignof(cudaIpcMemHandle_t));",
      "",
      "class CustomAllreduce {",
      " public:",
      "  int rank_;",
      "  int world_size_;",
      "  // Full NVLink or xGMI connection between GPUs.",
      "  bool fully_connected_;",
      "",
      "  RankSignals sg_;",
      "  // Stores a map from a pointer to its peer pointers from all ranks.",
      "  std::unordered_map<void*, RankData*> buffers_;",
      "  Signal* self_sg_;",
      "",
      "  // Stores rank data from all ranks. This is mainly for cuda graph purposes.",
      "  // For cuda graph to work, all kernel arguments must be fixed during graph",
      "  // capture time. However, the peer pointers are not known during graph",
      "  // capture time. Therefore, during capture, we increment the rank data",
      "  // pointer and use that as the argument to the kernel. The kernel arguments",
      "  // are stored in graph_unreg_buffers_. The actual peer pointers will be",
      "  // filled in at the memory pointed to by the pointers in",
      "  // graph_unreg_buffers_ when the IPC handles are exchanged between ranks.",
      "  //",
      "  // The overall process looks like this:",
      "  // 1. Graph capture.",
      "  // 2. Each rank obtains the IPC handles for each addresses used during cuda",
      "  // graph capture using get_graph_buffer_ipc_meta.",
      "  // 3. (In Python) all gather the IPC handles.",
      "  // 4. Obtain the peer pointers by opening the IPC handles, and store them in",
      "  // the rank data array at corresponding positions.",
      "  RankData *d_rank_data_base_, *d_rank_data_end_;",
      "  std::vector<void*> graph_unreg_buffers_;",
      "  // a map from IPC handles to opened IPC pointers",
      "  std::map<IPC_KEY, char*> ipc_handles_;",
      "",
      "  /**",
      "   * Signals are an array of ipc-enabled buffers from all ranks.",
      "   * For each of the buffer, the layout is as follows:",
      "   * | -- sizeof(Signal) -- | ------ a few MB ----- |",
      "   * The first section is for allreduce synchronization, and the second",
      "   * section is for storing the intermediate results required by some",
      "   * allreduce algos.",
      "   *",
      "   * Note: this class does not own any device memory. Any required buffers",
      "   * are passed in from the constructor.",
      "   */",
      "  CustomAllreduce(Signal** signals, void* rank_data, size_t rank_data_sz,",
      "                  int rank, int world_size, bool fully_connected = true)",
      "      : rank_(rank),",
      "        world_size_(world_size),",
      "        fully_connected_(fully_connected),",
      "        self_sg_(signals[rank]),",
      "        d_rank_data_base_(reinterpret_cast<RankData*>(rank_data)),",
      "        d_rank_data_end_(d_rank_data_base_ + rank_data_sz / sizeof(RankData)) {",
      "    for (int i = 0; i < world_size_; i++) {",
      "      sg_.signals[i] = signals[i];",
      "    }",
      "  }",
      "",
      "  char* open_ipc_handle(const void* ipc_handle) {",
      "    auto [it, new_handle] =",
      "        ipc_handles_.insert({*((IPC_KEY*)ipc_handle), nullptr});",
      "    if (new_handle) {",
      "      char* ipc_ptr;",
      "      CUDACHECK(cudaIpcOpenMemHandle((void**)&ipc_ptr,",
      "                                     *((const cudaIpcMemHandle_t*)ipc_handle),",
      "                                     cudaIpcMemLazyEnablePeerAccess));",
      "      it->second = ipc_ptr;",
      "    }",
      "    return it->second;",
      "  }",
      "",
      "  std::pair<std::string, std::vector<int64_t>> get_graph_buffer_ipc_meta() {",
      "    auto num_buffers = graph_unreg_buffers_.size();",
      "    auto handle_sz = sizeof(cudaIpcMemHandle_t);",
      "    std::string handles(handle_sz * num_buffers, static_cast<char>(0));",
      "    std::vector<int64_t> offsets(num_buffers);",
      "    for (int i = 0; i < num_buffers; i++) {",
      "      auto ptr = graph_unreg_buffers_[i];",
      "      void* base_ptr;",
      "      // note: must share the base address of each allocation, or we get wrong",
      "      // address",
      "      if (cuPointerGetAttribute(&base_ptr, rangeStartAddrAttr,",
      "                                (CUdeviceptr)ptr) != CUDA_SUCCESS)",
      "        throw std::runtime_error(\"failed to get pointer attr\");",
      "      CUDACHECK(cudaIpcGetMemHandle(",
      "          (cudaIpcMemHandle_t*)&handles[i * handle_sz], base_ptr));",
      "      offsets[i] = ((char*)ptr) - ((char*)base_ptr);",
      "    }",
      "    return std::make_pair(handles, offsets);",
      "  }",
      "",
      "  void check_rank_data_capacity(size_t num = 1) {",
      "    if (d_rank_data_base_ + num > d_rank_data_end_)",
      "      throw std::runtime_error(",
      "          \"Rank data buffer is overflowed by \" +",
      "          std::to_string(d_rank_data_base_ + num - d_rank_data_end_));",
      "  }",
      "",
      "  /**",
      "   * Register already-shared IPC pointers.",
      "   */",
      "  void register_buffer(void** ptrs) {",
      "    check_rank_data_capacity();",
      "    RankData data;",
      "    for (int i = 0; i < world_size_; i++) {",
      "      data.ptrs[i] = ptrs[i];",
      "    }",
      "    auto d_data = d_rank_data_base_++;",
      "    CUDACHECK(",
      "        cudaMemcpy(d_data, &data, sizeof(RankData), cudaMemcpyHostToDevice));",
      "    buffers_[ptrs[rank_]] = d_data;",
      "  }",
      "",
      "  // Note: when registering graph buffers, we intentionally choose to not",
      "  // deduplicate the addresses. That means if the allocator reuses some",
      "  // addresses, they will be registered again. This is to account for the",
      "  // remote possibility of different allocation patterns between ranks. For",
      "  // example, rank 1 may get the same input address for the second allreduce,",
      "  // but rank 2 got a different address. IPC handles have internal reference",
      "  // counting mechanism so overhead should be small.",
      "  void register_graph_buffers(",
      "      const std::vector<std::string>& handles,",
      "      const std::vector<std::vector<int64_t>>& offsets) {",
      "    auto num_buffers = graph_unreg_buffers_.size();",
      "    check_rank_data_capacity(num_buffers);",
      "    std::vector<RankData> rank_data(num_buffers);",
      "    for (int i = 0; i < num_buffers; i++) {",
      "      auto self_ptr = graph_unreg_buffers_[i];",
      "      auto& rd = rank_data[i];",
      "      for (int j = 0; j < world_size_; j++) {",
      "        if (j != rank_) {",
      "          char* handle =",
      "              open_ipc_handle(&handles[j][i * sizeof(cudaIpcMemHandle_t)]);",
      "          handle += offsets[j][i];",
      "          rd.ptrs[j] = handle;",
      "        } else {",
      "          rd.ptrs[j] = self_ptr;",
      "        }",
      "      }",
      "    }",
      "    CUDACHECK(cudaMemcpy(d_rank_data_base_, rank_data.data(),",
      "                         sizeof(RankData) * num_buffers,",
      "                         cudaMemcpyHostToDevice));",
      "    d_rank_data_base_ += num_buffers;",
      "    graph_unreg_buffers_.clear();",
      "  }",
      "",
      "  /**",
      "   * Performs allreduce, assuming input has already been registered.",
      "   *",
      "   * Block and grid default configs are results after careful grid search.",
      "   * Using 36 blocks give the best or close to the best runtime on the devices",
      "   * I tried: A100, A10, A30, T4, V100. You'll notice that NCCL kernels also",
      "   * only take a small amount of SMs. Not quite sure the underlying reason,",
      "   * but my guess is that too many SMs will cause contention on NVLink bus.",
      "   */",
      "  template <typename T>",
      "  void allreduce(cudaStream_t stream, T* input, T* output, int size,",
      "                 int threads = 512, int block_limit = defaultBlockLimit) {",
      "    auto d = packed_t<T>::P::size;",
      "    if (size % d != 0)",
      "      throw std::runtime_error(",
      "          \"custom allreduce currently requires input length to be multiple \"",
      "          \"of \" +",
      "          std::to_string(d));",
      "    if (block_limit > kMaxBlocks)",
      "      throw std::runtime_error(\"max supported block limit is \" +",
      "                               std::to_string(kMaxBlocks) + \". Got \" +",
      "                               std::to_string(block_limit));",
      "",
      "    RankData* ptrs;",
      "    cudaStreamCaptureStatus status;",
      "    CUDACHECK(cudaStreamIsCapturing(stream, &status));",
      "    if (status == cudaStreamCaptureStatusActive) {",
      "      ptrs = d_rank_data_base_ + graph_unreg_buffers_.size();",
      "      graph_unreg_buffers_.push_back(input);",
      "    } else {",
      "      auto it = buffers_.find(input);",
      "      if (it == buffers_.end())",
      "        throw std::runtime_error(",
      "            \"buffer address \" +",
      "            std::to_string(reinterpret_cast<uint64_t>(input)) +",
      "            \" is not registered!\");",
      "      ptrs = it->second;",
      "    }",
      "",
      "    size /= d;",
      "    auto bytes = size * sizeof(typename packed_t<T>::P);",
      "    int blocks = std::min(block_limit, (size + threads - 1) / threads);",
      "#define KL(ngpus, name)                                                       \\",
      "  name<T, ngpus><<<blocks, threads, 0, stream>>>(ptrs, sg_, self_sg_, output, \\",
      "                                                 rank_, size);",
      "#define REDUCE_CASE(ngpus)                            \\",
      "  case ngpus: {                                       \\",
      "    if (world_size_ == 2) {                           \\",
      "      KL(ngpus, cross_device_reduce_1stage);          \\",
      "    } else if (fully_connected_) {                    \\",
      "      if ((world_size_ <= 4 && bytes < 512 * 1024) || \\",
      "          (world_size_ <= 8 && bytes < 256 * 1024)) { \\",
      "        KL(ngpus, cross_device_reduce_1stage);        \\",
      "      } else {                                        \\",
      "        KL(ngpus, cross_device_reduce_2stage);        \\",
      "      }                                               \\",
      "    }                                                 \\",
      "    break;                                            \\",
      "  }",
      "",
      "    switch (world_size_) {",
      "      REDUCE_CASE(2)",
      "      REDUCE_CASE(4)",
      "      REDUCE_CASE(6)",
      "      REDUCE_CASE(8)",
      "      default:",
      "        throw std::runtime_error(",
      "            \"custom allreduce only supports num gpus in (2,4,6,8). Actual \"",
      "            \"num \"",
      "            \"gpus = \" +",
      "            std::to_string(world_size_));",
      "    }",
      "#undef REDUCE_CASE",
      "#undef KL",
      "  }",
      "",
      "  ~CustomAllreduce() {",
      "    for (auto [_, ptr] : ipc_handles_) {",
      "      CUDACHECK(cudaIpcCloseMemHandle(ptr));",
      "    }",
      "  }",
      "};",
      "",
      "/**",
      " * To inspect PTX/SASS, copy paste this header file to compiler explorer and",
      " add a template instantiation:",
      " * template void vllm::CustomAllreduce::allreduce<half>(cudaStream_t, half *,",
      " half *, int, int, int);",
      "*/",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/activation_kernels.cu",
    "source": [
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <torch/all.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#include <cmath>",
      "",
      "#include \"cuda_compat.h\"",
      "#include \"dispatch_utils.h\"",
      "",
      "namespace vllm {",
      "",
      "template <typename scalar_t, scalar_t (*ACT_FN)(const scalar_t&),",
      "          bool act_first>",
      "__device__ __forceinline__ scalar_t compute(const scalar_t& x,",
      "                                            const scalar_t& y) {",
      "  return act_first ? ACT_FN(x) * y : x * ACT_FN(y);",
      "}",
      "// Activation and gating kernel template.",
      "",
      "template <typename scalar_t, scalar_t (*ACT_FN)(const scalar_t&),",
      "          bool act_first>",
      "__global__ void act_and_mul_kernel(",
      "    scalar_t* __restrict__ out,          // [..., d]",
      "    const scalar_t* __restrict__ input,  // [..., 2, d]",
      "    const int d) {",
      "  const int64_t token_idx = blockIdx.x;",
      "  for (int64_t idx = threadIdx.x; idx < d; idx += blockDim.x) {",
      "    const scalar_t x = VLLM_LDG(&input[token_idx * 2 * d + idx]);",
      "    const scalar_t y = VLLM_LDG(&input[token_idx * 2 * d + d + idx]);",
      "    out[token_idx * d + idx] = compute<scalar_t, ACT_FN, act_first>(x, y);",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ T silu_kernel(const T& x) {",
      "  // x * sigmoid(x)",
      "  return (T)(((float)x) / (1.0f + expf((float)-x)));",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ T gelu_kernel(const T& x) {",
      "  // Equivalent to PyTorch GELU with 'none' approximation.",
      "  // Refer to:",
      "  // https://github.com/pytorch/pytorch/blob/8ac9b20d4b090c213799e81acf48a55ea8d437d6/aten/src/ATen/native/cuda/ActivationGeluKernel.cu#L36-L38",
      "  const float f = (float)x;",
      "  constexpr float ALPHA = M_SQRT1_2;",
      "  return (T)(f * 0.5f * (1.0f + ::erf(f * ALPHA)));",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ T gelu_tanh_kernel(const T& x) {",
      "  // Equivalent to PyTorch GELU with 'tanh' approximation.",
      "  // Refer to:",
      "  // https://github.com/pytorch/pytorch/blob/8ac9b20d4b090c213799e81acf48a55ea8d437d6/aten/src/ATen/native/cuda/ActivationGeluKernel.cu#L25-L30",
      "  const float f = (float)x;",
      "  constexpr float BETA = M_SQRT2 * M_2_SQRTPI * 0.5f;",
      "  constexpr float KAPPA = 0.044715;",
      "  float x_cube = f * f * f;",
      "  float inner = BETA * (f + KAPPA * x_cube);",
      "  return (T)(0.5f * f * (1.0f + ::tanhf(inner)));",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "// Launch activation and gating kernel.",
      "// Use ACT_FIRST (bool) indicating whether to apply the activation function",
      "// first.",
      "#define LAUNCH_ACTIVATION_GATE_KERNEL(KERNEL, ACT_FIRST)                 \\",
      "  int d = input.size(-1) / 2;                                            \\",
      "  int64_t num_tokens = input.numel() / input.size(-1);                   \\",
      "  dim3 grid(num_tokens);                                                 \\",
      "  dim3 block(std::min(d, 1024));                                         \\",
      "  if (num_tokens == 0) {                                                 \\",
      "    return;                                                              \\",
      "  }                                                                      \\",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));      \\",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();          \\",
      "  VLLM_DISPATCH_FLOATING_TYPES(                                          \\",
      "      input.scalar_type(), \"act_and_mul_kernel\", [&] {                   \\",
      "        vllm::act_and_mul_kernel<scalar_t, KERNEL<scalar_t>, ACT_FIRST>  \\",
      "            <<<grid, block, 0, stream>>>(out.data_ptr<scalar_t>(),       \\",
      "                                         input.data_ptr<scalar_t>(), d); \\",
      "      });",
      "",
      "void silu_and_mul(torch::Tensor& out,    // [..., d]",
      "                  torch::Tensor& input)  // [..., 2 * d]",
      "{",
      "  LAUNCH_ACTIVATION_GATE_KERNEL(vllm::silu_kernel, true);",
      "}",
      "",
      "void mul_and_silu(torch::Tensor& out,    // [..., d]",
      "                  torch::Tensor& input)  // [..., 2 * d]",
      "{",
      "  // The difference between mul_and_silu and silu_and_mul is that mul_and_silu",
      "  // applies the silu to the latter half of the input.",
      "  LAUNCH_ACTIVATION_GATE_KERNEL(vllm::silu_kernel, false);",
      "}",
      "",
      "void gelu_and_mul(torch::Tensor& out,    // [..., d]",
      "                  torch::Tensor& input)  // [..., 2 * d]",
      "{",
      "  LAUNCH_ACTIVATION_GATE_KERNEL(vllm::gelu_kernel, true);",
      "}",
      "",
      "void gelu_tanh_and_mul(torch::Tensor& out,    // [..., d]",
      "                       torch::Tensor& input)  // [..., 2 * d]",
      "{",
      "  LAUNCH_ACTIVATION_GATE_KERNEL(vllm::gelu_tanh_kernel, true);",
      "}",
      "",
      "namespace vllm {",
      "",
      "template <typename T>",
      "__device__ __forceinline__ T fatrelu_kernel(const T& x, const float threshold) {",
      "  const float f = (float)x;",
      "  return (T)(f > threshold ? f : 0.0f);",
      "}",
      "",
      "template <typename scalar_t, scalar_t (*ACT_FN)(const scalar_t&, const float)>",
      "__global__ void act_and_mul_kernel_with_param(",
      "    scalar_t* __restrict__ out, const scalar_t* __restrict__ input, const int d,",
      "    const float param) {",
      "  const int64_t token_idx = blockIdx.x;",
      "  for (int64_t idx = threadIdx.x; idx < d; idx += blockDim.x) {",
      "    const scalar_t x = VLLM_LDG(&input[token_idx * 2 * d + idx]);",
      "    const scalar_t y = VLLM_LDG(&input[token_idx * 2 * d + d + idx]);",
      "    out[token_idx * d + idx] = ACT_FN(x, param) * y;",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ T swigluoai_and_mul(const T& gate, const T& up,",
      "                                               float alpha, float limit) {",
      "  // clamp gate: min=None, max=limit",
      "  const float gate_f = (float)gate;",
      "  const float clamped_gate = gate_f > limit ? limit : gate_f;",
      "",
      "  // clamp up: min=-limit, max=limit",
      "  const float up_f = (float)up;",
      "  const float clamped_up =",
      "      up_f > limit ? limit : (up_f < -limit ? -limit : up_f);",
      "",
      "  // glu = gate * sigmoid(gate * alpha)",
      "  const float sigmoid_val = 1.0f / (1.0f + expf(-clamped_gate * alpha));",
      "  const float glu = clamped_gate * sigmoid_val;",
      "",
      "  // (up + 1) * glu",
      "  return (T)((clamped_up + 1.0f) * glu);",
      "}",
      "",
      "template <typename scalar_t,",
      "          scalar_t (*ACT_FN)(const scalar_t&, const scalar_t&, const float,",
      "                             const float)>",
      "__global__ void swigluoai_and_mul_kernel(",
      "    scalar_t* __restrict__ out,          // [..., d]",
      "    const scalar_t* __restrict__ input,  // [..., 2, d]",
      "    const int d, const float alpha, const float limit) {",
      "  const int64_t token_idx = blockIdx.x;",
      "  // TODO: Vectorize loads and stores.",
      "  for (int64_t idx = threadIdx.x; idx < d; idx += blockDim.x) {",
      "    // gate = x[..., ::2]  (even indices)",
      "    const scalar_t gate = VLLM_LDG(&input[token_idx * 2 * d + 2 * idx]);",
      "    // up = x[..., 1::2]   (odd indices)",
      "    const scalar_t up = VLLM_LDG(&input[token_idx * 2 * d + 2 * idx + 1]);",
      "",
      "    out[token_idx * d + idx] = ACT_FN(gate, up, alpha, limit);",
      "  }",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "#define LAUNCH_ACTIVATION_GATE_KERNEL_WITH_PARAM(KERNEL, PARAM)         \\",
      "  int d = input.size(-1) / 2;                                           \\",
      "  int64_t num_tokens = input.numel() / input.size(-1);                  \\",
      "  dim3 grid(num_tokens);                                                \\",
      "  dim3 block(std::min(d, 1024));                                        \\",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));     \\",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();         \\",
      "  VLLM_DISPATCH_FLOATING_TYPES(                                         \\",
      "      input.scalar_type(), \"act_and_mul_kernel_with_param\", [&] {       \\",
      "        vllm::act_and_mul_kernel_with_param<scalar_t, KERNEL<scalar_t>> \\",
      "            <<<grid, block, 0, stream>>>(out.data_ptr<scalar_t>(),      \\",
      "                                         input.data_ptr<scalar_t>(), d, \\",
      "                                         PARAM);                        \\",
      "      });",
      "",
      "#define LAUNCH_SIGLUOAI_AND_MUL(KERNEL, ALPHA, LIMIT)                          \\",
      "  int d = input.size(-1) / 2;                                                  \\",
      "  int64_t num_tokens = input.numel() / input.size(-1);                         \\",
      "  dim3 grid(num_tokens);                                                       \\",
      "  dim3 block(std::min(d, 1024));                                               \\",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));            \\",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();                \\",
      "  VLLM_DISPATCH_FLOATING_TYPES(                                                \\",
      "      input.scalar_type(), \"clamp_swiglu_kernel_with_params\", [&] {            \\",
      "        vllm::swigluoai_and_mul_kernel<scalar_t, KERNEL<scalar_t>>             \\",
      "            <<<grid, block, 0, stream>>>(out.data_ptr<scalar_t>(),             \\",
      "                                         input.data_ptr<scalar_t>(), d, ALPHA, \\",
      "                                         LIMIT);                               \\",
      "      });",
      "",
      "void fatrelu_and_mul(torch::Tensor& out,    // [..., d],",
      "                     torch::Tensor& input,  // [..., 2 * d]",
      "                     double threshold) {",
      "  LAUNCH_ACTIVATION_GATE_KERNEL_WITH_PARAM(vllm::fatrelu_kernel, threshold);",
      "}",
      "void swigluoai_and_mul(torch::Tensor& out,    // [..., d]",
      "                       torch::Tensor& input,  // [..., 2 * d]",
      "                       double alpha, double limit) {",
      "  LAUNCH_SIGLUOAI_AND_MUL(vllm::swigluoai_and_mul, alpha, limit);",
      "}",
      "namespace vllm {",
      "",
      "// Element-wise activation kernel template.",
      "template <typename scalar_t, scalar_t (*ACT_FN)(const scalar_t&)>",
      "__global__ void activation_kernel(",
      "    scalar_t* __restrict__ out,          // [..., d]",
      "    const scalar_t* __restrict__ input,  // [..., d]",
      "    const int d) {",
      "  const int64_t token_idx = blockIdx.x;",
      "  for (int64_t idx = threadIdx.x; idx < d; idx += blockDim.x) {",
      "    const scalar_t x = VLLM_LDG(&input[token_idx * d + idx]);",
      "    out[token_idx * d + idx] = ACT_FN(x);",
      "  }",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "// Launch element-wise activation kernel.",
      "#define LAUNCH_ACTIVATION_KERNEL(KERNEL)                                       \\",
      "  int d = input.size(-1);                                                      \\",
      "  int64_t num_tokens = input.numel() / d;                                      \\",
      "  dim3 grid(num_tokens);                                                       \\",
      "  dim3 block(std::min(d, 1024));                                               \\",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));            \\",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();                \\",
      "  VLLM_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"activation_kernel\", [&] { \\",
      "    vllm::activation_kernel<scalar_t, KERNEL<scalar_t>>                        \\",
      "        <<<grid, block, 0, stream>>>(out.data_ptr<scalar_t>(),                 \\",
      "                                     input.data_ptr<scalar_t>(), d);           \\",
      "  });",
      "",
      "namespace vllm {",
      "",
      "template <typename T>",
      "__device__ __forceinline__ T gelu_new_kernel(const T& x) {",
      "  const float x3 = (float)(x * x * x);",
      "  const T t = (T)tanhf((T)(0.79788456f * (float)(x + (T)(0.044715f * x3))));",
      "  return ((T)0.5) * x * (((T)1.0) + t);",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ T gelu_fast_kernel(const T& x) {",
      "  const float f = (float)x;",
      "  const T t =",
      "      (T)tanhf(((T)(f * 0.79788456f)) * (((T)1.0) + (T)(0.044715f * f) * x));",
      "  return ((T)0.5) * x * (((T)1.0) + t);",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ T gelu_quick_kernel(const T& x) {",
      "  // x * sigmoid(1.702 * x)",
      "  return (T)(((float)x) / (1.0f + expf(-1.702f * (float)x)));",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "void gelu_new(torch::Tensor& out,    // [..., d]",
      "              torch::Tensor& input)  // [..., d]",
      "{",
      "  LAUNCH_ACTIVATION_KERNEL(vllm::gelu_new_kernel);",
      "}",
      "",
      "void gelu_fast(torch::Tensor& out,    // [..., d]",
      "               torch::Tensor& input)  // [..., d]",
      "{",
      "  LAUNCH_ACTIVATION_KERNEL(vllm::gelu_fast_kernel);",
      "}",
      "",
      "void gelu_quick(torch::Tensor& out,    // [..., d]",
      "                torch::Tensor& input)  // [..., d]",
      "{",
      "  LAUNCH_ACTIVATION_KERNEL(vllm::gelu_quick_kernel);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/sampler.cu",
    "source": [
      "#include \"dispatch_utils.h\"",
      "",
      "#include <torch/cuda.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#ifndef USE_ROCM",
      "  #include <cub/cub.cuh>",
      "#else",
      "  #include <hipcub/hipcub.hpp>",
      "#endif",
      "",
      "namespace vllm {",
      "",
      "template <typename scalar_t>",
      "__global__ void apply_repetition_penalties_kernel(",
      "    scalar_t* __restrict__ logits,         // [num_seqs, vocab_size]",
      "    const bool* __restrict__ prompt_mask,  // [num_seqs, vocab_size]",
      "    const bool* __restrict__ output_mask,  // [num_seqs, vocab_size]",
      "    const scalar_t* __restrict__ repetition_penalties,  // [num_seqs]",
      "    const int num_seqs, const int vocab_size, const int tile_size) {",
      "  // Each block handles one sequence and a tile of vocab",
      "  const int seq_idx = blockIdx.x;",
      "  if (seq_idx >= num_seqs) return;",
      "",
      "  const int tile_start = blockIdx.y * tile_size;",
      "  const int tile_end = min(tile_start + tile_size, vocab_size);",
      "",
      "  // Load repetition penalty for this sequence",
      "  const scalar_t penalty = repetition_penalties[seq_idx];",
      "",
      "  // Each thread processes multiple vocab items within the tile",
      "  for (int vocab_idx = tile_start + threadIdx.x; vocab_idx < tile_end;",
      "       vocab_idx += blockDim.x) {",
      "    const int64_t idx = static_cast<int64_t>(seq_idx) * vocab_size + vocab_idx;",
      "    const bool is_repeated = prompt_mask[idx] || output_mask[idx];",
      "    if (is_repeated) {",
      "      scalar_t logit = logits[idx];",
      "      if (logit > 0) {",
      "        logits[idx] = logit / penalty;",
      "      } else {",
      "        logits[idx] = logit * penalty;",
      "      }",
      "    }",
      "  }",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "void apply_repetition_penalties_(",
      "    torch::Tensor& logits,             // [num_seqs, vocab_size], in-place",
      "    const torch::Tensor& prompt_mask,  // [num_seqs, vocab_size]",
      "    const torch::Tensor& output_mask,  // [num_seqs, vocab_size]",
      "    const torch::Tensor& repetition_penalties) {  // [num_seqs]",
      "  TORCH_CHECK(logits.is_contiguous());",
      "  TORCH_CHECK(prompt_mask.is_contiguous());",
      "  TORCH_CHECK(output_mask.is_contiguous());",
      "  TORCH_CHECK(repetition_penalties.is_contiguous());",
      "",
      "  int vocab_size = logits.size(-1);",
      "  int num_seqs = logits.size(0);",
      "",
      "  if (num_seqs == 0) return;",
      "",
      "  // Get number of SMs on the current device",
      "  int sms = 0;",
      "  cudaDeviceGetAttribute(&sms, cudaDevAttrMultiProcessorCount,",
      "                         logits.get_device());",
      "",
      "  // Compute tile_num and tile_size",
      "  int tile_num =",
      "      std::min(vocab_size, std::max(1, (sms + num_seqs - 1) / num_seqs));",
      "  int tile_size = (vocab_size + tile_num - 1) / tile_num;",
      "",
      "  // Each block handles one sequence and a tile of vocab",
      "  dim3 grid(num_seqs, tile_num);",
      "  dim3 block(std::min(tile_size, 1024));",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(logits));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  VLLM_DISPATCH_FLOATING_TYPES(",
      "      logits.scalar_type(), \"apply_repetition_penalties_kernel\", [&] {",
      "        vllm::apply_repetition_penalties_kernel<scalar_t>",
      "            <<<grid, block, 0, stream>>>(",
      "                logits.data_ptr<scalar_t>(), prompt_mask.data_ptr<bool>(),",
      "                output_mask.data_ptr<bool>(),",
      "                repetition_penalties.data_ptr<scalar_t>(), num_seqs, vocab_size,",
      "                tile_size);",
      "      });",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/custom_all_reduce_test.cu",
    "source": [
      "/**",
      " * This is a standalone test for custom allreduce.",
      " * To compile, make sure you have MPI and NCCL installed in your system.",
      " * export MPI_HOME=XXX",
      " * nvcc -O2 -arch=native -std=c++17 custom_all_reduce_test.cu -o",
      " * custom_all_reduce_test -lnccl -I${MPI_HOME}/include -lmpi",
      " *",
      " * Warning: this C++ test is not designed to be very readable and was used",
      " * during the rapid prototyping process.",
      " *",
      " * To run:",
      " * mpirun --allow-run-as-root -np 8 ./custom_all_reduce_test",
      " */",
      "#include <cuda.h>",
      "#include <curand_kernel.h>",
      "#include <stdio.h>",
      "#include <stdlib.h>",
      "",
      "#include <limits>",
      "#include <vector>",
      "",
      "#include \"cuda_profiler_api.h\"",
      "#include \"custom_all_reduce.cuh\"",
      "#include \"mpi.h\"",
      "#ifdef USE_ROCM",
      "  #include <hip/hip_bf16.h>",
      "typedef __hip_bfloat16 nv_bfloat16;",
      "  #include \"rccl/rccl.h\"",
      "  #include \"custom_all_reduce_hip.cuh\"",
      "#else",
      "  #include \"nccl.h\"",
      "  #include \"custom_all_reduce.cuh\"",
      "#endif",
      "",
      "#define MPICHECK(cmd)                                                  \\",
      "  do {                                                                 \\",
      "    int e = cmd;                                                       \\",
      "    if (e != MPI_SUCCESS) {                                            \\",
      "      printf(\"Failed: MPI error %s:%d '%d'\\n\", __FILE__, __LINE__, e); \\",
      "      exit(EXIT_FAILURE);                                              \\",
      "    }                                                                  \\",
      "  } while (0)",
      "",
      "#define NCCLCHECK(cmd)                                              \\",
      "  do {                                                              \\",
      "    ncclResult_t r = cmd;                                           \\",
      "    if (r != ncclSuccess) {                                         \\",
      "      printf(\"Failed, NCCL error %s:%d '%s'\\n\", __FILE__, __LINE__, \\",
      "             ncclGetErrorString(r));                                \\",
      "      exit(EXIT_FAILURE);                                           \\",
      "    }                                                               \\",
      "  } while (0)",
      "",
      "#ifdef USE_ROCM",
      "__global__ void dummy_kernel() {",
      "  for (int i = 0; i < 100; i++) {",
      "    uint64_t start = wall_clock64();",
      "    uint64_t cycles_elapsed;",
      "    do {",
      "      cycles_elapsed = wall_clock64() - start;",
      "    } while (cycles_elapsed < 100);",
      "  }",
      "  for (int i = 0; i < 100; i++) __nanosleep(1000000);  // 100ms",
      "}",
      "#else",
      "__global__ void dummy_kernel() {",
      "  #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 700",
      "  for (int i = 0; i < 100; i++) __nanosleep(1000000);  // 100ms",
      "  #else",
      "  for (int i = 0; i < 100; i++) {",
      "    long long int start = clock64();",
      "    while (clock64() - start < 150000000);  // approximately 98.4ms on P40",
      "  }",
      "  #endif",
      "}",
      "#endif",
      "",
      "template <typename T>",
      "__global__ void set_data(T* data, int size, int myRank) {",
      "  for (int idx = blockIdx.x * blockDim.x + threadIdx.x; idx < size;",
      "       idx += gridDim.x * blockDim.x) {",
      "    data[idx] = myRank * 0.11f;",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__global__ void convert_data(const T* data1, const T* data2, double* fdata1,",
      "                             double* fdata2, int size) {",
      "  for (int idx = blockIdx.x * blockDim.x + threadIdx.x; idx < size;",
      "       idx += gridDim.x * blockDim.x) {",
      "    fdata1[idx] = data1[idx];",
      "    fdata2[idx] = data2[idx];",
      "  }",
      "}",
      "",
      "__global__ void init_rand(curandState_t* state, int size, int nRanks) {",
      "  for (int idx = blockIdx.x * blockDim.x + threadIdx.x; idx < size;",
      "       idx += gridDim.x * blockDim.x) {",
      "    for (int i = 0; i < nRanks; i++) {",
      "      curand_init(i + 1, idx, 0, &state[idx * nRanks + i]);",
      "    }",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__global__ void gen_data(curandState_t* state, T* data, double* ground_truth,",
      "                         int myRank, int nRanks, int size) {",
      "  for (int idx = blockIdx.x * blockDim.x + threadIdx.x; idx < size;",
      "       idx += gridDim.x * blockDim.x) {",
      "    double sum = 0.0;",
      "    for (int i = 0; i < nRanks; i++) {",
      "      double val = curand_uniform_double(&state[idx * nRanks + i]) * 4;",
      "      T hval = val;  // downcast first",
      "      sum += static_cast<double>(hval);",
      "      if (i == myRank) data[idx] = hval;",
      "    }",
      "    ground_truth[idx] = sum;",
      "  }",
      "}",
      "",
      "template <typename T>",
      "void run(int myRank, int nRanks, ncclComm_t& comm, int threads, int block_limit,",
      "         int data_size, bool performance_test) {",
      "  T* result;",
      "  cudaStream_t stream;",
      "  CUDACHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));",
      "  CUDACHECK(cudaMalloc(&result, data_size * sizeof(T)));",
      "  CUDACHECK(cudaMemset(result, 0, data_size * sizeof(T)));",
      "",
      "  cudaIpcMemHandle_t self_data_handle;",
      "  cudaIpcMemHandle_t data_handles[8];",
      "  vllm::Signal* buffer;",
      "  T* self_data_copy;",
      "  /**",
      "   * Allocate IPC buffer",
      "   *",
      "   * The first section is a temporary buffer for storing intermediate allreduce",
      "   * results, if a particular algorithm requires it. The second section is for",
      "   * the input to the allreduce. The actual API takes the input pointer as an",
      "   * argument (that is, they can and usually should be allocated separately).",
      "   * But since the input pointers and the temporary buffer all require IPC",
      "   * registration, they are allocated and registered together in the test for",
      "   * convenience.",
      "   */",
      "#ifdef USE_ROCM",
      "  CUDACHECK(hipExtMallocWithFlags(",
      "      (void**)&buffer, 2 * data_size * sizeof(T) + sizeof(vllm::Signal),",
      "      hipDeviceMallocUncached));",
      "#else",
      "  CUDACHECK(",
      "      cudaMalloc(&buffer, 2 * data_size * sizeof(T) + sizeof(vllm::Signal)));",
      "#endif",
      "  CUDACHECK(",
      "      cudaMemset(buffer, 0, 2 * data_size * sizeof(T) + sizeof(vllm::Signal)));",
      "  CUDACHECK(cudaMalloc(&self_data_copy, data_size * sizeof(T)));",
      "  CUDACHECK(cudaIpcGetMemHandle(&self_data_handle, buffer));",
      "",
      "  MPICHECK(MPI_Allgather(&self_data_handle, sizeof(cudaIpcMemHandle_t),",
      "                         MPI_BYTE, data_handles, sizeof(cudaIpcMemHandle_t),",
      "                         MPI_BYTE, MPI_COMM_WORLD));",
      "",
      "  void* rank_data;",
      "  size_t rank_data_sz = 16 * 1024 * 1024;",
      "  CUDACHECK(cudaMalloc(&rank_data, rank_data_sz));",
      "  vllm::Signal* ipc_ptrs[8];",
      "  for (int i = 0; i < nRanks; i++) {",
      "    if (i == myRank)",
      "      ipc_ptrs[i] = buffer;",
      "    else",
      "      CUDACHECK(cudaIpcOpenMemHandle((void**)&ipc_ptrs[i], data_handles[i],",
      "                                     cudaIpcMemLazyEnablePeerAccess));",
      "  }",
      "  vllm::CustomAllreduce fa(ipc_ptrs, rank_data, rank_data_sz, myRank, nRanks);",
      "  auto* self_data =",
      "      reinterpret_cast<T*>(reinterpret_cast<char*>(buffer) +",
      "                           sizeof(vllm::Signal) + data_size * sizeof(T));",
      "  // hack buffer registration",
      "  {",
      "    void* data[8];",
      "    for (int i = 0; i < nRanks; i++) {",
      "      data[i] =",
      "          ((char*)ipc_ptrs[i]) + sizeof(vllm::Signal) + data_size * sizeof(T);",
      "    }",
      "    fa.register_buffer(data);",
      "  }",
      "",
      "  double* ground_truth;",
      "  CUDACHECK(cudaMallocHost(&ground_truth, data_size * sizeof(double)));",
      "  curandState_t* states;",
      "  CUDACHECK(cudaMalloc(&states, sizeof(curandState_t) * nRanks * data_size));",
      "  init_rand<<<108, 1024, 0, stream>>>(states, data_size, nRanks);",
      "  gen_data<T><<<108, 1024, 0, stream>>>(states, self_data, ground_truth, myRank,",
      "                                        nRanks, data_size);",
      "  CUDACHECK(cudaMemcpyAsync(self_data_copy, self_data, data_size * sizeof(T),",
      "                            cudaMemcpyDeviceToDevice, stream));",
      "  cudaEvent_t start, stop;",
      "  CUDACHECK(cudaEventCreate(&start));",
      "  CUDACHECK(cudaEventCreate(&stop));",
      "",
      "  ncclDataType_t ncclDtype;",
      "  if (std::is_same<T, half>::value) {",
      "    ncclDtype = ncclFloat16;",
      "  } else if (std::is_same<T, nv_bfloat16>::value) {",
      "    ncclDtype = ncclBfloat16;",
      "  } else {",
      "    ncclDtype = ncclFloat;",
      "  }",
      "  double *nccl_result, *my_result;",
      "  CUDACHECK(cudaMallocHost(&nccl_result, data_size * sizeof(double)));",
      "  CUDACHECK(cudaMallocHost(&my_result, data_size * sizeof(double)));",
      "  if (performance_test) {",
      "    dummy_kernel<<<1, 1, 0, stream>>>();",
      "    constexpr int warmup_iters = 5;",
      "    constexpr int num_iters = 100;",
      "    // warmup",
      "    for (int i = 0; i < warmup_iters; i++) {",
      "      NCCLCHECK(ncclAllReduce(result, result, data_size, ncclDtype, ncclSum,",
      "                              comm, stream));",
      "    }",
      "    CUDACHECK(cudaEventRecord(start, stream));",
      "    for (int i = 0; i < num_iters; i++) {",
      "      NCCLCHECK(ncclAllReduce(result, result, data_size, ncclDtype, ncclSum,",
      "                              comm, stream));",
      "    }",
      "    CUDACHECK(cudaEventRecord(stop, stream));",
      "    CUDACHECK(cudaStreamSynchronize(stream));",
      "    float allreduce_ms = 0;",
      "    cudaEventElapsedTime(&allreduce_ms, start, stop);",
      "",
      "    dummy_kernel<<<1, 1, 0, stream>>>();",
      "    // warm up",
      "    for (int i = 0; i < warmup_iters; i++) {",
      "      fa.allreduce<T>(stream, self_data, result, data_size, threads,",
      "                      block_limit);",
      "    }",
      "    CUDACHECK(cudaEventRecord(start, stream));",
      "    for (int i = 0; i < num_iters; i++) {",
      "      fa.allreduce<T>(stream, self_data, result, data_size, threads,",
      "                      block_limit);",
      "    }",
      "    CUDACHECK(cudaEventRecord(stop, stream));",
      "    CUDACHECK(cudaStreamSynchronize(stream));",
      "",
      "    float duration_ms = 0;",
      "    cudaEventElapsedTime(&duration_ms, start, stop);",
      "    if (myRank == 0)",
      "      printf(",
      "          \"Rank %d done, nGPUs:%d, sz (kb): %d, %d, %d, my time:%.2fus, nccl \"",
      "          \"time:%.2fus\\n\",",
      "          myRank, nRanks, data_size * sizeof(T) / 1024, threads, block_limit,",
      "          duration_ms * 1e3 / num_iters, allreduce_ms * 1e3 / num_iters);",
      "",
      "    // And wait for all the queued up work to complete",
      "    CUDACHECK(cudaStreamSynchronize(stream));",
      "",
      "    NCCLCHECK(ncclAllReduce(self_data_copy, self_data, data_size, ncclDtype,",
      "                            ncclSum, comm, stream));",
      "",
      "    convert_data<T><<<108, 1024, 0, stream>>>(self_data, result, nccl_result,",
      "                                              my_result, data_size);",
      "    CUDACHECK(cudaStreamSynchronize(stream));",
      "",
      "    for (unsigned long j = 0; j < data_size; j++) {",
      "      auto diff = abs(nccl_result[j] - my_result[j]);",
      "      if (diff >= 4e-2) {",
      "        printf(\"Rank %d: Verification mismatch at %lld: %f != (my) %f, gt=%f\\n\",",
      "               myRank, j, nccl_result[j], my_result[j], ground_truth[j]);",
      "        break;",
      "      }",
      "    }",
      "    long double nccl_diffs = 0.0;",
      "    long double my_diffs = 0.0;",
      "    for (int j = 0; j < data_size; j++) {",
      "      nccl_diffs += abs(nccl_result[j] - ground_truth[j]);",
      "      my_diffs += abs(my_result[j] - ground_truth[j]);",
      "    }",
      "    if (myRank == 0)",
      "      std::cout << \"average abs diffs: nccl: \" << nccl_diffs / data_size",
      "                << \" me: \" << my_diffs / data_size << std::endl;",
      "  } else {",
      "    for (int i = 0; i < 100; i++) {",
      "      fa.allreduce<T>(stream, self_data, result, data_size, threads,",
      "                      block_limit);",
      "      CUDACHECK(cudaStreamSynchronize(stream));",
      "      NCCLCHECK(ncclAllReduce(self_data, self_data_copy, data_size, ncclDtype,",
      "                              ncclSum, comm, stream));",
      "      convert_data<T><<<108, 1024, 0, stream>>>(",
      "          self_data_copy, result, nccl_result, my_result, data_size);",
      "      CUDACHECK(cudaStreamSynchronize(stream));",
      "",
      "      for (unsigned long j = 0; j < data_size; j++) {",
      "        auto diff = abs(nccl_result[j] - my_result[j]);",
      "        if (diff >= 4e-2) {",
      "          printf(",
      "              \"Rank %d: Verification mismatch at %lld: %f != (my) %f, gt=%f\\n\",",
      "              myRank, j, nccl_result[j], my_result[j], ground_truth[j]);",
      "          break;",
      "        }",
      "      }",
      "    }",
      "    if (myRank == 0)",
      "      printf(\"Test passed: nGPUs:%d, sz (kb): %d, %d, %d\\n\", nRanks,",
      "             data_size * sizeof(T) / 1024, threads, block_limit);",
      "    // long double nccl_diffs = 0.0;",
      "    // long double my_diffs = 0.0;",
      "    // for (int j = 0; j < data_size; j++) {",
      "    //   nccl_diffs += abs(nccl_result[j] - ground_truth[j]);",
      "    //   my_diffs += abs(my_result[j] - ground_truth[j]);",
      "    // }",
      "    // if (myRank == 0)",
      "    //   std::cout << \"average abs diffs: nccl: \" << nccl_diffs / data_size",
      "    //             << \" me: \" << my_diffs / data_size << std::endl;",
      "  }",
      "",
      "  CUDACHECK(cudaFree(result));",
      "  CUDACHECK(cudaFree(self_data_copy));",
      "  CUDACHECK(cudaFree(rank_data));",
      "  CUDACHECK(cudaFree(buffer));",
      "  CUDACHECK(cudaFree(states));",
      "  CUDACHECK(cudaFreeHost(ground_truth));",
      "  CUDACHECK(cudaFreeHost(nccl_result));",
      "  CUDACHECK(cudaFreeHost(my_result));",
      "  CUDACHECK(cudaStreamDestroy(stream));",
      "}",
      "",
      "int main(int argc, char** argv) {",
      "  int nRanks, myRank;",
      "  MPICHECK(MPI_Init(&argc, &argv));",
      "  MPICHECK(MPI_Comm_rank(MPI_COMM_WORLD, &myRank));",
      "  MPICHECK(MPI_Comm_size(MPI_COMM_WORLD, &nRanks));",
      "  CUDACHECK(cudaSetDevice(myRank));",
      "  ncclUniqueId id;",
      "  ncclComm_t comm;",
      "  if (myRank == 0) ncclGetUniqueId(&id);",
      "  MPICHECK(MPI_Bcast(static_cast<void*>(&id), sizeof(id), MPI_BYTE, 0,",
      "                     MPI_COMM_WORLD));",
      "  NCCLCHECK(ncclCommInitRank(&comm, nRanks, id, myRank));",
      "",
      "  bool performance_test = true;",
      "  cudaProfilerStart();",
      "// Uncomment to scan through different block size configs.",
      "// for (int threads : {256, 512, 1024}) {",
      "//   for (int block_limit = 16; block_limit < 112; block_limit += 4) {",
      "//     run<half>(myRank, nRanks, comm, threads, block_limit, 1024 * 1024,",
      "//     performance_test);",
      "//   }",
      "// }",
      "#ifdef USE_ROCM",
      "  const int block_limit = 16;",
      "#else",
      "  const int block_limit = 36;",
      "#endif",
      "  // Scan through different sizes to test performance.",
      "  for (int sz = 512; sz <= (8 << 20); sz *= 2) {",
      "    run<half>(myRank, nRanks, comm, 512, 36, sz + 8 * 47, performance_test);",
      "  }",
      "",
      "  cudaProfilerStop();",
      "  MPICHECK(MPI_Finalize());",
      "  return EXIT_SUCCESS;",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/type_convert.cuh",
    "source": [
      "#pragma once",
      "",
      "#include <torch/all.h>",
      "",
      "#ifndef USE_ROCM",
      "  #include <cuda_bf16.h>",
      "  #include <cuda_fp16.h>",
      "#else",
      "  #include <hip/hip_bf16.h>",
      "  #include <hip/hip_fp16.h>",
      "",
      "using __nv_bfloat16 = __hip_bfloat16;",
      "using __nv_bfloat162 = __hip_bfloat162;",
      "#endif",
      "",
      "namespace vllm {",
      "/* Converter structs for the conversion from torch types to HIP/CUDA types,",
      "   and the associated type conversions within HIP/CUDA. These helpers need",
      "   to be implemented for now because the relevant type conversion",
      "   operators/constructors are not consistently implemented by HIP/CUDA, so",
      "   a generic conversion via type casts cannot be implemented.",
      "",
      "   Each struct should have the member static constexpr bool `exists`:",
      "   If false, the optimized kernel is not used for the corresponding torch type.",
      "   If true, the struct should be fully defined as shown in the examples below.",
      " */",
      "template <typename torch_type>",
      "struct _typeConvert {",
      "  static constexpr bool exists = false;",
      "};",
      "",
      "#if defined(USE_ROCM) || (defined(CUDA_VERSION) && (CUDA_VERSION >= 12000))",
      "// CUDA < 12.0 runs into issues with packed type conversion",
      "template <>",
      "struct _typeConvert<c10::Half> {",
      "  static constexpr bool exists = true;",
      "  using hip_type = __half;",
      "  using packed_hip_type = __half2;",
      "",
      "  __device__ static inline float convert(hip_type x) { return __half2float(x); }",
      "  __device__ static inline float2 convert(packed_hip_type x) {",
      "    return __half22float2(x);",
      "  }",
      "  __device__ static inline hip_type convert(float x) {",
      "    return __float2half_rn(x);",
      "  }",
      "  __device__ static inline packed_hip_type convert(float2 x) {",
      "    return __float22half2_rn(x);",
      "  }",
      "};",
      "",
      "  #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800",
      "// CUDA_ARCH < 800 does not have BF16 support",
      "// TODO: Add in ROCm support once public headers handle bf16 maturely",
      "template <>",
      "struct _typeConvert<c10::BFloat16> {",
      "  static constexpr bool exists = true;",
      "  using hip_type = __nv_bfloat16;",
      "  using packed_hip_type = __nv_bfloat162;",
      "",
      "  __device__ static inline float convert(hip_type x) {",
      "    return __bfloat162float(x);",
      "  }",
      "  __device__ static inline float2 convert(packed_hip_type x) {",
      "    return __bfloat1622float2(x);",
      "  }",
      "  __device__ static inline hip_type convert(float x) {",
      "    return __float2bfloat16(x);",
      "  }",
      "  __device__ static inline packed_hip_type convert(float2 x) {",
      "    return __float22bfloat162_rn(x);",
      "  }",
      "};",
      "  #endif  // defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800",
      "#endif    // defined(USE_ROCM) || (defined(CUDA_VERSION) && (CUDA_VERSION >=",
      "          // 12000))",
      "",
      "/* Vector POD struct to generate vectorized and packed FP16/BF16 ops",
      "   for appropriate specializations of fused_add_rms_norm_kernel.",
      "   Only functions that are necessary in that kernel are implemented.",
      "   Alignment to 16 bytes is required to use 128-bit global memory ops.",
      " */",
      "template <typename scalar_t, int width>",
      "struct alignas(16) _f16Vec {",
      "  /* Not theoretically necessary that width is a power of 2 but should",
      "     almost always be the case for optimization purposes */",
      "  static_assert(width > 0 && (width & (width - 1)) == 0,",
      "                \"Width is not a positive power of 2!\");",
      "  using Converter = _typeConvert<scalar_t>;",
      "  using T1 = typename Converter::hip_type;",
      "  using T2 = typename Converter::packed_hip_type;",
      "  T1 data[width];",
      "",
      "  __device__ _f16Vec& operator+=(const _f16Vec<scalar_t, width>& other) {",
      "    if constexpr (width % 2 == 0) {",
      "#pragma unroll",
      "      for (int i = 0; i < width; i += 2) {",
      "        T2 temp{data[i], data[i + 1]};",
      "        temp += T2{other.data[i], other.data[i + 1]};",
      "        data[i] = temp.x;",
      "        data[i + 1] = temp.y;",
      "      }",
      "    } else {",
      "#pragma unroll",
      "      for (int i = 0; i < width; ++i) data[i] += other.data[i];",
      "    }",
      "    return *this;",
      "  }",
      "",
      "  __device__ _f16Vec& operator*=(const _f16Vec<scalar_t, width>& other) {",
      "    if constexpr (width % 2 == 0) {",
      "#pragma unroll",
      "      for (int i = 0; i < width; i += 2) {",
      "        T2 temp{data[i], data[i + 1]};",
      "        temp *= T2{other.data[i], other.data[i + 1]};",
      "        data[i] = temp.x;",
      "        data[i + 1] = temp.y;",
      "      }",
      "    } else {",
      "#pragma unroll",
      "      for (int i = 0; i < width; ++i) data[i] *= other.data[i];",
      "    }",
      "    return *this;",
      "  }",
      "",
      "  __device__ _f16Vec& operator*=(const float scale) {",
      "    if constexpr (width % 2 == 0) {",
      "#pragma unroll",
      "      for (int i = 0; i < width; i += 2) {",
      "        float2 temp_f = Converter::convert(T2{data[i], data[i + 1]});",
      "        temp_f.x *= scale;",
      "        temp_f.y *= scale;",
      "        T2 temp = Converter::convert(temp_f);",
      "        data[i] = temp.x;",
      "        data[i + 1] = temp.y;",
      "      }",
      "    } else {",
      "#pragma unroll",
      "      for (int i = 0; i < width; ++i) {",
      "        float temp = Converter::convert(data[i]) * scale;",
      "        data[i] = Converter::convert(temp);",
      "      }",
      "    }",
      "    return *this;",
      "  }",
      "",
      "  __device__ float sum_squares() const {",
      "    float result = 0.0f;",
      "    if constexpr (width % 2 == 0) {",
      "#pragma unroll",
      "      for (int i = 0; i < width; i += 2) {",
      "        float2 z = Converter::convert(T2{data[i], data[i + 1]});",
      "        result += z.x * z.x + z.y * z.y;",
      "      }",
      "    } else {",
      "#pragma unroll",
      "      for (int i = 0; i < width; ++i) {",
      "        float x = Converter::convert(data[i]);",
      "        result += x * x;",
      "      }",
      "    }",
      "    return result;",
      "  }",
      "};",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/layernorm_kernels.cu",
    "source": [
      "#include \"type_convert.cuh\"",
      "#include \"dispatch_utils.h\"",
      "",
      "#include <torch/cuda.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#ifndef USE_ROCM",
      "  #include <cub/cub.cuh>",
      "#else",
      "  #include <hipcub/hipcub.hpp>",
      "#endif",
      "",
      "namespace vllm {",
      "",
      "// TODO(woosuk): Further optimize this kernel.",
      "template <typename scalar_t>",
      "__global__ void rms_norm_kernel(",
      "    scalar_t* __restrict__ out,          // [..., hidden_size]",
      "    const scalar_t* __restrict__ input,  // [..., hidden_size]",
      "    const int64_t input_stride,",
      "    const scalar_t* __restrict__ weight,  // [hidden_size]",
      "    const float epsilon, const int num_tokens, const int hidden_size) {",
      "  __shared__ float s_variance;",
      "  float variance = 0.0f;",
      "",
      "  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {",
      "    const float x = (float)input[blockIdx.x * input_stride + idx];",
      "    variance += x * x;",
      "  }",
      "",
      "  using BlockReduce = cub::BlockReduce<float, 1024>;",
      "  __shared__ typename BlockReduce::TempStorage reduceStore;",
      "  variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);",
      "",
      "  if (threadIdx.x == 0) {",
      "    s_variance = rsqrtf(variance / hidden_size + epsilon);",
      "  }",
      "  __syncthreads();",
      "",
      "  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {",
      "    float x = (float)input[blockIdx.x * input_stride + idx];",
      "    out[blockIdx.x * hidden_size + idx] =",
      "        ((scalar_t)(x * s_variance)) * weight[idx];",
      "  }",
      "}",
      "",
      "/* Function specialization in the case of FP16/BF16 tensors.",
      "   Additional optimizations we can make in this case are",
      "   packed and vectorized operations, which help with the",
      "   memory latency bottleneck. */",
      "template <typename scalar_t, int width>",
      "__global__ std::enable_if_t<(width > 0) && _typeConvert<scalar_t>::exists>",
      "fused_add_rms_norm_kernel(",
      "    scalar_t* __restrict__ input,  // [..., hidden_size]",
      "    const int64_t input_stride,",
      "    scalar_t* __restrict__ residual,      // [..., hidden_size]",
      "    const scalar_t* __restrict__ weight,  // [hidden_size]",
      "    const float epsilon, const int num_tokens, const int hidden_size) {",
      "  // Sanity checks on our vector struct and type-punned pointer arithmetic",
      "  static_assert(std::is_pod_v<_f16Vec<scalar_t, width>>);",
      "  static_assert(sizeof(_f16Vec<scalar_t, width>) == sizeof(scalar_t) * width);",
      "",
      "  const int vec_hidden_size = hidden_size / width;",
      "  const int64_t vec_input_stride = input_stride / width;",
      "  __shared__ float s_variance;",
      "  float variance = 0.0f;",
      "  /* These and the argument pointers are all declared `restrict` as they are",
      "     not aliased in practice. Argument pointers should not be dereferenced",
      "     in this kernel as that would be undefined behavior */",
      "  auto* __restrict__ input_v =",
      "      reinterpret_cast<_f16Vec<scalar_t, width>*>(input);",
      "  auto* __restrict__ residual_v =",
      "      reinterpret_cast<_f16Vec<scalar_t, width>*>(residual);",
      "  auto* __restrict__ weight_v =",
      "      reinterpret_cast<const _f16Vec<scalar_t, width>*>(weight);",
      "",
      "  for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {",
      "    int id = blockIdx.x * vec_hidden_size + idx;",
      "    int64_t strided_id = blockIdx.x * vec_input_stride + idx;",
      "    _f16Vec<scalar_t, width> temp = input_v[strided_id];",
      "    temp += residual_v[id];",
      "    variance += temp.sum_squares();",
      "    residual_v[id] = temp;",
      "  }",
      "",
      "  using BlockReduce = cub::BlockReduce<float, 1024>;",
      "  __shared__ typename BlockReduce::TempStorage reduceStore;",
      "  variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);",
      "",
      "  if (threadIdx.x == 0) {",
      "    s_variance = rsqrtf(variance / hidden_size + epsilon);",
      "  }",
      "  __syncthreads();",
      "",
      "  for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {",
      "    int id = blockIdx.x * vec_hidden_size + idx;",
      "    int64_t strided_id = blockIdx.x * vec_input_stride + idx;",
      "    _f16Vec<scalar_t, width> temp = residual_v[id];",
      "    temp *= s_variance;",
      "    temp *= weight_v[idx];",
      "    input_v[strided_id] = temp;",
      "  }",
      "}",
      "",
      "/* Generic fused_add_rms_norm_kernel",
      "   The width field is not used here but necessary for other specializations.",
      " */",
      "template <typename scalar_t, int width>",
      "__global__ std::enable_if_t<(width == 0) || !_typeConvert<scalar_t>::exists>",
      "fused_add_rms_norm_kernel(",
      "    scalar_t* __restrict__ input,  // [..., hidden_size]",
      "    const int64_t input_stride,",
      "    scalar_t* __restrict__ residual,      // [..., hidden_size]",
      "    const scalar_t* __restrict__ weight,  // [hidden_size]",
      "    const float epsilon, const int num_tokens, const int hidden_size) {",
      "  __shared__ float s_variance;",
      "  float variance = 0.0f;",
      "",
      "  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {",
      "    scalar_t z = input[blockIdx.x * input_stride + idx];",
      "    z += residual[blockIdx.x * hidden_size + idx];",
      "    float x = (float)z;",
      "    variance += x * x;",
      "    residual[blockIdx.x * hidden_size + idx] = z;",
      "  }",
      "",
      "  using BlockReduce = cub::BlockReduce<float, 1024>;",
      "  __shared__ typename BlockReduce::TempStorage reduceStore;",
      "  variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);",
      "",
      "  if (threadIdx.x == 0) {",
      "    s_variance = rsqrtf(variance / hidden_size + epsilon);",
      "  }",
      "  __syncthreads();",
      "",
      "  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {",
      "    float x = (float)residual[blockIdx.x * hidden_size + idx];",
      "    input[blockIdx.x * input_stride + idx] =",
      "        ((scalar_t)(x * s_variance)) * weight[idx];",
      "  }",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "void rms_norm(torch::Tensor& out,     // [..., hidden_size]",
      "              torch::Tensor& input,   // [..., hidden_size]",
      "              torch::Tensor& weight,  // [hidden_size]",
      "              double epsilon) {",
      "  TORCH_CHECK(out.is_contiguous());",
      "  TORCH_CHECK(input.stride(-1) == 1);",
      "  TORCH_CHECK(weight.is_contiguous());",
      "",
      "  int hidden_size = input.size(-1);",
      "  int num_tokens = input.numel() / hidden_size;",
      "  int64_t input_stride = input.stride(-2);",
      "",
      "  dim3 grid(num_tokens);",
      "  dim3 block(std::min(hidden_size, 1024));",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  VLLM_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"rms_norm_kernel\", [&] {",
      "    vllm::rms_norm_kernel<scalar_t><<<grid, block, 0, stream>>>(",
      "        out.data_ptr<scalar_t>(), input.data_ptr<scalar_t>(), input_stride,",
      "        weight.data_ptr<scalar_t>(), epsilon, num_tokens, hidden_size);",
      "  });",
      "}",
      "",
      "#define LAUNCH_FUSED_ADD_RMS_NORM(width)                                    \\",
      "  VLLM_DISPATCH_FLOATING_TYPES(                                             \\",
      "      input.scalar_type(), \"fused_add_rms_norm_kernel\", [&] {               \\",
      "        vllm::fused_add_rms_norm_kernel<scalar_t, width>                    \\",
      "            <<<grid, block, 0, stream>>>(                                   \\",
      "                input.data_ptr<scalar_t>(), input_stride,                   \\",
      "                residual.data_ptr<scalar_t>(), weight.data_ptr<scalar_t>(), \\",
      "                epsilon, num_tokens, hidden_size);                          \\",
      "      });",
      "",
      "void fused_add_rms_norm(torch::Tensor& input,     // [..., hidden_size]",
      "                        torch::Tensor& residual,  // [..., hidden_size]",
      "                        torch::Tensor& weight,    // [hidden_size]",
      "                        double epsilon) {",
      "  TORCH_CHECK(residual.is_contiguous());",
      "  TORCH_CHECK(weight.is_contiguous());",
      "  int hidden_size = input.size(-1);",
      "  int64_t input_stride = input.stride(-2);",
      "  int num_tokens = input.numel() / hidden_size;",
      "",
      "  dim3 grid(num_tokens);",
      "  /* This kernel is memory-latency bound in many scenarios.",
      "     When num_tokens is large, a smaller block size allows",
      "     for increased block occupancy on CUs and better latency",
      "     hiding on global mem ops. */",
      "  const int max_block_size = (num_tokens < 256) ? 1024 : 256;",
      "  dim3 block(std::min(hidden_size, max_block_size));",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  /*If the tensor types are FP16/BF16, try to use the optimized kernel",
      "    with packed + vectorized ops.",
      "    Max optimization is achieved with a width-8 vector of FP16/BF16s",
      "    since we can load at most 128 bits at once in a global memory op.",
      "    However, this requires each tensor's data to be aligned to 16",
      "    bytes.",
      "   */",
      "  auto inp_ptr = reinterpret_cast<std::uintptr_t>(input.data_ptr());",
      "  auto res_ptr = reinterpret_cast<std::uintptr_t>(residual.data_ptr());",
      "  auto wt_ptr = reinterpret_cast<std::uintptr_t>(weight.data_ptr());",
      "  constexpr int vector_width = 8;",
      "  constexpr int req_alignment_bytes =",
      "      vector_width * 2;  // vector_width * sizeof(bfloat16 or float16) (float32",
      "                         // falls back to non-vectorized version anyway)",
      "  bool ptrs_are_aligned = inp_ptr % req_alignment_bytes == 0 &&",
      "                          res_ptr % req_alignment_bytes == 0 &&",
      "                          wt_ptr % req_alignment_bytes == 0;",
      "  bool offsets_are_multiple_of_vector_width =",
      "      hidden_size % vector_width == 0 && input_stride % vector_width == 0;",
      "  if (ptrs_are_aligned && offsets_are_multiple_of_vector_width) {",
      "    LAUNCH_FUSED_ADD_RMS_NORM(8);",
      "  } else {",
      "    LAUNCH_FUSED_ADD_RMS_NORM(0);",
      "  }",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/cache_kernels.cu",
    "source": [
      "#include <torch/all.h>",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <c10/cuda/CUDAException.h>",
      "",
      "#include \"cuda_utils.h\"",
      "#include \"cuda_compat.h\"",
      "#include \"dispatch_utils.h\"",
      "#include \"quantization/vectorization_utils.cuh\"",
      "",
      "#ifdef USE_ROCM",
      "  #include \"quantization/fp8/amd/quant_utils.cuh\"",
      "#else",
      "  #include \"quantization/fp8/nvidia/quant_utils.cuh\"",
      "#endif",
      "",
      "#include <algorithm>",
      "#include <cassert>",
      "#include <map>",
      "#include <vector>",
      "",
      "#ifdef USE_ROCM",
      "  #include <hip/hip_bf16.h>",
      "typedef __hip_bfloat16 __nv_bfloat16;",
      "#endif",
      "",
      "void swap_blocks(torch::Tensor& src, torch::Tensor& dst,",
      "                 const torch::Tensor& block_mapping) {",
      "  torch::Device src_device = src.device();",
      "  torch::Device dst_device = dst.device();",
      "  cudaMemcpyKind memcpy_type;",
      "  if (src_device.is_cuda() && dst_device.is_cuda()) {",
      "    TORCH_CHECK(src_device.index() == dst_device.index(),",
      "                \"src and dst must be on the same GPU\");",
      "    memcpy_type = cudaMemcpyDeviceToDevice;",
      "  } else if (src_device.is_cuda() && dst_device.is_cpu()) {",
      "    memcpy_type = cudaMemcpyDeviceToHost;",
      "  } else if (src_device.is_cpu() && dst_device.is_cuda()) {",
      "    memcpy_type = cudaMemcpyHostToDevice;",
      "  } else {",
      "    TORCH_CHECK(false, \"Invalid device combination\");",
      "  }",
      "",
      "  // NOTE(youkaichao): keep in mind that `block_mapping` should be",
      "  // a cpu tensor, otherwise every `item` call will require a gpu-cpu",
      "  // synchronization.",
      "  TORCH_CHECK(block_mapping.device().is_cpu(), \"block_mapping must be on CPU\");",
      "",
      "  char* src_ptr = static_cast<char*>(src.data_ptr());",
      "  char* dst_ptr = static_cast<char*>(dst.data_ptr());",
      "",
      "  // We use the stride instead of numel in case the cache is padded for memory",
      "  // alignment reasons, we assume the blocks data (inclusive of any padding)",
      "  // is contiguous in memory",
      "  const int64_t block_size_in_bytes = src.element_size() * src.stride(0);",
      "  const at::cuda::OptionalCUDAGuard device_guard(",
      "      src_device.is_cuda() ? src_device : dst_device);",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  // NOTE(woosuk): This can be slow if the number of blocks is large.",
      "  const int64_t num_blocks = block_mapping.size(0);",
      "  for (size_t i = 0; i < num_blocks; i++) {",
      "    int64_t src_block_number = block_mapping[i][0].item<int64_t>();",
      "    int64_t dst_block_number = block_mapping[i][1].item<int64_t>();",
      "    int64_t src_offset = src_block_number * block_size_in_bytes;",
      "    int64_t dst_offset = dst_block_number * block_size_in_bytes;",
      "    cudaMemcpyAsync(dst_ptr + dst_offset, src_ptr + src_offset,",
      "                    block_size_in_bytes, memcpy_type, stream);",
      "  }",
      "}",
      "",
      "namespace vllm {",
      "",
      "// Grid: (num_layers, num_pairs)",
      "template <typename scalar_t>",
      "__global__ void copy_blocks_kernel(int64_t* key_cache_ptrs,",
      "                                   int64_t* value_cache_ptrs,",
      "                                   const int64_t* __restrict__ block_mapping,",
      "                                   const int numel_per_block) {",
      "  const int layer_idx = blockIdx.x;",
      "  const int pair_idx = blockIdx.y;",
      "",
      "  scalar_t* key_cache = reinterpret_cast<scalar_t*>(key_cache_ptrs[layer_idx]);",
      "  scalar_t* value_cache =",
      "      reinterpret_cast<scalar_t*>(value_cache_ptrs[layer_idx]);",
      "  int64_t src_block_number = block_mapping[2 * pair_idx];",
      "  int64_t dst_block_number = block_mapping[2 * pair_idx + 1];",
      "",
      "  const int64_t src_block_offset = src_block_number * numel_per_block;",
      "  const int64_t dst_block_offset = dst_block_number * numel_per_block;",
      "  for (int i = threadIdx.x; i < numel_per_block; i += blockDim.x) {",
      "    int64_t src_offset = src_block_offset + i;",
      "    int64_t dst_offset = dst_block_offset + i;",
      "    key_cache[dst_offset] = key_cache[src_offset];",
      "  }",
      "  for (int i = threadIdx.x; i < numel_per_block; i += blockDim.x) {",
      "    int64_t src_offset = src_block_offset + i;",
      "    int64_t dst_offset = dst_block_offset + i;",
      "    value_cache[dst_offset] = value_cache[src_offset];",
      "  }",
      "}",
      "",
      "// Kernel for MLA, which works on a single joint kv_cache",
      "// Grid: (num_layers, num_pairs)",
      "template <typename scalar_t>",
      "__global__ void copy_blocks_mla_kernel(",
      "    int64_t* cache_ptrs, const int64_t* __restrict__ block_mapping,",
      "    const int mem_footprint_per_block) {",
      "  const int layer_idx = blockIdx.x;",
      "  const int pair_idx = blockIdx.y;",
      "  scalar_t* cache = reinterpret_cast<scalar_t*>(cache_ptrs[layer_idx]);",
      "  int64_t src_block = block_mapping[2 * pair_idx];",
      "  int64_t dst_block = block_mapping[2 * pair_idx + 1];",
      "  int64_t src_offset = src_block * mem_footprint_per_block;",
      "  int64_t dst_offset = dst_block * mem_footprint_per_block;",
      "  for (int i = threadIdx.x; i < mem_footprint_per_block; i += blockDim.x) {",
      "    cache[dst_offset + i] = cache[src_offset + i];",
      "  }",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "// Note: the key_caches and value_caches vectors are constant but",
      "// not the Tensors they contain. The vectors need to be const refs",
      "// in order to satisfy pytorch's C++ operator registration code.",
      "void copy_blocks(std::vector<torch::Tensor> const& key_caches,",
      "                 std::vector<torch::Tensor> const& value_caches,",
      "                 const torch::Tensor& block_mapping) {",
      "  int num_layers = key_caches.size();",
      "  TORCH_CHECK(num_layers == value_caches.size());",
      "  if (num_layers == 0) {",
      "    return;",
      "  }",
      "  torch::Device cache_device = key_caches[0].device();",
      "  TORCH_CHECK(cache_device.is_cuda());",
      "",
      "  // Create data structures for the kernel.",
      "  // Create an array of pointers to the key and value caches.",
      "  int64_t key_cache_ptrs[num_layers];",
      "  int64_t value_cache_ptrs[num_layers];",
      "  for (int layer_idx = 0; layer_idx < num_layers; ++layer_idx) {",
      "    key_cache_ptrs[layer_idx] =",
      "        reinterpret_cast<int64_t>(key_caches[layer_idx].data_ptr());",
      "    value_cache_ptrs[layer_idx] =",
      "        reinterpret_cast<int64_t>(value_caches[layer_idx].data_ptr());",
      "  }",
      "",
      "  // block_mapping is a 2D tensor with shape (num_pairs, 2).",
      "  int num_pairs = block_mapping.size(0);",
      "",
      "  // Move the data structures to the GPU.",
      "  // NOTE: This synchronizes the CPU and GPU.",
      "  torch::Tensor key_cache_ptrs_tensor =",
      "      torch::from_blob(key_cache_ptrs, {num_layers}, torch::kInt64)",
      "          .to(cache_device);",
      "  torch::Tensor value_cache_ptrs_tensor =",
      "      torch::from_blob(value_cache_ptrs, {num_layers}, torch::kInt64)",
      "          .to(cache_device);",
      "",
      "  // Launch the kernel.",
      "  const int numel_per_block = key_caches[0][0].numel();",
      "  dim3 grid(num_layers, num_pairs);",
      "  dim3 block(std::min(1024, numel_per_block));",
      "  const at::cuda::OptionalCUDAGuard device_guard(cache_device);",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  VLLM_DISPATCH_FLOATING_AND_BYTE_TYPES(",
      "      key_caches[0].scalar_type(), \"copy_blocks_kernel\", ([&] {",
      "        vllm::copy_blocks_kernel<scalar_t><<<grid, block, 0, stream>>>(",
      "            key_cache_ptrs_tensor.data_ptr<int64_t>(),",
      "            value_cache_ptrs_tensor.data_ptr<int64_t>(),",
      "            block_mapping.data_ptr<int64_t>(), numel_per_block);",
      "      }));",
      "}",
      "",
      "// copy blocks kernel for MLA (assumes a joint KV-cache)",
      "void copy_blocks_mla(std::vector<torch::Tensor> const& kv_caches,",
      "                     const torch::Tensor& block_mapping) {",
      "  int num_layers = kv_caches.size();",
      "  if (num_layers == 0) {",
      "    return;",
      "  }",
      "  torch::Device cache_device = kv_caches[0].device();",
      "  TORCH_CHECK(cache_device.is_cuda(), \"kv_cache must be on CUDA\");",
      "",
      "  std::vector<int64_t> cache_ptrs(num_layers);",
      "  for (int layer_idx = 0; layer_idx < num_layers; ++layer_idx) {",
      "    cache_ptrs[layer_idx] =",
      "        reinterpret_cast<int64_t>(kv_caches[layer_idx].data_ptr());",
      "  }",
      "  torch::Tensor cache_ptrs_tensor =",
      "      torch::from_blob(cache_ptrs.data(), {num_layers}, torch::kInt64)",
      "          .to(cache_device);",
      "",
      "  int num_pairs = block_mapping.size(0);",
      "  // We use the stride instead of numel in case the cache is padded for memory",
      "  // alignment reasons, we assume the blocks data (inclusive of any padding)",
      "  // is contiguous in memory",
      "  int mem_footprint_per_block = kv_caches[0].stride(0);",
      "  dim3 grid(num_layers, num_pairs);",
      "  dim3 block(std::min(1024, mem_footprint_per_block));",
      "  const at::cuda::OptionalCUDAGuard device_guard(cache_device);",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  VLLM_DISPATCH_FLOATING_AND_BYTE_TYPES(",
      "      kv_caches[0].scalar_type(), \"copy_blocks_mla_kernel\", ([&] {",
      "        vllm::copy_blocks_mla_kernel<scalar_t><<<grid, block, 0, stream>>>(",
      "            cache_ptrs_tensor.data_ptr<int64_t>(),",
      "            block_mapping.data_ptr<int64_t>(), mem_footprint_per_block);",
      "      }));",
      "}",
      "",
      "namespace vllm {",
      "",
      "template <typename scalar_t, typename cache_t, Fp8KVCacheDataType kv_dt>",
      "__global__ void reshape_and_cache_kernel(",
      "    const scalar_t* __restrict__ key,    // [num_tokens, num_heads, head_size]",
      "    const scalar_t* __restrict__ value,  // [num_tokens, num_heads, head_size]",
      "    cache_t* __restrict__ key_cache,     // [num_blocks, num_heads, head_size/x,",
      "                                         // block_size, x]",
      "    cache_t* __restrict__ value_cache,   // [num_blocks, num_heads, head_size,",
      "                                         // block_size]",
      "    const int64_t* __restrict__ slot_mapping,  // [num_tokens]",
      "    const int key_stride, const int value_stride, const int num_heads,",
      "    const int head_size, const int block_size, const int x,",
      "    const float* k_scale, const float* v_scale) {",
      "  const int64_t token_idx = blockIdx.x;",
      "  const int64_t slot_idx = slot_mapping[token_idx];",
      "  if (slot_idx < 0) {",
      "    // Padding token that should be ignored.",
      "    return;",
      "  }",
      "",
      "  const int64_t block_idx = slot_idx / block_size;",
      "  const int64_t block_offset = slot_idx % block_size;",
      "",
      "  const int n = num_heads * head_size;",
      "  for (int i = threadIdx.x; i < n; i += blockDim.x) {",
      "    const int64_t src_key_idx = token_idx * key_stride + i;",
      "    const int64_t src_value_idx = token_idx * value_stride + i;",
      "",
      "    const int head_idx = i / head_size;",
      "    const int head_offset = i % head_size;",
      "    const int x_idx = head_offset / x;",
      "    const int x_offset = head_offset % x;",
      "",
      "    const int64_t tgt_key_idx =",
      "        block_idx * num_heads * (head_size / x) * block_size * x +",
      "        head_idx * (head_size / x) * block_size * x + x_idx * block_size * x +",
      "        block_offset * x + x_offset;",
      "    const int64_t tgt_value_idx =",
      "        block_idx * num_heads * head_size * block_size +",
      "        head_idx * head_size * block_size + head_offset * block_size +",
      "        block_offset;",
      "    scalar_t tgt_key = key[src_key_idx];",
      "    scalar_t tgt_value = value[src_value_idx];",
      "    if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {",
      "      key_cache[tgt_key_idx] = tgt_key;",
      "      value_cache[tgt_value_idx] = tgt_value;",
      "    } else {",
      "      key_cache[tgt_key_idx] =",
      "          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_key, *k_scale);",
      "      value_cache[tgt_value_idx] =",
      "          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_value, *v_scale);",
      "    }",
      "  }",
      "}",
      "",
      "// Used by vectorization_utils to copy/convert one element",
      "template <typename OutT, typename InT, Fp8KVCacheDataType kv_dt>",
      "struct CopyWithScaleOp {",
      "  float scale;",
      "",
      "  __device__ __forceinline__ void operator()(OutT& dst, const InT src) const {",
      "    if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {",
      "      dst = static_cast<OutT>(src);",
      "    } else {",
      "      dst = fp8::scaled_convert<OutT, InT, kv_dt>(src, scale);",
      "    }",
      "  }",
      "};",
      "",
      "template <typename scalar_t, typename cache_t, Fp8KVCacheDataType kv_dt>",
      "__global__ void reshape_and_cache_flash_kernel(",
      "    const scalar_t* __restrict__ key,    // [num_tokens, num_heads, head_size]",
      "    const scalar_t* __restrict__ value,  // [num_tokens, num_heads, head_size]",
      "    cache_t* __restrict__ key_cache,     // NHD or HND, shape see comments below",
      "    cache_t* __restrict__ value_cache,   // same above",
      "    const int64_t* __restrict__ slot_mapping,  // [num_tokens]",
      "    const int64_t block_stride, const int64_t page_stride,",
      "    const int64_t head_stride, const int64_t key_stride,",
      "    const int64_t value_stride, const int num_heads, const int head_size,",
      "    const int block_size, const float* k_scale, const float* v_scale) {",
      "  const int64_t token_idx = blockIdx.x;",
      "  const int64_t slot_idx = slot_mapping[token_idx];",
      "  // NOTE: slot_idx can be -1 if the token is padded",
      "  if (slot_idx < 0) {",
      "    return;",
      "  }",
      "  const int64_t block_idx = slot_idx / block_size;",
      "  const int64_t block_offset = slot_idx % block_size;",
      "  const int n_elems = num_heads * head_size;",
      "",
      "  // pointers to the beginning of the source row for this token.",
      "  const scalar_t* __restrict__ key_src = key + token_idx * key_stride;",
      "  const scalar_t* __restrict__ value_src = value + token_idx * value_stride;",
      "",
      "  // find the start position inside the kv-cache for this token.",
      "  cache_t* __restrict__ key_dst =",
      "      key_cache + block_idx * block_stride + block_offset * page_stride;",
      "  cache_t* __restrict__ value_dst =",
      "      value_cache + block_idx * block_stride + block_offset * page_stride;",
      "",
      "  // this is true for the NHD layout where `head_stride == head_size`",
      "  const bool is_contiguous_heads = (head_stride == head_size);",
      "",
      "  float k_scale_val = (kv_dt == Fp8KVCacheDataType::kAuto) ? 0.f : *k_scale;",
      "  float v_scale_val = (kv_dt == Fp8KVCacheDataType::kAuto) ? 0.f : *v_scale;",
      "  constexpr int VEC_SIZE = (sizeof(scalar_t) == 2) ? 8 : 4;",
      "  CopyWithScaleOp<cache_t, scalar_t, kv_dt> k_op{k_scale_val};",
      "  CopyWithScaleOp<cache_t, scalar_t, kv_dt> v_op{v_scale_val};",
      "  if (is_contiguous_heads) {",
      "    // NHD layout",
      "    // kv cache: [num_blocks, block_size, num_heads, head_size]",
      "    vectorize_with_alignment<VEC_SIZE>(key_src, key_dst, n_elems, threadIdx.x,",
      "                                       blockDim.x, k_op);",
      "",
      "    vectorize_with_alignment<VEC_SIZE>(value_src, value_dst, n_elems,",
      "                                       threadIdx.x, blockDim.x, v_op);",
      "",
      "  } else {",
      "    // HND layout: heads are strided, but each head_size segment is contiguous",
      "    // kv cache: [num_blocks, num_heads, block_size, head_size]",
      "    const int lane = threadIdx.x & 31;     // 0..31 within warp",
      "    const int warp_id = threadIdx.x >> 5;  // warp index within block",
      "    const int warps_per_block = blockDim.x >> 5;",
      "",
      "    for (int head = warp_id; head < num_heads; head += warps_per_block) {",
      "      const scalar_t* __restrict__ k_src_h = key_src + head * head_size;",
      "      const scalar_t* __restrict__ v_src_h = value_src + head * head_size;",
      "",
      "      cache_t* __restrict__ k_dst_h =",
      "          key_dst + static_cast<int64_t>(head) * head_stride;",
      "      cache_t* __restrict__ v_dst_h =",
      "          value_dst + static_cast<int64_t>(head) * head_stride;",
      "",
      "      // within each head, let the 32 threads of the warp perform the vector",
      "      // copy",
      "      vectorize_with_alignment<VEC_SIZE>(k_src_h, k_dst_h, head_size, lane, 32,",
      "                                         k_op);",
      "",
      "      vectorize_with_alignment<VEC_SIZE>(v_src_h, v_dst_h, head_size, lane, 32,",
      "                                         v_op);",
      "    }",
      "  }",
      "}",
      "",
      "template <typename scalar_t, typename cache_t, Fp8KVCacheDataType kv_dt>",
      "__global__ void concat_and_cache_mla_kernel(",
      "    const scalar_t* __restrict__ kv_c,  // [num_tokens, kv_lora_rank]",
      "    const scalar_t* __restrict__ k_pe,  // [num_tokens, pe_dim]",
      "    cache_t* __restrict__ kv_cache,  // [num_blocks, block_size, (kv_lora_rank",
      "                                     // + pe_dim)]",
      "    const int64_t* __restrict__ slot_mapping,  // [num_tokens]",
      "    const int block_stride,                    //",
      "    const int entry_stride,                    //",
      "    const int kv_c_stride,                     //",
      "    const int k_pe_stride,                     //",
      "    const int kv_lora_rank,                    //",
      "    const int pe_dim,                          //",
      "    const int block_size,                      //",
      "    const float* scale                         //",
      ") {",
      "  const int64_t token_idx = blockIdx.x;",
      "  const int64_t slot_idx = slot_mapping[token_idx];",
      "  // NOTE: slot_idx can be -1 if the token is padded",
      "  if (slot_idx < 0) {",
      "    return;",
      "  }",
      "  const int64_t block_idx = slot_idx / block_size;",
      "  const int64_t block_offset = slot_idx % block_size;",
      "",
      "  auto copy = [&](const scalar_t* __restrict__ src, cache_t* __restrict__ dst,",
      "                  int src_stride, int dst_stride, int size, int offset) {",
      "    for (int i = threadIdx.x; i < size; i += blockDim.x) {",
      "      const int64_t src_idx = token_idx * src_stride + i;",
      "      const int64_t dst_idx =",
      "          block_idx * block_stride + block_offset * entry_stride + i + offset;",
      "      if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {",
      "        dst[dst_idx] = src[src_idx];",
      "      } else {",
      "        dst[dst_idx] =",
      "            fp8::scaled_convert<cache_t, scalar_t, kv_dt>(src[src_idx], *scale);",
      "      }",
      "    }",
      "  };",
      "",
      "  copy(kv_c, kv_cache, kv_c_stride, block_stride, kv_lora_rank, 0);",
      "  copy(k_pe, kv_cache, k_pe_stride, block_stride, pe_dim, kv_lora_rank);",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "// KV_T is the data type of key and value tensors.",
      "// CACHE_T is the stored data type of kv-cache.",
      "// KV_DTYPE is the real data type of kv-cache.",
      "#define CALL_RESHAPE_AND_CACHE(KV_T, CACHE_T, KV_DTYPE)               \\",
      "  vllm::reshape_and_cache_kernel<KV_T, CACHE_T, KV_DTYPE>             \\",
      "      <<<grid, block, 0, stream>>>(                                   \\",
      "          reinterpret_cast<KV_T*>(key.data_ptr()),                    \\",
      "          reinterpret_cast<KV_T*>(value.data_ptr()),                  \\",
      "          reinterpret_cast<CACHE_T*>(key_cache.data_ptr()),           \\",
      "          reinterpret_cast<CACHE_T*>(value_cache.data_ptr()),         \\",
      "          slot_mapping.data_ptr<int64_t>(), key_stride, value_stride, \\",
      "          num_heads, head_size, block_size, x,                        \\",
      "          reinterpret_cast<const float*>(k_scale.data_ptr()),         \\",
      "          reinterpret_cast<const float*>(v_scale.data_ptr()));",
      "",
      "void reshape_and_cache(",
      "    torch::Tensor& key,    // [num_tokens, num_heads, head_size]",
      "    torch::Tensor& value,  // [num_tokens, num_heads, head_size]",
      "    torch::Tensor&",
      "        key_cache,  // [num_blocks, num_heads, head_size/x, block_size, x]",
      "    torch::Tensor&",
      "        value_cache,  // [num_blocks, num_heads, head_size, block_size]",
      "    torch::Tensor& slot_mapping,  // [num_tokens]",
      "    const std::string& kv_cache_dtype, torch::Tensor& k_scale,",
      "    torch::Tensor& v_scale) {",
      "  int num_tokens = slot_mapping.size(0);",
      "  int num_heads = key.size(1);",
      "  int head_size = key.size(2);",
      "  int block_size = key_cache.size(3);",
      "  int x = key_cache.size(4);",
      "",
      "  int key_stride = key.stride(0);",
      "  int value_stride = value.stride(0);",
      "",
      "  dim3 grid(num_tokens);",
      "  dim3 block(std::min(num_heads * head_size, 512));",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(key));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "",
      "  DISPATCH_BY_KV_CACHE_DTYPE(key.dtype(), kv_cache_dtype,",
      "                             CALL_RESHAPE_AND_CACHE)",
      "}",
      "",
      "// KV_T is the data type of key and value tensors.",
      "// CACHE_T is the stored data type of kv-cache.",
      "// KV_DTYPE is the real data type of kv-cache.",
      "#define CALL_RESHAPE_AND_CACHE_FLASH(KV_T, CACHE_T, KV_DTYPE)             \\",
      "  vllm::reshape_and_cache_flash_kernel<KV_T, CACHE_T, KV_DTYPE>           \\",
      "      <<<grid, block, 0, stream>>>(                                       \\",
      "          reinterpret_cast<KV_T*>(key.data_ptr()),                        \\",
      "          reinterpret_cast<KV_T*>(value.data_ptr()),                      \\",
      "          reinterpret_cast<CACHE_T*>(key_cache.data_ptr()),               \\",
      "          reinterpret_cast<CACHE_T*>(value_cache.data_ptr()),             \\",
      "          slot_mapping.data_ptr<int64_t>(), block_stride, page_stride,    \\",
      "          head_stride, key_stride, value_stride, num_heads, head_size,    \\",
      "          block_size, reinterpret_cast<const float*>(k_scale.data_ptr()), \\",
      "          reinterpret_cast<const float*>(v_scale.data_ptr()));",
      "",
      "void reshape_and_cache_flash(",
      "    torch::Tensor& key,        // [num_tokens, num_heads, head_size]",
      "    torch::Tensor& value,      // [num_tokens, num_heads, head_size]",
      "    torch::Tensor& key_cache,  // [num_blocks, block_size, num_heads, head_size]",
      "    torch::Tensor&",
      "        value_cache,  // [num_blocks, block_size, num_heads, head_size]",
      "    torch::Tensor& slot_mapping,  // [num_tokens] or [num_actual_tokens]",
      "    const std::string& kv_cache_dtype, torch::Tensor& k_scale,",
      "    torch::Tensor& v_scale) {",
      "  // NOTE(woosuk): In vLLM V1, key.size(0) can be different from",
      "  // slot_mapping.size(0) because of padding for CUDA graphs.",
      "  // In vLLM V0, key.size(0) is always equal to slot_mapping.size(0) because",
      "  // both include padding.",
      "  // In vLLM V1, however, key.size(0) can be larger than slot_mapping.size(0)",
      "  // since key includes padding for CUDA graphs, while slot_mapping does not.",
      "  // In this case, slot_mapping.size(0) represents the actual number of tokens",
      "  // before padding.",
      "  // For compatibility with both cases, we use slot_mapping.size(0) as the",
      "  // number of tokens.",
      "  int num_tokens = slot_mapping.size(0);",
      "  int num_heads = key.size(1);",
      "  int head_size = key.size(2);",
      "  int block_size = key_cache.size(1);",
      "",
      "  int64_t key_stride = key.stride(0);",
      "  int64_t value_stride = value.stride(0);",
      "  int64_t block_stride = key_cache.stride(0);",
      "  int64_t page_stride = key_cache.stride(1);",
      "  int64_t head_stride = key_cache.stride(2);",
      "  TORCH_CHECK(key_cache.stride(0) == value_cache.stride(0));",
      "",
      "  dim3 grid(num_tokens);",
      "  dim3 block(std::min(num_heads * head_size, 512));",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(key));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "",
      "  DISPATCH_BY_KV_CACHE_DTYPE(key.dtype(), kv_cache_dtype,",
      "                             CALL_RESHAPE_AND_CACHE_FLASH);",
      "}",
      "",
      "// KV_T is the data type of key and value tensors.",
      "// CACHE_T is the stored data type of kv-cache.",
      "// KV_DTYPE is the real data type of kv-cache.",
      "#define CALL_CONCAT_AND_CACHE_MLA(KV_T, CACHE_T, KV_DTYPE)              \\",
      "  vllm::concat_and_cache_mla_kernel<KV_T, CACHE_T, KV_DTYPE>            \\",
      "      <<<grid, block, 0, stream>>>(                                     \\",
      "          reinterpret_cast<KV_T*>(kv_c.data_ptr()),                     \\",
      "          reinterpret_cast<KV_T*>(k_pe.data_ptr()),                     \\",
      "          reinterpret_cast<CACHE_T*>(kv_cache.data_ptr()),              \\",
      "          slot_mapping.data_ptr<int64_t>(), block_stride, entry_stride, \\",
      "          kv_c_stride, k_pe_stride, kv_lora_rank, pe_dim, block_size,   \\",
      "          reinterpret_cast<const float*>(scale.data_ptr()));",
      "",
      "void concat_and_cache_mla(",
      "    torch::Tensor& kv_c,          // [num_tokens, kv_lora_rank]",
      "    torch::Tensor& k_pe,          // [num_tokens, pe_dim]",
      "    torch::Tensor& kv_cache,      // [num_blocks, block_size, (kv_lora_rank +",
      "                                  // pe_dim)]",
      "    torch::Tensor& slot_mapping,  // [num_tokens] or [num_actual_tokens]",
      "    const std::string& kv_cache_dtype, torch::Tensor& scale) {",
      "  // NOTE(woosuk): In vLLM V1, key.size(0) can be different from",
      "  // slot_mapping.size(0) because of padding for CUDA graphs.",
      "  // In vLLM V0, key.size(0) is always equal to slot_mapping.size(0) because",
      "  // both include padding.",
      "  // In vLLM V1, however, key.size(0) can be larger than slot_mapping.size(0)",
      "  // since key includes padding for CUDA graphs, while slot_mapping does not.",
      "  // In this case, slot_mapping.size(0) represents the actual number of tokens",
      "  // before padding.",
      "  // For compatibility with both cases, we use slot_mapping.size(0) as the",
      "  // number of tokens.",
      "  int num_tokens = slot_mapping.size(0);",
      "  int kv_lora_rank = kv_c.size(1);",
      "  int pe_dim = k_pe.size(1);",
      "  int block_size = kv_cache.size(1);",
      "",
      "  TORCH_CHECK(kv_cache.size(2) == kv_lora_rank + pe_dim);",
      "",
      "  int kv_c_stride = kv_c.stride(0);",
      "  int k_pe_stride = k_pe.stride(0);",
      "  int block_stride = kv_cache.stride(0);",
      "  int entry_stride = kv_cache.stride(1);",
      "",
      "  dim3 grid(num_tokens);",
      "  dim3 block(std::min(kv_lora_rank, 512));",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(kv_c));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "",
      "  DISPATCH_BY_KV_CACHE_DTYPE(kv_c.dtype(), kv_cache_dtype,",
      "                             CALL_CONCAT_AND_CACHE_MLA);",
      "}",
      "",
      "namespace vllm {",
      "",
      "template <typename Tout, typename Tin, Fp8KVCacheDataType kv_dt>",
      "__global__ void convert_fp8_kernel(const Tin* __restrict__ src_cache,",
      "                                   Tout* __restrict__ dst_cache,",
      "                                   const float scale,",
      "                                   const int64_t block_stride) {",
      "  const int64_t block_idx = blockIdx.x;",
      "  for (int i = threadIdx.x; i < block_stride; i += blockDim.x) {",
      "    int64_t idx = block_idx * block_stride + i;",
      "    dst_cache[idx] =",
      "        fp8::scaled_convert<Tout, Tin, kv_dt>(src_cache[idx], scale);",
      "  }",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "#define CALL_CONVERT_FP8(Tout, Tin, KV_DTYPE)                                \\",
      "  vllm::convert_fp8_kernel<Tout, Tin, KV_DTYPE><<<grid, block, 0, stream>>>( \\",
      "      reinterpret_cast<Tin*>(src_cache.data_ptr()),                          \\",
      "      reinterpret_cast<Tout*>(dst_cache.data_ptr()), scale, block_stride);",
      "",
      "// Only for testing.",
      "void convert_fp8(torch::Tensor& dst_cache, torch::Tensor& src_cache,",
      "                 const double scale, const std::string& kv_cache_dtype) {",
      "  torch::Device src_device = src_cache.device();",
      "  torch::Device dst_device = dst_cache.device();",
      "  TORCH_CHECK(src_device.is_cuda(), \"src must be on a GPU\")",
      "  TORCH_CHECK(dst_device.is_cuda(), \"dst must be on a GPU\")",
      "  TORCH_CHECK(src_device.index() == dst_device.index(),",
      "              \"src and dst must be on the same GPU\");",
      "  at::cuda::OptionalCUDAGuard device_guard(src_device);",
      "",
      "  int64_t num_blocks = src_cache.size(0);",
      "  int64_t block_stride = src_cache.stride(0);",
      "",
      "  dim3 grid(num_blocks);",
      "  dim3 block(std::min(block_stride, int64_t(512)));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "",
      "  if (kv_cache_dtype == \"auto\") {",
      "    if (src_cache.dtype() == at::ScalarType::Float) {",
      "      CALL_CONVERT_FP8(uint8_t, float, vllm::Fp8KVCacheDataType::kAuto);",
      "    } else if (src_cache.dtype() == at::ScalarType::Half) {",
      "      CALL_CONVERT_FP8(uint8_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);",
      "    } else if (src_cache.dtype() == at::ScalarType::BFloat16) {",
      "      CALL_CONVERT_FP8(uint8_t, __nv_bfloat16, vllm::Fp8KVCacheDataType::kAuto);",
      "    } else if (dst_cache.dtype() == at::ScalarType::Float) {",
      "      CALL_CONVERT_FP8(float, uint8_t, vllm::Fp8KVCacheDataType::kAuto);",
      "    } else if (dst_cache.dtype() == at::ScalarType::Half) {",
      "      CALL_CONVERT_FP8(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kAuto);",
      "    } else if (dst_cache.dtype() == at::ScalarType::BFloat16) {",
      "      CALL_CONVERT_FP8(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kAuto);",
      "    }",
      "  } else if (kv_cache_dtype == \"fp8\" || kv_cache_dtype == \"fp8_e4m3\") {",
      "    if (src_cache.dtype() == at::ScalarType::Float) {",
      "      CALL_CONVERT_FP8(uint8_t, float, vllm::Fp8KVCacheDataType::kFp8E4M3);",
      "    } else if (src_cache.dtype() == at::ScalarType::Half) {",
      "      CALL_CONVERT_FP8(uint8_t, uint16_t, vllm::Fp8KVCacheDataType::kFp8E4M3);",
      "    } else if (src_cache.dtype() == at::ScalarType::BFloat16) {",
      "      CALL_CONVERT_FP8(uint8_t, __nv_bfloat16,",
      "                       vllm::Fp8KVCacheDataType::kFp8E4M3);",
      "    } else if (dst_cache.dtype() == at::ScalarType::Float) {",
      "      CALL_CONVERT_FP8(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);",
      "    } else if (dst_cache.dtype() == at::ScalarType::Half) {",
      "      CALL_CONVERT_FP8(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);",
      "    } else if (dst_cache.dtype() == at::ScalarType::BFloat16) {",
      "      CALL_CONVERT_FP8(__nv_bfloat16, uint8_t,",
      "                       vllm::Fp8KVCacheDataType::kFp8E4M3);",
      "    }",
      "  } else {",
      "    TORCH_CHECK(false, \"Unsupported data type: \", kv_cache_dtype);",
      "  }",
      "}",
      "",
      "namespace vllm {",
      "",
      "// grid is launched with dimensions (batch, num_splits)",
      "template <typename scalar_t, typename cache_t, Fp8KVCacheDataType kv_dt>",
      "__global__ void gather_and_maybe_dequant_cache(",
      "    const cache_t* __restrict__ src_cache,    // [NUM_BLOCKS, BLOCK_SIZE,",
      "                                              // ENTRIES...]",
      "    scalar_t* __restrict__ dst,               // [TOT_TOKENS, ENTRIES...]",
      "    const int32_t* __restrict__ block_table,  // [BATCH, BLOCK_INDICES]",
      "    const int32_t* __restrict__ cu_seq_lens,  // [BATCH+1]",
      "    const int32_t block_size, const int32_t entry_size,",
      "    const int64_t block_table_stride, const int64_t cache_block_stride,",
      "    const int64_t cache_entry_stride, const int64_t dst_entry_stride,",
      "    const float* __restrict__ scale,",
      "    const int32_t* __restrict__ seq_starts) {  // Optional: starting offsets per",
      "                                               // batch",
      "",
      "  const int64_t bid = blockIdx.x;  // Batch ID",
      "  const int32_t num_splits = gridDim.y;",
      "  const int32_t split = blockIdx.y;",
      "  const int32_t seq_start = cu_seq_lens[bid];",
      "  const int32_t seq_end = cu_seq_lens[bid + 1];",
      "  const int32_t seq_len = seq_end - seq_start;",
      "  const int32_t tot_blocks = cuda_utils::ceil_div(seq_len, block_size);",
      "  const int32_t split_blocks = cuda_utils::ceil_div(tot_blocks, num_splits);",
      "",
      "  const int32_t split_start = split * split_blocks;",
      "  const int32_t split_end = min((split + 1) * split_blocks, tot_blocks);",
      "",
      "  const bool is_active_split = (split_start < tot_blocks);",
      "  const bool is_last_split = (split_end == tot_blocks);",
      "",
      "  if (!is_active_split) return;",
      "",
      "  int32_t full_blocks_end = split_end;",
      "  int32_t partial_block_size = 0;",
      "",
      "  // Adjust the pointer for the block_table for this batch.",
      "  // If seq_starts is provided, compute an offset based on (seq_starts[bid] /",
      "  // page_size)",
      "  const int32_t batch_offset = bid * block_table_stride;",
      "  int32_t offset = 0;",
      "  if (seq_starts != nullptr) {",
      "    offset = seq_starts[bid] / block_size;",
      "  }",
      "  const int32_t* batch_block_table = block_table + batch_offset + offset;",
      "",
      "  // Adjust dst pointer based on the cumulative sequence lengths.",
      "  dst += seq_start * dst_entry_stride;",
      "",
      "  if (is_last_split) {",
      "    partial_block_size = seq_len % block_size;",
      "    if (partial_block_size) full_blocks_end -= 1;",
      "  }",
      "",
      "  auto copy_entry = [&](const cache_t* __restrict__ _src,",
      "                        scalar_t* __restrict__ _dst) {",
      "    for (int i = threadIdx.x; i < entry_size; i += blockDim.x) {",
      "      if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {",
      "        _dst[i] = static_cast<scalar_t>(_src[i]);",
      "      } else {",
      "        _dst[i] =",
      "            fp8::scaled_convert<scalar_t, cache_t, kv_dt>(_src[i], *scale);",
      "      }",
      "    }",
      "  };",
      "",
      "  for (int pid = split_start; pid < full_blocks_end; ++pid) {",
      "    auto block_id = batch_block_table[pid];",
      "    auto block_start_ptr = src_cache + block_id * cache_block_stride;",
      "    auto block_dst_ptr = dst + pid * block_size * dst_entry_stride;",
      "    for (int eid = 0; eid < block_size; ++eid) {",
      "      copy_entry(block_start_ptr + eid * cache_entry_stride,",
      "                 block_dst_ptr + eid * dst_entry_stride);",
      "    }",
      "  }",
      "",
      "  if (partial_block_size) {",
      "    auto block_id = batch_block_table[full_blocks_end];",
      "    auto block_start_ptr = src_cache + block_id * cache_block_stride;",
      "    auto block_dst_ptr = dst + full_blocks_end * block_size * dst_entry_stride;",
      "    for (int eid = 0; eid < partial_block_size; ++eid) {",
      "      copy_entry(block_start_ptr + eid * cache_entry_stride,",
      "                 block_dst_ptr + eid * dst_entry_stride);",
      "    }",
      "  }",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "// Macro to dispatch the kernel based on the data type.",
      "// SCALAR_T is the data type of the destination tensor.",
      "// CACHE_T is the stored data type of kv-cache.",
      "// KV_DTYPE is the real data type of kv-cache.",
      "#define CALL_GATHER_CACHE(SCALAR_T, CACHE_T, KV_DTYPE)                      \\",
      "  vllm::gather_and_maybe_dequant_cache<SCALAR_T, CACHE_T, KV_DTYPE>         \\",
      "      <<<grid, block, 0, stream>>>(                                         \\",
      "          reinterpret_cast<CACHE_T*>(src_cache.data_ptr()),                 \\",
      "          reinterpret_cast<SCALAR_T*>(dst.data_ptr()),                      \\",
      "          block_table.data_ptr<int32_t>(), cu_seq_lens.data_ptr<int32_t>(), \\",
      "          block_size, entry_size, block_table_stride, cache_block_stride,   \\",
      "          cache_entry_stride, dst_entry_stride,                             \\",
      "          reinterpret_cast<const float*>(scale.data_ptr()), seq_starts_ptr);",
      "",
      "// Gather sequences from the cache into the destination tensor.",
      "//  - cu_seq_lens contains the cumulative sequence lengths for each batch",
      "//  - block_table contains the cache block indices for each sequence",
      "//  - Optionally, seq_starts (if provided) offsets the starting block index by",
      "//  (seq_starts[bid] / page_size)",
      "void gather_and_maybe_dequant_cache(",
      "    torch::Tensor const& src_cache,    // [NUM_BLOCKS, BLOCK_SIZE, ENTRIES...]",
      "    torch::Tensor const& dst,          // [TOT_TOKENS, ENTRIES...]",
      "    torch::Tensor const& block_table,  // [BATCH, BLOCK_INDICES]",
      "    torch::Tensor const& cu_seq_lens,  // [BATCH+1]",
      "    int64_t batch_size, const std::string& kv_cache_dtype,",
      "    torch::Tensor const& scale,",
      "    std::optional<torch::Tensor> seq_starts = std::nullopt) {",
      "  at::cuda::OptionalCUDAGuard device_guard(src_cache.device());",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "",
      "  int32_t block_size = src_cache.size(1);",
      "  int32_t entry_size = src_cache.flatten(2, -1).size(2);",
      "",
      "  TORCH_CHECK(block_table.dtype() == torch::kInt32,",
      "              \"block_table must be int32\");",
      "  TORCH_CHECK(cu_seq_lens.dtype() == torch::kInt32,",
      "              \"cu_seq_lens must be int32\");",
      "  if (seq_starts.has_value()) {",
      "    TORCH_CHECK(seq_starts.value().dtype() == torch::kInt32,",
      "                \"seq_starts must be int32\");",
      "  }",
      "",
      "  TORCH_CHECK(src_cache.device() == dst.device(),",
      "              \"src_cache and dst must be on the same device\");",
      "  TORCH_CHECK(src_cache.device() == block_table.device(),",
      "              \"src_cache and block_table must be on the same device\");",
      "  TORCH_CHECK(src_cache.device() == cu_seq_lens.device(),",
      "              \"src_cache and cu_seq_lens must be on the same device\");",
      "  if (seq_starts.has_value()) {",
      "    TORCH_CHECK(src_cache.device() == seq_starts.value().device(),",
      "                \"src_cache and seq_starts must be on the same device\");",
      "  }",
      "",
      "  int64_t block_table_stride = block_table.stride(0);",
      "  int64_t cache_block_stride = src_cache.stride(0);",
      "  int64_t cache_entry_stride = src_cache.stride(1);",
      "  int64_t dst_entry_stride = dst.stride(0);",
      "",
      "  // Decide on the number of splits based on the batch size.",
      "  int num_splits = batch_size > 128 ? 2 : batch_size > 64 ? 4 : 16;",
      "  dim3 grid(batch_size, num_splits);",
      "  dim3 block(1024);",
      "",
      "  const int32_t* seq_starts_ptr =",
      "      seq_starts.has_value() ? seq_starts.value().data_ptr<int32_t>() : nullptr;",
      "",
      "  DISPATCH_BY_KV_CACHE_DTYPE(dst.dtype(), kv_cache_dtype, CALL_GATHER_CACHE);",
      "}",
      "",
      "namespace vllm {",
      "template <typename scalar_t>",
      "// Note(hc): The cp_gather_cache allows seq_starts to no longer be divisible by",
      "// block_size.",
      "__global__ void cp_gather_cache(",
      "    const scalar_t* __restrict__ src_cache,   // [NUM_BLOCKS, BLOCK_SIZE,",
      "                                              // ENTRY_SIZE]",
      "    scalar_t* __restrict__ dst,               // [TOT_TOKENS, ENTRY_SIZE]",
      "    const int32_t* __restrict__ block_table,  // [BATCH, BLOCK_INDICES]",
      "    const int32_t* __restrict__ cu_seq_lens,  // [BATCH+1]",
      "    const int32_t block_size, const int32_t entry_size,",
      "    const int64_t block_table_stride, const int64_t cache_block_stride,",
      "    const int64_t cache_entry_stride, const int64_t dst_entry_stride,",
      "    const int32_t* __restrict__ seq_starts  // Optional: starting offsets per",
      "                                            // batch",
      ") {",
      "  const int64_t bid = blockIdx.x;  // Batch ID",
      "  const int32_t num_splits = gridDim.y;",
      "  const int32_t split = blockIdx.y;",
      "  const int32_t seq_start = cu_seq_lens[bid];",
      "  const int32_t seq_end = cu_seq_lens[bid + 1];",
      "  const int32_t seq_len = seq_end - seq_start;",
      "  const int32_t tot_slots = seq_len;",
      "  const int32_t split_slots = cuda_utils::ceil_div(tot_slots, num_splits);",
      "",
      "  const int32_t split_start = split * split_slots;",
      "  const int32_t split_end = min((split + 1) * split_slots, tot_slots);",
      "",
      "  const bool is_active_split = (split_start < tot_slots);",
      "",
      "  if (!is_active_split) return;",
      "",
      "  // Adjust the pointer for the block_table for this batch.",
      "  // If seq_starts is provided, compute an offset based on it",
      "  const int32_t batch_offset = bid * block_table_stride;",
      "  int32_t offset = split_start;",
      "  if (seq_starts != nullptr) {",
      "    offset += seq_starts[bid];",
      "  }",
      "  int32_t offset_div = offset / block_size;",
      "  offset = offset % block_size;",
      "  const int32_t* batch_block_table = block_table + batch_offset;",
      "",
      "  // Adjust dst pointer based on the cumulative sequence lengths.",
      "  dst += seq_start * dst_entry_stride;",
      "",
      "  auto copy_entry = [&](const scalar_t* __restrict__ _src,",
      "                        scalar_t* __restrict__ _dst) {",
      "    for (int i = threadIdx.x; i < entry_size; i += blockDim.x)",
      "      _dst[i] = _src[i];",
      "  };",
      "",
      "  for (int pid = split_start; pid < split_end; ++pid) {",
      "    auto block_id = batch_block_table[offset_div];",
      "    auto block_start_ptr = src_cache + block_id * cache_block_stride;",
      "    auto block_dst_ptr = dst + pid * dst_entry_stride;",
      "    copy_entry(block_start_ptr + offset * cache_entry_stride, block_dst_ptr);",
      "    offset += 1;",
      "    // bump to next block",
      "    if (offset == block_size) {",
      "      offset_div += 1;",
      "      offset = 0;",
      "    }",
      "  }",
      "}",
      "}  // namespace vllm",
      "",
      "// Macro to dispatch the kernel based on the data type.",
      "#define CALL_CP_GATHER_CACHE(CPY_DTYPE)                                 \\",
      "  vllm::cp_gather_cache<CPY_DTYPE><<<grid, block, 0, stream>>>(         \\",
      "      reinterpret_cast<CPY_DTYPE*>(src_cache.data_ptr()),               \\",
      "      reinterpret_cast<CPY_DTYPE*>(dst.data_ptr()),                     \\",
      "      block_table.data_ptr<int32_t>(), cu_seq_lens.data_ptr<int32_t>(), \\",
      "      block_size, entry_size, block_table_stride, cache_block_stride,   \\",
      "      cache_entry_stride, dst_entry_stride, seq_starts_ptr);",
      "",
      "// Gather sequences from the cache into the destination tensor.",
      "//  - cu_seq_lens contains the cumulative sequence lengths for each batch",
      "//  - block_table contains the cache block indices for each sequence",
      "//  - Optionally, seq_starts (if provided) offsets the starting slot index by",
      "//  seq_starts[bid]",
      "void cp_gather_cache(",
      "    torch::Tensor const& src_cache,    // [NUM_BLOCKS, BLOCK_SIZE, ENTRIES...]",
      "    torch::Tensor const& dst,          // [TOT_TOKENS, ENTRIES...]",
      "    torch::Tensor const& block_table,  // [BATCH, BLOCK_INDICES]",
      "    torch::Tensor const& cu_seq_lens,  // [BATCH+1]",
      "    int64_t batch_size,",
      "    std::optional<torch::Tensor> seq_starts = std::nullopt) {",
      "  at::cuda::OptionalCUDAGuard device_guard(src_cache.device());",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "",
      "  int32_t block_size = src_cache.size(1);",
      "  int32_t entry_size = src_cache.flatten(2, -1).size(2);",
      "",
      "  TORCH_CHECK(block_table.dtype() == torch::kInt32,",
      "              \"block_table must be int32\");",
      "  TORCH_CHECK(cu_seq_lens.dtype() == torch::kInt32,",
      "              \"cu_seq_lens must be int32\");",
      "  if (seq_starts.has_value()) {",
      "    TORCH_CHECK(seq_starts.value().dtype() == torch::kInt32,",
      "                \"seq_starts must be int32\");",
      "  }",
      "",
      "  TORCH_CHECK(src_cache.device() == dst.device(),",
      "              \"src_cache and dst must be on the same device\");",
      "  TORCH_CHECK(src_cache.device() == block_table.device(),",
      "              \"src_cache and block_table must be on the same device\");",
      "  TORCH_CHECK(src_cache.device() == cu_seq_lens.device(),",
      "              \"src_cache and cu_seq_lens must be on the same device\");",
      "  if (seq_starts.has_value()) {",
      "    TORCH_CHECK(src_cache.device() == seq_starts.value().device(),",
      "                \"src_cache and seq_starts must be on the same device\");",
      "  }",
      "",
      "  int64_t block_table_stride = block_table.stride(0);",
      "  int64_t cache_block_stride = src_cache.stride(0);",
      "  int64_t cache_entry_stride = src_cache.stride(1);",
      "  int64_t dst_entry_stride = dst.stride(0);",
      "",
      "  // Decide on the number of splits based on the batch size.",
      "  int num_splits = batch_size > 128 ? 2 : batch_size > 64 ? 4 : 16;",
      "  dim3 grid(batch_size, num_splits);",
      "  dim3 block(1024);",
      "",
      "  TORCH_CHECK(src_cache.dtype() == dst.dtype(),",
      "              \"src_cache and dst must have the same dtype\");",
      "",
      "  const int dtype_bits = src_cache.element_size() * 8;",
      "  const int32_t* seq_starts_ptr =",
      "      seq_starts.has_value() ? seq_starts.value().data_ptr<int32_t>() : nullptr;",
      "",
      "  if (dtype_bits == 32) {",
      "    CALL_CP_GATHER_CACHE(uint32_t);",
      "  } else if (dtype_bits == 16) {",
      "    CALL_CP_GATHER_CACHE(uint16_t);",
      "  } else if (dtype_bits == 8) {",
      "    CALL_CP_GATHER_CACHE(uint8_t);",
      "  } else {",
      "    TORCH_CHECK(false, \"Unsupported data type width: \", dtype_bits);",
      "  }",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/custom_quickreduce.cu",
    "source": [
      "#include <ATen/cuda/Exceptions.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <c10/cuda/CUDAStream.h>",
      "#include <torch/all.h>",
      "",
      "#ifdef USE_ROCM",
      "",
      "  #include \"quickreduce/quick_reduce.h\"",
      "",
      "quickreduce::fptr_t init_custom_qr(int64_t rank, int64_t world_size,",
      "                                   std::optional<int64_t> qr_max_size) {",
      "  if (world_size > 8)",
      "    throw std::invalid_argument(\"world size > 8 is not supported\");",
      "  if (world_size == 6)",
      "    throw std::invalid_argument(\"world size == 6 is not supported\");",
      "  if (world_size % 2 != 0)",
      "    throw std::invalid_argument(\"Odd num gpus is not supported for now\");",
      "  if (rank < 0 || rank >= world_size)",
      "    throw std::invalid_argument(\"invalid rank passed in\");",
      "  quickreduce::DeviceComms* fptr = new quickreduce::DeviceComms();",
      "  fptr->init(world_size, rank, qr_max_size);",
      "  return (quickreduce::fptr_t)fptr;",
      "}",
      "",
      "void qr_destroy(quickreduce::fptr_t _fa) {",
      "  if (_fa) {",
      "    auto fa = reinterpret_cast<quickreduce::DeviceComms*>(_fa);",
      "    fa->destroy();",
      "    delete fa;",
      "  }",
      "}",
      "",
      "torch::Tensor qr_get_handle(quickreduce::fptr_t _fa) {",
      "  auto fa = reinterpret_cast<quickreduce::DeviceComms*>(_fa);",
      "  hipIpcMemHandle_t handle = fa->get_handle();",
      "  auto options =",
      "      torch::TensorOptions().dtype(torch::kUInt8).device(torch::kCPU);",
      "  auto data_handle =",
      "      torch::empty({static_cast<int64_t>(sizeof(hipIpcMemHandle_t))}, options);",
      "  std::memcpy(data_handle.data_ptr(), &handle, sizeof(hipIpcMemHandle_t));",
      "  return data_handle;",
      "}",
      "",
      "void qr_open_handles(quickreduce::fptr_t _fa,",
      "                     const std::vector<torch::Tensor>& handles) {",
      "  auto fa = reinterpret_cast<quickreduce::DeviceComms*>(_fa);",
      "  std::vector<hipIpcMemHandle_t> ipc_handles;",
      "  ipc_handles.reserve(handles.size());",
      "  for (auto& handle : handles) {",
      "    // Ensure the tensor is on the same device as the current device.",
      "    hipIpcMemHandle_t ipc_handle;",
      "    std::memcpy(&ipc_handle, handle.data_ptr(), sizeof(hipIpcMemHandle_t));",
      "    ipc_handles.push_back(ipc_handle);",
      "  }",
      "  fa->open_ipc_handles(ipc_handles);",
      "}",
      "",
      "void qr_all_reduce(quickreduce::fptr_t _fa, torch::Tensor& inp,",
      "                   torch::Tensor& out, int64_t quant_level, bool cast_bf2half) {",
      "  auto fa = reinterpret_cast<quickreduce::DeviceComms*>(_fa);",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(inp));",
      "  auto stream = at::cuda::getCurrentHIPStreamMasqueradingAsCUDA();",
      "",
      "  TORCH_CHECK_EQ(inp.scalar_type(), out.scalar_type());",
      "  TORCH_CHECK_EQ(inp.numel(), out.numel());",
      "  TORCH_CHECK_LE(out.numel(), fa->kMaxProblemSize);",
      "  if (out.scalar_type() == at::ScalarType::Half) {",
      "    fa->allreduce<half, false>(reinterpret_cast<half*>(inp.data_ptr()),",
      "                               reinterpret_cast<half*>(out.data_ptr()),",
      "                               out.numel(), quant_level, stream);",
      "  } else if (out.scalar_type() == at::ScalarType::BFloat16) {",
      "    if (cast_bf2half) {",
      "      fa->allreduce<half, true>(reinterpret_cast<half*>(inp.data_ptr()),",
      "                                reinterpret_cast<half*>(out.data_ptr()),",
      "                                out.numel(), quant_level, stream);",
      "    } else {",
      "      fa->allreduce<quickreduce::nv_bfloat16, false>(",
      "          reinterpret_cast<quickreduce::nv_bfloat16*>(inp.data_ptr()),",
      "          reinterpret_cast<quickreduce::nv_bfloat16*>(out.data_ptr()),",
      "          out.numel(), quant_level, stream);",
      "    }",
      "  } else {",
      "    throw std::runtime_error(",
      "        \"quick allreduce only supports float16 and bfloat16\");",
      "  }",
      "}",
      "",
      "int64_t qr_max_size() {",
      "  // The default is 2GB (2,147,483,648 bytes)",
      "  return static_cast<int64_t>(std::numeric_limits<int32_t>::max()) + 1;",
      "}",
      "",
      "  #define INSTANTIATE_FOR_WORLDSIZE(T, Codec, cast_bf2half)       \\",
      "    template struct quickreduce::AllReduceTwoshot<T, Codec<T, 2>, \\",
      "                                                  cast_bf2half>;  \\",
      "    template struct quickreduce::AllReduceTwoshot<T, Codec<T, 4>, \\",
      "                                                  cast_bf2half>;  \\",
      "    template struct quickreduce::AllReduceTwoshot<T, Codec<T, 8>, cast_bf2half>;",
      "",
      "INSTANTIATE_FOR_WORLDSIZE(quickreduce::nv_bfloat16, quickreduce::CodecFP, false)",
      "INSTANTIATE_FOR_WORLDSIZE(quickreduce::nv_bfloat16, quickreduce::CodecQ4, false)",
      "INSTANTIATE_FOR_WORLDSIZE(quickreduce::nv_bfloat16, quickreduce::CodecQ6, false)",
      "INSTANTIATE_FOR_WORLDSIZE(quickreduce::nv_bfloat16, quickreduce::CodecQ8, false)",
      "INSTANTIATE_FOR_WORLDSIZE(quickreduce::nv_bfloat16, quickreduce::CodecFP, true)",
      "INSTANTIATE_FOR_WORLDSIZE(quickreduce::nv_bfloat16, quickreduce::CodecQ4, true)",
      "INSTANTIATE_FOR_WORLDSIZE(quickreduce::nv_bfloat16, quickreduce::CodecQ6, true)",
      "INSTANTIATE_FOR_WORLDSIZE(quickreduce::nv_bfloat16, quickreduce::CodecQ8, true)",
      "",
      "INSTANTIATE_FOR_WORLDSIZE(half, quickreduce::CodecFP, false)",
      "INSTANTIATE_FOR_WORLDSIZE(half, quickreduce::CodecQ4, false)",
      "INSTANTIATE_FOR_WORLDSIZE(half, quickreduce::CodecQ6, false)",
      "INSTANTIATE_FOR_WORLDSIZE(half, quickreduce::CodecQ8, false)",
      "",
      "#endif  // USE_ROCM"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/permute_cols.cu",
    "source": [
      "#include <torch/all.h>",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#include <cuda_fp16.h>",
      "",
      "static constexpr int default_threads = 256;",
      "static constexpr int div_ceil(int a, int b) { return (a + b - 1) / b; }",
      "",
      "// For a given \"a\" of size [M,K] performs a permutation of the K columns based",
      "// on the given \"perm\" indices.",
      "// Currently only supports 16bit types (since we permute half types)",
      "__global__ void permute_cols_kernel(int4 const* __restrict__ a_int4_ptr,",
      "                                    int const* __restrict__ perm_int_ptr,",
      "                                    int4* __restrict__ out_int4_ptr, int size_m,",
      "                                    int size_k, int block_rows) {",
      "  int start_row = block_rows * blockIdx.x;",
      "  int finish_row = start_row + block_rows;",
      "  if (finish_row > size_m) {",
      "    finish_row = size_m;",
      "  }",
      "  int cur_block_rows = std::max(finish_row - start_row, 0);",
      "",
      "  int row_stride = size_k * sizeof(half) / 16;",
      "",
      "  auto permute_row = [&](int row) {",
      "    int iters = size_k / default_threads;",
      "    int rest = size_k % default_threads;",
      "",
      "    int offset = row * row_stride;",
      "",
      "    half const* a_row_half = reinterpret_cast<half const*>(a_int4_ptr + offset);",
      "    half* out_half = reinterpret_cast<half*>(out_int4_ptr + offset);",
      "",
      "    int base_k = 0;",
      "",
      "    for (int i = 0; i < iters; i++) {",
      "      int cur_k = base_k + threadIdx.x;",
      "      int src_pos = perm_int_ptr[cur_k];",
      "",
      "      out_half[cur_k] = a_row_half[src_pos];",
      "",
      "      base_k += default_threads;",
      "    }",
      "",
      "    if (rest) {",
      "      if (threadIdx.x < rest) {",
      "        int cur_k = base_k + threadIdx.x;",
      "        int src_pos = perm_int_ptr[cur_k];",
      "",
      "        out_half[cur_k] = a_row_half[src_pos];",
      "      }",
      "    }",
      "  };",
      "",
      "  for (int i = 0; i < cur_block_rows; i++) {",
      "    int cur_row = start_row + i;",
      "    if (cur_row < size_m) {",
      "      permute_row(cur_row);",
      "    }",
      "  }",
      "}",
      "",
      "// More efficient version of A[..., perm]",
      "//  taken from gptq_marlin.cu",
      "torch::Tensor permute_cols(torch::Tensor const& A, torch::Tensor const& perm) {",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(A));",
      "  auto dev = A.get_device();",
      "  auto stream = at::cuda::getCurrentCUDAStream(dev);",
      "",
      "  TORCH_CHECK(A.scalar_type() == at::kHalf || A.scalar_type() == at::kBFloat16,",
      "              \"Currently only 16bit types are supported\");",
      "  TORCH_CHECK(A.is_contiguous(), \"A must be contiguous\");",
      "  TORCH_CHECK(A.size(-1) % 8 == 0,",
      "              \"A columns must be a multiple of 8 (128bits)\");",
      "  auto A_2d = A.view({-1, A.size(-1)});",
      "",
      "  torch::Tensor D = torch::empty_like(A);",
      "  int sms;",
      "  cudaDeviceGetAttribute(&sms, cudaDevAttrMultiProcessorCount, dev);",
      "  int block_rows = div_ceil(A_2d.size(0), sms);",
      "  permute_cols_kernel<<<sms, default_threads, 0, stream>>>(",
      "      reinterpret_cast<int4 const*>(A_2d.const_data_ptr()),",
      "      perm.const_data_ptr<int>(), reinterpret_cast<int4*>(D.mutable_data_ptr()),",
      "      A_2d.size(0), A_2d.size(1), block_rows);",
      "  return D;",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/pos_encoding_kernels.cu",
    "source": [
      "#include <torch/all.h>",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#include \"cuda_compat.h\"",
      "#include \"dispatch_utils.h\"",
      "",
      "namespace vllm {",
      "",
      "template <typename scalar_t, bool IS_NEOX>",
      "inline __device__ void apply_token_rotary_embedding(",
      "    scalar_t* __restrict__ arr, const scalar_t* __restrict__ cos_ptr,",
      "    const scalar_t* __restrict__ sin_ptr, int rot_offset, int embed_dim) {",
      "  int x_index, y_index;",
      "  scalar_t cos, sin;",
      "  if (IS_NEOX) {",
      "    // GPT-NeoX style rotary embedding.",
      "    x_index = rot_offset;",
      "    y_index = embed_dim + rot_offset;",
      "    cos = VLLM_LDG(cos_ptr + x_index);",
      "    sin = VLLM_LDG(sin_ptr + x_index);",
      "  } else {",
      "    // GPT-J style rotary embedding.",
      "    x_index = 2 * rot_offset;",
      "    y_index = 2 * rot_offset + 1;",
      "    cos = VLLM_LDG(cos_ptr + x_index / 2);",
      "    sin = VLLM_LDG(sin_ptr + x_index / 2);",
      "  }",
      "",
      "  const scalar_t x = arr[x_index];",
      "  const scalar_t y = arr[y_index];",
      "  arr[x_index] = x * cos - y * sin;",
      "  arr[y_index] = y * cos + x * sin;",
      "}",
      "",
      "template <typename scalar_t, bool IS_NEOX>",
      "inline __device__ void apply_rotary_embedding(",
      "    scalar_t* __restrict__ query,  // [batch_size, seq_len, num_heads,",
      "                                   // head_size] or [num_tokens, num_heads,",
      "                                   // head_size]",
      "    scalar_t* __restrict__ key,    // nullptr or",
      "                                   // [batch_size, seq_len, num_kv_heads,",
      "                                   // head_size] or [num_tokens, num_kv_heads,",
      "                                   // head_size]",
      "    const scalar_t* cache_ptr, const int head_size, const int num_heads,",
      "    const int num_kv_heads, const int rot_dim, const int token_idx,",
      "    const int64_t query_stride, const int64_t key_stride,",
      "    const int64_t head_stride) {",
      "  const int embed_dim = rot_dim / 2;",
      "  const scalar_t* cos_ptr = cache_ptr;",
      "  const scalar_t* sin_ptr = cache_ptr + embed_dim;",
      "",
      "  const int nq = num_heads * embed_dim;",
      "  for (int i = threadIdx.x; i < nq; i += blockDim.x) {",
      "    const int head_idx = i / embed_dim;",
      "    const int64_t token_head =",
      "        token_idx * query_stride + head_idx * head_stride;",
      "    const int rot_offset = i % embed_dim;",
      "    apply_token_rotary_embedding<scalar_t, IS_NEOX>(",
      "        query + token_head, cos_ptr, sin_ptr, rot_offset, embed_dim);",
      "  }",
      "",
      "  if (key != nullptr) {",
      "    const int nk = num_kv_heads * embed_dim;",
      "    for (int i = threadIdx.x; i < nk; i += blockDim.x) {",
      "      const int head_idx = i / embed_dim;",
      "      const int64_t token_head =",
      "          token_idx * key_stride + head_idx * head_stride;",
      "      const int rot_offset = i % embed_dim;",
      "      apply_token_rotary_embedding<scalar_t, IS_NEOX>(",
      "          key + token_head, cos_ptr, sin_ptr, rot_offset, embed_dim);",
      "    }",
      "  }",
      "}",
      "",
      "template <typename scalar_t, bool IS_NEOX>",
      "__global__ void rotary_embedding_kernel(",
      "    const int64_t* __restrict__ positions,  // [batch_size, seq_len] or",
      "                                            // [num_tokens]",
      "    scalar_t* __restrict__ query,           // [batch_size, seq_len, num_heads,",
      "                                   // head_size] or [num_tokens, num_heads,",
      "                                   // head_size]",
      "    scalar_t* __restrict__ key,  // nullptr or",
      "                                 // [batch_size, seq_len, num_kv_heads,",
      "                                 // head_size] or [num_tokens, num_kv_heads,",
      "                                 // head_size]",
      "    const scalar_t* __restrict__ cos_sin_cache,  // [max_position, 2, rot_dim //",
      "                                                 // 2]",
      "    const int rot_dim, const int64_t query_stride, const int64_t key_stride,",
      "    const int64_t head_stride, const int num_heads, const int num_kv_heads,",
      "    const int head_size) {",
      "  // Each thread block is responsible for one token.",
      "  const int token_idx = blockIdx.x;",
      "  int64_t pos = positions[token_idx];",
      "  const scalar_t* cache_ptr = cos_sin_cache + pos * rot_dim;",
      "",
      "  apply_rotary_embedding<scalar_t, IS_NEOX>(",
      "      query, key, cache_ptr, head_size, num_heads, num_kv_heads, rot_dim,",
      "      token_idx, query_stride, key_stride, head_stride);",
      "}",
      "",
      "template <typename scalar_t, bool IS_NEOX>",
      "__global__ void batched_rotary_embedding_kernel(",
      "    const int64_t* __restrict__ positions,  // [batch_size, seq_len] or",
      "                                            // [num_tokens]",
      "    scalar_t* __restrict__ query,           // [batch_size, seq_len, num_heads,",
      "                                   // head_size] or [num_tokens, num_heads,",
      "                                   // head_size]",
      "    scalar_t* __restrict__ key,  // nullptr or",
      "                                 // [batch_size, seq_len, num_kv_heads,",
      "                                 // head_size] or [num_tokens, num_kv_heads,",
      "                                 // head_size]",
      "    const scalar_t* __restrict__ cos_sin_cache,  // [max_position, 2, rot_dim //",
      "                                                 // 2]",
      "    const int64_t* __restrict__ cos_sin_cache_offsets,  // [batch_size, seq_len]",
      "    const int rot_dim, const int64_t query_stride, const int64_t key_stride,",
      "    const int64_t head_stride, const int num_heads, const int num_kv_heads,",
      "    const int head_size) {",
      "  // Each thread block is responsible for one token.",
      "  const int token_idx = blockIdx.x;",
      "  int64_t pos = positions[token_idx];",
      "  int64_t cos_sin_cache_offset = cos_sin_cache_offsets[token_idx];",
      "  const scalar_t* cache_ptr =",
      "      cos_sin_cache + (cos_sin_cache_offset + pos) * rot_dim;",
      "",
      "  apply_rotary_embedding<scalar_t, IS_NEOX>(",
      "      query, key, cache_ptr, head_size, num_heads, num_kv_heads, rot_dim,",
      "      token_idx, query_stride, key_stride, head_stride);",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "void rotary_embedding(",
      "    torch::Tensor& positions,  // [batch_size, seq_len] or [num_tokens]",
      "    torch::Tensor& query,  // [batch_size, seq_len, num_heads * head_size] or",
      "                           // [num_tokens, num_heads * head_size] or",
      "                           // [batch_size, seq_len, num_heads, head_size] or",
      "                           // [num_tokens, num_heads, head_size]",
      "    std::optional<torch::Tensor> key,",
      "    // null or",
      "    // [batch_size, seq_len, num_kv_heads * head_size] or",
      "    // [num_tokens, num_kv_heads * head_size] or",
      "    // [batch_size, seq_len, num_heads, head_size] or",
      "    // [num_tokens, num_heads, head_size]",
      "    int64_t head_size,",
      "    torch::Tensor& cos_sin_cache,  // [max_position, rot_dim]",
      "    bool is_neox) {",
      "  // num_tokens = batch_size * seq_len",
      "  int64_t num_tokens = positions.numel();",
      "  int positions_ndim = positions.dim();",
      "",
      "  // Make sure num_tokens dim is consistent across positions, query, and key",
      "  TORCH_CHECK(",
      "      positions_ndim == 1 || positions_ndim == 2,",
      "      \"positions must have shape [num_tokens] or [batch_size, seq_len]\");",
      "  if (positions_ndim == 1) {",
      "    TORCH_CHECK(query.size(0) == positions.size(0) &&",
      "                    (!key.has_value() || key->size(0) == positions.size(0)),",
      "                \"query, key and positions must have the same number of tokens\");",
      "  }",
      "  if (positions_ndim == 2) {",
      "    TORCH_CHECK(",
      "        query.size(0) == positions.size(0) &&",
      "            (!key.has_value() || key->size(0) == positions.size(0)) &&",
      "            query.size(1) == positions.size(1) &&",
      "            (!key.has_value() || key->size(1) == positions.size(1)),",
      "        \"query, key and positions must have the same batch_size and seq_len\");",
      "  }",
      "",
      "  // Make sure head_size is valid for query and key",
      "  // hidden_size = num_heads * head_size",
      "  int query_hidden_size = query.numel() / num_tokens;",
      "  int key_hidden_size = key.has_value() ? key->numel() / num_tokens : 0;",
      "  TORCH_CHECK(query_hidden_size % head_size == 0);",
      "  TORCH_CHECK(key_hidden_size % head_size == 0);",
      "",
      "  // Make sure query and key have consistent number of heads",
      "  int num_heads = query_hidden_size / head_size;",
      "  int num_kv_heads = key.has_value() ? key_hidden_size / head_size : num_heads;",
      "  TORCH_CHECK(num_heads % num_kv_heads == 0);",
      "",
      "  int rot_dim = cos_sin_cache.size(1);",
      "  int seq_dim_idx = positions_ndim - 1;",
      "  int64_t query_stride = query.stride(seq_dim_idx);",
      "  int64_t key_stride = key.has_value() ? key->stride(seq_dim_idx) : 0;",
      "  // Determine head stride: for [*, heads, head_size] use stride of last dim;",
      "  // for flat [*, heads*head_size], heads blocks are contiguous of size",
      "  // head_size",
      "  int query_ndim = query.dim();",
      "  int64_t head_stride =",
      "      (query_ndim == positions_ndim + 2) ? query.stride(-2) : head_size;",
      "",
      "  dim3 grid(num_tokens);",
      "  dim3 block(std::min<int64_t>(num_heads * rot_dim / 2, 512));",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(query));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  VLLM_DISPATCH_FLOATING_TYPES(query.scalar_type(), \"rotary_embedding\", [&] {",
      "    if (is_neox) {",
      "      vllm::rotary_embedding_kernel<scalar_t, true><<<grid, block, 0, stream>>>(",
      "          positions.data_ptr<int64_t>(), query.data_ptr<scalar_t>(),",
      "          key.has_value() ? key->data_ptr<scalar_t>() : nullptr,",
      "          cos_sin_cache.data_ptr<scalar_t>(), rot_dim, query_stride, key_stride,",
      "          head_stride, num_heads, num_kv_heads, head_size);",
      "    } else {",
      "      vllm::rotary_embedding_kernel<scalar_t, false>",
      "          <<<grid, block, 0, stream>>>(",
      "              positions.data_ptr<int64_t>(), query.data_ptr<scalar_t>(),",
      "              key.has_value() ? key->data_ptr<scalar_t>() : nullptr,",
      "              cos_sin_cache.data_ptr<scalar_t>(), rot_dim, query_stride,",
      "              key_stride, head_stride, num_heads, num_kv_heads, head_size);",
      "    }",
      "  });",
      "}",
      "",
      "/*",
      "Batched version of rotary embedding, pack multiple LoRAs together",
      "and process in batched manner.",
      "*/",
      "void batched_rotary_embedding(",
      "    torch::Tensor& positions,  // [batch_size, seq_len] or [num_tokens]",
      "    torch::Tensor& query,  // [batch_size, seq_len, num_heads * head_size] or",
      "                           // [num_tokens, num_heads * head_size] or",
      "                           // [batch_size, seq_len, num_heads, head_size] or",
      "                           // [num_tokens, num_heads, head_size]",
      "    std::optional<torch::Tensor>",
      "        key,  // null or",
      "              // [batch_size, seq_len, num_kv_heads * head_size] or",
      "              // [num_tokens, num_kv_heads * head_size] or",
      "              // [batch_size, seq_len, num_heads, head_size] or",
      "              // [num_tokens, num_heads, head_size]",
      "    int64_t head_size,",
      "    torch::Tensor& cos_sin_cache,  // [max_position, rot_dim]",
      "    bool is_neox, int64_t rot_dim,",
      "    torch::Tensor& cos_sin_cache_offsets  // [num_tokens] or [batch_size]",
      ") {",
      "  // num_tokens = batch_size * seq_len",
      "  int64_t num_tokens = cos_sin_cache_offsets.size(0);",
      "  TORCH_CHECK(",
      "      positions.size(0) == num_tokens || positions.numel() == num_tokens,",
      "      \"positions must have the same num_tokens or batch_size as \"",
      "      \"cos_sin_cache_offsets\");",
      "",
      "  int positions_ndim = positions.dim();",
      "  // Make sure num_tokens dim is consistent across positions, query, and key",
      "  TORCH_CHECK(",
      "      positions_ndim == 1 || positions_ndim == 2,",
      "      \"positions must have shape [num_tokens] or [batch_size, seq_len]\");",
      "  if (positions_ndim == 1) {",
      "    TORCH_CHECK(query.size(0) == positions.size(0) &&",
      "                    (!key.has_value() || key->size(0) == positions.size(0)),",
      "                \"query, key and positions must have the same number of tokens\");",
      "  }",
      "  if (positions_ndim == 2) {",
      "    TORCH_CHECK(",
      "        query.size(0) == positions.size(0) &&",
      "            (!key.has_value() || key->size(0) == positions.size(0)) &&",
      "            query.size(1) == positions.size(1) &&",
      "            (!key.has_value() || key->size(1) == positions.size(1)),",
      "        \"query, key and positions must have the same batch_size and seq_len\");",
      "  }",
      "",
      "  // Make sure head_size is valid for query and key",
      "  int query_hidden_size = query.numel() / num_tokens;",
      "  int key_hidden_size = key.has_value() ? key->numel() / num_tokens : 0;",
      "  TORCH_CHECK(query_hidden_size % head_size == 0);",
      "  TORCH_CHECK(key_hidden_size % head_size == 0);",
      "",
      "  // Make sure query and key have concistent number of heads",
      "  int num_heads = query_hidden_size / head_size;",
      "  int num_kv_heads = key.has_value() ? key_hidden_size / head_size : num_heads;",
      "  TORCH_CHECK(num_heads % num_kv_heads == 0);",
      "",
      "  int seq_dim_idx = positions_ndim - 1;",
      "  int64_t query_stride = query.stride(seq_dim_idx);",
      "  int64_t key_stride = key.has_value() ? key->stride(seq_dim_idx) : 0;",
      "  // Determine head stride: for [*, heads, head_size] use stride of last dim;",
      "  // for flat [*, heads*head_size], heads blocks are contiguous of size",
      "  // head_size",
      "  int query_ndim = query.dim();",
      "  int64_t head_stride =",
      "      (query_ndim == positions_ndim + 2) ? query.stride(-2) : head_size;",
      "",
      "  dim3 grid(num_tokens);",
      "  dim3 block(std::min<int64_t>(num_heads * rot_dim / 2, 512));",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(query));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  VLLM_DISPATCH_FLOATING_TYPES(query.scalar_type(), \"rotary_embedding\", [&] {",
      "    if (is_neox) {",
      "      vllm::batched_rotary_embedding_kernel<scalar_t, true>",
      "          <<<grid, block, 0, stream>>>(",
      "              positions.data_ptr<int64_t>(), query.data_ptr<scalar_t>(),",
      "              key.has_value() ? key->data_ptr<scalar_t>() : nullptr,",
      "              cos_sin_cache.data_ptr<scalar_t>(),",
      "              cos_sin_cache_offsets.data_ptr<int64_t>(), rot_dim, query_stride,",
      "              key_stride, head_stride, num_heads, num_kv_heads, head_size);",
      "    } else {",
      "      vllm::batched_rotary_embedding_kernel<scalar_t, false>",
      "          <<<grid, block, 0, stream>>>(",
      "              positions.data_ptr<int64_t>(), query.data_ptr<scalar_t>(),",
      "              key.has_value() ? key->data_ptr<scalar_t>() : nullptr,",
      "              cos_sin_cache.data_ptr<scalar_t>(),",
      "              cos_sin_cache_offsets.data_ptr<int64_t>(), rot_dim, query_stride,",
      "              key_stride, head_stride, num_heads, num_kv_heads, head_size);",
      "    }",
      "  });",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/layernorm_quant_kernels.cu",
    "source": [
      "/*",
      " * This file contains the CUDA kernels for the fused quantized layernorm.",
      " * The kernels correspond to the kernels in layernorm_kernels.cu, except they",
      " * also produce quantized output directly.",
      " * Currently, only static fp8 quantization is supported.",
      " */",
      "",
      "#include \"type_convert.cuh\"",
      "#include \"quantization/fp8/common.cuh\"",
      "#include \"dispatch_utils.h\"",
      "",
      "#include <torch/cuda.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#ifndef USE_ROCM",
      "  #include <cub/cub.cuh>",
      "#else",
      "  #include <hipcub/hipcub.hpp>",
      "#endif",
      "",
      "namespace vllm {",
      "",
      "// TODO(woosuk): Further optimize this kernel.",
      "template <typename scalar_t, typename fp8_type>",
      "__global__ void rms_norm_static_fp8_quant_kernel(",
      "    fp8_type* __restrict__ out,          // [..., hidden_size]",
      "    const scalar_t* __restrict__ input,  // [..., hidden_size]",
      "    const int input_stride,",
      "    const scalar_t* __restrict__ weight,  // [hidden_size]",
      "    const float* __restrict__ scale,      // [1]",
      "    const float epsilon, const int num_tokens, const int hidden_size) {",
      "  __shared__ float s_variance;",
      "  float variance = 0.0f;",
      "",
      "  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {",
      "    const float x = (float)input[blockIdx.x * input_stride + idx];",
      "    variance += x * x;",
      "  }",
      "",
      "  using BlockReduce = cub::BlockReduce<float, 1024>;",
      "  __shared__ typename BlockReduce::TempStorage reduceStore;",
      "  variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);",
      "",
      "  if (threadIdx.x == 0) {",
      "    s_variance = rsqrtf(variance / hidden_size + epsilon);",
      "  }",
      "  __syncthreads();",
      "",
      "  // invert scale to avoid division",
      "  float const scale_inv = 1.0f / *scale;",
      "",
      "  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {",
      "    float x = (float)input[blockIdx.x * input_stride + idx];",
      "    float const out_norm = ((scalar_t)(x * s_variance)) * weight[idx];",
      "    out[blockIdx.x * hidden_size + idx] =",
      "        scaled_fp8_conversion<true, fp8_type>(out_norm, scale_inv);",
      "  }",
      "}",
      "",
      "/* Function specialization in the case of FP16/BF16 tensors.",
      "   Additional optimizations we can make in this case are",
      "   packed and vectorized operations, which help with the",
      "   memory latency bottleneck. */",
      "template <typename scalar_t, int width, typename fp8_type>",
      "__global__ std::enable_if_t<(width > 0) && _typeConvert<scalar_t>::exists>",
      "fused_add_rms_norm_static_fp8_quant_kernel(",
      "    fp8_type* __restrict__ out,    // [..., hidden_size]",
      "    scalar_t* __restrict__ input,  // [..., hidden_size]",
      "    const int input_stride,",
      "    scalar_t* __restrict__ residual,      // [..., hidden_size]",
      "    const scalar_t* __restrict__ weight,  // [hidden_size]",
      "    const float* __restrict__ scale,      // [1]",
      "    const float epsilon, const int num_tokens, const int hidden_size) {",
      "  // Sanity checks on our vector struct and type-punned pointer arithmetic",
      "  static_assert(std::is_pod_v<_f16Vec<scalar_t, width>>);",
      "  static_assert(sizeof(_f16Vec<scalar_t, width>) == sizeof(scalar_t) * width);",
      "",
      "  const int vec_hidden_size = hidden_size / width;",
      "  const int vec_input_stride = input_stride / width;",
      "  __shared__ float s_variance;",
      "  float variance = 0.0f;",
      "  /* These and the argument pointers are all declared `restrict` as they are",
      "     not aliased in practice. Argument pointers should not be dereferenced",
      "     in this kernel as that would be undefined behavior */",
      "  auto* __restrict__ input_v =",
      "      reinterpret_cast<_f16Vec<scalar_t, width>*>(input);",
      "  auto* __restrict__ residual_v =",
      "      reinterpret_cast<_f16Vec<scalar_t, width>*>(residual);",
      "  auto* __restrict__ weight_v =",
      "      reinterpret_cast<const _f16Vec<scalar_t, width>*>(weight);",
      "",
      "  for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {",
      "    int stride_id = blockIdx.x * vec_input_stride + idx;",
      "    int id = blockIdx.x * vec_hidden_size + idx;",
      "    _f16Vec<scalar_t, width> temp = input_v[stride_id];",
      "    temp += residual_v[id];",
      "    variance += temp.sum_squares();",
      "    residual_v[id] = temp;",
      "  }",
      "",
      "  using BlockReduce = cub::BlockReduce<float, 1024>;",
      "  __shared__ typename BlockReduce::TempStorage reduceStore;",
      "  variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);",
      "",
      "  if (threadIdx.x == 0) {",
      "    s_variance = rsqrtf(variance / hidden_size + epsilon);",
      "  }",
      "  __syncthreads();",
      "",
      "  // invert scale to avoid division",
      "  float const scale_inv = 1.0f / *scale;",
      "",
      "  for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {",
      "    int id = blockIdx.x * vec_hidden_size + idx;",
      "    _f16Vec<scalar_t, width> temp = residual_v[id];",
      "    temp *= s_variance;",
      "    temp *= weight_v[idx];",
      "#pragma unroll",
      "    for (int i = 0; i < width; ++i) {",
      "      out[id * width + i] =",
      "          scaled_fp8_conversion<true, fp8_type>(float(temp.data[i]), scale_inv);",
      "    }",
      "  }",
      "}",
      "",
      "/* Generic fused_add_rms_norm_kernel",
      "   The width field is not used here but necessary for other specializations.",
      " */",
      "template <typename scalar_t, int width, typename fp8_type>",
      "__global__ std::enable_if_t<(width == 0) || !_typeConvert<scalar_t>::exists>",
      "fused_add_rms_norm_static_fp8_quant_kernel(",
      "    fp8_type* __restrict__ out,    // [..., hidden_size]",
      "    scalar_t* __restrict__ input,  // [..., hidden_size]",
      "    const int input_stride,",
      "    scalar_t* __restrict__ residual,      // [..., hidden_size]",
      "    const scalar_t* __restrict__ weight,  // [hidden_size]",
      "    const float* __restrict__ scale,      // [1]",
      "    const float epsilon, const int num_tokens, const int hidden_size) {",
      "  __shared__ float s_variance;",
      "  float variance = 0.0f;",
      "",
      "  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {",
      "    scalar_t z = input[blockIdx.x * input_stride + idx];",
      "    z += residual[blockIdx.x * hidden_size + idx];",
      "    float x = (float)z;",
      "    variance += x * x;",
      "    residual[blockIdx.x * hidden_size + idx] = z;",
      "  }",
      "",
      "  using BlockReduce = cub::BlockReduce<float, 1024>;",
      "  __shared__ typename BlockReduce::TempStorage reduceStore;",
      "  variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);",
      "",
      "  if (threadIdx.x == 0) {",
      "    s_variance = rsqrtf(variance / hidden_size + epsilon);",
      "  }",
      "  __syncthreads();",
      "",
      "  // invert scale to avoid division",
      "  float const scale_inv = 1.0f / *scale;",
      "",
      "  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {",
      "    float x = (float)residual[blockIdx.x * hidden_size + idx];",
      "    float const out_norm = ((scalar_t)(x * s_variance)) * weight[idx];",
      "    out[blockIdx.x * hidden_size + idx] =",
      "        scaled_fp8_conversion<true, fp8_type>(out_norm, scale_inv);",
      "  }",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "void rms_norm_static_fp8_quant(torch::Tensor& out,     // [..., hidden_size]",
      "                               torch::Tensor& input,   // [..., hidden_size]",
      "                               torch::Tensor& weight,  // [hidden_size]",
      "                               torch::Tensor& scale,   // [1]",
      "                               double epsilon) {",
      "  TORCH_CHECK(out.is_contiguous());",
      "  int hidden_size = input.size(-1);",
      "  int input_stride = input.stride(-2);",
      "  int num_tokens = input.numel() / hidden_size;",
      "",
      "  dim3 grid(num_tokens);",
      "  dim3 block(std::min(hidden_size, 1024));",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  VLLM_DISPATCH_FLOATING_TYPES(",
      "      input.scalar_type(), \"rms_norm_kernel_scalar_type\", [&] {",
      "        VLLM_DISPATCH_FP8_TYPES(",
      "            out.scalar_type(), \"rms_norm_kernel_fp8_type\", [&] {",
      "              vllm::rms_norm_static_fp8_quant_kernel<scalar_t, fp8_t>",
      "                  <<<grid, block, 0, stream>>>(",
      "                      out.data_ptr<fp8_t>(), input.data_ptr<scalar_t>(),",
      "                      input_stride, weight.data_ptr<scalar_t>(),",
      "                      scale.data_ptr<float>(), epsilon, num_tokens,",
      "                      hidden_size);",
      "            });",
      "      });",
      "}",
      "",
      "#define LAUNCH_FUSED_ADD_RMS_NORM(width)                                     \\",
      "  VLLM_DISPATCH_FLOATING_TYPES(                                              \\",
      "      input.scalar_type(), \"fused_add_rms_norm_kernel_scalar_type\", [&] {    \\",
      "        VLLM_DISPATCH_FP8_TYPES(                                             \\",
      "            out.scalar_type(), \"fused_add_rms_norm_kernel_fp8_type\", [&] {   \\",
      "              vllm::fused_add_rms_norm_static_fp8_quant_kernel<scalar_t,     \\",
      "                                                               width, fp8_t> \\",
      "                  <<<grid, block, 0, stream>>>(                              \\",
      "                      out.data_ptr<fp8_t>(), input.data_ptr<scalar_t>(),     \\",
      "                      input_stride, residual.data_ptr<scalar_t>(),           \\",
      "                      weight.data_ptr<scalar_t>(), scale.data_ptr<float>(),  \\",
      "                      epsilon, num_tokens, hidden_size);                     \\",
      "            });                                                              \\",
      "      });",
      "void fused_add_rms_norm_static_fp8_quant(",
      "    torch::Tensor& out,       // [..., hidden_size],",
      "    torch::Tensor& input,     // [..., hidden_size]",
      "    torch::Tensor& residual,  // [..., hidden_size]",
      "    torch::Tensor& weight,    // [hidden_size]",
      "    torch::Tensor& scale,     // [1]",
      "    double epsilon) {",
      "  TORCH_CHECK(out.is_contiguous());",
      "  TORCH_CHECK(residual.is_contiguous());",
      "  int hidden_size = input.size(-1);",
      "  int input_stride = input.stride(-2);",
      "  int num_tokens = input.numel() / hidden_size;",
      "",
      "  dim3 grid(num_tokens);",
      "  /* This kernel is memory-latency bound in many scenarios.",
      "     When num_tokens is large, a smaller block size allows",
      "     for increased block occupancy on CUs and better latency",
      "     hiding on global mem ops. */",
      "  const int max_block_size = (num_tokens < 256) ? 1024 : 256;",
      "  dim3 block(std::min(hidden_size, max_block_size));",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  /*If the tensor types are FP16/BF16, try to use the optimized kernel",
      "    with packed + vectorized ops.",
      "    Max optimization is achieved with a width-8 vector of FP16/BF16s",
      "    since we can load at most 128 bits at once in a global memory op.",
      "    However, this requires each tensor's data to be aligned to 16",
      "    bytes.",
      "   */",
      "  auto inp_ptr = reinterpret_cast<std::uintptr_t>(input.data_ptr());",
      "  auto res_ptr = reinterpret_cast<std::uintptr_t>(residual.data_ptr());",
      "  auto wt_ptr = reinterpret_cast<std::uintptr_t>(weight.data_ptr());",
      "  bool ptrs_are_aligned =",
      "      inp_ptr % 16 == 0 && res_ptr % 16 == 0 && wt_ptr % 16 == 0;",
      "  if (ptrs_are_aligned && hidden_size % 8 == 0 && input_stride % 8 == 0) {",
      "    LAUNCH_FUSED_ADD_RMS_NORM(8);",
      "  } else {",
      "    LAUNCH_FUSED_ADD_RMS_NORM(0);",
      "  }",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/custom_all_reduce.cu",
    "source": [
      "#include <ATen/cuda/Exceptions.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <c10/cuda/CUDAStream.h>",
      "#include <torch/all.h>",
      "",
      "#include \"custom_all_reduce.cuh\"",
      "",
      "// Fake pointer type, must match fptr_t type in ops.h.",
      "// We use this type alias to indicate when pointers are passed in as int64_t.",
      "using fptr_t = int64_t;",
      "static_assert(sizeof(void*) == sizeof(fptr_t));",
      "",
      "fptr_t init_custom_ar(const std::vector<fptr_t>& fake_ipc_ptrs,",
      "                      torch::Tensor& rank_data, int64_t rank,",
      "                      bool fully_connected) {",
      "  int world_size = fake_ipc_ptrs.size();",
      "  if (world_size > 8)",
      "    throw std::invalid_argument(\"world size > 8 is not supported\");",
      "  if (world_size % 2 != 0)",
      "    throw std::invalid_argument(\"Odd num gpus is not supported for now\");",
      "  if (rank < 0 || rank >= world_size)",
      "    throw std::invalid_argument(\"invalid rank passed in\");",
      "",
      "  vllm::Signal* ipc_ptrs[8];",
      "  for (int i = 0; i < world_size; i++) {",
      "    ipc_ptrs[i] = reinterpret_cast<vllm::Signal*>(fake_ipc_ptrs[i]);",
      "  }",
      "  return (fptr_t) new vllm::CustomAllreduce(ipc_ptrs, rank_data.data_ptr(),",
      "                                            rank_data.numel(), rank, world_size,",
      "                                            fully_connected);",
      "}",
      "",
      "/**",
      " * Make sure tensor t's data lies completely within ((char)t.data_ptr()) +",
      " * t.numel() * t.element_size(). This is slightly weaker than t.is_contiguous()",
      " * because it allows transpose of contiguous slice (i.e. slicing the first",
      " * dimension). Currently, we require this because stride information is not",
      " * passed into the kernels and we treat input tensors as flat.",
      " *",
      " * Examples",
      " * A = torch.zeros(3, 3, 3)",
      " * 1. A: OK",
      " * 2. A[1:]: OK",
      " * 3. A.permute(2, 0, 1): OK",
      " * 4. A[1:].permute(2, 0, 1): OK",
      " * 5. A[None].expand(2, -1, -1, -1): Not OK",
      " * 6. A[:, 1:, 1:]: Not OK",
      " */",
      "bool _is_weak_contiguous(torch::Tensor& t) {",
      "  return t.is_contiguous() ||",
      "         (t.storage().nbytes() - t.storage_offset() * t.element_size() ==",
      "          t.numel() * t.element_size());",
      "}",
      "",
      "/**",
      " * Performs an out-of-place allreduce and stores result in out.",
      " *",
      " * If _reg_buffer is null, assumes inp.data_ptr() is already IPC-registered.",
      " * Otherwise, _reg_buffer is assumed to be IPC-registered and inp is first",
      " * copied into _reg_buffer.",
      " */",
      "void all_reduce(fptr_t _fa, torch::Tensor& inp, torch::Tensor& out,",
      "                fptr_t _reg_buffer, int64_t reg_buffer_sz_bytes) {",
      "  auto fa = reinterpret_cast<vllm::CustomAllreduce*>(_fa);",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(inp));",
      "  auto stream = c10::cuda::getCurrentCUDAStream().stream();",
      "",
      "  TORCH_CHECK_EQ(inp.scalar_type(), out.scalar_type());",
      "  TORCH_CHECK_EQ(inp.numel(), out.numel());",
      "  TORCH_CHECK(_is_weak_contiguous(out));",
      "  TORCH_CHECK(_is_weak_contiguous(inp));",
      "  auto input_size = inp.numel() * inp.element_size();",
      "  auto reg_buffer = reinterpret_cast<void*>(_reg_buffer);",
      "  if (reg_buffer) {",
      "    TORCH_CHECK_LE(input_size, reg_buffer_sz_bytes);",
      "    AT_CUDA_CHECK(cudaMemcpyAsync(reg_buffer, inp.data_ptr(), input_size,",
      "                                  cudaMemcpyDeviceToDevice, stream));",
      "  } else {",
      "    reg_buffer = inp.data_ptr();",
      "  }",
      "  switch (out.scalar_type()) {",
      "    case at::ScalarType::Float: {",
      "      fa->allreduce<float>(stream, reinterpret_cast<float*>(reg_buffer),",
      "                           reinterpret_cast<float*>(out.data_ptr()),",
      "                           out.numel());",
      "      break;",
      "    }",
      "    case at::ScalarType::Half: {",
      "      fa->allreduce<half>(stream, reinterpret_cast<half*>(reg_buffer),",
      "                          reinterpret_cast<half*>(out.data_ptr()), out.numel());",
      "      break;",
      "    }",
      "#if (__CUDA_ARCH__ >= 800 || !defined(__CUDA_ARCH__))",
      "    case at::ScalarType::BFloat16: {",
      "      fa->allreduce<nv_bfloat16>(",
      "          stream, reinterpret_cast<nv_bfloat16*>(reg_buffer),",
      "          reinterpret_cast<nv_bfloat16*>(out.data_ptr()), out.numel());",
      "      break;",
      "    }",
      "#endif",
      "    default:",
      "      throw std::runtime_error(",
      "          \"custom allreduce only supports float32, float16 and bfloat16\");",
      "  }",
      "}",
      "",
      "void dispose(fptr_t _fa) {",
      "  delete reinterpret_cast<vllm::CustomAllreduce*>(_fa);",
      "}",
      "",
      "int64_t meta_size() { return sizeof(vllm::Signal); }",
      "",
      "void register_buffer(fptr_t _fa, const std::vector<fptr_t>& fake_ipc_ptrs) {",
      "  auto fa = reinterpret_cast<vllm::CustomAllreduce*>(_fa);",
      "  TORCH_CHECK(fake_ipc_ptrs.size() == fa->world_size_);",
      "  void* ipc_ptrs[8];",
      "  for (int i = 0; i < fake_ipc_ptrs.size(); i++) {",
      "    ipc_ptrs[i] = reinterpret_cast<void*>(fake_ipc_ptrs[i]);",
      "  }",
      "  fa->register_buffer(ipc_ptrs);",
      "}",
      "",
      "// Use vector<int64_t> to represent byte data for python binding compatibility.",
      "std::tuple<std::vector<int64_t>, std::vector<int64_t>>",
      "get_graph_buffer_ipc_meta(fptr_t _fa) {",
      "  auto fa = reinterpret_cast<vllm::CustomAllreduce*>(_fa);",
      "  auto [handle, offsets] = fa->get_graph_buffer_ipc_meta();",
      "  std::vector<int64_t> bytes(handle.begin(), handle.end());",
      "  return std::make_tuple(bytes, offsets);",
      "}",
      "",
      "// Use vector<int64_t> to represent byte data for python binding compatibility.",
      "void register_graph_buffers(fptr_t _fa,",
      "                            const std::vector<std::vector<int64_t>>& handles,",
      "                            const std::vector<std::vector<int64_t>>& offsets) {",
      "  auto fa = reinterpret_cast<vllm::CustomAllreduce*>(_fa);",
      "  std::vector<std::string> bytes;",
      "  bytes.reserve(handles.size());",
      "  for (int i = 0; i < handles.size(); i++) {",
      "    bytes.emplace_back(handles[i].begin(), handles[i].end());",
      "  }",
      "  bytes.reserve(handles.size());",
      "  fa->register_graph_buffers(bytes, offsets);",
      "}",
      "",
      "std::tuple<fptr_t, torch::Tensor> allocate_shared_buffer_and_handle(",
      "    int64_t size) {",
      "  auto device_index = c10::cuda::current_device();",
      "  at::DeviceGuard device_guard(at::Device(at::DeviceType::CUDA, device_index));",
      "  void* buffer;",
      "  cudaStreamCaptureMode mode = cudaStreamCaptureModeRelaxed;",
      "  auto stream = c10::cuda::getCurrentCUDAStream().stream();",
      "  AT_CUDA_CHECK(cudaThreadExchangeStreamCaptureMode(&mode));",
      "",
      "  // Allocate buffer",
      "#if defined(USE_ROCM)",
      "  // data buffers need to be \"uncached\" for signal on MI200",
      "  AT_CUDA_CHECK(",
      "      hipExtMallocWithFlags((void**)&buffer, size, hipDeviceMallocUncached));",
      "#else",
      "  AT_CUDA_CHECK(cudaMalloc((void**)&buffer, size));",
      "#endif",
      "  AT_CUDA_CHECK(cudaMemsetAsync(buffer, 0, size, stream));",
      "  AT_CUDA_CHECK(cudaStreamSynchronize(stream));",
      "  AT_CUDA_CHECK(cudaThreadExchangeStreamCaptureMode(&mode));",
      "",
      "  // Create IPC memhandle for the allocated buffer.",
      "  // Will use it in open_mem_handle.",
      "  auto options =",
      "      torch::TensorOptions().dtype(torch::kUInt8).device(torch::kCPU);",
      "  auto handle =",
      "      torch::empty({static_cast<int64_t>(sizeof(cudaIpcMemHandle_t))}, options);",
      "  AT_CUDA_CHECK(",
      "      cudaIpcGetMemHandle((cudaIpcMemHandle_t*)handle.data_ptr(), buffer));",
      "",
      "  return std::make_tuple(reinterpret_cast<fptr_t>(buffer), handle);",
      "}",
      "",
      "fptr_t open_mem_handle(torch::Tensor& mem_handle) {",
      "  void* ipc_ptr;",
      "  AT_CUDA_CHECK(cudaIpcOpenMemHandle(",
      "      (void**)&ipc_ptr, *((const cudaIpcMemHandle_t*)mem_handle.data_ptr()),",
      "      cudaIpcMemLazyEnablePeerAccess));",
      "  return reinterpret_cast<fptr_t>(ipc_ptr);",
      "}",
      "",
      "void free_shared_buffer(fptr_t buffer) {",
      "  AT_CUDA_CHECK(cudaFree(reinterpret_cast<void*>(buffer)));",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/cuda_view.cu",
    "source": [
      "#include <torch/all.h>",
      "#include <torch/cuda.h>",
      "#include <cuda_runtime.h>",
      "",
      "// This function assumes that `cpu_tensor` is a CPU tensor allocated with pinned",
      "// memory, and that UVA (Unified Virtual Addressing) is enabled.",
      "torch::Tensor get_cuda_view_from_cpu_tensor(torch::Tensor& cpu_tensor) {",
      "  TORCH_CHECK(cpu_tensor.device().is_cpu(), \"Input tensor must be on CPU\");",
      "",
      "  // Get raw host pointer from CPU tensor",
      "  void* host_ptr = cpu_tensor.data_ptr();",
      "",
      "  // Get a device pointer corresponding to the pinned host memory",
      "  void* device_ptr = nullptr;",
      "  cudaError_t err = cudaHostGetDevicePointer(&device_ptr, host_ptr, 0);",
      "  TORCH_CHECK(err == cudaSuccess,",
      "              \"cudaHostGetDevicePointer failed: \", cudaGetErrorString(err));",
      "",
      "  // We'll use the same sizes, strides, and dtype as the CPU tensor.",
      "  // TODO: check if layout is respected.",
      "  auto sizes = cpu_tensor.sizes();",
      "  auto strides = cpu_tensor.strides();",
      "  auto options = cpu_tensor.options().device(torch::kCUDA);",
      "",
      "  // from_blob signature: from_blob(void *data, IntArrayRef sizes, ..., Deleter,",
      "  // const TensorOptions &) Provide a no-op deleter. The CPU tensor holds the",
      "  // memory, so we don't free it here.",
      "  auto deleter = [](void*) {",
      "    // no-op, since the memory is owned by the original CPU tensor",
      "  };",
      "",
      "  torch::Tensor cuda_tensor =",
      "      torch::from_blob(device_ptr, sizes, strides, deleter, options);",
      "",
      "  TORCH_CHECK(cuda_tensor.device().is_cuda(),",
      "              \"Resulting tensor is not on CUDA device\");",
      "",
      "  return cuda_tensor;",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/attention/paged_attention_v2.cu",
    "source": [
      "/*",
      " * Adapted from",
      " * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp",
      " * Copyright (c) 2023, The vLLM team.",
      " * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "#include \"attention_kernels.cuh\"",
      "#include \"../cuda_compat.h\"",
      "",
      "#define MAX(a, b) ((a) > (b) ? (a) : (b))",
      "#define MIN(a, b) ((a) < (b) ? (a) : (b))",
      "#define DIVIDE_ROUND_UP(a, b) (((a) + (b) - 1) / (b))",
      "",
      "#define LAUNCH_PAGED_ATTENTION_V2(HEAD_SIZE)                                   \\",
      "  vllm::paged_attention_v2_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,           \\",
      "                                  NUM_THREADS, KV_DTYPE, IS_BLOCK_SPARSE,      \\",
      "                                  PARTITION_SIZE>                              \\",
      "      <<<grid, block, shared_mem_size, stream>>>(                              \\",
      "          exp_sums_ptr, max_logits_ptr, tmp_out_ptr, query_ptr, key_cache_ptr, \\",
      "          value_cache_ptr, num_kv_heads, scale, block_tables_ptr,              \\",
      "          seq_lens_ptr, max_num_blocks_per_seq, alibi_slopes_ptr, q_stride,    \\",
      "          kv_block_stride, kv_head_stride, k_scale_ptr, v_scale_ptr, tp_rank,  \\",
      "          blocksparse_local_blocks, blocksparse_vert_stride,                   \\",
      "          blocksparse_block_size, blocksparse_head_sliding_step);              \\",
      "  vllm::paged_attention_v2_reduce_kernel<T, HEAD_SIZE, NUM_THREADS,            \\",
      "                                         PARTITION_SIZE>                       \\",
      "      <<<reduce_grid, block, reduce_shared_mem_size, stream>>>(                \\",
      "          out_ptr, exp_sums_ptr, max_logits_ptr, tmp_out_ptr, seq_lens_ptr,    \\",
      "          max_num_partitions);",
      "",
      "template <typename T, typename CACHE_T, int BLOCK_SIZE,",
      "          vllm::Fp8KVCacheDataType KV_DTYPE, bool IS_BLOCK_SPARSE,",
      "          int NUM_THREADS = 128, int PARTITION_SIZE = 512>",
      "void paged_attention_v2_launcher(",
      "    torch::Tensor& out, torch::Tensor& exp_sums, torch::Tensor& max_logits,",
      "    torch::Tensor& tmp_out, torch::Tensor& query, torch::Tensor& key_cache,",
      "    torch::Tensor& value_cache, int num_kv_heads, float scale,",
      "    torch::Tensor& block_tables, torch::Tensor& seq_lens, int max_seq_len,",
      "    const std::optional<torch::Tensor>& alibi_slopes, torch::Tensor& k_scale,",
      "    torch::Tensor& v_scale, const int tp_rank,",
      "    const int blocksparse_local_blocks, const int blocksparse_vert_stride,",
      "    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {",
      "  int num_seqs = query.size(0);",
      "  int num_heads = query.size(1);",
      "  int head_size = query.size(2);",
      "  int max_num_blocks_per_seq = block_tables.size(1);",
      "  int q_stride = query.stride(0);",
      "  int kv_block_stride = key_cache.stride(0);",
      "  int kv_head_stride = key_cache.stride(1);",
      "",
      "  // NOTE: alibi_slopes is optional.",
      "  const float* alibi_slopes_ptr =",
      "      alibi_slopes",
      "          ? reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())",
      "          : nullptr;",
      "",
      "  T* out_ptr = reinterpret_cast<T*>(out.data_ptr());",
      "  float* exp_sums_ptr = reinterpret_cast<float*>(exp_sums.data_ptr());",
      "  float* max_logits_ptr = reinterpret_cast<float*>(max_logits.data_ptr());",
      "  T* tmp_out_ptr = reinterpret_cast<T*>(tmp_out.data_ptr());",
      "  T* query_ptr = reinterpret_cast<T*>(query.data_ptr());",
      "  CACHE_T* key_cache_ptr = reinterpret_cast<CACHE_T*>(key_cache.data_ptr());",
      "  CACHE_T* value_cache_ptr = reinterpret_cast<CACHE_T*>(value_cache.data_ptr());",
      "  int* block_tables_ptr = block_tables.data_ptr<int>();",
      "  int* seq_lens_ptr = seq_lens.data_ptr<int>();",
      "  const float* k_scale_ptr = reinterpret_cast<const float*>(k_scale.data_ptr());",
      "  const float* v_scale_ptr = reinterpret_cast<const float*>(v_scale.data_ptr());",
      "",
      "  const int NUM_WARPS = NUM_THREADS / WARP_SIZE;",
      "  int max_num_partitions = DIVIDE_ROUND_UP(max_seq_len, PARTITION_SIZE);",
      "  int logits_size = PARTITION_SIZE * sizeof(float);",
      "  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);",
      "",
      "  // For paged attention v2 kernel.",
      "  dim3 grid(num_heads, num_seqs, max_num_partitions);",
      "  int shared_mem_size = std::max(logits_size, outputs_size);",
      "  // For paged attention v2 reduce kernel.",
      "  dim3 reduce_grid(num_heads, num_seqs);",
      "  int reduce_shared_mem_size = 2 * max_num_partitions * sizeof(float);",
      "",
      "  dim3 block(NUM_THREADS);",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(query));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  switch (head_size) {",
      "    // NOTE(woosuk): To reduce the compilation time, we only compile for the",
      "    // head sizes that we use in the model. However, we can easily extend this",
      "    // to support any head size which is a multiple of 16.",
      "    case 32:",
      "      LAUNCH_PAGED_ATTENTION_V2(32);",
      "      break;",
      "    case 64:",
      "      LAUNCH_PAGED_ATTENTION_V2(64);",
      "      break;",
      "    case 80:",
      "      LAUNCH_PAGED_ATTENTION_V2(80);",
      "      break;",
      "    case 96:",
      "      LAUNCH_PAGED_ATTENTION_V2(96);",
      "      break;",
      "    case 112:",
      "      LAUNCH_PAGED_ATTENTION_V2(112);",
      "      break;",
      "    case 120:",
      "      LAUNCH_PAGED_ATTENTION_V2(120);",
      "      break;",
      "    case 128:",
      "      LAUNCH_PAGED_ATTENTION_V2(128);",
      "      break;",
      "    case 192:",
      "      LAUNCH_PAGED_ATTENTION_V2(192);",
      "      break;",
      "    case 256:",
      "      LAUNCH_PAGED_ATTENTION_V2(256);",
      "      break;",
      "    default:",
      "      TORCH_CHECK(false, \"Unsupported head size: \", head_size);",
      "      break;",
      "  }",
      "}",
      "",
      "#define CALL_V2_LAUNCHER(T, CACHE_T, BLOCK_SIZE, KV_DTYPE, IS_BLOCK_SPARSE)   \\",
      "  paged_attention_v2_launcher<T, CACHE_T, BLOCK_SIZE, KV_DTYPE,               \\",
      "                              IS_BLOCK_SPARSE>(                               \\",
      "      out, exp_sums, max_logits, tmp_out, query, key_cache, value_cache,      \\",
      "      num_kv_heads, scale, block_tables, seq_lens, max_seq_len, alibi_slopes, \\",
      "      k_scale, v_scale, tp_rank, blocksparse_local_blocks,                    \\",
      "      blocksparse_vert_stride, blocksparse_block_size,                        \\",
      "      blocksparse_head_sliding_step);",
      "",
      "#define CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, BLOCK_SIZE, IS_FP8_KV_CACHE) \\",
      "  if (is_block_sparse) {                                                   \\",
      "    CALL_V2_LAUNCHER(T, CACHE_T, BLOCK_SIZE, IS_FP8_KV_CACHE, true);       \\",
      "  } else {                                                                 \\",
      "    CALL_V2_LAUNCHER(T, CACHE_T, BLOCK_SIZE, IS_FP8_KV_CACHE, false);      \\",
      "  }",
      "",
      "// NOTE(woosuk): To reduce the compilation time, we omitted block sizes",
      "// 1, 2, 4, 64, 128, 256.",
      "#define CALL_V2_LAUNCHER_BLOCK_SIZE(T, CACHE_T, KV_DTYPE)         \\",
      "  switch (block_size) {                                           \\",
      "    case 8:                                                       \\",
      "      CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\",
      "      break;                                                      \\",
      "    case 16:                                                      \\",
      "      CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\",
      "      break;                                                      \\",
      "    case 32:                                                      \\",
      "      CALL_V2_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\",
      "      break;                                                      \\",
      "    default:                                                      \\",
      "      TORCH_CHECK(false, \"Unsupported block size: \", block_size); \\",
      "      break;                                                      \\",
      "  }",
      "",
      "void paged_attention_v2(",
      "    torch::Tensor& out,         // [num_seqs, num_heads, head_size]",
      "    torch::Tensor& exp_sums,    // [num_seqs, num_heads, max_num_partitions]",
      "    torch::Tensor& max_logits,  // [num_seqs, num_heads, max_num_partitions]",
      "    torch::Tensor&",
      "        tmp_out,  // [num_seqs, num_heads, max_num_partitions, head_size]",
      "    torch::Tensor& query,  // [num_seqs, num_heads, head_size]",
      "    torch::Tensor&",
      "        key_cache,  // [num_blocks, num_heads, head_size/x, block_size, x]",
      "    torch::Tensor&",
      "        value_cache,       // [num_blocks, num_heads, head_size, block_size]",
      "    int64_t num_kv_heads,  // [num_heads]",
      "    double scale,",
      "    torch::Tensor& block_tables,  // [num_seqs, max_num_blocks_per_seq]",
      "    torch::Tensor& seq_lens,      // [num_seqs]",
      "    int64_t block_size, int64_t max_seq_len,",
      "    const std::optional<torch::Tensor>& alibi_slopes,",
      "    const std::string& kv_cache_dtype, torch::Tensor& k_scale,",
      "    torch::Tensor& v_scale, const int64_t tp_rank,",
      "    const int64_t blocksparse_local_blocks,",
      "    const int64_t blocksparse_vert_stride, const int64_t blocksparse_block_size,",
      "    const int64_t blocksparse_head_sliding_step) {",
      "  const bool is_block_sparse = (blocksparse_vert_stride > 1);",
      "  DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,",
      "                             CALL_V2_LAUNCHER_BLOCK_SIZE)",
      "}",
      "",
      "#undef MAX",
      "#undef MIN",
      "#undef DIVIDE_ROUND_UP"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/attention/paged_attention_v1.cu",
    "source": [
      "/*",
      " * Adapted from",
      " * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp",
      " * Copyright (c) 2023, The vLLM team.",
      " * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "#include \"attention_kernels.cuh\"",
      "#include \"../cuda_compat.h\"",
      "",
      "#define MAX(a, b) ((a) > (b) ? (a) : (b))",
      "#define MIN(a, b) ((a) < (b) ? (a) : (b))",
      "#define DIVIDE_ROUND_UP(a, b) (((a) + (b) - 1) / (b))",
      "",
      "#define LAUNCH_PAGED_ATTENTION_V1(HEAD_SIZE)                                \\",
      "  VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize(                     \\",
      "      ((void*)vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE,        \\",
      "                                              BLOCK_SIZE, NUM_THREADS,      \\",
      "                                              KV_DTYPE, IS_BLOCK_SPARSE>),  \\",
      "      shared_mem_size);                                                     \\",
      "  vllm::paged_attention_v1_kernel<T, CACHE_T, HEAD_SIZE, BLOCK_SIZE,        \\",
      "                                  NUM_THREADS, KV_DTYPE, IS_BLOCK_SPARSE>   \\",
      "      <<<grid, block, shared_mem_size, stream>>>(                           \\",
      "          out_ptr, query_ptr, key_cache_ptr, value_cache_ptr, num_kv_heads, \\",
      "          scale, block_tables_ptr, seq_lens_ptr, max_num_blocks_per_seq,    \\",
      "          alibi_slopes_ptr, q_stride, kv_block_stride, kv_head_stride,      \\",
      "          k_scale_ptr, v_scale_ptr, tp_rank, blocksparse_local_blocks,      \\",
      "          blocksparse_vert_stride, blocksparse_block_size,                  \\",
      "          blocksparse_head_sliding_step);",
      "",
      "// TODO(woosuk): Tune NUM_THREADS.",
      "template <typename T, typename CACHE_T, int BLOCK_SIZE,",
      "          vllm::Fp8KVCacheDataType KV_DTYPE, bool IS_BLOCK_SPARSE,",
      "          int NUM_THREADS = 128>",
      "void paged_attention_v1_launcher(",
      "    torch::Tensor& out, torch::Tensor& query, torch::Tensor& key_cache,",
      "    torch::Tensor& value_cache, int num_kv_heads, float scale,",
      "    torch::Tensor& block_tables, torch::Tensor& seq_lens, int max_seq_len,",
      "    const std::optional<torch::Tensor>& alibi_slopes, torch::Tensor& k_scale,",
      "    torch::Tensor& v_scale, const int tp_rank,",
      "    const int blocksparse_local_blocks, const int blocksparse_vert_stride,",
      "    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {",
      "  int num_seqs = query.size(0);",
      "  int num_heads = query.size(1);",
      "  int head_size = query.size(2);",
      "  int max_num_blocks_per_seq = block_tables.size(1);",
      "  int q_stride = query.stride(0);",
      "  int kv_block_stride = key_cache.stride(0);",
      "  int kv_head_stride = key_cache.stride(1);",
      "",
      "  // NOTE: alibi_slopes is optional.",
      "  const float* alibi_slopes_ptr =",
      "      alibi_slopes",
      "          ? reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())",
      "          : nullptr;",
      "",
      "  T* out_ptr = reinterpret_cast<T*>(out.data_ptr());",
      "  T* query_ptr = reinterpret_cast<T*>(query.data_ptr());",
      "  CACHE_T* key_cache_ptr = reinterpret_cast<CACHE_T*>(key_cache.data_ptr());",
      "  CACHE_T* value_cache_ptr = reinterpret_cast<CACHE_T*>(value_cache.data_ptr());",
      "  int* block_tables_ptr = block_tables.data_ptr<int>();",
      "  int* seq_lens_ptr = seq_lens.data_ptr<int>();",
      "  const float* k_scale_ptr = reinterpret_cast<const float*>(k_scale.data_ptr());",
      "  const float* v_scale_ptr = reinterpret_cast<const float*>(v_scale.data_ptr());",
      "",
      "  const int NUM_WARPS = NUM_THREADS / WARP_SIZE;",
      "  int padded_max_seq_len =",
      "      DIVIDE_ROUND_UP(max_seq_len, BLOCK_SIZE) * BLOCK_SIZE;",
      "  int logits_size = padded_max_seq_len * sizeof(float);",
      "  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);",
      "  // Python-side check in vllm.worker.worker._check_if_can_support_max_seq_len",
      "  // Keep that in sync with the logic here!",
      "  int shared_mem_size = std::max(logits_size, outputs_size);",
      "",
      "  dim3 grid(num_heads, num_seqs, 1);",
      "  dim3 block(NUM_THREADS);",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(query));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  switch (head_size) {",
      "    // NOTE(woosuk): To reduce the compilation time, we only compile for the",
      "    // head sizes that we use in the model. However, we can easily extend this",
      "    // to support any head size which is a multiple of 16.",
      "    case 32:",
      "      LAUNCH_PAGED_ATTENTION_V1(32);",
      "      break;",
      "    case 64:",
      "      LAUNCH_PAGED_ATTENTION_V1(64);",
      "      break;",
      "    case 80:",
      "      LAUNCH_PAGED_ATTENTION_V1(80);",
      "      break;",
      "    case 96:",
      "      LAUNCH_PAGED_ATTENTION_V1(96);",
      "      break;",
      "    case 112:",
      "      LAUNCH_PAGED_ATTENTION_V1(112);",
      "      break;",
      "    case 120:",
      "      LAUNCH_PAGED_ATTENTION_V1(120);",
      "      break;",
      "    case 128:",
      "      LAUNCH_PAGED_ATTENTION_V1(128);",
      "      break;",
      "    case 192:",
      "      LAUNCH_PAGED_ATTENTION_V1(192);",
      "      break;",
      "    case 256:",
      "      LAUNCH_PAGED_ATTENTION_V1(256);",
      "      break;",
      "    default:",
      "      TORCH_CHECK(false, \"Unsupported head size: \", head_size);",
      "      break;",
      "  }",
      "}",
      "",
      "#define CALL_V1_LAUNCHER(T, CACHE_T, BLOCK_SIZE, KV_DTYPE, IS_BLOCK_SPARSE)  \\",
      "  paged_attention_v1_launcher<T, CACHE_T, BLOCK_SIZE, KV_DTYPE,              \\",
      "                              IS_BLOCK_SPARSE>(                              \\",
      "      out, query, key_cache, value_cache, num_kv_heads, scale, block_tables, \\",
      "      seq_lens, max_seq_len, alibi_slopes, k_scale, v_scale, tp_rank,        \\",
      "      blocksparse_local_blocks, blocksparse_vert_stride,                     \\",
      "      blocksparse_block_size, blocksparse_head_sliding_step);",
      "",
      "#define CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, BLOCK_SIZE, IS_FP8_KV_CACHE) \\",
      "  if (is_block_sparse) {                                                   \\",
      "    CALL_V1_LAUNCHER(T, CACHE_T, BLOCK_SIZE, IS_FP8_KV_CACHE, true);       \\",
      "  } else {                                                                 \\",
      "    CALL_V1_LAUNCHER(T, CACHE_T, BLOCK_SIZE, IS_FP8_KV_CACHE, false);      \\",
      "  }",
      "",
      "// NOTE(woosuk): To reduce the compilation time, we omitted block sizes",
      "// 1, 2, 4, 64, 128, 256.",
      "#define CALL_V1_LAUNCHER_BLOCK_SIZE(T, CACHE_T, KV_DTYPE)         \\",
      "  switch (block_size) {                                           \\",
      "    case 8:                                                       \\",
      "      CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 8, KV_DTYPE);         \\",
      "      break;                                                      \\",
      "    case 16:                                                      \\",
      "      CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 16, KV_DTYPE);        \\",
      "      break;                                                      \\",
      "    case 32:                                                      \\",
      "      CALL_V1_LAUNCHER_SPARSITY(T, CACHE_T, 32, KV_DTYPE);        \\",
      "      break;                                                      \\",
      "    default:                                                      \\",
      "      TORCH_CHECK(false, \"Unsupported block size: \", block_size); \\",
      "      break;                                                      \\",
      "  }",
      "",
      "void paged_attention_v1(",
      "    torch::Tensor& out,    // [num_seqs, num_heads, head_size]",
      "    torch::Tensor& query,  // [num_seqs, num_heads, head_size]",
      "    torch::Tensor&",
      "        key_cache,  // [num_blocks, num_heads, head_size/x, block_size, x]",
      "    torch::Tensor&",
      "        value_cache,       // [num_blocks, num_heads, head_size, block_size]",
      "    int64_t num_kv_heads,  // [num_heads]",
      "    double scale,",
      "    torch::Tensor& block_tables,  // [num_seqs, max_num_blocks_per_seq]",
      "    torch::Tensor& seq_lens,      // [num_seqs]",
      "    int64_t block_size, int64_t max_seq_len,",
      "    const std::optional<torch::Tensor>& alibi_slopes,",
      "    const std::string& kv_cache_dtype, torch::Tensor& k_scale,",
      "    torch::Tensor& v_scale, const int64_t tp_rank,",
      "    const int64_t blocksparse_local_blocks,",
      "    const int64_t blocksparse_vert_stride, const int64_t blocksparse_block_size,",
      "    const int64_t blocksparse_head_sliding_step) {",
      "  const bool is_block_sparse = (blocksparse_vert_stride > 1);",
      "",
      "  DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,",
      "                             CALL_V1_LAUNCHER_BLOCK_SIZE)",
      "}",
      "",
      "#undef MAX",
      "#undef MIN",
      "#undef DIVIDE_ROUND_UP"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/attention/dtype_bfloat16.cuh",
    "source": [
      "/*",
      " * Adapted from",
      " * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp",
      " * and",
      " * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h",
      " * Copyright (c) 2023, The vLLM team.",
      " * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "#pragma once",
      "",
      "#include \"attention_generic.cuh\"",
      "#include \"dtype_float32.cuh\"",
      "",
      "#ifndef USE_ROCM",
      "  #include <cuda_bf16.h>",
      "  #include <cuda_fp16.h>",
      "#else",
      "  #include <hip/hip_bf16.h>",
      "  #include <hip/hip_fp16.h>",
      "",
      "typedef __hip_bfloat162 __nv_bfloat162;",
      "typedef __hip_bfloat16 __nv_bfloat16;",
      "#endif",
      "",
      "#include <stdint.h>",
      "",
      "namespace vllm {",
      "",
      "// Define custom BF16 vector data types.",
      "struct bf16_4_t {",
      "  __nv_bfloat162 x;",
      "  __nv_bfloat162 y;",
      "};",
      "",
      "struct bf16_8_t {",
      "  __nv_bfloat162 x;",
      "  __nv_bfloat162 y;",
      "  __nv_bfloat162 z;",
      "  __nv_bfloat162 w;",
      "};",
      "",
      "// BF16 vector types for Q, K, V.",
      "template <>",
      "struct Vec<__nv_bfloat16, 1> {",
      "  using Type = __nv_bfloat16;",
      "};",
      "template <>",
      "struct Vec<__nv_bfloat16, 2> {",
      "  using Type = __nv_bfloat162;",
      "};",
      "template <>",
      "struct Vec<__nv_bfloat16, 4> {",
      "  using Type = bf16_4_t;",
      "};",
      "template <>",
      "struct Vec<__nv_bfloat16, 8> {",
      "  using Type = bf16_8_t;",
      "};",
      "",
      "// FP32 accumulator vector types corresponding to Vec.",
      "template <>",
      "struct FloatVec<__nv_bfloat16> {",
      "  using Type = float;",
      "};",
      "template <>",
      "struct FloatVec<__nv_bfloat162> {",
      "  using Type = float2;",
      "};",
      "template <>",
      "struct FloatVec<bf16_4_t> {",
      "  using Type = Float4_;",
      "};",
      "template <>",
      "struct FloatVec<bf16_8_t> {",
      "  using Type = Float8_;",
      "};",
      "",
      "// Utility functions for type conversions.",
      "inline __device__ float2 bf1622float2(const __nv_bfloat162 val) {",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "  assert(false);",
      "#else",
      "  return __bfloat1622float2(val);",
      "#endif",
      "  __builtin_unreachable();  // Suppress missing return statement warning",
      "}",
      "",
      "inline __device__ __nv_bfloat162 bf162bf162(const __nv_bfloat16 val) {",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "  assert(false);",
      "#else",
      "  return __bfloat162bfloat162(val);",
      "#endif",
      "  __builtin_unreachable();  // Suppress missing return statement warning",
      "}",
      "",
      "// Vector addition.",
      "inline __device__ __nv_bfloat16 add(__nv_bfloat16 a, __nv_bfloat16 b) {",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "  assert(false);",
      "#else",
      "  #ifndef USE_ROCM",
      "  return a + b;",
      "  #else",
      "  return __hadd(a, b);",
      "  #endif",
      "#endif",
      "  __builtin_unreachable();  // Suppress missing return statement warning",
      "}",
      "",
      "inline __device__ __nv_bfloat162 add(__nv_bfloat162 a, __nv_bfloat162 b) {",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "  assert(false);",
      "#else",
      "  return __hadd2(a, b);",
      "#endif",
      "  __builtin_unreachable();  // Suppress missing return statement warning",
      "}",
      "",
      "inline __device__ bf16_4_t add(bf16_4_t a, bf16_4_t b) {",
      "  bf16_4_t c;",
      "  c.x = add(a.x, b.x);",
      "  c.y = add(a.y, b.y);",
      "  return c;",
      "}",
      "",
      "inline __device__ bf16_8_t add(bf16_8_t a, bf16_8_t b) {",
      "  bf16_8_t c;",
      "  c.x = add(a.x, b.x);",
      "  c.y = add(a.y, b.y);",
      "  c.z = add(a.z, b.z);",
      "  c.w = add(a.w, b.w);",
      "  return c;",
      "}",
      "",
      "inline __device__ float2 add(__nv_bfloat162 a, float2 fb) {",
      "  float2 fa = bf1622float2(a);",
      "  return add(fa, fb);",
      "}",
      "",
      "inline __device__ Float4_ add(bf16_4_t a, Float4_ fb) {",
      "  Float4_ fc;",
      "  fc.x = add(a.x, fb.x);",
      "  fc.y = add(a.y, fb.y);",
      "  return fc;",
      "}",
      "",
      "inline __device__ Float8_ add(bf16_8_t a, Float8_ fb) {",
      "  Float8_ fc;",
      "  fc.x = add(a.x, fb.x);",
      "  fc.y = add(a.y, fb.y);",
      "  fc.z = add(a.z, fb.z);",
      "  fc.w = add(a.w, fb.w);",
      "  return fc;",
      "}",
      "",
      "// Vector multiplication.",
      "template <>",
      "inline __device__ __nv_bfloat16 mul(__nv_bfloat16 a, __nv_bfloat16 b) {",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "  assert(false);",
      "#else",
      "  return __hmul(a, b);",
      "#endif",
      "  __builtin_unreachable();  // Suppress missing return statement warning",
      "}",
      "",
      "template <>",
      "inline __device__ __nv_bfloat162 mul(__nv_bfloat162 a, __nv_bfloat162 b) {",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "  assert(false);",
      "#else",
      "  return __hmul2(a, b);",
      "#endif",
      "  __builtin_unreachable();  // Suppress missing return statement warning",
      "}",
      "",
      "template <>",
      "inline __device__ __nv_bfloat162 mul(__nv_bfloat16 a, __nv_bfloat162 b) {",
      "  return mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(bf162bf162(a), b);",
      "}",
      "",
      "template <>",
      "inline __device__ bf16_4_t mul(bf16_4_t a, bf16_4_t b) {",
      "  bf16_4_t c;",
      "  c.x = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(a.x, b.x);",
      "  c.y = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(a.y, b.y);",
      "  return c;",
      "}",
      "",
      "template <>",
      "inline __device__ bf16_4_t mul(__nv_bfloat16 a, bf16_4_t b) {",
      "  __nv_bfloat162 s = bf162bf162(a);",
      "  bf16_4_t c;",
      "  c.x = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(s, b.x);",
      "  c.y = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(s, b.y);",
      "  return c;",
      "}",
      "",
      "template <>",
      "inline __device__ bf16_8_t mul(bf16_8_t a, bf16_8_t b) {",
      "  bf16_8_t c;",
      "  c.x = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(a.x, b.x);",
      "  c.y = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(a.y, b.y);",
      "  c.z = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(a.z, b.z);",
      "  c.w = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(a.w, b.w);",
      "  return c;",
      "}",
      "",
      "template <>",
      "inline __device__ bf16_8_t mul(__nv_bfloat16 a, bf16_8_t b) {",
      "  __nv_bfloat162 s = bf162bf162(a);",
      "  bf16_8_t c;",
      "  c.x = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(s, b.x);",
      "  c.y = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(s, b.y);",
      "  c.z = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(s, b.z);",
      "  c.w = mul<__nv_bfloat162, __nv_bfloat162, __nv_bfloat162>(s, b.w);",
      "  return c;",
      "}",
      "",
      "template <>",
      "inline __device__ float mul(__nv_bfloat16 a, __nv_bfloat16 b) {",
      "  float fa = __bfloat162float(a);",
      "  float fb = __bfloat162float(b);",
      "  return fa * fb;",
      "}",
      "",
      "template <>",
      "inline __device__ float2 mul(__nv_bfloat162 a, __nv_bfloat162 b) {",
      "  float2 fa = bf1622float2(a);",
      "  float2 fb = bf1622float2(b);",
      "  return mul<float2, float2, float2>(fa, fb);",
      "}",
      "",
      "template <>",
      "inline __device__ float2 mul(__nv_bfloat16 a, __nv_bfloat162 b) {",
      "  return mul<float2, __nv_bfloat162, __nv_bfloat162>(bf162bf162(a), b);",
      "}",
      "",
      "template <>",
      "inline __device__ Float4_ mul(bf16_4_t a, bf16_4_t b) {",
      "  Float4_ fc;",
      "  fc.x = mul<float2, __nv_bfloat162, __nv_bfloat162>(a.x, b.x);",
      "  fc.y = mul<float2, __nv_bfloat162, __nv_bfloat162>(a.y, b.y);",
      "  return fc;",
      "}",
      "",
      "template <>",
      "inline __device__ Float4_ mul(__nv_bfloat16 a, bf16_4_t b) {",
      "  __nv_bfloat162 s = bf162bf162(a);",
      "  Float4_ fc;",
      "  fc.x = mul<float2, __nv_bfloat162, __nv_bfloat162>(s, b.x);",
      "  fc.y = mul<float2, __nv_bfloat162, __nv_bfloat162>(s, b.y);",
      "  return fc;",
      "}",
      "",
      "template <>",
      "inline __device__ Float8_ mul(bf16_8_t a, bf16_8_t b) {",
      "  Float8_ fc;",
      "  fc.x = mul<float2, __nv_bfloat162, __nv_bfloat162>(a.x, b.x);",
      "  fc.y = mul<float2, __nv_bfloat162, __nv_bfloat162>(a.y, b.y);",
      "  fc.z = mul<float2, __nv_bfloat162, __nv_bfloat162>(a.z, b.z);",
      "  fc.w = mul<float2, __nv_bfloat162, __nv_bfloat162>(a.w, b.w);",
      "  return fc;",
      "}",
      "",
      "template <>",
      "inline __device__ Float8_ mul(__nv_bfloat16 a, bf16_8_t b) {",
      "  __nv_bfloat162 s = bf162bf162(a);",
      "  Float8_ fc;",
      "  fc.x = mul<float2, __nv_bfloat162, __nv_bfloat162>(s, b.x);",
      "  fc.y = mul<float2, __nv_bfloat162, __nv_bfloat162>(s, b.y);",
      "  fc.z = mul<float2, __nv_bfloat162, __nv_bfloat162>(s, b.z);",
      "  fc.w = mul<float2, __nv_bfloat162, __nv_bfloat162>(s, b.w);",
      "  return fc;",
      "}",
      "",
      "// Vector fused multiply-add.",
      "inline __device__ __nv_bfloat162 fma(__nv_bfloat162 a, __nv_bfloat162 b,",
      "                                     __nv_bfloat162 c) {",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "  assert(false);",
      "#else",
      "  return __hfma2(a, b, c);",
      "#endif",
      "  __builtin_unreachable();  // Suppress missing return statement warning",
      "}",
      "",
      "inline __device__ __nv_bfloat162 fma(__nv_bfloat16 a, __nv_bfloat162 b,",
      "                                     __nv_bfloat162 c) {",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "  assert(false);",
      "#else",
      "  return __hfma2(bf162bf162(a), b, c);",
      "#endif",
      "  __builtin_unreachable();  // Suppress missing return statement warning",
      "}",
      "",
      "inline __device__ bf16_4_t fma(bf16_4_t a, bf16_4_t b, bf16_4_t c) {",
      "  bf16_4_t d;",
      "  d.x = fma(a.x, b.x, c.x);",
      "  d.y = fma(a.y, b.y, c.y);",
      "  return d;",
      "}",
      "",
      "inline __device__ bf16_4_t fma(__nv_bfloat16 a, bf16_4_t b, bf16_4_t c) {",
      "  __nv_bfloat162 s = bf162bf162(a);",
      "  bf16_4_t d;",
      "  d.x = fma(s, b.x, c.x);",
      "  d.y = fma(s, b.y, c.y);",
      "  return d;",
      "}",
      "",
      "inline __device__ bf16_8_t fma(bf16_8_t a, bf16_8_t b, bf16_8_t c) {",
      "  bf16_8_t d;",
      "  d.x = fma(a.x, b.x, c.x);",
      "  d.y = fma(a.y, b.y, c.y);",
      "  d.z = fma(a.z, b.z, c.z);",
      "  d.w = fma(a.w, b.w, c.w);",
      "  return d;",
      "}",
      "",
      "inline __device__ bf16_8_t fma(__nv_bfloat16 a, bf16_8_t b, bf16_8_t c) {",
      "  __nv_bfloat162 s = bf162bf162(a);",
      "  bf16_8_t d;",
      "  d.x = fma(s, b.x, c.x);",
      "  d.y = fma(s, b.y, c.y);",
      "  d.z = fma(s, b.z, c.z);",
      "  d.w = fma(s, b.w, c.w);",
      "  return d;",
      "}",
      "",
      "inline __device__ float fma(__nv_bfloat16 a, __nv_bfloat16 b, float fc) {",
      "  return __bfloat162float(a) * __bfloat162float(b) + fc;",
      "}",
      "",
      "inline __device__ float2 fma(__nv_bfloat162 a, __nv_bfloat162 b, float2 fc) {",
      "  float2 fa = bf1622float2(a);",
      "  float2 fb = bf1622float2(b);",
      "  return fma(fa, fb, fc);",
      "}",
      "",
      "inline __device__ float2 fma(__nv_bfloat16 a, __nv_bfloat162 b, float2 fc) {",
      "  return fma(bf162bf162(a), b, fc);",
      "}",
      "",
      "inline __device__ Float4_ fma(bf16_4_t a, bf16_4_t b, Float4_ fc) {",
      "  Float4_ fd;",
      "  fd.x = fma(a.x, b.x, fc.x);",
      "  fd.y = fma(a.y, b.y, fc.y);",
      "  return fd;",
      "}",
      "",
      "inline __device__ Float4_ fma(__nv_bfloat16 a, bf16_4_t b, Float4_ fc) {",
      "  __nv_bfloat162 s = bf162bf162(a);",
      "  Float4_ fd;",
      "  fd.x = fma(s, b.x, fc.x);",
      "  fd.y = fma(s, b.y, fc.y);",
      "  return fd;",
      "}",
      "",
      "inline __device__ Float8_ fma(bf16_8_t a, bf16_8_t b, Float8_ fc) {",
      "  Float8_ fd;",
      "  fd.x = fma(a.x, b.x, fc.x);",
      "  fd.y = fma(a.y, b.y, fc.y);",
      "  fd.z = fma(a.z, b.z, fc.z);",
      "  fd.w = fma(a.w, b.w, fc.w);",
      "  return fd;",
      "}",
      "",
      "inline __device__ Float8_ fma(__nv_bfloat16 a, bf16_8_t b, Float8_ fc) {",
      "  __nv_bfloat162 s = bf162bf162(a);",
      "  Float8_ fd;",
      "  fd.x = fma(s, b.x, fc.x);",
      "  fd.y = fma(s, b.y, fc.y);",
      "  fd.z = fma(s, b.z, fc.z);",
      "  fd.w = fma(s, b.w, fc.w);",
      "  return fd;",
      "}",
      "",
      "// Vector sum.",
      "template <>",
      "inline __device__ float sum(__nv_bfloat16 v) {",
      "  return __bfloat162float(v);",
      "}",
      "",
      "template <>",
      "inline __device__ float sum(__nv_bfloat162 v) {",
      "  float2 vf = bf1622float2(v);",
      "  return vf.x + vf.y;",
      "}",
      "",
      "template <>",
      "inline __device__ float sum(bf16_4_t v) {",
      "  return sum(v.x) + sum(v.y);",
      "}",
      "",
      "template <>",
      "inline __device__ float sum(bf16_8_t v) {",
      "  return sum(v.x) + sum(v.y) + sum(v.z) + sum(v.w);",
      "}",
      "",
      "// From float32 to bfloat16.",
      "inline __device__ void from_float(__nv_bfloat16& dst, float src) {",
      "  dst = __float2bfloat16(src);",
      "}",
      "",
      "inline __device__ void from_float(__nv_bfloat162& dst, float2 src) {",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "  assert(false);",
      "#else",
      "  dst = __float22bfloat162_rn(src);",
      "#endif",
      "}",
      "",
      "inline __device__ void from_float(bf16_4_t& dst, Float4_ src) {",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "  assert(false);",
      "#else",
      "  dst.x = __float22bfloat162_rn(src.x);",
      "  dst.y = __float22bfloat162_rn(src.y);",
      "#endif",
      "}",
      "",
      "inline __device__ void from_float(bf16_8_t& dst, Float8_ src) {",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "  assert(false);",
      "#else",
      "  dst.x = __float22bfloat162_rn(src.x);",
      "  dst.y = __float22bfloat162_rn(src.y);",
      "  dst.z = __float22bfloat162_rn(src.z);",
      "  dst.w = __float22bfloat162_rn(src.w);",
      "#endif",
      "}",
      "",
      "// From bfloat16 to float32.",
      "inline __device__ float to_float(__nv_bfloat16 u) {",
      "  return __bfloat162float(u);",
      "}",
      "",
      "// Zero-out a variable.",
      "inline __device__ void zero(__nv_bfloat16& dst) {",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "  assert(false);",
      "#else",
      "  // Same as CUDART_ZERO_BF16 introduced in CUDA 12.2.",
      "  dst = __ushort_as_bfloat16((unsigned short)0x0000U);",
      "#endif",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/attention/dtype_fp8.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"attention_generic.cuh\"",
      "",
      "#include <stdint.h>",
      "#ifdef ENABLE_FP8",
      "  #ifndef USE_ROCM",
      "    #include <cuda_fp8.h>",
      "  #endif  // USE_ROCM",
      "#endif    // ENABLE_FP8",
      "",
      "namespace vllm {",
      "",
      "enum class Fp8KVCacheDataType {",
      "  kAuto = 0,",
      "  kFp8E4M3 = 1,",
      "  kFp8E5M2 = 2,",
      "};",
      "",
      "// fp8 vector types for quantization of kv cache",
      "template <>",
      "struct Vec<uint8_t, 1> {",
      "  using Type = uint8_t;",
      "};",
      "",
      "template <>",
      "struct Vec<uint8_t, 2> {",
      "  using Type = uint16_t;",
      "};",
      "",
      "template <>",
      "struct Vec<uint8_t, 4> {",
      "  using Type = uint32_t;",
      "};",
      "",
      "template <>",
      "struct Vec<uint8_t, 8> {",
      "  using Type = uint2;",
      "};",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/attention/dtype_float16.cuh",
    "source": [
      "/*",
      " * Adapted from",
      " * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp",
      " * and",
      " * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h",
      " * Copyright (c) 2023, The vLLM team.",
      " * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "#pragma once",
      "",
      "#include \"attention_generic.cuh\"",
      "#include \"dtype_float32.cuh\"",
      "",
      "#ifdef USE_ROCM",
      "  #include <hip/hip_fp16.h>",
      "#endif",
      "",
      "#include <stdint.h>",
      "",
      "namespace vllm {",
      "",
      "// FP16 vector types for Q, K, V.",
      "template <>",
      "struct Vec<uint16_t, 1> {",
      "  using Type = uint16_t;",
      "};",
      "template <>",
      "struct Vec<uint16_t, 2> {",
      "  using Type = uint32_t;",
      "};",
      "template <>",
      "struct Vec<uint16_t, 4> {",
      "  using Type = uint2;",
      "};",
      "template <>",
      "struct Vec<uint16_t, 8> {",
      "  using Type = uint4;",
      "};",
      "",
      "// FP32 accumulator vector types corresponding to Vec.",
      "template <>",
      "struct FloatVec<uint16_t> {",
      "  using Type = float;",
      "};",
      "template <>",
      "struct FloatVec<uint32_t> {",
      "  using Type = float2;",
      "};",
      "template <>",
      "struct FloatVec<uint2> {",
      "  using Type = Float4_;",
      "};",
      "template <>",
      "struct FloatVec<uint4> {",
      "  using Type = Float8_;",
      "};",
      "",
      "// Utility functions for type conversions.",
      "inline __device__ uint32_t h0_h0(uint16_t a) {",
      "#ifndef USE_ROCM",
      "  uint32_t b;",
      "  asm volatile(\"mov.b32 %0, {%1, %1};\" : \"=r\"(b) : \"h\"(a));",
      "  return b;",
      "#else",
      "  union {",
      "    uint32_t u32;",
      "    uint16_t u16[2];",
      "  } tmp;",
      "  tmp.u16[0] = a;",
      "  tmp.u16[1] = a;",
      "  return tmp.u32;",
      "#endif",
      "}",
      "",
      "inline __device__ float half_to_float(uint16_t h) {",
      "  float f;",
      "#ifndef USE_ROCM",
      "  asm volatile(\"cvt.f32.f16 %0, %1;\\n\" : \"=f\"(f) : \"h\"(h));",
      "#else",
      "  asm volatile(\"v_cvt_f32_f16 %0, %1;\" : \"=v\"(f) : \"v\"(h));",
      "#endif",
      "  return f;",
      "}",
      "",
      "inline __device__ float2 half2_to_float2(uint32_t v) {",
      "#ifndef USE_ROCM",
      "  uint16_t lo, hi;",
      "  asm volatile(\"mov.b32 {%0, %1}, %2;\\n\" : \"=h\"(lo), \"=h\"(hi) : \"r\"(v));",
      "  return make_float2(half_to_float(lo), half_to_float(hi));",
      "#else",
      "  union {",
      "    uint32_t u32;",
      "    uint16_t u16[2];",
      "  } tmp;",
      "  tmp.u32 = v;",
      "  float2 ret;",
      "  ret.x = half_to_float(tmp.u16[0]);",
      "  ret.y = half_to_float(tmp.u16[1]);",
      "  return ret;",
      "#endif",
      "}",
      "",
      "inline __device__ uint16_t float_to_half(float f) {",
      "  union {",
      "    uint32_t u32;",
      "    uint16_t u16[2];",
      "  } tmp;",
      "#ifndef USE_ROCM",
      "  asm volatile(\"cvt.rn.f16.f32 %0, %1;\\n\" : \"=h\"(tmp.u16[0]) : \"f\"(f));",
      "#else",
      "  asm volatile(\"v_cvt_f16_f32 %0, %1;\\n\" : \"=v\"(tmp.u32) : \"v\"(f));",
      "#endif",
      "  return tmp.u16[0];",
      "}",
      "",
      "inline __device__ uint32_t float2_to_half2(float2 f) {",
      "  union {",
      "    uint32_t u32;",
      "    uint16_t u16[2];",
      "  } tmp;",
      "#ifndef USE_ROCM",
      "  #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800",
      "  asm volatile(\"cvt.rn.f16x2.f32 %0, %1, %2;\\n\"",
      "               : \"=r\"(tmp.u32)",
      "               : \"f\"(f.y), \"f\"(f.x));",
      "  #else",
      "  asm volatile(\"cvt.rn.f16.f32 %0, %1;\\n\" : \"=h\"(tmp.u16[0]) : \"f\"(f.x));",
      "  asm volatile(\"cvt.rn.f16.f32 %0, %1;\\n\" : \"=h\"(tmp.u16[1]) : \"f\"(f.y));",
      "  #endif",
      "#else",
      "  tmp.u16[0] = float_to_half(f.x);",
      "  tmp.u16[1] = float_to_half(f.y);",
      "#endif",
      "  return tmp.u32;",
      "}",
      "",
      "// Vector addition.",
      "inline __device__ uint16_t add(uint16_t a, uint16_t b) {",
      "  uint16_t c;",
      "#ifndef USE_ROCM",
      "  asm volatile(\"add.f16 %0, %1, %2;\\n\" : \"=h\"(c) : \"h\"(a), \"h\"(b));",
      "#else",
      "  asm volatile(\"v_add_f16 %0, %1, %2;\\n\" : \"=v\"(c) : \"v\"(a), \"v\"(b));",
      "#endif",
      "  return c;",
      "}",
      "",
      "inline __device__ uint32_t add(uint32_t a, uint32_t b) {",
      "  uint32_t c;",
      "#ifndef USE_ROCM",
      "  asm volatile(\"add.f16x2 %0, %1, %2;\\n\" : \"=r\"(c) : \"r\"(a), \"r\"(b));",
      "#else",
      "  asm volatile(\"v_pk_add_f16 %0, %1, %2;\\n\" : \"=v\"(c) : \"v\"(a), \"v\"(b));",
      "#endif",
      "  return c;",
      "}",
      "",
      "inline __device__ uint2 add(uint2 a, uint2 b) {",
      "  uint2 c;",
      "  c.x = add(a.x, b.x);",
      "  c.y = add(a.y, b.y);",
      "  return c;",
      "}",
      "",
      "inline __device__ uint4 add(uint4 a, uint4 b) {",
      "  uint4 c;",
      "  c.x = add(a.x, b.x);",
      "  c.y = add(a.y, b.y);",
      "  c.z = add(a.z, b.z);",
      "  c.w = add(a.w, b.w);",
      "  return c;",
      "}",
      "",
      "inline __device__ float2 add(uint32_t a, float2 fb) {",
      "  float2 fa = half2_to_float2(a);",
      "  return add(fa, fb);",
      "}",
      "",
      "inline __device__ Float4_ add(uint2 a, Float4_ fb) {",
      "  Float4_ fc;",
      "  fc.x = add(a.x, fb.x);",
      "  fc.y = add(a.y, fb.y);",
      "  return fc;",
      "}",
      "",
      "inline __device__ Float8_ add(uint4 a, Float8_ fb) {",
      "  Float8_ fc;",
      "  fc.x = add(a.x, fb.x);",
      "  fc.y = add(a.y, fb.y);",
      "  fc.z = add(a.z, fb.z);",
      "  fc.w = add(a.w, fb.w);",
      "  return fc;",
      "}",
      "",
      "// Vector multiplication.",
      "template <>",
      "inline __device__ uint16_t mul(uint16_t a, uint16_t b) {",
      "  uint16_t c;",
      "#ifndef USE_ROCM",
      "  asm volatile(\"mul.f16 %0, %1, %2;\\n\" : \"=h\"(c) : \"h\"(a), \"h\"(b));",
      "#else",
      "  asm volatile(\"v_mul_f16 %0, %1, %2;\\n\" : \"=v\"(c) : \"v\"(a), \"v\"(b));",
      "#endif",
      "  return c;",
      "}",
      "",
      "template <>",
      "inline __device__ uint32_t mul(uint32_t a, uint32_t b) {",
      "  uint32_t c;",
      "#ifndef USE_ROCM",
      "  asm volatile(\"mul.f16x2 %0, %1, %2;\\n\" : \"=r\"(c) : \"r\"(a), \"r\"(b));",
      "#else",
      "  asm volatile(\"v_pk_mul_f16 %0, %1, %2;\\n\" : \"=v\"(c) : \"v\"(a), \"v\"(b));",
      "#endif",
      "  return c;",
      "}",
      "",
      "template <>",
      "inline __device__ uint32_t mul(uint16_t a, uint32_t b) {",
      "  return mul<uint32_t, uint32_t, uint32_t>(h0_h0(a), b);",
      "}",
      "",
      "template <>",
      "inline __device__ uint2 mul(uint2 a, uint2 b) {",
      "  uint2 c;",
      "  c.x = mul<uint32_t, uint32_t, uint32_t>(a.x, b.x);",
      "  c.y = mul<uint32_t, uint32_t, uint32_t>(a.y, b.y);",
      "  return c;",
      "}",
      "",
      "template <>",
      "inline __device__ uint2 mul(uint16_t a, uint2 b) {",
      "  uint32_t s = h0_h0(a);",
      "  uint2 c;",
      "  c.x = mul<uint32_t, uint32_t, uint32_t>(s, b.x);",
      "  c.y = mul<uint32_t, uint32_t, uint32_t>(s, b.y);",
      "  return c;",
      "}",
      "",
      "template <>",
      "inline __device__ uint4 mul(uint4 a, uint4 b) {",
      "  uint4 c;",
      "  c.x = mul<uint32_t, uint32_t, uint32_t>(a.x, b.x);",
      "  c.y = mul<uint32_t, uint32_t, uint32_t>(a.y, b.y);",
      "  c.z = mul<uint32_t, uint32_t, uint32_t>(a.z, b.z);",
      "  c.w = mul<uint32_t, uint32_t, uint32_t>(a.w, b.w);",
      "  return c;",
      "}",
      "",
      "template <>",
      "inline __device__ uint4 mul(uint16_t a, uint4 b) {",
      "  uint32_t s = h0_h0(a);",
      "  uint4 c;",
      "  c.x = mul<uint32_t, uint32_t, uint32_t>(s, b.x);",
      "  c.y = mul<uint32_t, uint32_t, uint32_t>(s, b.y);",
      "  c.z = mul<uint32_t, uint32_t, uint32_t>(s, b.z);",
      "  c.w = mul<uint32_t, uint32_t, uint32_t>(s, b.w);",
      "  return c;",
      "}",
      "",
      "template <>",
      "inline __device__ float mul(uint16_t a, uint16_t b) {",
      "  float fa = half_to_float(a);",
      "  float fb = half_to_float(b);",
      "  return fa * fb;",
      "}",
      "",
      "template <>",
      "inline __device__ float2 mul(uint32_t a, uint32_t b) {",
      "  float2 fa = half2_to_float2(a);",
      "  float2 fb = half2_to_float2(b);",
      "  return mul<float2, float2, float2>(fa, fb);",
      "}",
      "",
      "template <>",
      "inline __device__ float2 mul(uint16_t a, uint32_t b) {",
      "  return mul<float2, uint32_t, uint32_t>(h0_h0(a), b);",
      "}",
      "",
      "template <>",
      "inline __device__ Float4_ mul(uint2 a, uint2 b) {",
      "  Float4_ fc;",
      "  fc.x = mul<float2, uint32_t, uint32_t>(a.x, b.x);",
      "  fc.y = mul<float2, uint32_t, uint32_t>(a.y, b.y);",
      "  return fc;",
      "}",
      "",
      "template <>",
      "inline __device__ Float4_ mul(uint16_t a, uint2 b) {",
      "  uint32_t s = h0_h0(a);",
      "  Float4_ fc;",
      "  fc.x = mul<float2, uint32_t, uint32_t>(s, b.x);",
      "  fc.y = mul<float2, uint32_t, uint32_t>(s, b.y);",
      "  return fc;",
      "}",
      "",
      "template <>",
      "inline __device__ Float8_ mul(uint4 a, uint4 b) {",
      "  Float8_ fc;",
      "  fc.x = mul<float2, uint32_t, uint32_t>(a.x, b.x);",
      "  fc.y = mul<float2, uint32_t, uint32_t>(a.y, b.y);",
      "  fc.z = mul<float2, uint32_t, uint32_t>(a.z, b.z);",
      "  fc.w = mul<float2, uint32_t, uint32_t>(a.w, b.w);",
      "  return fc;",
      "}",
      "",
      "template <>",
      "inline __device__ Float8_ mul(uint16_t a, uint4 b) {",
      "  uint32_t s = h0_h0(a);",
      "  Float8_ fc;",
      "  fc.x = mul<float2, uint32_t, uint32_t>(s, b.x);",
      "  fc.y = mul<float2, uint32_t, uint32_t>(s, b.y);",
      "  fc.z = mul<float2, uint32_t, uint32_t>(s, b.z);",
      "  fc.w = mul<float2, uint32_t, uint32_t>(s, b.w);",
      "  return fc;",
      "}",
      "",
      "// Vector fused multiply-add.",
      "inline __device__ uint32_t fma(uint32_t a, uint32_t b, uint32_t c) {",
      "  uint32_t d;",
      "#ifndef USE_ROCM",
      "  asm volatile(\"fma.rn.f16x2 %0, %1, %2, %3;\\n\"",
      "               : \"=r\"(d)",
      "               : \"r\"(a), \"r\"(b), \"r\"(c));",
      "#else",
      "  asm volatile(\"v_pk_fma_f16 %0, %1, %2, %3;\\n\"",
      "               : \"=v\"(d)",
      "               : \"v\"(a), \"v\"(b), \"v\"(c));",
      "#endif",
      "  return d;",
      "}",
      "",
      "inline __device__ uint32_t fma(uint16_t a, uint32_t b, uint32_t c) {",
      "  return fma(h0_h0(a), b, c);",
      "}",
      "",
      "inline __device__ uint2 fma(uint2 a, uint2 b, uint2 c) {",
      "  uint2 d;",
      "  d.x = fma(a.x, b.x, c.x);",
      "  d.y = fma(a.y, b.y, c.y);",
      "  return d;",
      "}",
      "",
      "inline __device__ uint2 fma(uint16_t a, uint2 b, uint2 c) {",
      "  uint32_t s = h0_h0(a);",
      "  uint2 d;",
      "  d.x = fma(s, b.x, c.x);",
      "  d.y = fma(s, b.y, c.y);",
      "  return d;",
      "}",
      "",
      "inline __device__ uint4 fma(uint4 a, uint4 b, uint4 c) {",
      "  uint4 d;",
      "  d.x = fma(a.x, b.x, c.x);",
      "  d.y = fma(a.y, b.y, c.y);",
      "  d.z = fma(a.z, b.z, c.z);",
      "  d.w = fma(a.w, b.w, c.w);",
      "  return d;",
      "}",
      "",
      "inline __device__ uint4 fma(uint16_t a, uint4 b, uint4 c) {",
      "  uint32_t s = h0_h0(a);",
      "  uint4 d;",
      "  d.x = fma(s, b.x, c.x);",
      "  d.y = fma(s, b.y, c.y);",
      "  d.z = fma(s, b.z, c.z);",
      "  d.w = fma(s, b.w, c.w);",
      "  return d;",
      "}",
      "",
      "inline __device__ float fma(uint16_t a, uint16_t b, float fc) {",
      "  float fa = half_to_float(a);",
      "  float fb = half_to_float(b);",
      "  return fa * fb + fc;",
      "}",
      "",
      "inline __device__ float2 fma(uint32_t a, uint32_t b, float2 fc) {",
      "  float2 fa = half2_to_float2(a);",
      "  float2 fb = half2_to_float2(b);",
      "  return fma(fa, fb, fc);",
      "}",
      "",
      "inline __device__ float2 fma(uint16_t a, uint32_t b, float2 fc) {",
      "  return fma(h0_h0(a), b, fc);",
      "}",
      "",
      "inline __device__ Float4_ fma(uint2 a, uint2 b, Float4_ fc) {",
      "  Float4_ fd;",
      "  fd.x = fma(a.x, b.x, fc.x);",
      "  fd.y = fma(a.y, b.y, fc.y);",
      "  return fd;",
      "}",
      "",
      "inline __device__ Float4_ fma(uint16_t a, uint2 b, Float4_ fc) {",
      "  uint32_t s = h0_h0(a);",
      "  Float4_ fd;",
      "  fd.x = fma(s, b.x, fc.x);",
      "  fd.y = fma(s, b.y, fc.y);",
      "  return fd;",
      "}",
      "",
      "inline __device__ Float8_ fma(uint4 a, uint4 b, Float8_ fc) {",
      "  Float8_ fd;",
      "  fd.x = fma(a.x, b.x, fc.x);",
      "  fd.y = fma(a.y, b.y, fc.y);",
      "  fd.z = fma(a.z, b.z, fc.z);",
      "  fd.w = fma(a.w, b.w, fc.w);",
      "  return fd;",
      "}",
      "",
      "inline __device__ Float8_ fma(uint16_t a, uint4 b, Float8_ fc) {",
      "  uint32_t s = h0_h0(a);",
      "  Float8_ fd;",
      "  fd.x = fma(s, b.x, fc.x);",
      "  fd.y = fma(s, b.y, fc.y);",
      "  fd.z = fma(s, b.z, fc.z);",
      "  fd.w = fma(s, b.w, fc.w);",
      "  return fd;",
      "}",
      "",
      "// Vector sum.",
      "template <>",
      "inline __device__ float sum(uint16_t v) {",
      "  return half_to_float(v);",
      "}",
      "",
      "template <>",
      "inline __device__ float sum(uint32_t v) {",
      "  float2 tmp = half2_to_float2(v);",
      "  return tmp.x + tmp.y;",
      "}",
      "",
      "template <>",
      "inline __device__ float sum(uint2 v) {",
      "  uint32_t c = add(v.x, v.y);",
      "  return sum(c);",
      "}",
      "",
      "template <>",
      "inline __device__ float sum(uint4 v) {",
      "  uint32_t c = add(v.x, v.y);",
      "  c = add(c, v.z);",
      "  c = add(c, v.w);",
      "  return sum(c);",
      "}",
      "",
      "// From float32 to float16.",
      "inline __device__ void from_float(uint16_t& dst, float src) {",
      "  dst = float_to_half(src);",
      "}",
      "",
      "inline __device__ void from_float(uint32_t& dst, float2 src) {",
      "  dst = float2_to_half2(src);",
      "}",
      "",
      "inline __device__ void from_float(uint2& dst, Float4_ src) {",
      "  dst.x = float2_to_half2(src.x);",
      "  dst.y = float2_to_half2(src.y);",
      "}",
      "",
      "inline __device__ void from_float(uint4& dst, Float8_ src) {",
      "  dst.x = float2_to_half2(src.x);",
      "  dst.y = float2_to_half2(src.y);",
      "  dst.z = float2_to_half2(src.z);",
      "  dst.w = float2_to_half2(src.w);",
      "}",
      "",
      "// From float16 to float32.",
      "inline __device__ float to_float(uint16_t u) { return half_to_float(u); }",
      "",
      "inline __device__ float2 to_float(uint32_t u) { return half2_to_float2(u); }",
      "",
      "inline __device__ Float4_ to_float(uint2 u) {",
      "  Float4_ tmp;",
      "  tmp.x = half2_to_float2(u.x);",
      "  tmp.y = half2_to_float2(u.y);",
      "  return tmp;",
      "}",
      "",
      "inline __device__ Float8_ to_float(uint4 u) {",
      "  Float8_ tmp;",
      "  tmp.x = half2_to_float2(u.x);",
      "  tmp.y = half2_to_float2(u.y);",
      "  tmp.z = half2_to_float2(u.z);",
      "  tmp.w = half2_to_float2(u.w);",
      "  return tmp;",
      "}",
      "",
      "// Zero-out a variable.",
      "inline __device__ void zero(uint16_t& dst) { dst = uint16_t(0); }",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/attention/dtype_float32.cuh",
    "source": [
      "/*",
      " * Adapted from",
      " * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp",
      " * and",
      " * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h",
      " * Copyright (c) 2023, The vLLM team.",
      " * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "#pragma once",
      "",
      "#include \"attention_generic.cuh\"",
      "",
      "#include <stdint.h>",
      "",
      "namespace vllm {",
      "",
      "// Define custom FP32 vector data types.",
      "struct Float4_ {",
      "  float2 x;",
      "  float2 y;",
      "};",
      "",
      "struct Float8_ {",
      "  float2 x;",
      "  float2 y;",
      "  float2 z;",
      "  float2 w;",
      "};",
      "",
      "// FP32 vector types for Q, K, V.",
      "template <>",
      "struct Vec<float, 1> {",
      "  using Type = float;",
      "};",
      "template <>",
      "struct Vec<float, 2> {",
      "  using Type = float2;",
      "};",
      "template <>",
      "struct Vec<float, 4> {",
      "  using Type = float4;",
      "};",
      "",
      "// FP32 accumulator vector types corresponding to Vec.",
      "template <>",
      "struct FloatVec<float> {",
      "  using Type = float;",
      "};",
      "template <>",
      "struct FloatVec<float2> {",
      "  using Type = float2;",
      "};",
      "template <>",
      "struct FloatVec<float4> {",
      "  using Type = float4;",
      "};",
      "",
      "// Vector addition.",
      "inline __device__ float add(float a, float b) { return a + b; }",
      "",
      "inline __device__ float2 add(float2 a, float2 b) {",
      "  float2 c;",
      "  c.x = add(a.x, b.x);",
      "  c.y = add(a.y, b.y);",
      "  return c;",
      "}",
      "",
      "inline __device__ float4 add(float4 a, float4 b) {",
      "  float4 c;",
      "  c.x = add(a.x, b.x);",
      "  c.y = add(a.y, b.y);",
      "  c.z = add(a.z, b.z);",
      "  c.w = add(a.w, b.w);",
      "  return c;",
      "}",
      "",
      "// Vector multiplication.",
      "template <>",
      "inline __device__ float mul<float, float>(float a, float b) {",
      "  return a * b;",
      "}",
      "",
      "template <>",
      "inline __device__ float2 mul(float2 a, float2 b) {",
      "  float2 c;",
      "  c.x = a.x * b.x;",
      "  c.y = a.y * b.y;",
      "  return c;",
      "}",
      "",
      "template <>",
      "inline __device__ float2 mul(float a, float2 b) {",
      "  float2 c;",
      "  c.x = a * b.x;",
      "  c.y = a * b.y;",
      "  return c;",
      "}",
      "",
      "template <>",
      "inline __device__ float4 mul(float4 a, float4 b) {",
      "  float4 c;",
      "  c.x = a.x * b.x;",
      "  c.y = a.y * b.y;",
      "  c.z = a.z * b.z;",
      "  c.w = a.w * b.w;",
      "  return c;",
      "}",
      "",
      "template <>",
      "inline __device__ float4 mul(float a, float4 b) {",
      "  float4 c;",
      "  c.x = a * b.x;",
      "  c.y = a * b.y;",
      "  c.z = a * b.z;",
      "  c.w = a * b.w;",
      "  return c;",
      "}",
      "",
      "// Vector fused multiply-add.",
      "inline __device__ float fma(float a, float b, float c) { return a * b + c; }",
      "",
      "inline __device__ float2 fma(float2 a, float2 b, float2 c) {",
      "  float2 d;",
      "  d.x = fma(a.x, b.x, c.x);",
      "  d.y = fma(a.y, b.y, c.y);",
      "  return d;",
      "}",
      "",
      "inline __device__ float2 fma(float a, float2 b, float2 c) {",
      "  float2 d;",
      "  d.x = fma(a, b.x, c.x);",
      "  d.y = fma(a, b.y, c.y);",
      "  return d;",
      "}",
      "",
      "inline __device__ float4 fma(float4 a, float4 b, float4 c) {",
      "  float4 d;",
      "  d.x = fma(a.x, b.x, c.x);",
      "  d.y = fma(a.y, b.y, c.y);",
      "  d.z = fma(a.z, b.z, c.z);",
      "  d.w = fma(a.w, b.w, c.w);",
      "  return d;",
      "}",
      "",
      "inline __device__ float4 fma(float a, float4 b, float4 c) {",
      "  float4 d;",
      "  d.x = fma(a, b.x, c.x);",
      "  d.y = fma(a, b.y, c.y);",
      "  d.z = fma(a, b.z, c.z);",
      "  d.w = fma(a, b.w, c.w);",
      "  return d;",
      "}",
      "",
      "inline __device__ Float4_ fma(float a, Float4_ b, Float4_ c) {",
      "  Float4_ d;",
      "  d.x = fma(a, b.x, c.x);",
      "  d.y = fma(a, b.y, c.y);",
      "  return d;",
      "}",
      "",
      "inline __device__ Float8_ fma(float a, Float8_ b, Float8_ c) {",
      "  Float8_ d;",
      "  d.x = fma(a, b.x, c.x);",
      "  d.y = fma(a, b.y, c.y);",
      "  d.z = fma(a, b.z, c.z);",
      "  d.w = fma(a, b.w, c.w);",
      "  return d;",
      "}",
      "",
      "// Vector sum.",
      "template <>",
      "inline __device__ float sum(float v) {",
      "  return v;",
      "}",
      "",
      "template <>",
      "inline __device__ float sum(float2 v) {",
      "  return v.x + v.y;",
      "}",
      "",
      "template <>",
      "inline __device__ float sum(float4 v) {",
      "  return v.x + v.y + v.z + v.w;",
      "}",
      "",
      "template <>",
      "inline __device__ float sum(Float4_ v) {",
      "  return v.x.x + v.x.y + v.y.x + v.y.y;",
      "}",
      "",
      "template <>",
      "inline __device__ float sum(Float8_ v) {",
      "  return v.x.x + v.x.y + v.y.x + v.y.y + v.z.x + v.z.y + v.w.x + v.w.y;",
      "}",
      "",
      "// Vector dot product.",
      "inline __device__ float dot(float a, float b) { return a * b; }",
      "",
      "inline __device__ float dot(float2 a, float2 b) {",
      "  float2 c = mul<float2, float2, float2>(a, b);",
      "  return c.x + c.y;",
      "}",
      "",
      "inline __device__ float dot(Float4_ a, Float4_ b) {",
      "  float2 acc = mul<float2, float2, float2>(a.x, b.x);",
      "  acc = fma(a.y, b.y, acc);",
      "  return acc.x + acc.y;",
      "}",
      "",
      "inline __device__ float dot(Float8_ a, Float8_ b) {",
      "  float2 acc = mul<float2, float2, float2>(a.x, b.x);",
      "  acc = fma(a.y, b.y, acc);",
      "  acc = fma(a.z, b.z, acc);",
      "  acc = fma(a.w, b.w, acc);",
      "  return acc.x + acc.y;",
      "}",
      "",
      "// From float to float.",
      "inline __device__ void from_float(float& dst, float src) { dst = src; }",
      "",
      "inline __device__ void from_float(float2& dst, float2 src) { dst = src; }",
      "",
      "inline __device__ void from_float(float4& dst, float4 src) { dst = src; }",
      "",
      "// From float to float.",
      "inline __device__ float to_float(float u) { return u; }",
      "",
      "inline __device__ float2 to_float(float2 u) { return u; }",
      "",
      "inline __device__ float4 to_float(float4 u) { return u; }",
      "",
      "inline __device__ Float4_ to_float(Float4_ u) { return u; }",
      "",
      "inline __device__ Float8_ to_float(Float8_ u) { return u; }",
      "",
      "// Zero-out a variable.",
      "inline __device__ void zero(float& dst) { dst = 0.f; }",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/attention/vertical_slash_index.cu",
    "source": [
      "// Copyright (c) Microsoft Corporation.",
      "// Licensed under the MIT license.",
      "",
      "#include <assert.h>",
      "",
      "#include <cuda.h>",
      "",
      "#include <torch/all.h>",
      "",
      "__device__ int64_t save_blocks(int* block_offset, int64_t range_start,",
      "                               int64_t range_end, int64_t block_size,",
      "                               int64_t input_block_count, int64_t kv_seqlen) {",
      "  if (range_start >= kv_seqlen) {",
      "    return input_block_count;",
      "  }",
      "  if (range_end > kv_seqlen) {",
      "    range_end = kv_seqlen;",
      "  }",
      "  int64_t current_block_count = input_block_count;",
      "  for (int idx = range_start; idx < range_end; idx += block_size) {",
      "    block_offset[current_block_count++] = idx;",
      "  }",
      "  return current_block_count;",
      "}",
      "",
      "__global__ void convert_vertical_slash_indexes_kernel(",
      "    const int* q_seqlens,         // [BATCH, ]",
      "    const int* kv_seqlens,        // [BATCH, ]",
      "    const int* vertical_indexes,  // [BATCH, N_HEADS, NNZ_V]",
      "    const int* slash_indexes,     // [BATCH, N_HEADS, NNZ_S]",
      "    int* block_count,             // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M)]",
      "    int* block_offset,  // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M), NNZ_S]",
      "    int* column_count,  // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M)]",
      "    int* column_index,  // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M), NNZ_V]",
      "    int64_t N_HEADS, int64_t N_ROWS, int64_t BLOCK_SIZE_M, int64_t BLOCK_SIZE_N,",
      "    int64_t NNZ_V, int64_t NNZ_S,",
      "    bool causal  // True for intra, False for succ",
      ") {",
      "  const int batch_idx = blockIdx.y;",
      "  const int head_idx = blockIdx.x;",
      "  const int group_idx = blockIdx.z;",
      "",
      "  int64_t q_seqlen = q_seqlens[batch_idx];",
      "  int64_t kv_seqlen = kv_seqlens[batch_idx];",
      "  int64_t block_idx_m = group_idx * blockDim.x + threadIdx.x;",
      "  int64_t start_m = block_idx_m * BLOCK_SIZE_M;",
      "  if (start_m >= q_seqlen) {",
      "    return;",
      "  }",
      "  int64_t end_m = start_m + BLOCK_SIZE_M;",
      "  vertical_indexes += (batch_idx * N_HEADS + head_idx) * NNZ_V;",
      "  slash_indexes += (batch_idx * N_HEADS + head_idx) * NNZ_S;",
      "  int64_t row_offset = (batch_idx * N_HEADS + head_idx) * N_ROWS + block_idx_m;",
      "  block_count += row_offset;",
      "  block_offset += row_offset * NNZ_S;",
      "  column_count += row_offset;",
      "  column_index += row_offset * NNZ_V;",
      "",
      "  bool has_slash = true;",
      "  int64_t tmp_col_cnt = 0, tmp_blk_cnt = 0;",
      "  int64_t s = 0, v = 0;",
      "  int64_t v_idx = vertical_indexes[v++];",
      "  int64_t s_idx = slash_indexes[s++];",
      "  if (causal) {",
      "    while (s_idx >= end_m + (kv_seqlen - q_seqlen) && s < NNZ_S) {",
      "      s_idx = slash_indexes[s++];",
      "    }",
      "    if (s_idx > end_m + (kv_seqlen - q_seqlen)) has_slash = false;",
      "    s_idx = max((kv_seqlen - q_seqlen) + end_m - s_idx, BLOCK_SIZE_M);",
      "  } else {",
      "    while (s_idx >= end_m + kv_seqlen && s < NNZ_S) {",
      "      s_idx = slash_indexes[s++];",
      "    }",
      "    if (s_idx > end_m + kv_seqlen) has_slash = false;",
      "    s_idx = max(kv_seqlen + end_m - s_idx, BLOCK_SIZE_M);",
      "  }",
      "",
      "  int64_t range_start = s_idx - BLOCK_SIZE_M, range_end = s_idx;",
      "  if (!has_slash) {",
      "    if (causal) {",
      "      range_start = (kv_seqlen - q_seqlen) + end_m;",
      "      range_end = (kv_seqlen - q_seqlen) + end_m + BLOCK_SIZE_N;",
      "    } else {",
      "      range_start = kv_seqlen;",
      "      range_end = kv_seqlen + BLOCK_SIZE_N;",
      "    }",
      "  }",
      "",
      "  bool slash_finished = false;",
      "  while (1) {",
      "    if (v_idx < range_end) {",
      "      if (v_idx < range_start) {",
      "        column_index[tmp_col_cnt++] = v_idx;",
      "      }",
      "      if (v < NNZ_V) {",
      "        v_idx = vertical_indexes[v++];",
      "      } else {",
      "        if (causal)",
      "          v_idx = end_m + BLOCK_SIZE_N + (kv_seqlen - q_seqlen);",
      "        else",
      "          v_idx = end_m + BLOCK_SIZE_N + kv_seqlen;",
      "      }",
      "    } else {",
      "      if ((s < NNZ_S && causal) ||",
      "          (s < NNZ_S && !causal && slash_indexes[s] >= start_m)) {",
      "        if (causal)",
      "          s_idx = max((kv_seqlen - q_seqlen) + end_m - slash_indexes[s++],",
      "                      BLOCK_SIZE_M);",
      "        else",
      "          s_idx = max(kv_seqlen + end_m - slash_indexes[s++], BLOCK_SIZE_M);",
      "      } else {",
      "        if (v == NNZ_V || (v_idx > range_start && causal)) {",
      "          // add the last vertical if no more slash",
      "          if (v == NNZ_V && !causal && v_idx < kv_seqlen) {",
      "            column_index[tmp_col_cnt++] = v_idx;",
      "          }",
      "          tmp_blk_cnt = save_blocks(block_offset, range_start, range_end,",
      "                                    BLOCK_SIZE_N, tmp_blk_cnt, kv_seqlen);",
      "          break;",
      "        } else {",
      "          if (causal) {",
      "            range_start = (kv_seqlen - q_seqlen) + end_m;",
      "            range_end = (kv_seqlen - q_seqlen) + end_m + BLOCK_SIZE_N;",
      "          } else {",
      "            // if slash_finished but there are vertical left, save current",
      "            // blocks",
      "            tmp_blk_cnt = save_blocks(block_offset, range_start, range_end,",
      "                                      BLOCK_SIZE_N, tmp_blk_cnt, kv_seqlen);",
      "            range_start = kv_seqlen;",
      "            range_end = kv_seqlen + BLOCK_SIZE_N;",
      "          }",
      "          slash_finished = true;",
      "        }",
      "      }",
      "      if (!slash_finished) {",
      "        if (s_idx > range_end + BLOCK_SIZE_M) {",
      "          tmp_blk_cnt = save_blocks(block_offset, range_start, range_end,",
      "                                    BLOCK_SIZE_N, tmp_blk_cnt, kv_seqlen);",
      "          range_start = s_idx - BLOCK_SIZE_M;",
      "          range_end = s_idx;",
      "        } else if (s_idx > range_end) {",
      "          range_end += BLOCK_SIZE_M;",
      "        }",
      "      }",
      "    }",
      "  }",
      "",
      "  block_count[0] = tmp_blk_cnt;",
      "  column_count[0] = tmp_col_cnt;",
      "}",
      "",
      "void convert_vertical_slash_indexes_64x64(",
      "    const int* q_seqlens,         // [BATCH, ]",
      "    const int* kv_seqlens,        // [BATCH, ]",
      "    const int* vertical_indexes,  // [BATCH, N_HEADS, NNZ_V]",
      "    const int* slash_indexes,     // [BATCH, N_HEADS, NNZ_S]",
      "    int* block_count,             // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M)]",
      "    int* block_offset,  // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M), NNZ_S]",
      "    int* column_count,  // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M)]",
      "    int* column_index,  // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M), NNZ_V]",
      "    int64_t BATCH_SIZE, int64_t N_HEADS, int64_t N_ROWS, int64_t BLOCK_SIZE_M,",
      "    int64_t BLOCK_SIZE_N, int64_t NNZ_V, int64_t NNZ_S, bool causal) {",
      "  const int N_THREADS = 64;",
      "  const dim3 dimBlock(N_THREADS);",
      "  const dim3 dimGrid(N_HEADS, BATCH_SIZE, (N_ROWS + N_THREADS - 1) / N_THREADS);",
      "  convert_vertical_slash_indexes_kernel<<<dimGrid, dimBlock>>>(",
      "      q_seqlens, kv_seqlens, vertical_indexes, slash_indexes, block_count,",
      "      block_offset, column_count, column_index, N_HEADS, N_ROWS, BLOCK_SIZE_M,",
      "      BLOCK_SIZE_N, NNZ_V, NNZ_S, causal);",
      "}",
      "",
      "/**",
      " * Implements the Algorithm 4 in paper https://arxiv.org/abs/2407.02490.",
      " *",
      " * This function builds the index of each row of blocks from vertical indices",
      " * and slash indices. The vertical indices are treated as points, while the",
      " * slash indices are converted as ranges. The output consists of the merged",
      " * ranges and separate column indices, where the ranges are represented by",
      " * block indices.",
      " *",
      " * The implementation is referenced from the original MInference repo:",
      " * https://github.com/microsoft/MInference/blob/main/csrc/vertical_slash_index.cu.",
      " */",
      "void convert_vertical_slash_indexes(",
      "    torch::Tensor& block_count,      // [BATCH, N_HEADS, NUM_ROWS]",
      "    torch::Tensor& block_offset,     // [BATCH, N_HEADS, NUM_ROWS, NNZ_S]",
      "    torch::Tensor& column_count,     // [BATCH, N_HEADS, NUM_ROWS]",
      "    torch::Tensor& column_index,     // [BATCH, N_HEADS, NUM_ROWS, NNZ_V]",
      "    torch::Tensor q_seqlens,         // [BATCH, ]",
      "    torch::Tensor kv_seqlens,        // [BATCH, ]",
      "    torch::Tensor vertical_indexes,  // [BATCH, N_HEADS, NNZ_V]",
      "    torch::Tensor slash_indexes,     // [BATCH, N_HEADS, NNZ_S]",
      "    int64_t context_size, int64_t block_size_M, int64_t block_size_N,",
      "    bool causal) {",
      "  cudaSetDevice(q_seqlens.get_device());",
      "",
      "  int batch_size = slash_indexes.size(0);",
      "  int num_heads = slash_indexes.size(1);",
      "  int nnz_slash = slash_indexes.size(2);",
      "  int nnz_vertical = vertical_indexes.size(2);",
      "  int num_rows = (context_size + block_size_M - 1) / block_size_M;",
      "",
      "  convert_vertical_slash_indexes_64x64(",
      "      q_seqlens.data_ptr<int>(), kv_seqlens.data_ptr<int>(),",
      "      vertical_indexes.data_ptr<int>(), slash_indexes.data_ptr<int>(),",
      "      block_count.data_ptr<int>(), block_offset.data_ptr<int>(),",
      "      column_count.data_ptr<int>(), column_index.data_ptr<int>(), batch_size,",
      "      num_heads, num_rows, block_size_M, block_size_N, nnz_vertical, nnz_slash,",
      "      causal);",
      "}",
      "",
      "__global__ void convert_vertical_slash_indexes_kernel_mergehead(",
      "    const int* q_seqlens,         // [BATCH, ]",
      "    const int* kv_seqlens,        // [BATCH, ]",
      "    const int* vertical_indexes,  // [BATCH, N_HEADS, NNZ_V]",
      "    const int* slash_indexes,     // [BATCH, N_HEADS, NNZ_S]",
      "    const int* per_head_vertical_topkv, const int* per_head_slash_topkv,",
      "    int* block_count,   // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M)]",
      "    int* block_offset,  // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M), NNZ_S]",
      "    int* column_count,  // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M)]",
      "    int* column_index,  // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M), NNZ_V]",
      "    int64_t N_HEADS, int64_t N_ROWS, int64_t BLOCK_SIZE_M, int64_t BLOCK_SIZE_N,",
      "    int64_t NNZ_V, int64_t NNZ_S,",
      "    bool causal  // True for intra, False for succ",
      ") {",
      "  const int batch_idx = blockIdx.y;",
      "  const int head_idx = blockIdx.x;",
      "  const int group_idx = blockIdx.z;",
      "",
      "  int64_t q_seqlen = q_seqlens[batch_idx];",
      "  int64_t kv_seqlen = kv_seqlens[batch_idx];",
      "  int64_t block_idx_m = group_idx * blockDim.x + threadIdx.x;",
      "  int64_t start_m = block_idx_m * BLOCK_SIZE_M;",
      "  if (start_m >= q_seqlen) {",
      "    return;",
      "  }",
      "  int64_t end_m = start_m + BLOCK_SIZE_M;",
      "  vertical_indexes += (batch_idx * N_HEADS + head_idx) * NNZ_V;",
      "  slash_indexes += (batch_idx * N_HEADS + head_idx) * NNZ_S;",
      "  int64_t row_offset = (batch_idx * N_HEADS + head_idx) * N_ROWS + block_idx_m;",
      "  block_count += row_offset;",
      "  block_offset += row_offset * NNZ_S;",
      "  column_count += row_offset;",
      "  column_index += row_offset * NNZ_V;",
      "",
      "  // MergeHead: each head has it's unique max topk NNZ_V\uff0cNNZ_S. (NNZ_V\uff0cNNZ_S",
      "  // above is buffer size, use to compute offset)",
      "  NNZ_S = per_head_slash_topkv[head_idx];",
      "  NNZ_V = per_head_vertical_topkv[head_idx];",
      "",
      "  bool has_slash = true;",
      "  int64_t tmp_col_cnt = 0, tmp_blk_cnt = 0;",
      "  int64_t s = 0, v = 0;",
      "  int64_t v_idx = vertical_indexes[v++];",
      "  int64_t s_idx = slash_indexes[s++];",
      "  if (causal) {",
      "    while (s_idx >= end_m + (kv_seqlen - q_seqlen) && s < NNZ_S) {",
      "      s_idx = slash_indexes[s++];",
      "    }",
      "    if (s_idx > end_m + (kv_seqlen - q_seqlen)) has_slash = false;",
      "    s_idx = max((kv_seqlen - q_seqlen) + end_m - s_idx, BLOCK_SIZE_M);",
      "  } else {",
      "    while (s_idx >= end_m + kv_seqlen && s < NNZ_S) {",
      "      s_idx = slash_indexes[s++];",
      "    }",
      "    if (s_idx > end_m + kv_seqlen) has_slash = false;",
      "    s_idx = max(kv_seqlen + end_m - s_idx, BLOCK_SIZE_M);",
      "  }",
      "",
      "  int64_t range_start = s_idx - BLOCK_SIZE_M, range_end = s_idx;",
      "  if (!has_slash) {",
      "    if (causal) {",
      "      range_start = (kv_seqlen - q_seqlen) + end_m;",
      "      range_end = (kv_seqlen - q_seqlen) + end_m + BLOCK_SIZE_N;",
      "    } else {",
      "      range_start = kv_seqlen;",
      "      range_end = kv_seqlen + BLOCK_SIZE_N;",
      "    }",
      "  }",
      "",
      "  bool slash_finished = false;",
      "  while (1) {",
      "    if (v_idx < range_end) {",
      "      if (v_idx < range_start) {",
      "        column_index[tmp_col_cnt++] = v_idx;",
      "      }",
      "      if (v < NNZ_V) {",
      "        v_idx = vertical_indexes[v++];",
      "      } else {",
      "        if (causal)",
      "          v_idx = end_m + BLOCK_SIZE_N + (kv_seqlen - q_seqlen);",
      "        else",
      "          v_idx = end_m + BLOCK_SIZE_N + kv_seqlen;",
      "      }",
      "    } else {",
      "      if ((s < NNZ_S && causal) ||",
      "          (s < NNZ_S && !causal && slash_indexes[s] >= start_m)) {",
      "        if (causal)",
      "          s_idx = max((kv_seqlen - q_seqlen) + end_m - slash_indexes[s++],",
      "                      BLOCK_SIZE_M);",
      "        else",
      "          s_idx = max(kv_seqlen + end_m - slash_indexes[s++], BLOCK_SIZE_M);",
      "      } else {",
      "        if (v == NNZ_V || (v_idx > range_start && causal)) {",
      "          // add the last vertical if no more slash",
      "          if (v == NNZ_V && !causal && v_idx < kv_seqlen) {",
      "            column_index[tmp_col_cnt++] = v_idx;",
      "          }",
      "          tmp_blk_cnt = save_blocks(block_offset, range_start, range_end,",
      "                                    BLOCK_SIZE_N, tmp_blk_cnt, kv_seqlen);",
      "          break;",
      "        } else {",
      "          if (causal) {",
      "            range_start = (kv_seqlen - q_seqlen) + end_m;",
      "            range_end = (kv_seqlen - q_seqlen) + end_m + BLOCK_SIZE_N;",
      "          } else {",
      "            // if slash_finished but there are vertical left, save current",
      "            // blocks",
      "            tmp_blk_cnt = save_blocks(block_offset, range_start, range_end,",
      "                                      BLOCK_SIZE_N, tmp_blk_cnt, kv_seqlen);",
      "            range_start = kv_seqlen;",
      "            range_end = kv_seqlen + BLOCK_SIZE_N;",
      "          }",
      "          slash_finished = true;",
      "        }",
      "      }",
      "      if (!slash_finished) {",
      "        if (s_idx > range_end + BLOCK_SIZE_M) {",
      "          tmp_blk_cnt = save_blocks(block_offset, range_start, range_end,",
      "                                    BLOCK_SIZE_N, tmp_blk_cnt, kv_seqlen);",
      "          range_start = s_idx - BLOCK_SIZE_M;",
      "          range_end = s_idx;",
      "        } else if (s_idx > range_end) {",
      "          range_end += BLOCK_SIZE_M;",
      "        }",
      "      }",
      "    }",
      "  }",
      "",
      "  block_count[0] = tmp_blk_cnt;",
      "  column_count[0] = tmp_col_cnt;",
      "}",
      "",
      "void convert_vertical_slash_indexes_64x64_mergehead(",
      "    const int* q_seqlens,         // [BATCH, ]",
      "    const int* kv_seqlens,        // [BATCH, ]",
      "    const int* vertical_indexes,  // [BATCH, N_HEADS, NNZ_V]",
      "    const int* slash_indexes,     // [BATCH, N_HEADS, NNZ_S]",
      "    int* per_head_vertical_topkv, int* per_head_slash_topkv,",
      "    int* block_count,   // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M)]",
      "    int* block_offset,  // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M), NNZ_S]",
      "    int* column_count,  // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M)]",
      "    int* column_index,  // [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M), NNZ_V]",
      "    int64_t BATCH_SIZE, int64_t N_HEADS, int64_t N_ROWS, int64_t BLOCK_SIZE_M,",
      "    int64_t BLOCK_SIZE_N, int64_t NNZ_V, int64_t NNZ_S, bool causal) {",
      "  const int N_THREADS = 64;",
      "  const dim3 dimBlock(N_THREADS);",
      "  const dim3 dimGrid(N_HEADS, BATCH_SIZE, (N_ROWS + N_THREADS - 1) / N_THREADS);",
      "  convert_vertical_slash_indexes_kernel_mergehead<<<dimGrid, dimBlock>>>(",
      "      q_seqlens, kv_seqlens, vertical_indexes, slash_indexes,",
      "      per_head_vertical_topkv, per_head_slash_topkv, block_count, block_offset,",
      "      column_count, column_index, N_HEADS, N_ROWS, BLOCK_SIZE_M, BLOCK_SIZE_N,",
      "      NNZ_V, NNZ_S, causal);",
      "}",
      "",
      "/**",
      " * Implements the Algorithm 4 in paper https://arxiv.org/abs/2407.02490.",
      " *",
      " * Like the above convert_vertical_slash_indexes, but with",
      " * pre-computed vertical and slash counts.",
      " */",
      "void convert_vertical_slash_indexes_mergehead(",
      "    torch::Tensor& block_count,            // [BATCH, N_HEADS, NUM_ROWS]",
      "    torch::Tensor& block_offset,           // [BATCH, N_HEADS, NUM_ROWS, NNZ_S]",
      "    torch::Tensor& column_count,           // [BATCH, N_HEADS, NUM_ROWS]",
      "    torch::Tensor& column_index,           // [BATCH, N_HEADS, NUM_ROWS, NNZ_V]",
      "    torch::Tensor q_seqlens,               // [BATCH, ]",
      "    torch::Tensor kv_seqlens,              // [BATCH, ]",
      "    torch::Tensor vertical_indexes,        // [BATCH, N_HEADS, NNZ_V]",
      "    torch::Tensor slash_indexes,           // [BATCH, N_HEADS, NNZ_S]",
      "    torch::Tensor vertical_indices_count,  // [N_HEADS, ]",
      "    torch::Tensor slash_indices_count,     // [N_HEADS, ]",
      "    int64_t context_size, int64_t block_size_M, int64_t block_size_N,",
      "    bool causal) {",
      "  cudaSetDevice(q_seqlens.get_device());",
      "",
      "  int batch_size = slash_indexes.size(0);",
      "  int num_heads = slash_indexes.size(1);",
      "  int nnz_slash = slash_indexes.size(2);",
      "  int nnz_vertical = vertical_indexes.size(2);",
      "  int num_rows = (context_size + block_size_M - 1) / block_size_M;",
      "",
      "  convert_vertical_slash_indexes_64x64_mergehead(",
      "      q_seqlens.data_ptr<int>(), kv_seqlens.data_ptr<int>(),",
      "      vertical_indexes.data_ptr<int>(), slash_indexes.data_ptr<int>(),",
      "      vertical_indices_count.data_ptr<int>(),",
      "      slash_indices_count.data_ptr<int>(), block_count.data_ptr<int>(),",
      "      block_offset.data_ptr<int>(), column_count.data_ptr<int>(),",
      "      column_index.data_ptr<int>(), batch_size, num_heads, num_rows,",
      "      block_size_M, block_size_N, nnz_vertical, nnz_slash, causal);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/attention/attention_utils.cuh",
    "source": [
      "/*",
      " * Adapted from",
      " * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp",
      " * Copyright (c) 2023, The vLLM team.",
      " * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "#pragma once",
      "",
      "#include \"../cuda_compat.h\"",
      "#include \"attention_dtypes.h\"",
      "",
      "#include <float.h>",
      "#include <type_traits>",
      "",
      "namespace vllm {",
      "",
      "// Q*K^T operation.",
      "template <int THREAD_GROUP_SIZE, typename Vec, int N>",
      "inline __device__ float qk_dot_(const Vec (&q)[N], const Vec (&k)[N]) {",
      "  using A_vec = typename FloatVec<Vec>::Type;",
      "  // Compute the parallel products for Q*K^T (treat vector lanes separately).",
      "  A_vec qk_vec = mul<A_vec, Vec, Vec>(q[0], k[0]);",
      "#pragma unroll",
      "  for (int ii = 1; ii < N; ++ii) {",
      "    qk_vec = vllm::fma(q[ii], k[ii], qk_vec);",
      "  }",
      "",
      "  // Finalize the reduction across lanes.",
      "  float qk = sum(qk_vec);",
      "#pragma unroll",
      "  for (int mask = THREAD_GROUP_SIZE / 2; mask >= 1; mask /= 2) {",
      "    qk += VLLM_SHFL_XOR_SYNC(qk, mask);",
      "  }",
      "  return qk;",
      "}",
      "",
      "template <typename T, int THREAD_GROUP_SIZE>",
      "struct Qk_dot {",
      "  template <typename Vec, int N>",
      "  static inline __device__ float dot(const Vec (&q)[N], const Vec (&k)[N]) {",
      "    return qk_dot_<THREAD_GROUP_SIZE>(q, k);",
      "  }",
      "};",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/attention/merge_attn_states.cu",
    "source": [
      "#include <optional>",
      "#include <torch/all.h>",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <algorithm>",
      "",
      "#include \"attention_dtypes.h\"",
      "#include \"attention_utils.cuh\"",
      "",
      "namespace vllm {",
      "",
      "// Implements section 2.2 of https://www.arxiv.org/pdf/2501.01005",
      "// can be used to combine partial attention results (in the split-KV case)",
      "template <typename scalar_t, const uint NUM_THREADS>",
      "__global__ void merge_attn_states_kernel(",
      "    scalar_t* output, float* output_lse, const scalar_t* prefix_output,",
      "    const float* prefix_lse, const scalar_t* suffix_output,",
      "    const float* suffix_lse, const uint num_tokens, const uint num_heads,",
      "    const uint head_size) {",
      "  using pack_128b_t = uint4;",
      "  const uint pack_size = 16 / sizeof(scalar_t);",
      "  const uint threads_per_head = head_size / pack_size;",
      "",
      "  const uint global_idx = blockIdx.x * NUM_THREADS + threadIdx.x;",
      "  const uint token_head_threads = num_tokens * num_heads * threads_per_head;",
      "",
      "  if (global_idx >= token_head_threads) return;",
      "",
      "  // global_idx -> token_idx + head_idx + pack_idx",
      "  const uint token_head_idx = global_idx / threads_per_head;",
      "  const uint pack_idx = global_idx % threads_per_head;",
      "",
      "  const uint token_idx = token_head_idx / num_heads;",
      "  const uint head_idx = token_head_idx % num_heads;",
      "",
      "  const uint pack_offset = pack_idx * pack_size;  // (0~15)*8, etc.",
      "  const uint head_offset =",
      "      token_idx * num_heads * head_size + head_idx * head_size;",
      "  const scalar_t* prefix_head_ptr = prefix_output + head_offset;",
      "  const scalar_t* suffix_head_ptr = suffix_output + head_offset;",
      "  scalar_t* output_head_ptr = output + head_offset;",
      "",
      "  float p_lse = prefix_lse[head_idx * num_tokens + token_idx];",
      "  float s_lse = suffix_lse[head_idx * num_tokens + token_idx];",
      "  p_lse = std::isinf(p_lse) ? -std::numeric_limits<float>::infinity() : p_lse;",
      "  s_lse = std::isinf(s_lse) ? -std::numeric_limits<float>::infinity() : s_lse;",
      "",
      "  const float max_lse = fmaxf(p_lse, s_lse);",
      "  p_lse = p_lse - max_lse;",
      "  s_lse = s_lse - max_lse;",
      "  const float p_se = expf(p_lse);",
      "  const float s_se = expf(s_lse);",
      "  const float out_se = p_se + s_se;",
      "  const float p_scale = p_se / out_se;",
      "  const float s_scale = s_se / out_se;",
      "",
      "  if (pack_offset < head_size) {",
      "    // Pack 128b load",
      "    pack_128b_t p_out_pack = reinterpret_cast<const pack_128b_t*>(",
      "        prefix_head_ptr)[pack_offset / pack_size];",
      "    pack_128b_t s_out_pack = reinterpret_cast<const pack_128b_t*>(",
      "        suffix_head_ptr)[pack_offset / pack_size];",
      "    pack_128b_t o_out_pack;",
      "",
      "#pragma unroll",
      "    for (uint i = 0; i < pack_size; ++i) {",
      "      // Always use float for FMA to keep high precision.",
      "      // half(uint16_t), bfloat16, float -> float.",
      "      const float p_out_f =",
      "          vllm::to_float(reinterpret_cast<const scalar_t*>(&p_out_pack)[i]);",
      "      const float s_out_f =",
      "          vllm::to_float(reinterpret_cast<const scalar_t*>(&s_out_pack)[i]);",
      "      // fma: a * b + c = p_out_f * p_scale + (s_out_f * s_scale)",
      "      const float o_out_f = p_out_f * p_scale + (s_out_f * s_scale);",
      "      // float -> half(uint16_t), bfloat16, float.",
      "      vllm::from_float(reinterpret_cast<scalar_t*>(&o_out_pack)[i], o_out_f);",
      "    }",
      "",
      "    // Pack 128b storage",
      "    reinterpret_cast<pack_128b_t*>(output_head_ptr)[pack_offset / pack_size] =",
      "        o_out_pack;",
      "  }",
      "  // We only need to write to output_lse once per head.",
      "  if (output_lse != nullptr && pack_idx == 0) {",
      "    float out_lse = logf(out_se) + max_lse;",
      "    output_lse[head_idx * num_tokens + token_idx] = out_lse;",
      "  }",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "// The following macro is used to dispatch the conversion function based on",
      "// the output data type. The FN is a macro that calls a function with",
      "// template<typename scalar_t>.",
      "#define DISPATCH_BY_SCALAR_DTYPE(scalar_dtype, fn)                      \\",
      "  {                                                                     \\",
      "    if (scalar_dtype == at::ScalarType::Float) {                        \\",
      "      fn(float);                                                        \\",
      "    } else if (scalar_dtype == at::ScalarType::Half) {                  \\",
      "      fn(uint16_t);                                                     \\",
      "    } else if (scalar_dtype == at::ScalarType::BFloat16) {              \\",
      "      fn(__nv_bfloat16);                                                \\",
      "    } else {                                                            \\",
      "      TORCH_CHECK(false, \"Unsupported data type of O: \", scalar_dtype); \\",
      "    }                                                                   \\",
      "  }",
      "",
      "#define LAUNCH_MERGE_ATTN_STATES(scalar_t, NUM_THREADS)                     \\",
      "  {                                                                         \\",
      "    vllm::merge_attn_states_kernel<scalar_t, NUM_THREADS>                   \\",
      "        <<<grid, block, 0, stream>>>(                                       \\",
      "            reinterpret_cast<scalar_t*>(output.data_ptr()), output_lse_ptr, \\",
      "            reinterpret_cast<scalar_t*>(prefix_output.data_ptr()),          \\",
      "            reinterpret_cast<float*>(prefix_lse.data_ptr()),                \\",
      "            reinterpret_cast<scalar_t*>(suffix_output.data_ptr()),          \\",
      "            reinterpret_cast<float*>(suffix_lse.data_ptr()), num_tokens,    \\",
      "            num_heads, head_size);                                          \\",
      "  }",
      "",
      "/*@brief Merges the attention states from prefix and suffix",
      " * into the output tensor. NUM_TOKENS: n, NUM_HEADS: h, HEAD_SIZE: d",
      " *",
      " * @param output [n,h,d] The output tensor to store the merged attention states.",
      " * @param output_lse [h,d] Optional tensor to store the log-sum-exp values.",
      " * @param prefix_output [n,h,d] The prefix attention states.",
      " * @param prefix_lse [h,n] The log-sum-exp values for the prefix attention",
      " * states.",
      " * @param suffix_output [n,h,d] The suffix attention states.",
      " * @param suffix_lse [h,n] The log-sum-exp values for the suffix attention",
      " * states.",
      " */",
      "template <typename scalar_t>",
      "void merge_attn_states_launcher(torch::Tensor& output,",
      "                                std::optional<torch::Tensor> output_lse,",
      "                                const torch::Tensor& prefix_output,",
      "                                const torch::Tensor& prefix_lse,",
      "                                const torch::Tensor& suffix_output,",
      "                                const torch::Tensor& suffix_lse) {",
      "  constexpr uint NUM_THREADS = 128;",
      "  const uint num_tokens = output.size(0);",
      "  const uint num_heads = output.size(1);",
      "  const uint head_size = output.size(2);",
      "  const uint pack_size = 16 / sizeof(scalar_t);",
      "  TORCH_CHECK(head_size % pack_size == 0,",
      "              \"headsize must be multiple of pack_size:\", pack_size);",
      "  TORCH_CHECK(output.stride(-2) == head_size && output.stride(-1) == 1,",
      "              \"output heads must be contiguous in memory\");",
      "  TORCH_CHECK(",
      "      prefix_output.stride(-2) == head_size && prefix_output.stride(-1) == 1,",
      "      \"prefix_output heads must be contiguous in memory\");",
      "  TORCH_CHECK(",
      "      suffix_output.stride(-2) == head_size && suffix_output.stride(-1) == 1,",
      "      \"suffix_output heads must be contiguous in memory\");",
      "  float* output_lse_ptr = nullptr;",
      "  if (output_lse.has_value()) {",
      "    output_lse_ptr = output_lse.value().data_ptr<float>();",
      "  }",
      "  // Process one pack elements per thread. for float, the",
      "  // pack_size is 4 for half/bf16, the pack_size is 8.",
      "  const uint threads_per_head = head_size / pack_size;",
      "  const uint total_threads = num_tokens * num_heads * threads_per_head;",
      "",
      "  dim3 block(NUM_THREADS);",
      "  dim3 grid((total_threads + NUM_THREADS - 1) / NUM_THREADS);",
      "",
      "  const c10::cuda::OptionalCUDAGuard device_guard(prefix_output.device());",
      "  auto stream = at::cuda::getCurrentCUDAStream();",
      "",
      "  LAUNCH_MERGE_ATTN_STATES(scalar_t, NUM_THREADS);",
      "}",
      "",
      "#define CALL_MERGE_ATTN_STATES_LAUNCHER(scalar_t)                           \\",
      "  {                                                                         \\",
      "    merge_attn_states_launcher<scalar_t>(output, output_lse, prefix_output, \\",
      "                                         prefix_lse, suffix_output,         \\",
      "                                         suffix_lse);                       \\",
      "  }",
      "",
      "void merge_attn_states(torch::Tensor& output,",
      "                       std::optional<torch::Tensor> output_lse,",
      "                       const torch::Tensor& prefix_output,",
      "                       const torch::Tensor& prefix_lse,",
      "                       const torch::Tensor& suffix_output,",
      "                       const torch::Tensor& suffix_lse) {",
      "  DISPATCH_BY_SCALAR_DTYPE(output.dtype(), CALL_MERGE_ATTN_STATES_LAUNCHER);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/attention/attention_kernels.cuh",
    "source": [
      "/*",
      " * Adapted from",
      " * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp",
      " * Copyright (c) 2023, The vLLM team.",
      " * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "",
      "#include <torch/all.h>",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <algorithm>",
      "",
      "#include \"attention_dtypes.h\"",
      "#include \"attention_utils.cuh\"",
      "#include \"../cuda_compat.h\"",
      "",
      "#ifdef USE_ROCM",
      "  #include <hip/hip_bf16.h>",
      "  #include \"../quantization/fp8/amd/quant_utils.cuh\"",
      "typedef __hip_bfloat16 __nv_bfloat16;",
      "#else",
      "  #include \"../quantization/fp8/nvidia/quant_utils.cuh\"",
      "#endif",
      "",
      "#define MAX(a, b) ((a) > (b) ? (a) : (b))",
      "#define MIN(a, b) ((a) < (b) ? (a) : (b))",
      "#define DIVIDE_ROUND_UP(a, b) (((a) + (b) - 1) / (b))",
      "",
      "namespace vllm {",
      "",
      "// Utility function for attention softmax.",
      "template <int NUM_WARPS>",
      "inline __device__ float block_sum(float* red_smem, float sum) {",
      "  // Decompose the thread index into warp / lane.",
      "  int warp = threadIdx.x / WARP_SIZE;",
      "  int lane = threadIdx.x % WARP_SIZE;",
      "",
      "  // Compute the sum per warp.",
      "#pragma unroll",
      "  for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {",
      "    sum += VLLM_SHFL_XOR_SYNC(sum, mask);",
      "  }",
      "",
      "  // Warp leaders store the data to shared memory.",
      "  if (lane == 0) {",
      "    red_smem[warp] = sum;",
      "  }",
      "",
      "  // Make sure the data is in shared memory.",
      "  __syncthreads();",
      "",
      "  // The warps compute the final sums.",
      "  if (lane < NUM_WARPS) {",
      "    sum = red_smem[lane];",
      "  }",
      "",
      "  // Parallel reduction inside the warp.",
      "#pragma unroll",
      "  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {",
      "    sum += VLLM_SHFL_XOR_SYNC(sum, mask);",
      "  }",
      "",
      "  // Broadcast to other threads.",
      "  return VLLM_SHFL_SYNC(sum, 0);",
      "}",
      "",
      "// TODO(woosuk): Merge the last two dimensions of the grid.",
      "// Grid: (num_heads, num_seqs, max_num_partitions).",
      "template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,",
      "          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,",
      "          bool IS_BLOCK_SPARSE,",
      "          int PARTITION_SIZE = 0>  // Zero means no partitioning.",
      "__device__ void paged_attention_kernel(",
      "    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]",
      "    float* __restrict__ max_logits,  // [num_seqs, num_heads,",
      "                                     // max_num_partitions]",
      "    scalar_t* __restrict__ out,  // [num_seqs, num_heads, max_num_partitions,",
      "                                 // head_size]",
      "    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]",
      "    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,",
      "                                          // head_size/x, block_size, x]",
      "    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,",
      "                                          // head_size, block_size]",
      "    const int num_kv_heads,               // [num_heads]",
      "    const float scale,",
      "    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]",
      "    const int* __restrict__ seq_lens,      // [num_seqs]",
      "    const int max_num_blocks_per_seq,",
      "    const float* __restrict__ alibi_slopes,  // [num_heads]",
      "    const int q_stride, const int kv_block_stride, const int kv_head_stride,",
      "    const float* k_scale, const float* v_scale, const int tp_rank,",
      "    const int blocksparse_local_blocks, const int blocksparse_vert_stride,",
      "    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {",
      "  const int seq_idx = blockIdx.y;",
      "  const int partition_idx = blockIdx.z;",
      "  const int max_num_partitions = gridDim.z;",
      "  constexpr bool USE_PARTITIONING = PARTITION_SIZE > 0;",
      "  const int seq_len = seq_lens[seq_idx];",
      "  if (USE_PARTITIONING && partition_idx * PARTITION_SIZE >= seq_len) {",
      "    // No work to do. Terminate the thread block.",
      "    return;",
      "  }",
      "",
      "  const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);",
      "  const int num_blocks_per_partition =",
      "      USE_PARTITIONING ? PARTITION_SIZE / BLOCK_SIZE : num_seq_blocks;",
      "",
      "  // [start_block_idx, end_block_idx) is the range of blocks to process.",
      "  const int start_block_idx =",
      "      USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;",
      "  const int end_block_idx =",
      "      MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);",
      "  const int num_blocks = end_block_idx - start_block_idx;",
      "",
      "  // [start_token_idx, end_token_idx) is the range of tokens to process.",
      "  const int start_token_idx = start_block_idx * BLOCK_SIZE;",
      "  const int end_token_idx =",
      "      MIN(start_token_idx + num_blocks * BLOCK_SIZE, seq_len);",
      "  const int num_tokens = end_token_idx - start_token_idx;",
      "",
      "  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);",
      "  constexpr int NUM_THREAD_GROUPS =",
      "      NUM_THREADS / THREAD_GROUP_SIZE;  // Note: This assumes THREAD_GROUP_SIZE",
      "                                        // divides NUM_THREADS",
      "  assert(NUM_THREADS % THREAD_GROUP_SIZE == 0);",
      "  constexpr int NUM_TOKENS_PER_THREAD_GROUP =",
      "      DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);",
      "  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;",
      "  const int thread_idx = threadIdx.x;",
      "  const int warp_idx = thread_idx / WARP_SIZE;",
      "  const int lane = thread_idx % WARP_SIZE;",
      "",
      "  const int head_idx = blockIdx.x;",
      "  const int num_heads = gridDim.x;",
      "  const int num_queries_per_kv = num_heads / num_kv_heads;",
      "  const int kv_head_idx = head_idx / num_queries_per_kv;",
      "  const float alibi_slope =",
      "      alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];",
      "",
      "  // A vector type to store a part of a key or a query.",
      "  // The vector size is configured in such a way that the threads in a thread",
      "  // group fetch or compute 16 bytes at a time. For example, if the size of a",
      "  // thread group is 4 and the data type is half, then the vector size is 16 /",
      "  // (4 * sizeof(half)) == 2.",
      "  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1);",
      "  using K_vec = typename Vec<scalar_t, VEC_SIZE>::Type;",
      "  using Q_vec = typename Vec<scalar_t, VEC_SIZE>::Type;",
      "  using Quant_vec = typename Vec<cache_t, VEC_SIZE>::Type;",
      "",
      "  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;",
      "  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;",
      "",
      "  const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;",
      "  const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;",
      "",
      "  // Load the query to registers.",
      "  // Each thread in a thread group has a different part of the query.",
      "  // For example, if the thread group size is 4, then the first thread in",
      "  // the group has 0, 4, 8, ... th vectors of the query, and the second thread",
      "  // has 1, 5, 9, ... th vectors of the query, and so on. NOTE(woosuk): Because",
      "  // q is split from a qkv tensor, it may not be contiguous.",
      "  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;",
      "  __shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];",
      "#pragma unroll",
      "  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;",
      "       i += NUM_THREAD_GROUPS) {",
      "    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;",
      "    q_vecs[thread_group_offset][i] =",
      "        *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);",
      "  }",
      "  __syncthreads();  // TODO(naed90): possible speedup if this is replaced with a",
      "                    // memory wall right before we use q_vecs",
      "",
      "  // Memory planning.",
      "  extern __shared__ char shared_mem[];",
      "  // NOTE(woosuk): We use FP32 for the softmax logits for better accuracy.",
      "  float* logits = reinterpret_cast<float*>(shared_mem);",
      "  // Workspace for reduction.",
      "  __shared__ float red_smem[2 * NUM_WARPS];",
      "",
      "  // x == THREAD_GROUP_SIZE * VEC_SIZE",
      "  // Each thread group fetches x elements from the key at a time.",
      "  constexpr int x = 16 / sizeof(cache_t);",
      "  float qk_max = -FLT_MAX;",
      "",
      "  // Iterate over the key blocks.",
      "  // Each warp fetches a block of keys for each iteration.",
      "  // Each thread group in a warp fetches a key from the block, and computes",
      "  // dot product with the query.",
      "  const int* block_table = block_tables + seq_idx * max_num_blocks_per_seq;",
      "",
      "  // blocksparse specific vars",
      "  int bs_block_offset;",
      "  int q_bs_block_id;",
      "  if constexpr (IS_BLOCK_SPARSE) {",
      "    // const int num_blocksparse_blocks = DIVIDE_ROUND_UP(seq_len,",
      "    // blocksparse_block_size);",
      "    q_bs_block_id = (seq_len - 1) / blocksparse_block_size;",
      "    if (blocksparse_head_sliding_step >= 0)",
      "      // sliding on q heads",
      "      bs_block_offset =",
      "          (tp_rank * num_heads + head_idx) * blocksparse_head_sliding_step + 1;",
      "    else",
      "      // sliding on kv heads",
      "      bs_block_offset = (tp_rank * num_kv_heads + kv_head_idx) *",
      "                            (-blocksparse_head_sliding_step) +",
      "                        1;",
      "  }",
      "",
      "  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;",
      "       block_idx += NUM_WARPS) {",
      "    // NOTE(woosuk): The block number is stored in int32. However, we cast it to",
      "    // int64 because int32 can lead to overflow when this variable is multiplied",
      "    // by large numbers (e.g., kv_block_stride).",
      "    // For blocksparse attention: skip computation on blocks that are not",
      "    // attended",
      "    if constexpr (IS_BLOCK_SPARSE) {",
      "      const int k_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;",
      "      const bool is_remote =",
      "          ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);",
      "      const bool is_local =",
      "          (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);",
      "      if (!is_remote && !is_local) {",
      "        for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {",
      "          const int physical_block_offset =",
      "              (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;",
      "          const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;",
      "",
      "          if (thread_group_offset == 0) {",
      "            // NOTE(linxihui): assign very large number to skipped tokens to",
      "            // avoid contribution to the sumexp softmax normalizer. This will",
      "            // not be used at computing sum(softmax*v) as the blocks will be",
      "            // skipped.",
      "            logits[token_idx - start_token_idx] = -FLT_MAX;",
      "          }",
      "        }",
      "        continue;",
      "      }",
      "    }",
      "    const int64_t physical_block_number =",
      "        static_cast<int64_t>(block_table[block_idx]);",
      "",
      "    // Load a key to registers.",
      "    // Each thread in a thread group has a different part of the key.",
      "    // For example, if the thread group size is 4, then the first thread in",
      "    // the group has 0, 4, 8, ... th vectors of the key, and the second thread",
      "    // has 1, 5, 9, ... th vectors of the key, and so on.",
      "    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {",
      "      const int physical_block_offset =",
      "          (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;",
      "      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;",
      "      K_vec k_vecs[NUM_VECS_PER_THREAD];",
      "",
      "#pragma unroll",
      "      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {",
      "        const cache_t* k_ptr =",
      "            k_cache + physical_block_number * kv_block_stride +",
      "            kv_head_idx * kv_head_stride + physical_block_offset * x;",
      "        const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;",
      "        const int offset1 = (vec_idx * VEC_SIZE) / x;",
      "        const int offset2 = (vec_idx * VEC_SIZE) % x;",
      "",
      "        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {",
      "          k_vecs[j] = *reinterpret_cast<const K_vec*>(",
      "              k_ptr + offset1 * BLOCK_SIZE * x + offset2);",
      "        } else {",
      "          // Vector conversion from Quant_vec to K_vec.",
      "          Quant_vec k_vec_quant = *reinterpret_cast<const Quant_vec*>(",
      "              k_ptr + offset1 * BLOCK_SIZE * x + offset2);",
      "          k_vecs[j] = fp8::scaled_convert<K_vec, Quant_vec, KV_DTYPE>(",
      "              k_vec_quant, *k_scale);",
      "        }",
      "      }",
      "",
      "      // Compute dot product.",
      "      // This includes a reduction across the threads in the same thread group.",
      "      float qk = scale * Qk_dot<scalar_t, THREAD_GROUP_SIZE>::dot(",
      "                             q_vecs[thread_group_offset], k_vecs);",
      "      // Add the ALiBi bias if slopes are given.",
      "      qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;",
      "",
      "      if (thread_group_offset == 0) {",
      "        // Store the partial reductions to shared memory.",
      "        // NOTE(woosuk): It is required to zero out the masked logits.",
      "        const bool mask = token_idx >= seq_len;",
      "        logits[token_idx - start_token_idx] = mask ? 0.f : qk;",
      "        // Update the max value.",
      "        qk_max = mask ? qk_max : fmaxf(qk_max, qk);",
      "      }",
      "    }",
      "  }",
      "",
      "  // Perform reduction across the threads in the same warp to get the",
      "  // max qk value for each \"warp\" (not across the thread block yet).",
      "  // The 0-th thread of each thread group already has its max qk value.",
      "#pragma unroll",
      "  for (int mask = WARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {",
      "    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));",
      "  }",
      "  if (lane == 0) {",
      "    red_smem[warp_idx] = qk_max;",
      "  }",
      "  __syncthreads();",
      "",
      "  // TODO(woosuk): Refactor this part.",
      "  // Get the max qk value for the sequence.",
      "  qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;",
      "#pragma unroll",
      "  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {",
      "    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));",
      "  }",
      "  // Broadcast the max qk value to all threads.",
      "  qk_max = VLLM_SHFL_SYNC(qk_max, 0);",
      "",
      "  // Get the sum of the exp values.",
      "  float exp_sum = 0.f;",
      "  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {",
      "    float val = __expf(logits[i] - qk_max);",
      "    logits[i] = val;",
      "    exp_sum += val;",
      "  }",
      "  exp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);",
      "",
      "  // Compute softmax.",
      "  const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f);",
      "  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {",
      "    logits[i] *= inv_sum;",
      "  }",
      "  __syncthreads();",
      "",
      "  // If partitioning is enabled, store the max logit and exp_sum.",
      "  if (USE_PARTITIONING && thread_idx == 0) {",
      "    float* max_logits_ptr = max_logits +",
      "                            seq_idx * num_heads * max_num_partitions +",
      "                            head_idx * max_num_partitions + partition_idx;",
      "    *max_logits_ptr = qk_max;",
      "    float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions +",
      "                          head_idx * max_num_partitions + partition_idx;",
      "    *exp_sums_ptr = exp_sum;",
      "  }",
      "",
      "  // Each thread will fetch 16 bytes from the value cache at a time.",
      "  constexpr int V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE);",
      "  using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;",
      "  using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;",
      "  using V_quant_vec = typename Vec<cache_t, V_VEC_SIZE>::Type;",
      "  using Float_L_vec = typename FloatVec<L_vec>::Type;",
      "",
      "  constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;",
      "  constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;",
      "  constexpr int NUM_ROWS_PER_THREAD =",
      "      DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER);",
      "",
      "  // NOTE(woosuk): We use FP32 for the accumulator for better accuracy.",
      "  float accs[NUM_ROWS_PER_THREAD];",
      "#pragma unroll",
      "  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {",
      "    accs[i] = 0.f;",
      "  }",
      "",
      "  scalar_t zero_value;",
      "  zero(zero_value);",
      "  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;",
      "       block_idx += NUM_WARPS) {",
      "    // NOTE(woosuk): The block number is stored in int32. However, we cast it to",
      "    // int64 because int32 can lead to overflow when this variable is multiplied",
      "    // by large numbers (e.g., kv_block_stride).",
      "    // For blocksparse attention: skip computation on blocks that are not",
      "    // attended",
      "    if constexpr (IS_BLOCK_SPARSE) {",
      "      int v_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;",
      "      if (!((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) &&",
      "          !((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {",
      "        continue;",
      "      }",
      "    }",
      "    const int64_t physical_block_number =",
      "        static_cast<int64_t>(block_table[block_idx]);",
      "    const int physical_block_offset = (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;",
      "    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;",
      "    L_vec logits_vec;",
      "    from_float(logits_vec, *reinterpret_cast<Float_L_vec*>(logits + token_idx -",
      "                                                           start_token_idx));",
      "",
      "    const cache_t* v_ptr = v_cache + physical_block_number * kv_block_stride +",
      "                           kv_head_idx * kv_head_stride;",
      "#pragma unroll",
      "    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {",
      "      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;",
      "      if (row_idx < HEAD_SIZE) {",
      "        const int offset = row_idx * BLOCK_SIZE + physical_block_offset;",
      "        V_vec v_vec;",
      "",
      "        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {",
      "          v_vec = *reinterpret_cast<const V_vec*>(v_ptr + offset);",
      "        } else {",
      "          V_quant_vec v_quant_vec =",
      "              *reinterpret_cast<const V_quant_vec*>(v_ptr + offset);",
      "          // Vector conversion from V_quant_vec to V_vec.",
      "          v_vec = fp8::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec,",
      "                                                                    *v_scale);",
      "        }",
      "        if (block_idx == num_seq_blocks - 1) {",
      "          // NOTE(woosuk): When v_vec contains the tokens that are out of the",
      "          // context, we should explicitly zero out the values since they may",
      "          // contain NaNs. See",
      "          // https://github.com/vllm-project/vllm/issues/641#issuecomment-1682544472",
      "          scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vec);",
      "#pragma unroll",
      "          for (int j = 0; j < V_VEC_SIZE; j++) {",
      "            v_vec_ptr[j] = token_idx + j < seq_len ? v_vec_ptr[j] : zero_value;",
      "          }",
      "        }",
      "        accs[i] += dot(logits_vec, v_vec);",
      "      }",
      "    }",
      "  }",
      "",
      "  // Perform reduction within each warp.",
      "#pragma unroll",
      "  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {",
      "    float acc = accs[i];",
      "#pragma unroll",
      "    for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {",
      "      acc += VLLM_SHFL_XOR_SYNC(acc, mask);",
      "    }",
      "    accs[i] = acc;",
      "  }",
      "",
      "  // NOTE(woosuk): A barrier is required because the shared memory space for",
      "  // logits is reused for the output.",
      "  __syncthreads();",
      "",
      "  // Perform reduction across warps.",
      "  float* out_smem = reinterpret_cast<float*>(shared_mem);",
      "#pragma unroll",
      "  for (int i = NUM_WARPS; i > 1; i /= 2) {",
      "    int mid = i / 2;",
      "    // Upper warps write to shared memory.",
      "    if (warp_idx >= mid && warp_idx < i) {",
      "      float* dst = &out_smem[(warp_idx - mid) * HEAD_SIZE];",
      "#pragma unroll",
      "      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {",
      "        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;",
      "        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {",
      "          dst[row_idx] = accs[i];",
      "        }",
      "      }",
      "    }",
      "    __syncthreads();",
      "",
      "    // Lower warps update the output.",
      "    if (warp_idx < mid) {",
      "      const float* src = &out_smem[warp_idx * HEAD_SIZE];",
      "#pragma unroll",
      "      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {",
      "        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;",
      "        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {",
      "          accs[i] += src[row_idx];",
      "        }",
      "      }",
      "    }",
      "    __syncthreads();",
      "  }",
      "",
      "  // Write the final output.",
      "  if (warp_idx == 0) {",
      "    scalar_t* out_ptr =",
      "        out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +",
      "        head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE;",
      "#pragma unroll",
      "    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {",
      "      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;",
      "      if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {",
      "        from_float(*(out_ptr + row_idx), accs[i]);",
      "      }",
      "    }",
      "  }",
      "}",
      "",
      "// Grid: (num_heads, num_seqs, 1).",
      "template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,",
      "          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,",
      "          bool IS_BLOCK_SPARSE>",
      "__global__ void paged_attention_v1_kernel(",
      "    scalar_t* __restrict__ out,           // [num_seqs, num_heads, head_size]",
      "    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]",
      "    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,",
      "                                          // head_size/x, block_size, x]",
      "    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,",
      "                                          // head_size, block_size]",
      "    const int num_kv_heads,               // [num_heads]",
      "    const float scale,",
      "    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]",
      "    const int* __restrict__ seq_lens,      // [num_seqs]",
      "    const int max_num_blocks_per_seq,",
      "    const float* __restrict__ alibi_slopes,  // [num_heads]",
      "    const int q_stride, const int kv_block_stride, const int kv_head_stride,",
      "    const float* k_scale, const float* v_scale, const int tp_rank,",
      "    const int blocksparse_local_blocks, const int blocksparse_vert_stride,",
      "    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {",
      "  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,",
      "                         KV_DTYPE, IS_BLOCK_SPARSE>(",
      "      /* exp_sums */ nullptr, /* max_logits */ nullptr, out, q, k_cache,",
      "      v_cache, num_kv_heads, scale, block_tables, seq_lens,",
      "      max_num_blocks_per_seq, alibi_slopes, q_stride, kv_block_stride,",
      "      kv_head_stride, k_scale, v_scale, tp_rank, blocksparse_local_blocks,",
      "      blocksparse_vert_stride, blocksparse_block_size,",
      "      blocksparse_head_sliding_step);",
      "}",
      "",
      "// Grid: (num_heads, num_seqs, max_num_partitions).",
      "template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,",
      "          int NUM_THREADS, vllm::Fp8KVCacheDataType KV_DTYPE,",
      "          bool IS_BLOCK_SPARSE,",
      "          int PARTITION_SIZE>",
      "__global__ void paged_attention_v2_kernel(",
      "    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]",
      "    float* __restrict__ max_logits,       // [num_seqs, num_heads,",
      "                                          // max_num_partitions]",
      "    scalar_t* __restrict__ tmp_out,       // [num_seqs, num_heads,",
      "                                          // max_num_partitions, head_size]",
      "    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]",
      "    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,",
      "                                          // head_size/x, block_size, x]",
      "    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,",
      "                                          // head_size, block_size]",
      "    const int num_kv_heads,               // [num_heads]",
      "    const float scale,",
      "    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]",
      "    const int* __restrict__ seq_lens,      // [num_seqs]",
      "    const int max_num_blocks_per_seq,",
      "    const float* __restrict__ alibi_slopes,  // [num_heads]",
      "    const int q_stride, const int kv_block_stride, const int kv_head_stride,",
      "    const float* k_scale, const float* v_scale, const int tp_rank,",
      "    const int blocksparse_local_blocks, const int blocksparse_vert_stride,",
      "    const int blocksparse_block_size, const int blocksparse_head_sliding_step) {",
      "  paged_attention_kernel<scalar_t, cache_t, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS,",
      "                         KV_DTYPE, IS_BLOCK_SPARSE, PARTITION_SIZE>(",
      "      exp_sums, max_logits, tmp_out, q, k_cache, v_cache, num_kv_heads, scale,",
      "      block_tables, seq_lens, max_num_blocks_per_seq, alibi_slopes, q_stride,",
      "      kv_block_stride, kv_head_stride, k_scale, v_scale, tp_rank,",
      "      blocksparse_local_blocks, blocksparse_vert_stride, blocksparse_block_size,",
      "      blocksparse_head_sliding_step);",
      "}",
      "",
      "// Grid: (num_heads, num_seqs).",
      "template <typename scalar_t, int HEAD_SIZE, int NUM_THREADS,",
      "          int PARTITION_SIZE>",
      "__global__ void paged_attention_v2_reduce_kernel(",
      "    scalar_t* __restrict__ out,            // [num_seqs, num_heads, head_size]",
      "    const float* __restrict__ exp_sums,    // [num_seqs, num_heads,",
      "                                           // max_num_partitions]",
      "    const float* __restrict__ max_logits,  // [num_seqs, num_heads,",
      "                                           // max_num_partitions]",
      "    const scalar_t* __restrict__ tmp_out,  // [num_seqs, num_heads,",
      "                                           // max_num_partitions, head_size]",
      "    const int* __restrict__ seq_lens,      // [num_seqs]",
      "    const int max_num_partitions) {",
      "  const int num_heads = gridDim.x;",
      "  const int head_idx = blockIdx.x;",
      "  const int seq_idx = blockIdx.y;",
      "  const int seq_len = seq_lens[seq_idx];",
      "  const int num_partitions = DIVIDE_ROUND_UP(seq_len, PARTITION_SIZE);",
      "  if (num_partitions == 1) {",
      "    // No need to reduce. Only copy tmp_out to out.",
      "    scalar_t* out_ptr =",
      "        out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;",
      "    const scalar_t* tmp_out_ptr =",
      "        tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +",
      "        head_idx * max_num_partitions * HEAD_SIZE;",
      "    for (int i = threadIdx.x; i < HEAD_SIZE; i += blockDim.x) {",
      "      out_ptr[i] = tmp_out_ptr[i];",
      "    }",
      "    // Terminate the thread block.",
      "    return;",
      "  }",
      "",
      "  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;",
      "  const int warp_idx = threadIdx.x / WARP_SIZE;",
      "  const int lane = threadIdx.x % WARP_SIZE;",
      "",
      "  // Size: 2 * num_partitions.",
      "  extern __shared__ char shared_mem[];",
      "  // Workspace for reduction.",
      "  __shared__ float red_smem[2 * NUM_WARPS];",
      "",
      "  // Load max logits to shared memory.",
      "  float* shared_max_logits = reinterpret_cast<float*>(shared_mem);",
      "  const float* max_logits_ptr = max_logits +",
      "                                seq_idx * num_heads * max_num_partitions +",
      "                                head_idx * max_num_partitions;",
      "  float max_logit = -FLT_MAX;",
      "  for (int i = threadIdx.x; i < num_partitions; i += blockDim.x) {",
      "    const float l = max_logits_ptr[i];",
      "    shared_max_logits[i] = l;",
      "    max_logit = fmaxf(max_logit, l);",
      "  }",
      "  __syncthreads();",
      "",
      "  // Get the global max logit.",
      "  // Reduce within the warp.",
      "#pragma unroll",
      "  for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {",
      "    max_logit = fmaxf(max_logit, VLLM_SHFL_XOR_SYNC(max_logit, mask));",
      "  }",
      "  if (lane == 0) {",
      "    red_smem[warp_idx] = max_logit;",
      "  }",
      "  __syncthreads();",
      "  // Reduce across warps.",
      "  max_logit = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;",
      "#pragma unroll",
      "  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {",
      "    max_logit = fmaxf(max_logit, VLLM_SHFL_XOR_SYNC(max_logit, mask));",
      "  }",
      "  // Broadcast the max value to all threads.",
      "  max_logit = VLLM_SHFL_SYNC(max_logit, 0);",
      "",
      "  // Load rescaled exp sums to shared memory.",
      "  float* shared_exp_sums =",
      "      reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions);",
      "  const float* exp_sums_ptr = exp_sums +",
      "                              seq_idx * num_heads * max_num_partitions +",
      "                              head_idx * max_num_partitions;",
      "  float global_exp_sum = 0.0f;",
      "  for (int i = threadIdx.x; i < num_partitions; i += blockDim.x) {",
      "    float l = shared_max_logits[i];",
      "    float rescaled_exp_sum = exp_sums_ptr[i] * expf(l - max_logit);",
      "    global_exp_sum += rescaled_exp_sum;",
      "    shared_exp_sums[i] = rescaled_exp_sum;",
      "  }",
      "  __syncthreads();",
      "  global_exp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], global_exp_sum);",
      "  const float inv_global_exp_sum = __fdividef(1.0f, global_exp_sum + 1e-6f);",
      "",
      "  // Aggregate tmp_out to out.",
      "  const scalar_t* tmp_out_ptr =",
      "      tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +",
      "      head_idx * max_num_partitions * HEAD_SIZE;",
      "  scalar_t* out_ptr =",
      "      out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;",
      "#pragma unroll",
      "  for (int i = threadIdx.x; i < HEAD_SIZE; i += NUM_THREADS) {",
      "    float acc = 0.0f;",
      "    for (int j = 0; j < num_partitions; ++j) {",
      "      acc += to_float(tmp_out_ptr[j * HEAD_SIZE + i]) * shared_exp_sums[j] *",
      "             inv_global_exp_sum;",
      "    }",
      "    from_float(out_ptr[i], acc);",
      "  }",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "#undef MAX",
      "#undef MIN",
      "#undef DIVIDE_ROUND_UP"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/attention/attention_generic.cuh",
    "source": [
      "/*",
      " * Adapted from",
      " * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h",
      " * Copyright (c) 2023, The vLLM team.",
      " * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "#pragma once",
      "",
      "#include <stdint.h>",
      "",
      "namespace vllm {",
      "",
      "// A vector type to store Q, K, V elements.",
      "template <typename T, int VEC_SIZE>",
      "struct Vec {};",
      "",
      "// A vector type to store FP32 accumulators.",
      "template <typename T>",
      "struct FloatVec {};",
      "",
      "// Template vector operations.",
      "template <typename Acc, typename A, typename B>",
      "inline __device__ Acc mul(A a, B b);",
      "",
      "template <typename T>",
      "inline __device__ float sum(T v);",
      "",
      "template <typename T>",
      "inline __device__ float dot(T a, T b) {",
      "  return sum(mul<T, T, T>(a, b));",
      "}",
      "",
      "template <typename A, typename T>",
      "inline __device__ float dot(T a, T b) {",
      "  return sum(mul<A, T, T>(a, b));",
      "}",
      "",
      "template <typename T>",
      "inline __device__ void zero(T& dst) {",
      "  constexpr int WORDS = sizeof(T) / 4;",
      "  union {",
      "    T raw;",
      "    uint32_t words[WORDS];",
      "  } tmp;",
      "",
      "#pragma unroll",
      "  for (int ii = 0; ii < WORDS; ++ii) {",
      "    tmp.words[ii] = 0u;",
      "  }",
      "  dst = tmp.raw;",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/attention/mla/cutlass_mla_kernels.cu",
    "source": [
      "/*",
      " * Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "",
      "#include <torch/all.h>",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#include \"cute/tensor.hpp\"",
      "",
      "#include \"cutlass/cutlass.h\"",
      "#include \"cutlass/kernel_hardware_info.h\"",
      "",
      "#include \"cutlass_extensions/common.hpp\"",
      "",
      "#include \"device/sm100_mla.hpp\"",
      "#include \"kernel/sm100_mla_tile_scheduler.hpp\"",
      "",
      "using namespace cute;",
      "using namespace cutlass::fmha::kernel;",
      "",
      "template <typename T, bool PersistenceOption = true>",
      "struct MlaSm100 {",
      "  using Element = T;",
      "  using ElementAcc = float;",
      "  using ElementOut = T;",
      "",
      "  using TileShape = Shape<_128, _128, Shape<_512, _64>>;",
      "  using TileShapeH = cute::tuple_element_t<0, TileShape>;",
      "  using TileShapeD = cute::tuple_element_t<2, TileShape>;",
      "",
      "  // H K (D_latent D_rope) B",
      "  using ProblemShape = cute::tuple<TileShapeH, int, TileShapeD, int>;",
      "",
      "  using StrideQ = cute::tuple<int64_t, _1, int64_t>;  // H D B",
      "  using StrideK = cute::tuple<int64_t, _1, int64_t>;  // K D B",
      "  using StrideO = StrideK;                            // H D B",
      "  using StrideLSE = cute::tuple<_1, int>;             // H B",
      "",
      "  using TileScheduler =",
      "      std::conditional_t<PersistenceOption, Sm100MlaPersistentTileScheduler,",
      "                         Sm100MlaIndividualTileScheduler>;",
      "",
      "  using FmhaKernel =",
      "      cutlass::fmha::kernel::Sm100FmhaMlaKernelTmaWarpspecialized<",
      "          TileShape, Element, ElementAcc, ElementOut, ElementAcc, TileScheduler,",
      "          /*kIsCpAsync=*/true>;",
      "  using Fmha = cutlass::fmha::device::MLA<FmhaKernel>;",
      "};",
      "",
      "template <typename T>",
      "typename T::Fmha::Arguments args_from_options(",
      "    at::Tensor const& out, at::Tensor const& q_nope, at::Tensor const& q_pe,",
      "    at::Tensor const& kv_c_and_k_pe_cache, at::Tensor const& seq_lens,",
      "    at::Tensor const& page_table, double scale) {",
      "  cutlass::KernelHardwareInfo hw_info;",
      "  hw_info.device_id = q_nope.device().index();",
      "  hw_info.sm_count =",
      "      cutlass::KernelHardwareInfo::query_device_multiprocessor_count(",
      "          hw_info.device_id);",
      "",
      "  int batches = q_nope.sizes()[0];",
      "  int page_count_per_seq = page_table.sizes()[1];",
      "  int page_count_total = kv_c_and_k_pe_cache.sizes()[0];",
      "  int page_size = kv_c_and_k_pe_cache.sizes()[1];",
      "  int max_seq_len = page_size * page_count_per_seq;",
      "  using TileShapeH = typename T::TileShapeH;",
      "  using TileShapeD = typename T::TileShapeD;",
      "  auto problem_shape =",
      "      cute::make_tuple(TileShapeH{}, max_seq_len, TileShapeD{}, batches);",
      "",
      "  auto [H, K, D, B] = problem_shape;",
      "  auto [D_latent, D_rope] = D;",
      "",
      "  using StrideQ = typename T::StrideQ;",
      "  using StrideK = typename T::StrideK;",
      "  using StrideO = typename T::StrideO;",
      "  using StrideLSE = typename T::StrideLSE;",
      "",
      "  StrideQ stride_Q_latent = cute::make_tuple(",
      "      static_cast<int64_t>(D_latent), _1{}, static_cast<int64_t>(H * D_latent));",
      "  StrideQ stride_Q_rope = cute::make_tuple(static_cast<int64_t>(D_rope), _1{},",
      "                                           static_cast<int64_t>(H * D_rope));",
      "  StrideK stride_C =",
      "      cute::make_tuple(static_cast<int64_t>(D_latent + D_rope), _1{},",
      "                       static_cast<int64_t>(page_size * (D_latent + D_rope)));",
      "  StrideLSE stride_PT = cute::make_stride(_1{}, page_count_per_seq);",
      "  StrideLSE stride_LSE = cute::make_tuple(_1{}, static_cast<int>(H));",
      "  StrideO stride_O = cute::make_tuple(static_cast<int64_t>(D_latent), _1{},",
      "                                      static_cast<int64_t>(H * D_latent));",
      "",
      "  using Element = typename T::Element;",
      "  using ElementOut = typename T::ElementOut;",
      "  using ElementAcc = typename T::ElementAcc;",
      "  auto Q_latent_ptr = static_cast<Element*>(q_nope.data_ptr());",
      "  auto Q_rope_ptr = static_cast<Element*>(q_pe.data_ptr());",
      "  auto C_ptr = static_cast<Element*>(kv_c_and_k_pe_cache.data_ptr());",
      "  auto scale_f = static_cast<float>(scale);",
      "  typename T::Fmha::Arguments arguments{",
      "      problem_shape,",
      "      {scale_f, Q_latent_ptr, stride_Q_latent, Q_rope_ptr, stride_Q_rope, C_ptr,",
      "       stride_C, C_ptr + D_latent, stride_C,",
      "       static_cast<int*>(seq_lens.data_ptr()),",
      "       static_cast<int*>(page_table.data_ptr()), stride_PT, page_count_total,",
      "       page_size},",
      "      {static_cast<ElementOut*>(out.data_ptr()), stride_O,",
      "       static_cast<ElementAcc*>(nullptr), stride_LSE},",
      "      hw_info,",
      "      1,        // split_kv",
      "      nullptr,  // is_var_split_kv",
      "  };",
      "  // TODO(kaixih@nvidia): When split_kv=-1 and is_var_split_kv=false, we compute",
      "  // split_kv automatically based on batch size and sequence length to balance",
      "  // workload across available SMs. Consider using var_split_kv for manual",
      "  // control if needed.",
      "  T::Fmha::set_split_kv(arguments);",
      "  return arguments;",
      "}",
      "",
      "template <typename Element>",
      "void runMla(at::Tensor const& out, at::Tensor const& q_nope,",
      "            at::Tensor const& q_pe, at::Tensor const& kv_c_and_k_pe_cache,",
      "            at::Tensor const& seq_lens, at::Tensor const& page_table,",
      "            float scale, cudaStream_t stream) {",
      "  using MlaSm100Type = MlaSm100<Element>;",
      "  typename MlaSm100Type::Fmha fmha;",
      "  auto arguments = args_from_options<MlaSm100Type>(",
      "      out, q_nope, q_pe, kv_c_and_k_pe_cache, seq_lens, page_table, scale);",
      "  size_t workspace_size = MlaSm100Type::Fmha::get_workspace_size(arguments);",
      "  auto const workspace_options =",
      "      torch::TensorOptions().dtype(torch::kUInt8).device(q_nope.device());",
      "  auto workspace = torch::empty(workspace_size, workspace_options);",
      "",
      "  CUTLASS_CHECK(fmha.can_implement(arguments));",
      "",
      "  CUTLASS_CHECK(fmha.initialize(arguments, workspace.data_ptr(), stream));",
      "",
      "  CUTLASS_CHECK(fmha.run(arguments, workspace.data_ptr(), stream));",
      "}",
      "",
      "void cutlass_mla_decode_sm100a(torch::Tensor const& out,",
      "                               torch::Tensor const& q_nope,",
      "                               torch::Tensor const& q_pe,",
      "                               torch::Tensor const& kv_c_and_k_pe_cache,",
      "                               torch::Tensor const& seq_lens,",
      "                               torch::Tensor const& page_table, double scale) {",
      "  TORCH_CHECK(q_nope.device().is_cuda(), \"q_nope must be on CUDA\");",
      "  TORCH_CHECK(q_nope.dim() == 3, \"q_nope must be a 3D tensor\");",
      "  TORCH_CHECK(q_pe.dim() == 3, \"q_pe must be a 3D tensor\");",
      "  TORCH_CHECK(kv_c_and_k_pe_cache.dim() == 3,",
      "              \"kv_c_and_k_pe_cache must be a 3D tensor\");",
      "  TORCH_CHECK(seq_lens.dim() == 1, \"seq_lens must be a 1D tensor\");",
      "  TORCH_CHECK(page_table.dim() == 2, \"page_table must be a 2D tensor\");",
      "  TORCH_CHECK(out.dim() == 3, \"out must be a 3D tensor\");",
      "",
      "  auto B_q_nope = q_nope.size(0);",
      "  auto H_q_nope = q_nope.size(1);",
      "  auto D_q_nope = q_nope.size(2);",
      "  auto B_q_pe = q_pe.size(0);",
      "  auto H_q_pe = q_pe.size(1);",
      "  auto D_q_pe = q_pe.size(2);",
      "  auto B_pt = page_table.size(0);",
      "  auto PAGE_NUM = page_table.size(1);",
      "  auto PAGE_SIZE = kv_c_and_k_pe_cache.size(1);",
      "  auto D_ckv = kv_c_and_k_pe_cache.size(2);",
      "  auto B_o = out.size(0);",
      "  auto H_o = out.size(1);",
      "  auto D_o = out.size(2);",
      "",
      "  TORCH_CHECK(D_q_nope == 512, \"D_q_nope must be equal to 512\");",
      "  TORCH_CHECK(D_q_pe == 64, \"D_q_pe must be equal to 64\");",
      "  TORCH_CHECK(D_ckv == 576, \"D_ckv must be equal to 576\");",
      "  TORCH_CHECK(H_q_nope == H_q_pe && H_q_nope == H_o && H_o == 128,",
      "              \"H_q_nope, H_q_pe, and H_o must be equal to 128\");",
      "  TORCH_CHECK(PAGE_SIZE > 0 && (PAGE_SIZE & (PAGE_SIZE - 1)) == 0,",
      "              \"PAGE_SIZE must be a power of 2\");",
      "  TORCH_CHECK(",
      "      B_q_nope == B_q_pe && B_q_nope == B_pt && B_q_nope == B_o,",
      "      \"Batch dims must be same for page_table, q_nope and q_pe, and out\");",
      "  TORCH_CHECK(PAGE_NUM % (128 / PAGE_SIZE) == 0,",
      "              \"PAGE_NUM must be divisible by 128 / PAGE_SIZE\");",
      "  TORCH_CHECK(D_o == 512, \"D_o must be equal to 512\");",
      "",
      "  TORCH_CHECK(q_nope.dtype() == at::ScalarType::Half ||",
      "                  q_nope.dtype() == at::ScalarType::BFloat16 ||",
      "                  q_nope.dtype() == at::ScalarType::Float8_e4m3fn,",
      "              \"q_nope must be a half, bfloat16, or float8_e4m3fn tensor\");",
      "  TORCH_CHECK(kv_c_and_k_pe_cache.dtype() == q_nope.dtype() &&",
      "                  q_nope.dtype() == q_pe.dtype(),",
      "              \"kv_c_and_k_pe_cache, q_nope, and q_pe must be the same type\");",
      "  TORCH_CHECK(seq_lens.dtype() == torch::kInt32,",
      "              \"seq_lens must be a 32-bit integer tensor\");",
      "  TORCH_CHECK(page_table.dtype() == torch::kInt32,",
      "              \"page_table must be a 32-bit integer tensor\");",
      "",
      "  auto in_dtype = q_nope.dtype();",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(q_nope));",
      "  const cudaStream_t stream =",
      "      at::cuda::getCurrentCUDAStream(q_nope.get_device());",
      "  if (in_dtype == at::ScalarType::Half) {",
      "    runMla<cutlass::half_t>(out, q_nope, q_pe, kv_c_and_k_pe_cache, seq_lens,",
      "                            page_table, scale, stream);",
      "  } else if (in_dtype == at::ScalarType::BFloat16) {",
      "    runMla<cutlass::bfloat16_t>(out, q_nope, q_pe, kv_c_and_k_pe_cache,",
      "                                seq_lens, page_table, scale, stream);",
      "  } else if (in_dtype == at::ScalarType::Float8_e4m3fn) {",
      "    runMla<cutlass::float_e4m3_t>(out, q_nope, q_pe, kv_c_and_k_pe_cache,",
      "                                  seq_lens, page_table, scale, stream);",
      "  } else {",
      "    TORCH_CHECK(false, \"Unsupported input data type of MLA\");",
      "  }",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/attention/mla/sm100_cutlass_mla_kernel.cu",
    "source": [
      "/*",
      "Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.",
      "Copyright 2025 SGLang Team. All Rights Reserved.",
      "",
      "Licensed under the Apache License, Version 2.0 (the \"License\");",
      "you may not use this file except in compliance with the License.",
      "You may obtain a copy of the License at",
      "",
      "    http://www.apache.org/licenses/LICENSE-2.0",
      "",
      "Unless required by applicable law or agreed to in writing, software",
      "distributed under the License is distributed on an \"AS IS\" BASIS,",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      "See the License for the specific language governing permissions and",
      "limitations under the License.",
      "==============================================================================*/",
      "/*",
      " * Taken from SGLANG PR https://github.com/sgl-project/sglang/pull/6929",
      " * by Alcanderian JieXin Liang",
      " */",
      "#include \"core/registration.h\"",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <cutlass/cutlass.h>",
      "#include <cutlass/kernel_hardware_info.h>",
      "#include <torch/all.h>",
      "",
      "#include <cute/tensor.hpp>",
      "#include <iostream>",
      "",
      "#include \"cutlass_sm100_mla/device/sm100_mla.hpp\"",
      "#include \"cutlass_sm100_mla/kernel/sm100_mla_tile_scheduler.hpp\"",
      "",
      "// clang-format off",
      "#if !defined(CUDA_VERSION) || CUDA_VERSION < 12040",
      "void sm100_cutlass_mla_decode(",
      "    torch::Tensor const& out,",
      "    torch::Tensor const& lse,",
      "    torch::Tensor const& q_nope,",
      "    torch::Tensor const& q_pe,",
      "    torch::Tensor const& kv_c_and_k_pe_cache,",
      "    torch::Tensor const& seq_lens,",
      "    torch::Tensor const& page_table,",
      "    torch::Tensor const& workspace,",
      "    int64_t num_kv_splits) {",
      "  TORCH_CHECK(false, \"CUDA version must be >= 12.4 for cutlass_mla_decode\");",
      "}",
      "int64_t sm100_cutlass_mla_get_workspace_size(int64_t max_seq_len, int64_t num_batches, int64_t sm_count, int64_t num_kv_splits) {",
      "  TORCH_CHECK(false, \"CUDA version must be >= 12.4 for cutlass_mla_get_workspace_size\");",
      "}",
      "#else",
      "",
      "#define CUTLASS_CHECK(status)                                                       \\",
      "  {                                                                                 \\",
      "    cutlass::Status error = status;                                                 \\",
      "    TORCH_CHECK(error == cutlass::Status::kSuccess, cutlassGetStatusString(error)); \\",
      "  }",
      "",
      "using namespace cute;",
      "using namespace cutlass::fmha::kernel;",
      "",
      "template <bool v>",
      "struct IsPersistent {",
      "  static const bool value = v;",
      "};",
      "",
      "template <typename T, typename TOut, bool IsPaged128, typename PersistenceOption = IsPersistent<true>>",
      "struct MlaSm100 {",
      "  using Element = T;",
      "  using ElementAcc = float;",
      "  using ElementOut = TOut;",
      "",
      "  using TileShape = Shape<_128, _128, Shape<_512, _64>>;",
      "  using TileShapeH = cute::tuple_element_t<0, TileShape>;",
      "  using TileShapeD = cute::tuple_element_t<2, TileShape>;",
      "",
      "  // H K (D_latent D_rope) B",
      "  using ProblemShape = cute::tuple<TileShapeH, int, TileShapeD, int>;",
      "",
      "  using StrideQ = cute::tuple<int64_t, _1, int64_t>;  // H D B",
      "  using StrideK = cute::tuple<int64_t, _1, int64_t>;  // K D B",
      "  using StrideO = StrideK;                            // H D B",
      "  using StrideLSE = cute::tuple<_1, int>;             // H B",
      "",
      "  using TileScheduler =",
      "      std::conditional_t<PersistenceOption::value, Sm100MlaPersistentTileScheduler, Sm100MlaIndividualTileScheduler>;",
      "",
      "  using FmhaKernel = cutlass::fmha::kernel::Sm100FmhaMlaKernelTmaWarpspecialized<",
      "      TileShape,",
      "      Element,",
      "      ElementAcc,",
      "      ElementOut,",
      "      ElementAcc,",
      "      TileScheduler,",
      "      /*kIsCpAsync=*/!IsPaged128>;",
      "  using Fmha = cutlass::fmha::device::MLA<FmhaKernel>;",
      "};",
      "",
      "template <typename T>",
      "typename T::Fmha::Arguments args_from_options(",
      "    at::Tensor const& out,",
      "    at::Tensor const& lse,",
      "    at::Tensor const& q_nope,",
      "    at::Tensor const& q_pe,",
      "    at::Tensor const& kv_c_and_k_pe_cache,",
      "    at::Tensor const& seq_lens,",
      "    at::Tensor const& page_table,",
      "    double sm_scale,",
      "    int64_t num_kv_splits) {",
      "  cutlass::KernelHardwareInfo hw_info;",
      "  hw_info.device_id = q_nope.device().index();",
      "  hw_info.sm_count = cutlass::KernelHardwareInfo::query_device_multiprocessor_count(hw_info.device_id);",
      "",
      "  int batches = q_nope.sizes()[0];",
      "  int page_count_per_seq = page_table.sizes()[1];",
      "  int page_count_total = kv_c_and_k_pe_cache.sizes()[0];",
      "  int page_size = kv_c_and_k_pe_cache.sizes()[1];",
      "  int max_seq_len = page_size * page_count_per_seq;",
      "  using TileShapeH = typename T::TileShapeH;",
      "  using TileShapeD = typename T::TileShapeD;",
      "  auto problem_shape = cute::make_tuple(TileShapeH{}, max_seq_len, TileShapeD{}, batches);",
      "",
      "  auto [H, K, D, B] = problem_shape;",
      "  auto [D_latent, D_rope] = D;",
      "",
      "  float scale = float(sm_scale);",
      "",
      "  using StrideQ = typename T::StrideQ;",
      "  using StrideK = typename T::StrideK;",
      "  using StrideO = typename T::StrideO;",
      "  using StrideLSE = typename T::StrideLSE;",
      "",
      "  StrideQ stride_Q_nope = cute::make_tuple(",
      "      static_cast<int64_t>(q_nope.stride(1)), _1{}, static_cast<int64_t>(q_nope.stride(0)));",
      "  StrideQ stride_Q_pe = cute::make_tuple(",
      "      static_cast<int64_t>(q_pe.stride(1)), _1{}, static_cast<int64_t>(q_pe.stride(0)));",
      "",
      "  StrideK stride_C = cute::make_tuple(",
      "      static_cast<int64_t>(0 + D_latent + D_rope), _1{}, static_cast<int64_t>(page_size * (D_latent + D_rope)));",
      "  StrideLSE stride_PT = cute::make_stride(_1{}, page_count_per_seq);",
      "  StrideLSE stride_LSE = cute::make_tuple(_1{}, 0 + H);",
      "  StrideO stride_O = cute::make_tuple(static_cast<int64_t>(0 + D_latent), _1{}, static_cast<int64_t>(0 + H * D_latent));",
      "",
      "  using Element = typename T::Element;",
      "  using ElementOut = typename T::ElementOut;",
      "  using ElementAcc = typename T::ElementAcc;",
      "  auto Q_nope_ptr = static_cast<Element*>(q_nope.data_ptr());",
      "  auto Q_pe_ptr = static_cast<Element*>(q_pe.data_ptr());",
      "  auto C_ptr = static_cast<Element*>(kv_c_and_k_pe_cache.data_ptr());",
      "  typename T::Fmha::Arguments arguments{",
      "      problem_shape,",
      "      {scale,",
      "       Q_nope_ptr,",
      "       stride_Q_nope,",
      "       Q_pe_ptr,",
      "       stride_Q_pe,",
      "       C_ptr,",
      "       stride_C,",
      "       C_ptr + D_latent,",
      "       stride_C,",
      "       static_cast<int*>(seq_lens.data_ptr()),",
      "       static_cast<int*>(page_table.data_ptr()),",
      "       stride_PT,",
      "       page_count_total,",
      "       page_size},",
      "      {static_cast<ElementOut*>(out.data_ptr()),",
      "       stride_O,",
      "       static_cast<ElementAcc*>(lse.defined() ? lse.data_ptr() : nullptr),",
      "       stride_LSE},",
      "      hw_info,",
      "      // TODO(trevor-m): Change split_kv back to -1 when",
      "      // https://github.com/NVIDIA/cutlass/issues/2274 is fixed. Split_kv=1 will",
      "      // perform worse with larger context length and smaller batch sizes.",
      "      static_cast<int>(num_kv_splits), // split_kv",
      "      nullptr,       // is_var_split_kv",
      "  };",
      "  // TODO(kaixih@nvidia): When split_kv=-1 and is_var_split_kv=false, we compute",
      "  // split_kv automatically based on batch size and sequence length to balance",
      "  // workload across available SMs. Consider using var_split_kv for manual",
      "  // control if needed.",
      "  T::Fmha::set_split_kv(arguments);",
      "  return arguments;",
      "}",
      "",
      "template <typename Element, typename ElementOut, bool IsPaged128, typename PersistenceOption>",
      "void runMla(",
      "    at::Tensor const& out,",
      "    at::Tensor const& lse,",
      "    at::Tensor const& q_nope,",
      "    at::Tensor const& q_pe,",
      "    at::Tensor const& kv_c_and_k_pe_cache,",
      "    at::Tensor const& seq_lens,",
      "    at::Tensor const& page_table,",
      "    at::Tensor const& workspace,",
      "    double sm_scale,",
      "    int64_t num_kv_splits,",
      "    cudaStream_t stream) {",
      "  using MlaSm100Type = MlaSm100<Element, ElementOut, IsPaged128, PersistenceOption>;",
      "  typename MlaSm100Type::Fmha fmha;",
      "  auto arguments = args_from_options<MlaSm100Type>(out, lse, q_nope, q_pe, kv_c_and_k_pe_cache, seq_lens, page_table, sm_scale, num_kv_splits);",
      "",
      "  CUTLASS_CHECK(fmha.can_implement(arguments));",
      "",
      "  CUTLASS_CHECK(fmha.initialize(arguments, workspace.data_ptr(), stream));",
      "",
      "  CUTLASS_CHECK(fmha.run(arguments, workspace.data_ptr(), stream));",
      "}",
      "",
      "#define DISPATCH_BOOL(expr, const_expr, ...) \\",
      "  [&]() -> bool {                            \\",
      "    if (expr) {                              \\",
      "      constexpr bool const_expr = true;      \\",
      "      return __VA_ARGS__();                  \\",
      "    } else {                                 \\",
      "      constexpr bool const_expr = false;     \\",
      "      return __VA_ARGS__();                  \\",
      "    }                                        \\",
      "  }()",
      "",
      "void sm100_cutlass_mla_decode(",
      "    torch::Tensor const& out,",
      "    torch::Tensor const& lse,",
      "    torch::Tensor const& q_nope,",
      "    torch::Tensor const& q_pe,",
      "    torch::Tensor const& kv_c_and_k_pe_cache,",
      "    torch::Tensor const& seq_lens,",
      "    torch::Tensor const& page_table,",
      "    torch::Tensor const& workspace,",
      "    double sm_scale,",
      "    int64_t num_kv_splits) {",
      "  auto in_dtype = q_nope.dtype();",
      "  at::cuda::CUDAGuard device_guard{(char)q_nope.get_device()};",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream(q_nope.get_device());",
      "  const int page_size = kv_c_and_k_pe_cache.sizes()[1];",
      "  ",
      "  // NOTE(alcanderian): IsPersistent has bug with manual split_kv.",
      "  // Kernel will hang if batch is too large with large num_kv_splits. (for example bs=8, num_kv_splits=8)",
      "  // Maybe per batch split kv will fix this.",
      "  DISPATCH_BOOL(page_size == 128, IsPaged128, [&] {",
      "    DISPATCH_BOOL(num_kv_splits <= 1, NotManualSplitKV, [&] {",
      "      if (in_dtype == at::ScalarType::Half) {",
      "        runMla<cutlass::half_t, cutlass::half_t, IsPaged128, IsPersistent<NotManualSplitKV>>(",
      "          out, lse, q_nope, q_pe, kv_c_and_k_pe_cache, seq_lens, page_table, workspace, sm_scale, num_kv_splits, stream);",
      "      } else if (in_dtype == at::ScalarType::BFloat16) {",
      "        runMla<cutlass::bfloat16_t, cutlass::bfloat16_t, IsPaged128, IsPersistent<NotManualSplitKV>>(",
      "          out, lse, q_nope, q_pe, kv_c_and_k_pe_cache, seq_lens, page_table, workspace, sm_scale, num_kv_splits, stream);",
      "      } else if (in_dtype == at::ScalarType::Float8_e4m3fn) {",
      "        runMla<cutlass::float_e4m3_t, cutlass::bfloat16_t, IsPaged128, IsPersistent<NotManualSplitKV>>(",
      "          out, lse, q_nope, q_pe, kv_c_and_k_pe_cache, seq_lens, page_table, workspace, sm_scale, num_kv_splits, stream);",
      "      } else {",
      "        TORCH_CHECK(false, \"Unsupported input data type of MLA\");",
      "      }",
      "      return true;",
      "    });",
      "    return true;",
      "  });",
      "}",
      "",
      "int64_t sm100_cutlass_mla_get_workspace_size(int64_t max_seq_len, int64_t num_batches, int64_t sm_count, int64_t num_kv_splits) {",
      "  // Workspace size depends on ElementAcc and ElementLSE (same as ElementAcc)",
      "  // which are float, so Element type here doesn't matter.",
      "  using MlaSm100Type = MlaSm100<cutlass::half_t, cutlass::half_t, true>;",
      "",
      "  // Get split kv. Requires problem shape and sm_count only.",
      "  typename MlaSm100Type::Fmha::Arguments arguments;",
      "  using TileShapeH = typename MlaSm100Type::TileShapeH;",
      "  using TileShapeD = typename MlaSm100Type::TileShapeD;",
      "  arguments.problem_shape =",
      "      cute::make_tuple(TileShapeH{}, static_cast<int>(max_seq_len), TileShapeD{}, static_cast<int>(num_batches));",
      "  // Assumes device 0 when getting sm_count.",
      "  arguments.hw_info.sm_count =",
      "      sm_count <= 0 ? cutlass::KernelHardwareInfo::query_device_multiprocessor_count(/*device_id=*/0) : sm_count;",
      "  arguments.split_kv = static_cast<int>(num_kv_splits);",
      "  MlaSm100Type::Fmha::set_split_kv(arguments);",
      "",
      "  return MlaSm100Type::Fmha::get_workspace_size(arguments);",
      "}",
      "",
      "#endif",
      "",
      "TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, CUDA, m) {",
      "  m.impl(\"sm100_cutlass_mla_decode\", &sm100_cutlass_mla_decode);",
      "}",
      "",
      "TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, CatchAll, m) {",
      "  m.impl(\"sm100_cutlass_mla_get_workspace_size\", &sm100_cutlass_mla_get_workspace_size);",
      "}",
      "",
      "// clang-format on"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/attention/mla/cutlass_mla_entry.cu",
    "source": [
      "/*",
      " * Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "",
      "#include <torch/all.h>",
      "",
      "#if defined ENABLE_CUTLASS_MLA && ENABLE_CUTLASS_MLA",
      "void cutlass_mla_decode_sm100a(torch::Tensor const& out,",
      "                               torch::Tensor const& q_nope,",
      "                               torch::Tensor const& q_pe,",
      "                               torch::Tensor const& kv_c_and_k_pe_cache,",
      "                               torch::Tensor const& seq_lens,",
      "                               torch::Tensor const& page_table, double scale);",
      "#endif",
      "",
      "void cutlass_mla_decode(torch::Tensor const& out, torch::Tensor const& q_nope,",
      "                        torch::Tensor const& q_pe,",
      "                        torch::Tensor const& kv_c_and_k_pe_cache,",
      "                        torch::Tensor const& seq_lens,",
      "                        torch::Tensor const& page_table, double scale) {",
      "#if defined ENABLE_CUTLASS_MLA && ENABLE_CUTLASS_MLA",
      "  return cutlass_mla_decode_sm100a(out, q_nope, q_pe, kv_c_and_k_pe_cache,",
      "                                   seq_lens, page_table, scale);",
      "#endif",
      "  TORCH_CHECK_NOT_IMPLEMENTED(false, \"No compiled cutlass MLA\");",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/cutlass_extensions/vllm_collective_builder.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"cutlass_extensions/gemm/collective/collective_builder.hpp\"",
      "",
      "namespace cutlass::gemm::collective {",
      "using namespace cute;",
      "",
      "//",
      "// VLLMCollectiveBuilder is a wrapper around CollectiveBuilder that allows for",
      "// for custom kernel tags, allowing you to build custom collectives. Without",
      "// touching the cutlass library headers, using `CutlassKernelTag` will mean it",
      "// will resort to using the standard cutlass collective builder.",
      "//",
      "",
      "// Use the default Cutlass collective builder, i.e. use an unmodified cutless",
      "// collective",
      "struct CutlassKernelTag {};",
      "",
      "template <class KernelTag, class ArchTag, class OpClass, class ElementA,",
      "          class GmemLayoutA, int AlignmentA, class ElementB, class GmemLayoutB,",
      "          int AlignmentB, class ElementAccumulator, class TileShape_MNK,",
      "          class ClusterShape_MNK, class StageCountType,",
      "          class KernelScheduleType, class Enable = void>",
      "struct VLLMCollectiveBuilder {",
      "  static_assert(sizeof(ElementA) == 0,",
      "                \"Could not build a collective for given parameters.\");",
      "};",
      "",
      "template <class ArchTag, class OpClass, class ElementA, class GmemLayoutA,",
      "          int AlignmentA, class ElementB, class GmemLayoutB, int AlignmentB,",
      "          class ElementAccumulator, class TileShape_MNK, class ClusterShape_MNK,",
      "          class StageCountType, class KernelScheduleType>",
      "struct VLLMCollectiveBuilder<",
      "    CutlassKernelTag, ArchTag, OpClass, ElementA, GmemLayoutA, AlignmentA,",
      "    ElementB, GmemLayoutB, AlignmentB, ElementAccumulator, TileShape_MNK,",
      "    ClusterShape_MNK, StageCountType, KernelScheduleType> {",
      "  using CollectiveOp = typename CollectiveBuilder<",
      "      ArchTag, OpClass, ElementA, GmemLayoutA, AlignmentA, ElementB,",
      "      GmemLayoutB, AlignmentB, ElementAccumulator, TileShape_MNK,",
      "      ClusterShape_MNK, StageCountType, KernelScheduleType>::CollectiveOp;",
      "};",
      "",
      "};  // namespace cutlass::gemm::collective"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/cutlass_extensions/vllm_numeric_conversion.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"cutlass/numeric_conversion.h\"",
      "#include \"cutlass_extensions/vllm_custom_types.cuh\"",
      "#include \"cutlass_extensions/cute_utils.cuh\"",
      "#include \"cutlass_extensions/vllm_type_utils.cuh\"",
      "",
      "// this file extends:",
      "//   https://github.com/NVIDIA/cutlass/blob/cutlass-3.5.0/include/cutlass/numeric_conversion.h",
      "// with vllm specific type conversions, namely: vllm_uint4b8_t, vllm_uint8b128_t",
      "// as well as adds interleaved numeric array converters for specific types.",
      "// (interleaved numeric array converters can be more efficient for subbyte",
      "// types)",
      "",
      "namespace cutlass {",
      "",
      "// InterleavedNumericArrayConverter is like NumericArrayConverter but also",
      "// deinterleaves converted elements based on IlvBlkLayout, interleaving can",
      "// make subbyte converts more efficient by allowing for efficient extraction",
      "// of subbyte elements from a 32bit register.",
      "template <typename IlvBlkLayout, typename T, typename S, int N,",
      "          FloatRoundStyle Round = FloatRoundStyle::round_to_nearest,",
      "          class Enable = void>",
      "struct InterleavedNumericArrayConverter {",
      "  using Converter = NumericArrayConverter<T, S, N, Round>;",
      "",
      "  using result_type = typename Converter::result_type;",
      "  using source_type = typename Converter::source_type;",
      "",
      "  CUTLASS_DEVICE",
      "  static result_type convert(source_type const& source) {",
      "    if (cute::elect_one_sync()) {",
      "      if constexpr (std::is_same_v<IlvBlkLayout, void>) {",
      "        printf(",
      "            \"Convert %s <= %s (N = %d, IlvBlkLayout = void), not implemented\\n\",",
      "            nameof_v<T>, nameof_v<S>, N);",
      "      } else {",
      "        printf(",
      "            \"Convert %s <= %s (N = %d, size(IlvBlkLayout{}) = %d), not \"",
      "            \"implemented\\n\",",
      "            nameof_v<T>, nameof_v<S>, N, size(IlvBlkLayout{}));",
      "      }",
      "      __brkpt();",
      "    }",
      "    return {};",
      "  }",
      "",
      "  CUTLASS_DEVICE",
      "  result_type operator()(source_type const& s) const { return convert(s); }",
      "};",
      "",
      "template <typename IlvBlkLayout, typename T, typename S, int N,",
      "          FloatRoundStyle Round>",
      "struct InterleavedNumericArrayConverter<",
      "    IlvBlkLayout, T, S, N, Round,",
      "    std::enable_if_t<is_identity_layout<IlvBlkLayout>()>> {",
      "  using Converter = NumericArrayConverter<T, S, N, Round>;",
      "",
      "  using result_type = typename Converter::result_type;",
      "  using source_type = typename Converter::source_type;",
      "",
      "  CUTLASS_DEVICE",
      "  static result_type convert(source_type const& source) {",
      "    return Converter::convert(source);",
      "  }",
      "",
      "  CUTLASS_DEVICE",
      "  result_type operator()(source_type const& s) const { return convert(s); }",
      "};",
      "",
      "template <typename RegConvert32bit, typename T, typename S, int N>",
      "struct ArrayConverterPacked32Bit {",
      "  using result_type = Array<T, N>;",
      "  using source_type = Array<S, N>;",
      "",
      "  using result_packed_8_t = Array<T, 8>;",
      "  using result_packed_4_t = Array<T, 4>;",
      "  using result_packed_2_t = Array<T, 2>;",
      "  using src_packed_8_t = Array<S, 8>;",
      "  using src_packed_4_t = Array<S, 4>;",
      "  using src_packed_2_t = Array<S, 2>;",
      "",
      "  static_assert(N % 2 == 0, \"N must be a multiple of 2\");",
      "  static_assert(cutlass::sizeof_bits_v<S> >= 4);  // TODO: add 16 packed sources",
      "  static_assert(32 % cutlass::sizeof_bits_v<S> == 0);",
      "  static constexpr auto src_elems_per_32bit_reg =",
      "      32 / cutlass::sizeof_bits_v<S>;",
      "",
      "  // Maybe not Valid. ScalarConverter will not actually work unless",
      "  // NumericConverter<T, S, Round> is implemented. However it won't be used",
      "  // anyways since we assert N % 2 == 0, just here for compliance with",
      "  // VectorizedConverter.",
      "  using ScalarConverter = NumericConverter<T, S>;",
      "",
      "  template <typename PackedSrc>",
      "  CUTLASS_DEVICE static auto to_regs(PackedSrc const& src) {",
      "    if constexpr (sizeof(PackedSrc) == 1) {",
      "      return Array<uint32_t, 1>{reinterpret_cast<uint8_t const&>(src)};",
      "    } else if constexpr (sizeof(PackedSrc) == 2) {",
      "      return Array<uint32_t, 1>{reinterpret_cast<uint16_t const&>(src)};",
      "    } else if constexpr (sizeof(PackedSrc) == 4) {",
      "      return Array<uint32_t, 1>{reinterpret_cast<uint32_t const&>(src)};",
      "    } else {",
      "      static_assert(sizeof(PackedSrc) == 8);",
      "      return reinterpret_cast<Array<uint32_t, 2> const&>(src);",
      "    }",
      "  }",
      "",
      "  // The core converter uses bit tricks to construct a known FP16 number, then",
      "  // does a subtraction in FP16 for the final result.",
      "  template <typename PackedResultType, typename PackedSrcType>",
      "  CUTLASS_DEVICE static PackedResultType packed_convert(",
      "      PackedSrcType const& source) {",
      "    static_assert(PackedSrcType::kElements == PackedResultType::kElements);",
      "    static_assert(PackedResultType::kElements == 2 ||",
      "                      PackedResultType::kElements == 4 ||",
      "                      PackedResultType::kElements == 8,",
      "                  \"Invalid PackedResultType must be 2, 4 or 8.\");",
      "    static_assert(std::is_same_v<typename PackedSrcType::Element, S>);",
      "    static_assert(std::is_same_v<typename PackedResultType::Element, T>);",
      "",
      "    return RegConvert32bit::template convert<PackedResultType>(to_regs(source));",
      "  }",
      "",
      "  friend class detail::VectorizedConverter;",
      "",
      " public:",
      "  CUTLASS_DEVICE static result_type convert(source_type const& source) {",
      "    result_type result;",
      "    using ConverterType =",
      "        ArrayConverterPacked32Bit<RegConvert32bit,",
      "                                  typename result_type::Element,",
      "                                  typename source_type::Element, N>;",
      "",
      "    if constexpr (src_elems_per_32bit_reg >= 8) {",
      "      detail::VectorizedConverter::convert<",
      "          ConverterType, result_packed_8_t, src_packed_8_t, result_packed_4_t,",
      "          src_packed_4_t, result_packed_2_t, src_packed_2_t>(result, source);",
      "    } else if constexpr (src_elems_per_32bit_reg >= 4) {",
      "      detail::VectorizedConverter::convert<ConverterType, result_packed_4_t,",
      "                                           src_packed_4_t, result_packed_2_t,",
      "                                           src_packed_2_t>(result, source);",
      "    } else {",
      "      detail::VectorizedConverter::convert<ConverterType, result_packed_2_t,",
      "                                           src_packed_2_t>(result, source);",
      "    }",
      "",
      "    return result;",
      "  }",
      "};",
      "",
      "// Convert 8 4bit values packed into a 32bit register to 8 8bit values packed",
      "// into 2 32bit register.",
      "template <uint8_t LUT0, uint8_t LUT1, uint8_t LUT2, uint8_t LUT3,    //",
      "          uint8_t LUT4, uint8_t LUT5, uint8_t LUT6, uint8_t LUT7,    //",
      "          uint8_t LUT8, uint8_t LUT9, uint8_t LUT10, uint8_t LUT11,  //",
      "          uint8_t LUT12, uint8_t LUT13, uint8_t LUT14, uint8_t LUT15>",
      "CUTLASS_DEVICE cutlass::AlignedArray<uint32_t, 2> lut_4bit_to_8bit_convert(",
      "    uint32_t src) {",
      "  cutlass::AlignedArray<uint32_t, 2> r;",
      "  // Determines if the value is in the top half of the LUT if set or",
      "  //  (i.e. LUT[8:15]) in the bottom half (i.e. LUT[0:7]) if not set. Then move",
      "  //  into bit position 0x4 of each nibble so when or'd with final_prmt_base it",
      "  //  selects the correct candidate. When elements in final_prmt_base",
      "  //  are >= 0x4, the high candidate is selected (i.e. LUT[8:15]), when elements",
      "  //  are  < 0x4, the low candidate is selected (i.e. LUT[0:7])",
      "  uint32_t high_bit = (src & 0x88888888) >> 1;",
      "",
      "  // `high_bit` is OR'd with 0x31203120 to find the correct value in the LUT",
      "  // (selects correct high or low candidate)",
      "  const uint32_t final_prmt_base = 0x32103210;",
      "",
      "  // Ignore the high bit when indexing into LUT, for each 4bit value",
      "  //  we index into both the high and low candidates then use",
      "  //  high_bit | final_prmt_base to select the correct candidate",
      "  uint32_t lut_idx = (src & 0x77777777);",
      "",
      "  auto pack = [](uint8_t a, uint8_t b, uint8_t c, uint8_t d) {",
      "    return uint32_t(a) | (uint32_t(b) << 8) | (uint32_t(c) << 16) |",
      "           (uint32_t(d) << 24);",
      "  };",
      "",
      "  static constexpr uint32_t LOW_0 = pack(LUT0, LUT1, LUT2, LUT3);",
      "  static constexpr uint32_t LOW_1 = pack(LUT4, LUT5, LUT6, LUT7);",
      "  static constexpr uint32_t HIGH_0 = pack(LUT8, LUT9, LUT10, LUT11);",
      "  static constexpr uint32_t HIGH_1 = pack(LUT12, LUT13, LUT14, LUT15);",
      "",
      "  CUTLASS_PRAGMA_UNROLL",
      "  for (int ii = 0; ii < 2; ++ii, lut_idx >>= 16, high_bit >>= 16) {",
      "    uint32_t final_prmt_idx = final_prmt_base | high_bit;",
      "",
      "    // This uses a look up table to convert packed int4s to packed int8s,",
      "    // using the int4 value as the index to prmt. It first select both the",
      "    // high and low candidates, then uses the high bit (i.e. `high_bit`) to",
      "    // select the correct candidate.",
      "    asm volatile(",
      "        \"{\\n\"",
      "        \"  .reg .b32 low, high;\\n\"",
      "        \"  prmt.b32 low, %1, %2, %5;\\n\"",
      "        \"  prmt.b32 high, %3, %4, %5;\\n\"",
      "        \"  prmt.b32 %0, low, high, %6;\\n\"",
      "        \"}\\n\"",
      "        : \"=r\"(r[ii])",
      "        : \"n\"(LOW_0), \"n\"(LOW_1), \"n\"(HIGH_0), \"n\"(HIGH_1), \"r\"(lut_idx),",
      "          \"r\"(final_prmt_idx));",
      "  }",
      "",
      "  return r;",
      "};",
      "",
      "// for Array<int8_t, N> <= Array<vllm_uint4b8_t, N>",
      "template <FloatRoundStyle Round, int N>",
      "struct NumericArrayConverter<int8_t, vllm_uint4b8_t, N, Round> {",
      "  using result_type = Array<int8_t, N>;",
      "  using source_type = Array<vllm_uint4b8_t, N>;",
      "",
      "  static FloatRoundStyle const round_style = Round;",
      "",
      " private:",
      "  struct RegConvert {",
      "    template <typename PackedResultType>",
      "    CUTLASS_DEVICE static PackedResultType convert(Array<uint32_t, 1> src_) {",
      "      // [-8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7] as int8s",
      "      auto r = lut_4bit_to_8bit_convert<0xF8, 0xF9, 0xFA, 0xFB,  //",
      "                                        0xFC, 0xFD, 0xFE, 0xFF,  //",
      "                                        0x00, 0x01, 0x02, 0x03,  //",
      "                                        0x04, 0x05, 0x06, 0x07>(src_[0]);",
      "      return reinterpret_cast<PackedResultType&>(r);",
      "    };",
      "  };",
      "",
      " public:",
      "  CUTLASS_DEVICE",
      "  static result_type convert(source_type const& source) {",
      "    return ArrayConverterPacked32Bit<RegConvert, typename result_type::Element,",
      "                                     typename source_type::Element,",
      "                                     N>::convert(source);",
      "  }",
      "",
      "  CUTLASS_DEVICE",
      "  result_type operator()(source_type const& s) const { return convert(s); }",
      "};",
      "",
      "// for Array<cutlass::float_e4m3_t, N> <= Array<vllm_uint4b8_t, N>",
      "template <FloatRoundStyle Round, int N>",
      "struct NumericArrayConverter<cutlass::float_e4m3_t, vllm_uint4b8_t, N, Round> {",
      "  using result_type = Array<cutlass::float_e4m3_t, N>;",
      "  using source_type = Array<vllm_uint4b8_t, N>;",
      "",
      "  static FloatRoundStyle const round_style = Round;",
      "",
      " private:",
      "  struct RegConvert {",
      "    template <typename PackedResultType>",
      "    CUTLASS_DEVICE static PackedResultType convert(Array<uint32_t, 1> src_) {",
      "      // [-8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7] as fp8s",
      "      auto r = lut_4bit_to_8bit_convert<0xD0, 0xCE, 0xCC, 0xCA,  //",
      "                                        0xC8, 0xC4, 0xC0, 0xB8,  //",
      "                                        0x00, 0x38, 0x40, 0x44,  //",
      "                                        0x48, 0x4A, 0x4C, 0x4E>(src_[0]);",
      "      return reinterpret_cast<PackedResultType&>(r);",
      "    };",
      "  };",
      "",
      " public:",
      "  CUTLASS_DEVICE",
      "  static result_type convert(source_type const& source) {",
      "    return ArrayConverterPacked32Bit<RegConvert, typename result_type::Element,",
      "                                     typename source_type::Element,",
      "                                     N>::convert(source);",
      "  }",
      "",
      "  CUTLASS_DEVICE",
      "  result_type operator()(source_type const& s) const { return convert(s); }",
      "};",
      "",
      "// for Array<cutlass::half_t, N> <= Array<vllm_uint4b8_t, N>",
      "template <FloatRoundStyle Round, int N>",
      "struct NumericArrayConverter<cutlass::half_t, vllm_uint4b8_t, N, Round> {",
      "  using result_type = Array<cutlass::half_t, N>;",
      "  using source_type = Array<vllm_uint4b8_t, N>;",
      "",
      "  struct RegConvert {",
      "    template <typename PackedResultType>",
      "    CUTLASS_DEVICE static PackedResultType convert(Array<uint32_t, 1> src_) {",
      "      uint32_t src = src_[0];",
      "      using RegArray =",
      "          cutlass::AlignedArray<uint32_t, PackedResultType::kElements / 2,",
      "                                sizeof(PackedResultType)>;",
      "      RegArray r;",
      "",
      "      // Below constructs the following temporary:",
      "      // fp16s_01 = {0x00, i4_01, 0x00, i4_01}",
      "      // fp16s_23 = {0x00, i4_23, 0x00, i4_23}",
      "      // fp16s_45 = {0x00, i4_45, 0x00, i4_45}",
      "      // fp16s_67 = {0x00, i4_67, 0x00, i4_67}",
      "      // We use inline asm instead of __byte_perm intrinsic since we don't want",
      "      // the documented (& 0x7) on the index. NVCC might be able to optimize it",
      "      // out since the index is a constexpr, but we choose to be safe about it",
      "      // here.",
      "      uint32_t prmt_indices[4] = {0x4040, 0x4141, 0x4242, 0x4343};",
      "      static_assert(RegArray::kElements <= 4,",
      "                    \"Too many inputs for F16 -> I4 vector converter\");",
      "      CUTLASS_PRAGMA_UNROLL",
      "      for (int ii = 0; ii < RegArray::kElements; ++ii) {",
      "        asm volatile(",
      "            \"{\\n\"",
      "            \"  prmt.b32 %0, %1, %2, %3;\\n\"",
      "            \"}\\n\"",
      "            : \"=r\"(r[ii])",
      "            : \"r\"(src), \"n\"(0), \"r\"(prmt_indices[ii]));",
      "      }",
      "",
      "      // Since the stored 4bit values are biased by 8 we get stored_val = (x+8)",
      "      //  we are trying to construct x and a fp16 value",
      "      // The below XOR does the following:",
      "      //  1) Sets the exponent bits of the FP16 to the correct value for the",
      "      //  FP16 magic_num. We will be constructing {1024+16*(x1+8), 1024+(x0+8)},",
      "      //  where x1 in the high nibble and x0 is the low nibble then using hfma",
      "      //  to subtract 1032 from that",
      "      // The AND does the following:",
      "      //  1) Clear the set bits for the int4 we will ignore.",
      "      // We use lop3 so that we can use 1 instruction for AND and XOR.",
      "      static constexpr uint32_t xor_mask = 0x64006400;",
      "      static constexpr uint32_t and_mask = 0xFFF0FF0F;",
      "      static constexpr uint32_t immLut = (0xf0 & 0xcc) ^ 0xaa;",
      "",
      "      // For each operand, computes:",
      "      // r[i] = (r[i] & and_mask) ^ xor_mask",
      "      CUTLASS_PRAGMA_UNROLL",
      "      for (int ii = 0; ii < RegArray::kElements; ++ii) {",
      "        asm volatile(",
      "            \"{\\n\"",
      "            \"  lop3.b32 %0, %0, %1, %2, %3;\\n\"",
      "            \"}\\n\"",
      "            : \"+r\"(r[ii])",
      "            : \"n\"(and_mask), \"n\"(xor_mask), \"n\"(immLut));",
      "      }",
      "",
      "      // We will issue 2 hfmas that do the following:",
      "      // {x1, x0} = {1024+16*(x1+8), 1024+(x0+8)} * {1/16, 1} - {72, 1032}",
      "      //          = {x1 + 1152, x0 + 1032} * {1/16, 1} - {72, 1032}",
      "      static constexpr uint32_t hfma_bias_rep = 0xD480E408;   // {72, 1032}",
      "      static constexpr uint32_t hfma_scale_rep = 0x2C003C00;  // {1 / 16, 1}",
      "",
      "      const half2& hfma_bias = reinterpret_cast<const half2&>(hfma_bias_rep);",
      "      const half2& hfma_scale = reinterpret_cast<const half2&>(hfma_scale_rep);",
      "      CUTLASS_PRAGMA_UNROLL",
      "      for (int ii = 0; ii < RegArray::kElements; ++ii) {",
      "        half2& fp16x2_val = reinterpret_cast<__half2&>(r[ii]);",
      "        fp16x2_val = __hfma2(hfma_scale, fp16x2_val, hfma_bias);",
      "      }",
      "",
      "      return reinterpret_cast<PackedResultType&>(r);",
      "    };",
      "  };",
      "",
      " public:",
      "  CUTLASS_DEVICE",
      "  static result_type convert(source_type const& source) {",
      "    return ArrayConverterPacked32Bit<RegConvert, typename result_type::Element,",
      "                                     typename source_type::Element,",
      "                                     N>::convert(source);",
      "  }",
      "",
      "  CUTLASS_DEVICE",
      "  result_type operator()(source_type const& s) const { return convert(s); }",
      "};",
      "",
      "// for Array<cutlass::half_t, N> <= Array<vllm_uint4b8_t, N>",
      "//   for IlvdLayout: (2, 4):(4, 1)",
      "template <FloatRoundStyle Round, int N>",
      "struct InterleavedNumericArrayConverter<Layout<Shape<_2, _4>, Stride<_4, _1>>,",
      "                                        cutlass::half_t, vllm_uint4b8_t, N,",
      "                                        Round, void> {",
      "  using IlvdLayout = Layout<Shape<_2, _4>, Stride<_4, _1>>;",
      "  static_assert(N % size(IlvdLayout{}) == 0);",
      "",
      "  using result_type = Array<cutlass::half_t, N>;",
      "  using source_type = Array<vllm_uint4b8_t, N>;",
      "",
      "  static FloatRoundStyle const round_style = Round;",
      "",
      " private:",
      "  struct RegConvert {",
      "    template <typename PackedResultType>",
      "    CUTLASS_DEVICE static PackedResultType convert(Array<uint32_t, 1> src_) {",
      "      uint32_t src = src_[0];",
      "      using RegArray =",
      "          cutlass::AlignedArray<uint32_t, PackedResultType::kElements / 2,",
      "                                sizeof(PackedResultType)>;",
      "      RegArray r;",
      "",
      "      static_assert(PackedResultType::kElements <= size(IlvdLayout{}));",
      "      static constexpr uint32_t xor_mask = 0x64006400;",
      "",
      "      for (int ii = 0; ii < RegArray::kElements; ii += 2) {",
      "        auto src_ = src >> (4 * (ii));",
      "        r[ii + 0] = src_;",
      "        r[ii + 1] = src_;",
      "",
      "        static constexpr uint32_t and_xor_imm_lut = (0xf0 & 0xcc) ^ 0xaa;",
      "",
      "        static constexpr uint32_t low_nib_mask = 0x000F000F;",
      "        static constexpr uint32_t high_nib_mask = 0x00F000F0;",
      "",
      "        asm volatile(",
      "            \"{\\n\"",
      "            \"  lop3.b32 %0, %0, %1, %2, %3;\\n\"",
      "            \"}\\n\"",
      "            : \"+r\"(r[ii + 0])",
      "            : \"n\"(low_nib_mask), \"n\"(xor_mask), \"n\"(and_xor_imm_lut));",
      "",
      "        asm volatile(",
      "            \"{\\n\"",
      "            \"  lop3.b32 %0, %0, %1, %2, %3;\\n\"",
      "            \"}\\n\"",
      "            : \"+r\"(r[ii + 1])",
      "            : \"n\"(high_nib_mask), \"n\"(xor_mask), \"n\"(and_xor_imm_lut));",
      "",
      "        // For low nibble:",
      "        //  {x1, x0} = {1024+(x1+8), 1024+(x0+8)} * {1, 1} - {1032, 1032}",
      "        // For high nibble:",
      "        //  {x1, x0} = {1024+16*(x1+8), 1024+16*(x0+8)} * {1/16, 1/16}",
      "        //             - {72, 72}",
      "        static constexpr uint32_t low_nib_bias = 0x64086408;    // {1032, 1032}",
      "        static constexpr uint32_t high_nib_scale = 0x2C002C00;  // {1/16, 1/16}",
      "        static constexpr uint32_t high_nib_bias = 0xD480D480;   // {-72, -72}",
      "",
      "        {",
      "          half2& fp16x2_val = reinterpret_cast<__half2&>(r[ii + 0]);",
      "          fp16x2_val =",
      "              __hsub2(fp16x2_val, reinterpret_cast<const half2&>(low_nib_bias));",
      "        }",
      "",
      "        {",
      "          half2& fp16x2_val = reinterpret_cast<__half2&>(r[ii + 1]);",
      "          fp16x2_val = __hfma2(fp16x2_val,",
      "                               reinterpret_cast<const half2&>(high_nib_scale),",
      "                               reinterpret_cast<const half2&>(high_nib_bias));",
      "        }",
      "      }",
      "",
      "      return reinterpret_cast<PackedResultType&>(r);",
      "    };",
      "  };",
      "",
      " public:",
      "  CUTLASS_DEVICE",
      "  static result_type convert(source_type const& source) {",
      "    return ArrayConverterPacked32Bit<RegConvert, typename result_type::Element,",
      "                                     typename source_type::Element,",
      "                                     N>::convert(source);",
      "  }",
      "",
      "  CUTLASS_DEVICE",
      "  result_type operator()(source_type const& s) const { return convert(s); }",
      "};",
      "",
      "// for Array<cutlass::half_t, N> <= Array<uint4_t, N>",
      "//   for IlvdLayout: (2, 4):(4, 1)",
      "template <FloatRoundStyle Round, int N>",
      "struct InterleavedNumericArrayConverter<Layout<Shape<_2, _4>, Stride<_4, _1>>,",
      "                                        cutlass::half_t, uint4_t, N, Round,",
      "                                        void> {",
      "  using IlvdLayout = Layout<Shape<_2, _4>, Stride<_4, _1>>;",
      "  static_assert(N % size(IlvdLayout{}) == 0);",
      "",
      "  using result_type = Array<cutlass::half_t, N>;",
      "  using source_type = Array<uint4_t, N>;",
      "",
      "  static FloatRoundStyle const round_style = Round;",
      "",
      " private:",
      "  struct RegConvert {",
      "    template <typename PackedResultType>",
      "    CUTLASS_DEVICE static PackedResultType convert(Array<uint32_t, 1> src_) {",
      "      uint32_t src = src_[0];",
      "      using RegArray =",
      "          cutlass::AlignedArray<uint32_t, PackedResultType::kElements / 2,",
      "                                sizeof(PackedResultType)>;",
      "      RegArray r;",
      "",
      "      static_assert(PackedResultType::kElements <= size(IlvdLayout{}));",
      "      static constexpr uint32_t xor_mask = 0x64006400;",
      "",
      "      for (int ii = 0; ii < RegArray::kElements; ii += 2) {",
      "        auto src_ = src >> (4 * (ii));",
      "        r[ii + 0] = src_;",
      "        r[ii + 1] = src_;",
      "",
      "        static constexpr uint32_t and_xor_imm_lut = (0xf0 & 0xcc) ^ 0xaa;",
      "",
      "        static constexpr uint32_t low_nib_mask = 0x000F000F;",
      "        static constexpr uint32_t high_nib_mask = 0x00F000F0;",
      "",
      "        asm volatile(",
      "            \"{\\n\"",
      "            \"  lop3.b32 %0, %0, %1, %2, %3;\\n\"",
      "            \"}\\n\"",
      "            : \"+r\"(r[ii + 0])",
      "            : \"n\"(low_nib_mask), \"n\"(xor_mask), \"n\"(and_xor_imm_lut));",
      "",
      "        asm volatile(",
      "            \"{\\n\"",
      "            \"  lop3.b32 %0, %0, %1, %2, %3;\\n\"",
      "            \"}\\n\"",
      "            : \"+r\"(r[ii + 1])",
      "            : \"n\"(high_nib_mask), \"n\"(xor_mask), \"n\"(and_xor_imm_lut));",
      "",
      "        // For low nibble:",
      "        //  {x1, x0} = {1024+x1, 1024+x0} - {1024, 1024}",
      "        // For high nibble:",
      "        //  {x1, x0} = {1024+16*x1, 1024+16*x0} * {1/16, 1/16} - {64, 64}",
      "        static constexpr uint32_t low_nib_bias = 0x64006400;    // {1024, 1024}",
      "        static constexpr uint32_t high_nib_scale = 0x2C002C00;  // {1/16, 1/16}",
      "        static constexpr uint32_t high_nib_bias = 0xD400D400;   // {-64, -64}",
      "",
      "        {",
      "          half2& fp16x2_val = reinterpret_cast<__half2&>(r[ii + 0]);",
      "          fp16x2_val =",
      "              __hsub2(fp16x2_val, reinterpret_cast<const half2&>(low_nib_bias));",
      "        }",
      "",
      "        {",
      "          half2& fp16x2_val = reinterpret_cast<__half2&>(r[ii + 1]);",
      "          fp16x2_val = __hfma2(fp16x2_val,",
      "                               reinterpret_cast<const half2&>(high_nib_scale),",
      "                               reinterpret_cast<const half2&>(high_nib_bias));",
      "        }",
      "      }",
      "",
      "      return reinterpret_cast<PackedResultType&>(r);",
      "    };",
      "  };",
      "",
      " public:",
      "  CUTLASS_DEVICE",
      "  static result_type convert(source_type const& source) {",
      "    return ArrayConverterPacked32Bit<RegConvert, typename result_type::Element,",
      "                                     typename source_type::Element,",
      "                                     N>::convert(source);",
      "  }",
      "",
      "  CUTLASS_DEVICE",
      "  result_type operator()(source_type const& s) const { return convert(s); }",
      "};",
      "",
      "// for Array<cutlass::half_t, N> <= Array<vllm_uint8b128_t, N>",
      "template <FloatRoundStyle Round, int N>",
      "struct NumericArrayConverter<cutlass::half_t, vllm_uint8b128_t, N, Round> {",
      "  using result_type = Array<cutlass::half_t, N>;",
      "  using source_type = Array<vllm_uint8b128_t, N>;",
      "",
      "  struct RegConvert {",
      "    template <typename PackedResultType>",
      "    CUTLASS_DEVICE static PackedResultType convert(Array<uint32_t, 1> src_) {",
      "      uint32_t src = src_[0];",
      "      // Hold output FP16s in reg. We need 1 reg for every 2 elements",
      "      using RegArray =",
      "          cutlass::AlignedArray<uint32_t, PackedResultType::kElements / 2,",
      "                                sizeof(PackedResultType)>;",
      "      RegArray r;",
      "",
      "      uint32_t const prmt_indices[2] = {0x5150, 0x5352};",
      "      static constexpr uint32_t start_byte_for_fp16 = 0x64646464;",
      "",
      "      for (int ii = 0; ii < RegArray::kElements; ++ii) {",
      "        asm volatile(\"prmt.b32 %0,%1,%2,%3;\\n\"",
      "                     : \"=r\"(r[ii])",
      "                     : \"r\"(src), \"n\"(start_byte_for_fp16),",
      "                       \"r\"(prmt_indices[ii]));",
      "      }",
      "",
      "      // -128 is folded into bias subtraction, i.e. the 0x80 in the low bytes",
      "      static constexpr uint32_t bias_rep = 0x64806480;",
      "      const half2& bias = reinterpret_cast<const half2&>(bias_rep);",
      "      CUTLASS_PRAGMA_UNROLL",
      "      for (int ii = 0; ii < RegArray::kElements; ++ii) {",
      "        half2& fp16x2_val = reinterpret_cast<__half2&>(r[ii]);",
      "        fp16x2_val = __hsub2(fp16x2_val, bias);",
      "      }",
      "",
      "      return reinterpret_cast<PackedResultType&>(r);",
      "    };",
      "  };",
      "",
      " public:",
      "  CUTLASS_DEVICE",
      "  static result_type convert(source_type const& source) {",
      "    return ArrayConverterPacked32Bit<RegConvert, typename result_type::Element,",
      "                                     typename source_type::Element,",
      "                                     N>::convert(source);",
      "  }",
      "",
      "  CUTLASS_DEVICE",
      "  result_type operator()(source_type const& s) const { return convert(s); }",
      "};",
      "",
      "// for Array<cutlass::float, N> <= Array<vllm_uint8b128_t, N>",
      "template <FloatRoundStyle Round, int N>",
      "struct NumericArrayConverter<float, vllm_uint8b128_t, N, Round> {",
      "  using result_type = Array<float, N>;",
      "  using source_type = Array<vllm_uint8b128_t, N>;",
      "  static FloatRoundStyle const round_style = Round;",
      "",
      " private:",
      "  struct RegConvert {",
      "    template <typename PackedResultType>",
      "    CUTLASS_DEVICE static PackedResultType convert(Array<uint32_t, 1> src_) {",
      "      uint32_t src = src_[0];",
      "      PackedResultType r;",
      "",
      "      // __byte_perm simulates the add.u32 0x4B000000 to every u8 element of",
      "      // u8x4 source and stores the result in r (without introducing extra",
      "      // cvt.u32.u8 instruction)",
      "      uint32_t const prmt_indices[4] = {0x7650, 0x7651, 0x7652, 0x7653};",
      "      uint32_t* result_as_int = reinterpret_cast<uint32_t*>(&r);",
      "      for (int ii = 0; ii < PackedResultType::kElements; ++ii) {",
      "        result_as_int[ii] = __byte_perm(src, 0x4B000000, prmt_indices[ii]);",
      "        // Subtract the magic number 0x4B000000 from tmp in floating-point",
      "        // arithmetic to obtain final result",
      "        r[ii] -= (8388608.f + 128.f);  // fold in -128 bias",
      "      }",
      "",
      "      return r;",
      "    };",
      "  };",
      "",
      " public:",
      "  CUTLASS_DEVICE",
      "  static result_type convert(source_type const& source) {",
      "    return ArrayConverterPacked32Bit<RegConvert, typename result_type::Element,",
      "                                     typename source_type::Element,",
      "                                     N>::convert(source);",
      "  }",
      "",
      "  CUTLASS_DEVICE",
      "  result_type operator()(source_type const& s) const { return convert(s); }",
      "};",
      "",
      "#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)",
      "",
      "// for Array<cutlass::bfloat16_t, N> <= Array<vllm_uint4b8_t, N>",
      "template <FloatRoundStyle Round, int N>",
      "struct NumericArrayConverter<cutlass::bfloat16_t, vllm_uint4b8_t, N, Round> {",
      "  using result_type = Array<cutlass::bfloat16_t, N>;",
      "  using source_type = Array<vllm_uint4b8_t, N>;",
      "",
      "  static FloatRoundStyle const round_style = Round;",
      "",
      " private:",
      "  struct RegConvert {",
      "    template <typename PackedResultType>",
      "    CUTLASS_DEVICE static PackedResultType convert(Array<uint32_t, 1> src_) {",
      "      uint32_t src_reg = src_[0];",
      "      // Hold output BF16s in reg. We need 1 reg for every 2 elements",
      "      using RegArray =",
      "          cutlass::AlignedArray<uint32_t, PackedResultType::kElements / 2,",
      "                                sizeof(PackedResultType)>;",
      "      RegArray r;",
      "      uint32_t src_reg_shifted = src_reg >> 4;",
      "",
      "      // Below constructs the following temporary:",
      "      uint32_t const prmt_indices[4] = {0xF4F0, 0xF5F1, 0xF6F2, 0xF7F3};",
      "      static_assert(RegArray::kElements <= 4,",
      "                    \"Too many inputs for uint4b8_t -> BF16 vector converter\");",
      "      CUTLASS_PRAGMA_UNROLL",
      "      for (int ii = 0; ii < RegArray::kElements; ++ii) {",
      "        asm volatile(",
      "            \"{\\n\"",
      "            \"  prmt.b32 %0, %1, %2, %3;\\n\"",
      "            \"}\\n\"",
      "            : \"=r\"(r[ii])",
      "            : \"r\"(src_reg), \"r\"(src_reg_shifted), \"r\"(prmt_indices[ii]));",
      "      }",
      "",
      "      // Since the stored 4bit values are biased by 8 we get stored_val = (x+8)",
      "      //  we are trying to construct x and a BF16 value",
      "      // The below XOR does the following:",
      "      //  1) Sets the exponent bits of the BF16 to the correct value for the",
      "      //  BF16 magic_num. We will be constructing {128 + (x1+8), 128 + (x0+8)}",
      "      //  and subtracting 136 to get {x1, x0}",
      "      static constexpr uint32_t xor_mask = 0x43004300;",
      "      static constexpr uint32_t and_mask = 0x000F000F;",
      "      static constexpr uint32_t immLut = (0xf0 & 0xcc) ^ 0xaa;",
      "",
      "      // For each operand, computes:",
      "      // r[i] = (r[i] & and_mask) ^ xor_mask",
      "      CUTLASS_PRAGMA_UNROLL",
      "      for (int ii = 0; ii < RegArray::kElements; ++ii) {",
      "        asm volatile(",
      "            \"{\\n\"",
      "            \"  lop3.b32 %0, %0, %1, %2, %3;\\n\"",
      "            \"}\\n\"",
      "            : \"+r\"(r[ii])",
      "            : \"n\"(and_mask), \"n\"(xor_mask), \"n\"(immLut));",
      "      }",
      "",
      "      // We will issue 2 bfmas that do the following:",
      "      // high BF16:",
      "      // hi_bf16 - 136, lo_bf16 - 136",
      "",
      "      // This is the BF16 {136, 136} represented as an integer.",
      "      static constexpr uint32_t bias_rep = 0x43084308;",
      "      const __nv_bfloat162& bias =",
      "          reinterpret_cast<const __nv_bfloat162&>(bias_rep);",
      "",
      "      CUTLASS_PRAGMA_UNROLL",
      "      for (int ii = 0; ii < RegArray::kElements; ++ii) {",
      "        __nv_bfloat162& bf16x2_val = reinterpret_cast<__nv_bfloat162&>(r[ii]);",
      "        bf16x2_val = __hsub2(bf16x2_val, bias);",
      "      }",
      "",
      "      return reinterpret_cast<PackedResultType&>(r);",
      "    }",
      "  };",
      "",
      " public:",
      "  CUTLASS_DEVICE",
      "  static result_type convert(source_type const& source) {",
      "    return ArrayConverterPacked32Bit<RegConvert, typename result_type::Element,",
      "                                     typename source_type::Element,",
      "                                     N>::convert(source);",
      "  }",
      "",
      "  CUTLASS_DEVICE",
      "  result_type operator()(source_type const& s) const { return convert(s); }",
      "};",
      "",
      "// for Array<cutlass::bfloat16_t, N> <= Array<vllm_uint4b8_t, N>",
      "//   for IlvdLayout: (2, 4):(4, 1)",
      "template <FloatRoundStyle Round, int N>",
      "struct InterleavedNumericArrayConverter<Layout<Shape<_2, _4>, Stride<_4, _1>>,",
      "                                        cutlass::bfloat16_t, vllm_uint4b8_t, N,",
      "                                        Round, void> {",
      "  using IlvdLayout = Layout<Shape<_2, _4>, Stride<_4, _1>>;",
      "  static_assert(N % size(IlvdLayout{}) == 0);",
      "",
      "  using result_type = Array<cutlass::bfloat16_t, N>;",
      "  using source_type = Array<vllm_uint4b8_t, N>;",
      "",
      " private:",
      "  struct RegConvert {",
      "    template <typename PackedResultType>",
      "    CUTLASS_DEVICE static PackedResultType convert(Array<uint32_t, 1> src_) {",
      "      uint32_t src = src_[0];",
      "      using RegArray =",
      "          cutlass::AlignedArray<uint32_t, PackedResultType::kElements / 2,",
      "                                sizeof(PackedResultType)>;",
      "      RegArray r;",
      "",
      "      static_assert(PackedResultType::kElements <= size(IlvdLayout{}));",
      "      static constexpr uint32_t or_mask = 0x43004300;",
      "",
      "      // Unlike float16 where the mantissa is large enough to contain 2",
      "      // nibbles, bfloat16 can only fit one, so we can only convert one",
      "      // nibble at a time",
      "      for (int ii = 0; ii < RegArray::kElements; ++ii) {",
      "        r[ii] = src >> (4 * ii);",
      "",
      "        static constexpr uint32_t and_or_imm_lut = (0xf0 & 0xcc) | 0xaa;",
      "        static constexpr uint32_t low_nib_mask = 0x000F000F;",
      "",
      "        asm volatile(",
      "            \"{\\n\"",
      "            \"  lop3.b32 %0, %0, %1, %2, %3;\\n\"",
      "            \"}\\n\"",
      "            : \"+r\"(r[ii + 0])",
      "            : \"n\"(low_nib_mask), \"n\"(or_mask), \"n\"(and_or_imm_lut));",
      "",
      "        // For low nibble:",
      "        //  {x1, x0} = {128+(x1+8), 128+(x0+8)} * {1, 1} - {136, 136}",
      "        static constexpr uint32_t low_nib_bias = 0x43084308;  // {136, 136}",
      "",
      "        {",
      "          __nv_bfloat162& fp16x2_val = reinterpret_cast<__nv_bfloat162&>(r[ii]);",
      "          fp16x2_val =",
      "              __hsub2(fp16x2_val,",
      "                      reinterpret_cast<const __nv_bfloat162&>(low_nib_bias));",
      "        }",
      "      }",
      "",
      "      return reinterpret_cast<PackedResultType&>(r);",
      "    };",
      "  };",
      "",
      " public:",
      "  CUTLASS_DEVICE",
      "  static result_type convert(source_type const& source) {",
      "    return ArrayConverterPacked32Bit<RegConvert, typename result_type::Element,",
      "                                     typename source_type::Element,",
      "                                     N>::convert(source);",
      "  }",
      "",
      "  CUTLASS_DEVICE",
      "  result_type operator()(source_type const& s) const { return convert(s); }",
      "};",
      "",
      "// for Array<cutlass::bfloat16_t, N> <= Array<uint4_t, N>",
      "//   for IlvdLayout: (2, 4):(4, 1)",
      "template <FloatRoundStyle Round, int N>",
      "struct InterleavedNumericArrayConverter<Layout<Shape<_2, _4>, Stride<_4, _1>>,",
      "                                        cutlass::bfloat16_t, uint4_t, N, Round,",
      "                                        void> {",
      "  using IlvdLayout = Layout<Shape<_2, _4>, Stride<_4, _1>>;",
      "  static_assert(N % size(IlvdLayout{}) == 0);",
      "",
      "  using result_type = Array<cutlass::bfloat16_t, N>;",
      "  using source_type = Array<uint4_t, N>;",
      "",
      " private:",
      "  struct RegConvert {",
      "    template <typename PackedResultType>",
      "    CUTLASS_DEVICE static PackedResultType convert(Array<uint32_t, 1> src_) {",
      "      uint32_t src = src_[0];",
      "      using RegArray =",
      "          cutlass::AlignedArray<uint32_t, PackedResultType::kElements / 2,",
      "                                sizeof(PackedResultType)>;",
      "      RegArray r;",
      "",
      "      static_assert(PackedResultType::kElements <= size(IlvdLayout{}));",
      "      static constexpr uint32_t or_mask = 0x43004300;",
      "",
      "      // Unlike float16 where the mantissa is large enough to contain 2",
      "      // nibbles, bfloat16 can only fit one, so we can only convert one",
      "      // nibble at a time",
      "      for (int ii = 0; ii < RegArray::kElements; ++ii) {",
      "        r[ii] = src >> (4 * ii);",
      "",
      "        static constexpr uint32_t and_or_imm_lut = (0xf0 & 0xcc) | 0xaa;",
      "        static constexpr uint32_t low_nib_mask = 0x000F000F;",
      "",
      "        asm volatile(",
      "            \"{\\n\"",
      "            \"  lop3.b32 %0, %0, %1, %2, %3;\\n\"",
      "            \"}\\n\"",
      "            : \"+r\"(r[ii])",
      "            : \"n\"(low_nib_mask), \"n\"(or_mask), \"n\"(and_or_imm_lut));",
      "",
      "        // For low nibble:",
      "        //  {x1, x0} = {128 + x1, 128 + x0} * {1, 1} - {128, 128}",
      "        static constexpr uint32_t low_nib_bias = 0x43004300;  // {128, 128}",
      "",
      "        {",
      "          __nv_bfloat162& fp16x2_val = reinterpret_cast<__nv_bfloat162&>(r[ii]);",
      "          fp16x2_val =",
      "              __hsub2(fp16x2_val,",
      "                      reinterpret_cast<const __nv_bfloat162&>(low_nib_bias));",
      "        }",
      "      }",
      "",
      "      return reinterpret_cast<PackedResultType&>(r);",
      "    };",
      "  };",
      "",
      " public:",
      "  CUTLASS_DEVICE",
      "  static result_type convert(source_type const& source) {",
      "    return ArrayConverterPacked32Bit<RegConvert, typename result_type::Element,",
      "                                     typename source_type::Element,",
      "                                     N>::convert(source);",
      "  }",
      "",
      "  CUTLASS_DEVICE",
      "  result_type operator()(source_type const& s) const { return convert(s); }",
      "};",
      "",
      "// for Array<cutlass::bfloat16_t, N> <= Array<vllm_uint8b128_t, N>",
      "template <FloatRoundStyle Round, int N>",
      "struct NumericArrayConverter<cutlass::bfloat16_t, vllm_uint8b128_t, N, Round> {",
      "  using result_type = Array<cutlass::bfloat16_t, N>;",
      "  using source_type = Array<vllm_uint8b128_t, N>;",
      "  static FloatRoundStyle const round_style = Round;",
      "",
      " private:",
      "  using result_packed_4_t = Array<cutlass::bfloat16_t, 4>;",
      "  using result_packed_2_t = Array<cutlass::bfloat16_t, 2>;",
      "  using src_packed_4_t = Array<vllm_uint8b128_t, 4>;",
      "  using src_packed_2_t = Array<vllm_uint8b128_t, 2>;",
      "",
      "  // Not Valid, not supported, only here to satisfy the interface and to avoid",
      "  //  a compile error. ScalarConverter will not actually work until",
      "  //  NumericConverter<cutlass::bfloat16_t, vllm_uint8b128_t, Round> is",
      "  //  implemented",
      "  using ScalarConverter =",
      "      NumericConverter<cutlass::bfloat16_t, vllm_uint8b128_t, Round>;",
      "",
      "  template <typename PackedResultType, typename PackedSrcType>",
      "  CUTLASS_DEVICE static PackedResultType packed_convert(",
      "      PackedSrcType const& source) {",
      "    static_assert(",
      "        (platform::is_same<PackedSrcType, src_packed_2_t>::value &&",
      "         platform::is_same<PackedResultType, result_packed_2_t>::value) ||",
      "            (platform::is_same<PackedSrcType, src_packed_4_t>::value &&",
      "             platform::is_same<PackedResultType, result_packed_4_t>::value),",
      "        \"Invalid PackedSrcType/PackedResultType must be 2 or 4 to use private \"",
      "        \"convert dispatch.\");",
      "",
      "    NumericArrayConverter<float, vllm_uint8b128_t, PackedResultType::kElements,",
      "                          Round>",
      "        convert_uint8_to_f32;",
      "    Array<float, PackedResultType::kElements> tmp =",
      "        convert_uint8_to_f32(source);",
      "    NumericArrayConverter<cutlass::bfloat16_t, float,",
      "                          PackedResultType::kElements, Round>",
      "        convert_f32_to_bf16_;",
      "    return convert_f32_to_bf16_(tmp);",
      "  }",
      "",
      "  friend class detail::VectorizedConverter;",
      "",
      " public:",
      "  CUTLASS_DEVICE",
      "  static result_type convert(source_type const& source) {",
      "    result_type result;",
      "    using ConverterType =",
      "        NumericArrayConverter<typename result_type::Element,",
      "                              typename source_type::Element, N, Round>;",
      "    detail::VectorizedConverter::convert<ConverterType, result_packed_4_t,",
      "                                         src_packed_4_t, result_packed_2_t,",
      "                                         src_packed_2_t>(result, source);",
      "",
      "    return result;",
      "  }",
      "",
      "  CUTLASS_DEVICE",
      "  result_type operator()(source_type const& s) const { return convert(s); }",
      "};",
      "",
      "#endif",
      "",
      "// for Array<int8_t, N> <= Array<cutlass::half_t, N>",
      "//   FastFP16toINT8 from https://arxiv.org/pdf/2406.09904",
      "template <FloatRoundStyle Round, int N>",
      "struct NumericArrayConverter<int8_t, cutlass::half_t, N, Round> {",
      "  using result_type = Array<int8_t, N>;",
      "  using source_type = Array<cutlass::half_t, N>;",
      "",
      "  struct RegConvert {",
      "    // FastFP16toINT8 from https://arxiv.org/pdf/2406.09904",
      "    template <typename PackedResultType, int src_regs>",
      "    CUTLASS_DEVICE static PackedResultType convert(",
      "        Array<uint32_t, src_regs> src) {",
      "      // Hold output int8s in reg. We need 1 reg for every 4 elements",
      "      using RegArray = cutlass::AlignedArray<",
      "          uint32_t, std::max(PackedResultType::kElements / 4, size_t(1))>;",
      "      RegArray r;",
      "",
      "      static constexpr uint32_t MAGIC_BIAS_ = 0x64806480;",
      "      auto MAGIC_BIAS = *reinterpret_cast<const half2*>(&MAGIC_BIAS_);",
      "",
      "      *reinterpret_cast<half2*>(&src[0]) =",
      "          __hadd2(*reinterpret_cast<half2*>(&src[0]), MAGIC_BIAS);",
      "",
      "      if constexpr (src_regs > 1) {",
      "        *reinterpret_cast<half2*>(&src[1]) =",
      "            __hadd2(*reinterpret_cast<half2*>(&src[1]), MAGIC_BIAS);",
      "      }",
      "",
      "      static_assert(PackedResultType::kElements <= 4);",
      "      uint32_t uint8s;",
      "      static constexpr uint32_t MASK_0246 = 0x6420;",
      "      static constexpr uint32_t UINT8s_TO_INT8s_MASK = 0x80808080;",
      "      asm volatile(\"prmt.b32 %0,%1,%2,%3;\\n\"",
      "                   : \"=r\"(uint8s)",
      "                   : \"r\"(src[0]), \"r\"((src_regs > 1) ? src[1] : src[0]),",
      "                     \"n\"(MASK_0246));",
      "",
      "      uint32_t int8s = (uint8s ^ UINT8s_TO_INT8s_MASK);",
      "",
      "      return reinterpret_cast<PackedResultType&>(int8s);",
      "    };",
      "  };",
      "",
      " public:",
      "  CUTLASS_DEVICE",
      "  static result_type convert(source_type const& source) {",
      "    return ArrayConverterPacked32Bit<RegConvert, typename result_type::Element,",
      "                                     typename source_type::Element,",
      "                                     N>::convert(source);",
      "  }",
      "",
      "  CUTLASS_DEVICE",
      "  result_type operator()(source_type const& s) const { return convert(s); }",
      "};",
      "",
      "/////////////////////////////////////////////////////////////////////////////////////////////////",
      "",
      "}  // namespace cutlass",
      "",
      "/////////////////////////////////////////////////////////////////////////////////////////////////"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/cutlass_extensions/vllm_custom_types.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"cutlass/integer_subbyte.h\"",
      "",
      "namespace cutlass {",
      "",
      "///////////////////////////////////////////////////////////////////////////////////////////////////",
      "",
      "template <int Bits, int Bias, bool Signed = false>",
      "struct vllm_biased_integer_subbyte : public integer_subbyte<Bits, Signed> {",
      "  using Base = integer_subbyte<Bits, Signed>;",
      "",
      "  using Storage = typename Base::Storage;",
      "  using xint_t = typename Base::xint_t;",
      "",
      "  using Base::bits_mask_;",
      "  using Base::sign_mask_;",
      "  using Base::storage;",
      "",
      "  //",
      "  // Methods",
      "  //",
      "",
      "  /// No operation",
      "  vllm_biased_integer_subbyte() = default;",
      "",
      "  /// Conversion from integer type",
      "  CUTLASS_HOST_DEVICE explicit vllm_biased_integer_subbyte(int value)",
      "      : Base(value) {}",
      "  CUTLASS_HOST_DEVICE explicit vllm_biased_integer_subbyte(unsigned value)",
      "      : Base(value) {}",
      "  CUTLASS_HOST_DEVICE explicit vllm_biased_integer_subbyte(double value)",
      "      : Base(value) {}",
      "};",
      "///////////////////////////////////////////////////////////////////////////////////////////////////",
      "",
      "// \"GPTQ\" types, i.e. symmetric quantization",
      "using vllm_uint4b8_t = vllm_biased_integer_subbyte<4, 8>;      // u4b8",
      "using vllm_uint8b128_t = vllm_biased_integer_subbyte<8, 128>;  // u8b128",
      "",
      "///////////////////////////////////////////////////////////////////////////////////////////////////",
      "",
      "template <int Bits, int Bias, bool Signed>",
      "struct sizeof_bits<vllm_biased_integer_subbyte<Bits, Bias, Signed>> {",
      "  static constexpr int value = Bits;",
      "};",
      "",
      "///////////////////////////////////////////////////////////////////////////////////////////////////",
      "",
      "}  // namespace cutlass"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/cutlass_extensions/cute_utils.cuh",
    "source": [
      "#pragma once",
      "",
      "#include <cute/tensor.hpp>",
      "#include <torch/all.h>",
      "namespace cute {",
      "",
      "////////////////////////////////////////////////////////////////////",
      "// layout utils",
      "////////////////////////////////////////////////////////////////////",
      "",
      "// Permute layout based on indices, example:",
      "//   permute_layout<1, 0>(layout) will swap the two dimensions",
      "//   permute_layout<0, 2, 1>(layout) will swap the last two dimensions",
      "template <size_t... I, typename Layout>",
      "CUTE_HOST_DEVICE static constexpr auto permute_layout(Layout l) {",
      "  static_assert(rank(l) == sizeof...(I), \"Invalid permutation, rank mismatch\");",
      "  return cute::make_layout(cute::get<I>(l)...);",
      "}",
      "",
      "// is the layout f(x) = x",
      "template <typename Layout>",
      "CUTE_HOST_DEVICE static constexpr bool is_identity_layout() {",
      "  if constexpr (std::is_same_v<Layout, void>) {",
      "    return true;",
      "  } else {",
      "    constexpr auto coalesced_layout = coalesce(Layout{});",
      "    if constexpr (rank(coalesced_layout) == 1 &&",
      "                  stride<0>(coalesced_layout) == 1) {",
      "      return true;",
      "    }",
      "    return false;",
      "  }",
      "}",
      "",
      "////////////////////////////////////////////////////////////////////",
      "// Pointer utils",
      "////////////////////////////////////////////////////////////////////",
      "",
      "template <class PointerType>",
      "static constexpr auto get_logical_ptr(PointerType* ptr) {",
      "  if constexpr (cute::sizeof_bits_v<PointerType> < 8) {",
      "    return cute::subbyte_iterator<PointerType>(ptr);",
      "  } else {",
      "    return ptr;",
      "  }",
      "}",
      "",
      "////////////////////////////////////////////////////////////////////",
      "// Misc utils",
      "////////////////////////////////////////////////////////////////////",
      "",
      "template <typename T, typename Elements>",
      "CUTE_HOST_DEVICE static constexpr auto create_auto_vectorizing_copy() {",
      "  constexpr auto bits = sizeof_bits_v<T> * Elements{};",
      "  if constexpr (bits % 128 == 0) {",
      "    return AutoVectorizingCopyWithAssumedAlignment<128>{};",
      "  } else if constexpr (bits % 64 == 0) {",
      "    return AutoVectorizingCopyWithAssumedAlignment<64>{};",
      "  } else if constexpr (bits % 32 == 0) {",
      "    return AutoVectorizingCopyWithAssumedAlignment<32>{};",
      "  } else if constexpr (bits % 16 == 0) {",
      "    return AutoVectorizingCopyWithAssumedAlignment<16>{};",
      "  } else {",
      "    return AutoVectorizingCopyWithAssumedAlignment<8>{};",
      "  }",
      "}",
      "",
      "};  // namespace cute"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/cutlass_extensions/vllm_type_utils.cuh",
    "source": [
      "#include \"cutlass/bfloat16.h\"",
      "#include \"cutlass/half.h\"",
      "#include \"cuda_bf16.h\"",
      "",
      "#include \"cutlass_extensions/vllm_custom_types.cuh\"",
      "",
      "namespace cutlass {",
      "",
      "template <typename T>",
      "struct nameof {",
      "  static constexpr char const* value = \"unknown\";",
      "};",
      "",
      "template <typename T>",
      "inline constexpr auto nameof_v = nameof<T>::value;",
      "",
      "#define NAMEOF_TYPE(T)                       \\",
      "  template <>                                \\",
      "  struct nameof<T> {                         \\",
      "    static constexpr char const* value = #T; \\",
      "  };",
      "",
      "NAMEOF_TYPE(float_e4m3_t)",
      "NAMEOF_TYPE(float_e5m2_t)",
      "NAMEOF_TYPE(half_t)",
      "NAMEOF_TYPE(nv_bfloat16)",
      "NAMEOF_TYPE(bfloat16_t)",
      "NAMEOF_TYPE(float)",
      "",
      "NAMEOF_TYPE(int4b_t)",
      "NAMEOF_TYPE(int8_t)",
      "NAMEOF_TYPE(int32_t)",
      "NAMEOF_TYPE(int64_t)",
      "",
      "NAMEOF_TYPE(vllm_uint4b8_t)",
      "NAMEOF_TYPE(uint4b_t)",
      "NAMEOF_TYPE(uint8_t)",
      "NAMEOF_TYPE(vllm_uint8b128_t)",
      "NAMEOF_TYPE(uint32_t)",
      "NAMEOF_TYPE(uint64_t)",
      "",
      "};  // namespace cutlass"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/sparse/cutlass/sparse_compressor_c3x.cuh",
    "source": [
      "#pragma once",
      "",
      "// clang-format will break include orders",
      "// clang-format off",
      "#include <cudaTypedefs.h>",
      "",
      "#if defined CUDA_VERSION && CUDA_VERSION >= 12020",
      "#include \"sparse_scaled_mm_c3x.cuh\"",
      "",
      "#include \"cutlass/numeric_conversion.h\"",
      "#include \"cutlass/transform/device/transform_universal_adapter.hpp\"",
      "#include \"cutlass/transform/kernel/sparse_gemm_compressor.hpp\"",
      "#include \"cutlass/epilogue/collective/default_epilogue.hpp\"",
      "",
      "// clang-format on",
      "",
      "using namespace cute;",
      "using namespace vllm;",
      "",
      "using CompressorResult = std::tuple<torch::Tensor, torch::Tensor>;",
      "/// Make A structured sparse by replacing elements with 0 and compress it",
      "template <typename Gemm>",
      "CompressorResult cutlass_sparse_compress(torch::Tensor const& a) {",
      "  // Checks for conformality",
      "  TORCH_CHECK(a.dtype() == torch::kInt8 || a.dtype() == torch::kFloat8_e4m3fn ||",
      "              a.dtype() == torch::kFloat16 || a.dtype() == torch::kBFloat16);",
      "  TORCH_CHECK(a.dim() == 2)",
      "  // Check for strides and alignment",
      "  TORCH_CHECK(a.stride(0) % 4 == 0)  // Required for semi-structured sparsity",
      "  TORCH_CHECK(a.stride(1) == 1)",
      "",
      "  using GemmKernel = typename Gemm::KernelType;",
      "  using ElementA = typename Gemm::ElementAB;",
      "  using ElementE = typename GemmKernel::CollectiveMainloop::ElementE;",
      "",
      "  int m = a.size(0);",
      "  int k = a.size(1);",
      "  using ProblemShape = typename GemmKernel::ProblemShape;",
      "  ProblemShape prob_shape{m, 1, k, 1};",
      "",
      "  int64_t lda = a.stride(0);",
      "  using StrideA = Stride<int64_t, Int<1>, int64_t>;",
      "  StrideA a_stride{lda, Int<1>{}, 0};",
      "",
      "  using CompressorUtility = typename Gemm::CompressorUtility;",
      "  CompressorUtility compressor_utility(prob_shape, a_stride);",
      "",
      "  // Allocate buffers for the metadata E and the compressed matrix A",
      "  int ME = compressor_utility.get_metadata_m_physical();",
      "  int KE = compressor_utility.get_metadata_k_physical();",
      "  int MC = compressor_utility.get_tensorA_m_physical();",
      "  int KC = compressor_utility.get_tensorA_k_physical();",
      "",
      "  auto const a_meta_options =",
      "      torch::TensorOptions().dtype(torch::kUInt8).device(a.device());",
      "  auto const a_nzs_options =",
      "      torch::TensorOptions().dtype(a.dtype()).device(a.device());",
      "",
      "  auto a_meta = torch::zeros({ME, KE}, a_meta_options);",
      "  auto a_nzs = torch::zeros({MC, KC}, a_nzs_options);",
      "",
      "  auto a_ptr = static_cast<ElementA*>(a.data_ptr());",
      "  auto a_nzs_ptr = static_cast<ElementA*>(a_nzs.data_ptr());",
      "  auto a_meta_ptr = static_cast<ElementE*>(a_meta.data_ptr());",
      "",
      "  cutlass::KernelHardwareInfo hw_info;",
      "  hw_info.device_id = a.device().index();",
      "  hw_info.sm_count =",
      "      cutlass::KernelHardwareInfo::query_device_multiprocessor_count(",
      "          hw_info.device_id);",
      "",
      "  using Compressor = typename Gemm::Compressor;",
      "  typename Compressor::Arguments arguments{",
      "      prob_shape, {a_ptr, a_stride, a_nzs_ptr, a_meta_ptr}, {hw_info}};",
      "",
      "  Compressor compressor_op;",
      "  size_t workspace_size = Compressor::get_workspace_size(arguments);",
      "  auto const workspace_options =",
      "      torch::TensorOptions().dtype(torch::kUInt8).device(a.device());",
      "  auto workspace = torch::empty(workspace_size, workspace_options);",
      "",
      "  CUTLASS_CHECK(compressor_op.can_implement(arguments));",
      "  CUTLASS_CHECK(compressor_op.initialize(arguments, workspace.data_ptr()));",
      "  CUTLASS_CHECK(compressor_op.run());",
      "  CUDA_CHECK(cudaDeviceSynchronize());",
      "",
      "  return {a_meta, a_nzs};",
      "}",
      "",
      "#endif"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/sparse/cutlass/sparse_scaled_mm_c3x.cu",
    "source": [
      "// clang-format will break include orders",
      "// clang-format off",
      "#include <cudaTypedefs.h>",
      "",
      "#if defined CUDA_VERSION && CUDA_VERSION >= 12020",
      "#include \"sparse_scaled_mm_c3x.cuh\"",
      "// clang-format on",
      "",
      "using namespace cute;",
      "using namespace vllm;",
      "",
      "struct GemmCallerTraits {",
      "  using return_type = void;",
      "",
      "  template <typename GemmConfig, typename... Args>",
      "  static return_type invoke(Args&&... args) {",
      "    return cutlass_sparse_gemm_caller<GemmConfig>(std::forward<Args>(args)...);",
      "  }",
      "};",
      "",
      "struct GemmCompressorTraits {",
      "  using return_type = CompressorResult;",
      "",
      "  template <typename GemmConfig, typename... Args>",
      "  static return_type invoke(Args&&... args) {",
      "    return cutlass_sparse_compress<GemmConfig>(std::forward<Args>(args)...);",
      "  }",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue,",
      "          typename DispatchFunc, typename... Args>",
      "typename DispatchFunc::return_type cutlass_gemm_sm90_fp8_dispatch(",
      "    uint32_t m, uint32_t n, Args&&... args) {",
      "  static_assert(std::is_same_v<InType, cutlass::float_e4m3_t>);",
      "",
      "  using Cutlass3xGemmDefault =",
      "      typename sm90_config_default<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM64 =",
      "      typename sm90_fp8_config_M64<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM128 =",
      "      typename sm90_fp8_config_M128<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM256 =",
      "      typename sm90_fp8_config_M256<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM512 =",
      "      typename sm90_fp8_config_M512<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "",
      "  using Cutlass3xGemm1 =",
      "      typename sm90_fp8_config_1<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemm2 =",
      "      typename sm90_fp8_config_2<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemm3 =",
      "      typename sm90_fp8_config_3<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemm4 =",
      "      typename sm90_fp8_config_4<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemm5 =",
      "      typename sm90_fp8_config_5<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemm6 =",
      "      typename sm90_fp8_config_6<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemm7 =",
      "      typename sm90_fp8_config_7<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemm8 =",
      "      typename sm90_fp8_config_8<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "",
      "  uint32_t const mp2 =",
      "      std::max(static_cast<uint32_t>(64), next_pow_2(m));  // next power of 2",
      "",
      "  if (mp2 <= 64) {",
      "    if (n == 28672) {",
      "      return DispatchFunc::template invoke<Cutlass3xGemm2>(",
      "          std::forward<Args>(args)...);",
      "    } else if (n == 4096 || n == 6144) {",
      "      return DispatchFunc::template invoke<Cutlass3xGemm1>(",
      "          std::forward<Args>(args)...);",
      "    }",
      "  } else if (mp2 <= 128) {",
      "    if (n == 4096) {",
      "      return DispatchFunc::template invoke<Cutlass3xGemm3>(",
      "          std::forward<Args>(args)...);",
      "    } else if (n == 28672) {",
      "      return DispatchFunc::template invoke<Cutlass3xGemm5>(",
      "          std::forward<Args>(args)...);",
      "    } else if (n == 6144) {",
      "      return DispatchFunc::template invoke<Cutlass3xGemm4>(",
      "          std::forward<Args>(args)...);",
      "    }",
      "  } else if (mp2 <= 256) {",
      "    if (n == 4096) {",
      "      return DispatchFunc::template invoke<Cutlass3xGemm6>(",
      "          std::forward<Args>(args)...);",
      "    } else if (n == 28672) {",
      "      return DispatchFunc::template invoke<Cutlass3xGemm8>(",
      "          std::forward<Args>(args)...);",
      "    } else if (n == 6144) {",
      "      return DispatchFunc::template invoke<Cutlass3xGemm7>(",
      "          std::forward<Args>(args)...);",
      "    }",
      "  } else {",
      "    if (n == 6144 || n == 28672) {",
      "      return DispatchFunc::template invoke<Cutlass3xGemm8>(",
      "          std::forward<Args>(args)...);",
      "    } else if (n == 4096) {",
      "      return DispatchFunc::template invoke<Cutlass3xGemm7>(",
      "          std::forward<Args>(args)...);",
      "    }",
      "  }",
      "",
      "  // Otherwise the default heuristic",
      "  if (mp2 <= 64) {",
      "    // n in [1, 64]",
      "    return DispatchFunc::template invoke<Cutlass3xGemmM64>(",
      "        std::forward<Args>(args)...);",
      "  } else if (mp2 <= 128) {",
      "    // n in (64, 128]",
      "    return DispatchFunc::template invoke<Cutlass3xGemmM128>(",
      "        std::forward<Args>(args)...);",
      "  } else if (mp2 <= 256) {",
      "    // n in (128, 256]",
      "    return DispatchFunc::template invoke<Cutlass3xGemmM256>(",
      "        std::forward<Args>(args)...);",
      "  } else {",
      "    // n in (256, inf)",
      "    return DispatchFunc::template invoke<Cutlass3xGemmM512>(",
      "        std::forward<Args>(args)...);",
      "  }",
      "}",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue,",
      "          typename DispatchFunc, typename... Args>",
      "typename DispatchFunc::return_type cutlass_gemm_sm90_16bit_dispatch(",
      "    uint32_t m, uint32_t n, Args&&... args) {",
      "  using Cutlass3xGemmDefault =",
      "      typename sm90_config_default<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "",
      "  return DispatchFunc::template invoke<Cutlass3xGemmDefault>(",
      "      std::forward<Args>(args)...);",
      "}",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue,",
      "          typename DispatchFunc, typename... Args>",
      "typename DispatchFunc::return_type cutlass_gemm_sm90_int8_dispatch(",
      "    uint32_t m, uint32_t n, Args&&... args) {",
      "  static_assert(std::is_same_v<InType, int8_t>);",
      "",
      "  using Cutlass3xGemmDefault =",
      "      typename sm90_config_default<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM128 =",
      "      typename sm90_int8_config_M128<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM64 =",
      "      typename sm90_int8_config_M64<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM32NBig =",
      "      typename sm90_int8_config_M32_NBig<InType, OutType,",
      "                                         Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM32NSmall =",
      "      typename sm90_int8_config_M32_NSmall<InType, OutType,",
      "                                           Epilogue>::Cutlass3xGemm;",
      "",
      "  bool const is_small_n = n < 8192;",
      "  uint32_t const mp2 =",
      "      std::max(static_cast<uint32_t>(32), next_pow_2(m));  // next power of 2",
      "",
      "  if (mp2 <= 32) {",
      "    // m in [1, 32]",
      "    if (is_small_n) {",
      "      return DispatchFunc::template invoke<Cutlass3xGemmM32NSmall>(",
      "          std::forward<Args>(args)...);",
      "    } else {",
      "      return DispatchFunc::template invoke<Cutlass3xGemmM32NBig>(",
      "          std::forward<Args>(args)...);",
      "    }",
      "  } else if (mp2 <= 64) {",
      "    // m in (32, 64]",
      "    return DispatchFunc::template invoke<Cutlass3xGemmM64>(",
      "        std::forward<Args>(args)...);",
      "  } else if (mp2 <= 128) {",
      "    // m in (64, 128]",
      "    return DispatchFunc::template invoke<Cutlass3xGemmM128>(",
      "        std::forward<Args>(args)...);",
      "  } else {",
      "    // m in (128, inf)",
      "    return DispatchFunc::template invoke<Cutlass3xGemmDefault>(",
      "        std::forward<Args>(args)...);",
      "  }",
      "}",
      "",
      "// Dispatch to GEMM implementations based on element types",
      "template <template <typename, typename, typename> typename Epilogue,",
      "          typename... EpilogueArgs>",
      "void cutlass_scaled_sparse_mm_sm90_epilogue(torch::Tensor& out,",
      "                                            torch::Tensor const& a,",
      "                                            torch::Tensor const& bt_nzs,",
      "                                            torch::Tensor const& bt_meta,",
      "                                            EpilogueArgs&&... epilogue_args) {",
      "  uint32_t const m = out.size(0);",
      "  uint32_t const n = out.size(1);",
      "",
      "  // TODO: add dispatch functions to all of these",
      "  TORCH_CHECK(bt_meta.dtype() == torch::kUInt8);",
      "  if (a.dtype() == torch::kInt8) {",
      "    TORCH_CHECK(bt_nzs.dtype() == torch::kInt8);",
      "",
      "    if (out.dtype() == torch::kBFloat16) {",
      "      return cutlass_gemm_sm90_int8_dispatch<int8_t, cutlass::bfloat16_t,",
      "                                             Epilogue, GemmCallerTraits>(",
      "          m, n, out, a, bt_nzs, bt_meta,",
      "          std::forward<EpilogueArgs>(epilogue_args)...);",
      "    } else {",
      "      TORCH_CHECK(out.dtype() == torch::kFloat16);",
      "      return cutlass_gemm_sm90_int8_dispatch<int8_t, cutlass::half_t, Epilogue,",
      "                                             GemmCallerTraits>(",
      "          m, n, out, a, bt_nzs, bt_meta,",
      "          std::forward<EpilogueArgs>(epilogue_args)...);",
      "    }",
      "  } else if (a.dtype() == torch::kFloat8_e4m3fn) {",
      "    TORCH_CHECK(bt_nzs.dtype() == torch::kFloat8_e4m3fn);",
      "",
      "    if (out.dtype() == torch::kBFloat16) {",
      "      return cutlass_gemm_sm90_fp8_dispatch<cutlass::float_e4m3_t,",
      "                                            cutlass::bfloat16_t, Epilogue,",
      "                                            GemmCallerTraits>(",
      "          m, n, out, a, bt_nzs, bt_meta,",
      "          std::forward<EpilogueArgs>(epilogue_args)...);",
      "    } else {",
      "      TORCH_CHECK(out.dtype() == torch::kFloat16);",
      "      return cutlass_gemm_sm90_fp8_dispatch<",
      "          cutlass::float_e4m3_t, cutlass::half_t, Epilogue, GemmCallerTraits>(",
      "          m, n, out, a, bt_nzs, bt_meta,",
      "          std::forward<EpilogueArgs>(epilogue_args)...);",
      "    }",
      "  } else if (a.dtype() == torch::kFloat16) {",
      "    TORCH_CHECK(bt_nzs.dtype() == torch::kFloat16);",
      "    TORCH_CHECK(out.dtype() == torch::kFloat16);",
      "",
      "    return cutlass_gemm_sm90_16bit_dispatch<cutlass::half_t, cutlass::half_t,",
      "                                            Epilogue, GemmCallerTraits>(",
      "        m, n, out, a, bt_nzs, bt_meta,",
      "        std::forward<EpilogueArgs>(epilogue_args)...);",
      "  } else {  // a.dtype() == torch::kBFloat16",
      "    TORCH_CHECK(a.dtype() == torch::kBFloat16);",
      "    TORCH_CHECK(bt_nzs.dtype() == torch::kBFloat16);",
      "    TORCH_CHECK(out.dtype() == torch::kBFloat16);",
      "",
      "    return cutlass_gemm_sm90_16bit_dispatch<",
      "        cutlass::bfloat16_t, cutlass::bfloat16_t, Epilogue, GemmCallerTraits>(",
      "        m, n, out, a, bt_nzs, bt_meta,",
      "        std::forward<EpilogueArgs>(epilogue_args)...);",
      "  }",
      "}",
      "",
      "void cutlass_scaled_sparse_mm_sm90(torch::Tensor& out, torch::Tensor const& a,",
      "                                   torch::Tensor const& bt_nzs,",
      "                                   torch::Tensor const& bt_meta,",
      "                                   torch::Tensor const& a_scales,",
      "                                   torch::Tensor const& b_scales,",
      "                                   std::optional<torch::Tensor> const& bias) {",
      "  TORCH_CHECK(bt_meta.dtype() == torch::kUInt8);",
      "  TORCH_CHECK(a_scales.dtype() == torch::kFloat32);",
      "  TORCH_CHECK(b_scales.dtype() == torch::kFloat32);",
      "",
      "  if (bias) {",
      "    TORCH_CHECK(bias->dtype() == out.dtype(),",
      "                \"CUTLASS scaled_mm bias dtype must match output dtype \",",
      "                out.dtype());",
      "    return cutlass_scaled_sparse_mm_sm90_epilogue<",
      "        c3x::ScaledEpilogueColumnBias>(out, a, bt_nzs, bt_meta, b_scales,",
      "                                       a_scales, *bias);",
      "  } else {",
      "    return cutlass_scaled_sparse_mm_sm90_epilogue<c3x::ScaledEpilogue>(",
      "        out, a, bt_nzs, bt_meta, b_scales, a_scales);",
      "  }",
      "}",
      "",
      "CompressorResult cutlass_sparse_compress_sm90(torch::Tensor const& a) {",
      "  // These m and n variables are fordispatching to different GEMM algorithms.",
      "  uint32_t const m = 1;  // Set M to 1 for compression",
      "  uint32_t const n = a.size(1);",
      "",
      "  // Note: For correctness, the compressed format must be invariant in:",
      "  //  - M, the flattened number of tokens",
      "  //  - Whether output dtype is fp16 or bf16",
      "  //  - CUTLASS epilogues",
      "",
      "  if (a.dtype() == torch::kInt8) {",
      "    return cutlass_gemm_sm90_int8_dispatch<int8_t, cutlass::bfloat16_t,",
      "                                           c3x::TrivialEpilogue,",
      "                                           GemmCompressorTraits>(m, n, a);",
      "  } else if (a.dtype() == torch::kFloat8_e4m3fn) {",
      "    return cutlass_gemm_sm90_fp8_dispatch<",
      "        cutlass::float_e4m3_t, cutlass::bfloat16_t, c3x::TrivialEpilogue,",
      "        GemmCompressorTraits>(m, n, a);",
      "  } else if (a.dtype() == torch::kFloat16) {",
      "    return cutlass_gemm_sm90_16bit_dispatch<",
      "        cutlass::bfloat16_t, cutlass::bfloat16_t, c3x::TrivialEpilogue,",
      "        GemmCompressorTraits>(m, n, a);",
      "  } else {",
      "    TORCH_CHECK(a.dtype() == torch::kBFloat16,",
      "                \"cutlass_sparse_compress only supports int8, fp8_e4m3, fp16, \"",
      "                \"and bf16 datatypes\");",
      "    return cutlass_gemm_sm90_16bit_dispatch<cutlass::half_t, cutlass::half_t,",
      "                                            c3x::TrivialEpilogue,",
      "                                            GemmCompressorTraits>(m, n, a);",
      "  }",
      "}",
      "",
      "#endif"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/sparse/cutlass/sparse_scaled_mm_c3x.cuh",
    "source": [
      "#pragma once",
      "",
      "// clang-format will break include orders",
      "// clang-format off",
      "#include <cudaTypedefs.h>",
      "",
      "#include <torch/all.h>",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "",
      "#include \"cuda_utils.h\"",
      "",
      "#include \"cutlass/cutlass.h\"",
      "",
      "#include \"cutlass/gemm/device/gemm_universal_adapter.h\"",
      "#include \"cutlass/epilogue/collective/collective_builder.hpp\"",
      "#include \"cutlass/gemm/collective/collective_builder.hpp\"",
      "",
      "#include \"cutlass/transform/device/transform_universal_adapter.hpp\"",
      "#include \"cutlass/transform/kernel/sparse_gemm_compressor.hpp\"",
      "",
      "#include \"core/math.hpp\"",
      "#include \"cutlass_extensions/cute_utils.cuh\"",
      "#include \"cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp\"",
      "#include \"cutlass_extensions/common.hpp\"",
      "#include \"cutlass_extensions/torch_utils.hpp\"",
      "// clang-format on",
      "",
      "using namespace cute;",
      "",
      "/*",
      "   This file defines 2:4 sparse GEMM operations using the CUTLASS 3.x API,",
      "   for NVIDIA GPUs with sm90a (Hopper) or later.",
      "*/",
      "",
      "namespace {",
      "",
      "// A wrapper for the GEMM kernel that is used to guard against compilation on",
      "// architectures that will never use the kernel. The purpose of this is to",
      "// reduce the size of the compiled binary.",
      "// __CUDA_ARCH__ is not defined in host code, so this lets us smuggle the ifdef",
      "// into code that will be executed on the device where it is defined.",
      "template <typename Kernel>",
      "struct enable_sm90_or_later : Kernel {",
      "  template <typename... Args>",
      "  CUTLASS_DEVICE void operator()(Args&&... args) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 900",
      "    Kernel::operator()(std::forward<Args>(args)...);",
      "#endif",
      "  }",
      "};",
      "",
      "using GemmUniversalMode = cutlass::gemm::GemmUniversalMode;",
      "",
      "/*",
      " * cutlass_sparse_3x_gemm defines a 2:4 sparse GEMM kernel via CUTLASS",
      " * for SM90 Hopper systems.",
      " */",
      "template <typename ElementAB_, typename ElementD_,",
      "          template <typename, typename, typename> typename Epilogue_,",
      "          typename TileShape, typename ClusterShape, typename KernelSchedule,",
      "          typename EpilogueSchedule>",
      "struct cutlass_sparse_3x_gemm {",
      "  using ElementAB = ElementAB_;",
      "  using ElementD = ElementD_;",
      "  using ElementAcc =",
      "      typename std::conditional<std::is_same_v<ElementAB, int8_t>, int32_t,",
      "                                float>::type;",
      "",
      "  using Epilogue = Epilogue_<ElementAcc, ElementD, TileShape>;",
      "",
      "  using ElementC = void;",
      "  using LayoutC = cutlass::layout::RowMajor;",
      "  using LayoutC_Transpose =",
      "      typename cutlass::layout::LayoutTranspose<LayoutC>::type;",
      "",
      "  using EVTCompute = typename Epilogue::EVTCompute;",
      "",
      "  // These are the minimum alignments needed for the kernels to compile",
      "  static constexpr int AlignmentAB =",
      "      128 / cutlass::sizeof_bits<ElementAB>::value;",
      "  static constexpr int AlignmentCD =",
      "      128 / cutlass::sizeof_bits<ElementD>::value;",
      "",
      "  using CollectiveEpilogue =",
      "      typename cutlass::epilogue::collective::CollectiveBuilder<",
      "          cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp, TileShape,",
      "          ClusterShape, cutlass::epilogue::collective::EpilogueTileAuto,",
      "          ElementAcc, float, ElementC, LayoutC_Transpose, AlignmentCD, ElementD,",
      "          LayoutC_Transpose, AlignmentCD, EpilogueSchedule,",
      "          EVTCompute>::CollectiveOp;",
      "",
      "  static constexpr size_t CEStorageSize =",
      "      sizeof(typename CollectiveEpilogue::SharedStorage);",
      "  using Stages = typename cutlass::gemm::collective::StageCountAutoCarveout<",
      "      static_cast<int>(CEStorageSize)>;",
      "",
      "  // clang-format off",
      "  using CollectiveMainloop =",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          cutlass::arch::Sm90, cutlass::arch::OpClassSparseTensorOp,",
      "          ElementAB, cutlass::layout::RowMajor, AlignmentAB,",
      "          ElementAB, cutlass::layout::ColumnMajor, AlignmentAB,",
      "          ElementAcc, TileShape, ClusterShape,",
      "          Stages,",
      "          KernelSchedule>::CollectiveOp;",
      "  // clang-format on",
      "",
      "  using KernelType = enable_sm90_or_later<cutlass::gemm::kernel::GemmUniversal<",
      "      cute::Shape<int, int, int, int>, CollectiveMainloop, CollectiveEpilogue,",
      "      cutlass::gemm::PersistentScheduler>>;",
      "",
      "  struct GemmKernel : public KernelType {};",
      "",
      "  // Sparse compressor definitions",
      "  using SparseConfig = typename GemmKernel::CollectiveMainloop::SparseConfig;",
      "  using LayoutTagA = cutlass::layout::RowMajor;",
      "  using CompressorUtility =",
      "      cutlass::transform::kernel::StructuredSparseCompressorUtility<",
      "          typename GemmKernel::ProblemShape, ElementAB, LayoutTagA,",
      "          SparseConfig>;",
      "  using CompressorKernel =",
      "      cutlass::transform::kernel::StructuredSparseCompressor<",
      "          typename GemmKernel::ProblemShape, ElementAB, LayoutTagA,",
      "          SparseConfig, cutlass::arch::Sm90>;",
      "  using Compressor =",
      "      cutlass::transform::device::TransformUniversalAdapter<CompressorKernel>;",
      "};",
      "",
      "/*",
      " * This class defines kernel to compress a 2:4 sparse matrix.",
      " * The particular format is defined by the Gemm template parameter,",
      " * which is a cutlass_sparse_3x_gemm.",
      " */",
      "using CompressorResult = std::tuple<torch::Tensor, torch::Tensor>;",
      "/// Make A structured sparse by replacing elements with 0 and compress it",
      "template <typename Gemm>",
      "CompressorResult cutlass_sparse_compress(torch::Tensor const& a) {",
      "  // Checks for conformality",
      "  TORCH_CHECK(a.dtype() == torch::kInt8 || a.dtype() == torch::kFloat8_e4m3fn ||",
      "              a.dtype() == torch::kFloat16 || a.dtype() == torch::kBFloat16);",
      "  TORCH_CHECK(a.dim() == 2)",
      "  // Check for strides and alignment",
      "  TORCH_CHECK(a.stride(0) % 4 == 0)  // Required for semi-structured sparsity",
      "  TORCH_CHECK(a.stride(1) == 1)",
      "",
      "  using GemmKernel = typename Gemm::KernelType;",
      "  using ElementA = typename Gemm::ElementAB;",
      "  using ElementE = typename GemmKernel::CollectiveMainloop::ElementE;",
      "",
      "  int m = a.size(0);",
      "  int k = a.size(1);",
      "  using ProblemShape = typename GemmKernel::ProblemShape;",
      "  ProblemShape prob_shape{m, 1, k, 1};",
      "",
      "  int64_t lda = a.stride(0);",
      "  using StrideA = Stride<int64_t, Int<1>, int64_t>;",
      "  StrideA a_stride{lda, Int<1>{}, 0};",
      "",
      "  using CompressorUtility = typename Gemm::CompressorUtility;",
      "  CompressorUtility compressor_utility(prob_shape, a_stride);",
      "",
      "  // Allocate buffers for the metadata E and the compressed matrix A",
      "  int ME = compressor_utility.get_metadata_m_physical();",
      "  int KE = compressor_utility.get_metadata_k_physical();",
      "  int MC = compressor_utility.get_tensorA_m_physical();",
      "  int KC = compressor_utility.get_tensorA_k_physical();",
      "",
      "  auto const a_meta_options =",
      "      torch::TensorOptions().dtype(torch::kUInt8).device(a.device());",
      "  auto const a_nzs_options =",
      "      torch::TensorOptions().dtype(a.dtype()).device(a.device());",
      "",
      "  auto a_meta = torch::zeros({ME, KE}, a_meta_options);",
      "  auto a_nzs = torch::zeros({MC, KC}, a_nzs_options);",
      "",
      "  auto a_ptr = static_cast<ElementA*>(a.data_ptr());",
      "  auto a_nzs_ptr = static_cast<ElementA*>(a_nzs.data_ptr());",
      "  auto a_meta_ptr = static_cast<ElementE*>(a_meta.data_ptr());",
      "",
      "  cutlass::KernelHardwareInfo hw_info;",
      "  hw_info.device_id = a.device().index();",
      "  hw_info.sm_count =",
      "      cutlass::KernelHardwareInfo::query_device_multiprocessor_count(",
      "          hw_info.device_id);",
      "",
      "  using Compressor = typename Gemm::Compressor;",
      "  typename Compressor::Arguments arguments{",
      "      prob_shape, {a_ptr, a_stride, a_nzs_ptr, a_meta_ptr}, {hw_info}};",
      "",
      "  Compressor compressor_op;",
      "  size_t workspace_size = Compressor::get_workspace_size(arguments);",
      "  auto const workspace_options =",
      "      torch::TensorOptions().dtype(torch::kUInt8).device(a.device());",
      "  auto workspace = torch::empty(workspace_size, workspace_options);",
      "",
      "  CUTLASS_CHECK(compressor_op.can_implement(arguments));",
      "  CUTLASS_CHECK(compressor_op.initialize(arguments, workspace.data_ptr()));",
      "  CUTLASS_CHECK(compressor_op.run());",
      "  CUDA_CHECK(cudaDeviceSynchronize());",
      "",
      "  return {a_meta, a_nzs};",
      "}",
      "",
      "template <typename Gemm, typename... EpilogueArgs>",
      "void cutlass_sparse_gemm_caller(torch::Tensor& out, torch::Tensor const& a,",
      "                                torch::Tensor const& bt_nzs,",
      "                                torch::Tensor const& bt_meta,",
      "                                EpilogueArgs&&... epilogue_params) {",
      "  using ElementAB = typename Gemm::ElementAB;",
      "  using ElementD = typename Gemm::ElementD;",
      "",
      "  // Interface stride expected from the argument a (will get transposed)",
      "  // We compute C^T = B^T * A^T, but we assume B is transposed before",
      "  // compression and hence the bt_* naming",
      "  using LayoutB = typename Gemm::GemmKernel::CollectiveMainloop::LayoutA;",
      "  using LayoutE = typename Gemm::GemmKernel::CollectiveMainloop::LayoutE;",
      "",
      "  // M, N, K after transposition",
      "  int32_t m = out.size(1);",
      "  int32_t n = out.size(0);",
      "  int32_t k = a.size(1);",
      "",
      "  int64_t lda = a.stride(0);",
      "  int64_t ldc = out.stride(0);",
      "",
      "  using StrideA = Stride<int64_t, Int<1>, int64_t>;",
      "  using StrideC = Stride<Int<1>, int64_t, int64_t>;",
      "",
      "  StrideA a_stride{lda, Int<1>{}, Int<0>{}};",
      "  StrideC c_stride{Int<1>{}, ldc, Int<0>{}};",
      "",
      "  using GemmKernel = typename Gemm::GemmKernel;",
      "  typename GemmKernel::ProblemShape prob_shape{m, n, k, 1};",
      "",
      "  using ElementE = typename GemmKernel::CollectiveMainloop::ElementE;",
      "  using SparseConfig = typename GemmKernel::CollectiveMainloop::SparseConfig;",
      "",
      "  LayoutB b_layout = SparseConfig::fill_layoutA(prob_shape);",
      "  LayoutE e_layout = SparseConfig::fill_layoutE(prob_shape);",
      "",
      "  auto a_ptr = static_cast<ElementAB*>(a.data_ptr());",
      "  auto b_ptr = static_cast<ElementAB*>(bt_nzs.data_ptr());",
      "  auto e_ptr = static_cast<ElementE*>(bt_meta.data_ptr());",
      "  typename GemmKernel::MainloopArguments mainloop_args{",
      "      b_ptr, b_layout, a_ptr, a_stride, e_ptr, e_layout};",
      "",
      "  auto c_ptr = static_cast<ElementD*>(out.data_ptr());",
      "  typename GemmKernel::EpilogueArguments epilogue_args{",
      "      Gemm::Epilogue::prepare_args(",
      "          std::forward<EpilogueArgs>(epilogue_params)...),",
      "      c_ptr, c_stride, c_ptr, c_stride};",
      "",
      "  typename GemmKernel::Arguments args{cutlass::gemm::GemmUniversalMode::kGemm,",
      "                                      prob_shape, mainloop_args, epilogue_args};",
      "",
      "  // Launch the CUTLASS GEMM kernel.",
      "  using GemmOp = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;",
      "  GemmOp gemm_op;",
      "  CUTLASS_CHECK(gemm_op.can_implement(args));",
      "",
      "  size_t workspace_size = gemm_op.get_workspace_size(args);",
      "  auto const workspace_options =",
      "      torch::TensorOptions().dtype(torch::kUInt8).device(a.device());",
      "  auto workspace = torch::empty(workspace_size, workspace_options);",
      "",
      "  auto stream = at::cuda::getCurrentCUDAStream(a.get_device());",
      "",
      "  cutlass::Status status = gemm_op.run(args, workspace.data_ptr(), stream);",
      "  CUTLASS_CHECK(status);",
      "}",
      "",
      "//////////////////////////////////////////////////",
      "// Gemm Configs are defined below",
      "//////////////////////////////////////////////////",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_config_default {};",
      "",
      "template <typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_config_default<half_t, OutType, Epilogue> {",
      "  using KernelSchedule = cutlass::gemm::KernelTmaWarpSpecialized;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_128, _128, _128>;",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<half_t, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_config_default<cutlass::bfloat16_t, OutType, Epilogue> {",
      "  using KernelSchedule = cutlass::gemm::KernelTmaWarpSpecialized;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_128, _128, _128>;",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<cutlass::bfloat16_t, OutType, Epilogue, TileShape,",
      "                             ClusterShape, KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "//////////////////////// Cherry-Picking Kernels ////////////////////////",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_1 {",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule = cutlass::gemm::KernelTmaWarpSpecializedFP8FastAccum;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _64, _256>;",
      "  using ClusterShape = Shape<_8, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_2 {",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelTmaWarpSpecializedCooperativeFP8FastAccum;",
      "  using EpilogueSchedule =",
      "      typename cutlass::epilogue::TmaWarpSpecializedCooperative;",
      "  using TileShape = Shape<_128, _64, _256>;",
      "  using ClusterShape = Shape<_8, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_3 {",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule = cutlass::gemm::KernelTmaWarpSpecializedFP8FastAccum;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _64, _256>;",
      "  using ClusterShape = Shape<_1, _2, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_4 {",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule = cutlass::gemm::KernelTmaWarpSpecializedFP8FastAccum;",
      "  using EpilogueSchedule =",
      "      typename cutlass::epilogue::TmaWarpSpecializedCooperative;",
      "  using TileShape = Shape<_64, _128, _256>;",
      "  using ClusterShape = Shape<_8, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_5 {",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelTmaWarpSpecializedPingpongFP8FastAccum;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_128, _128, _256>;",
      "  using ClusterShape = Shape<_8, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_6 {",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule = cutlass::gemm::KernelTmaWarpSpecializedFP8FastAccum;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _128, _256>;",
      "  using ClusterShape = Shape<_1, _2, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_7 {",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelTmaWarpSpecializedCooperativeFP8FastAccum;",
      "  using EpilogueSchedule =",
      "      typename cutlass::epilogue::TmaWarpSpecializedCooperative;",
      "  using TileShape = Shape<_128, _128, _256>;",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_8 {",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelTmaWarpSpecializedCooperativeFP8FastAccum;",
      "  using EpilogueSchedule =",
      "      typename cutlass::epilogue::TmaWarpSpecializedCooperative;",
      "  using TileShape = Shape<_128, _256, _128>;",
      "  using ClusterShape = Shape<_8, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "////////////////////////////////////////////////////////////////////////",
      "",
      "template <typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_config_default<cutlass::float_e4m3_t, OutType, Epilogue> {",
      "  // M in (128, inf)",
      "  using KernelSchedule = cutlass::gemm::KernelTmaWarpSpecializedFP8FastAccum;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_128, _128, _128>;",
      "  using ClusterShape = Shape<_1, _2, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<cutlass::float_e4m3_t, OutType, Epilogue,",
      "                             TileShape, ClusterShape, KernelSchedule,",
      "                             EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_M64 {",
      "  // M in [1, 64]",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule = cutlass::gemm::KernelTmaWarpSpecializedFP8FastAccum;",
      "  using EpilogueSchedule =",
      "      typename cutlass::epilogue::TmaWarpSpecializedCooperative;",
      "  using TileShape = Shape<_64, _64, _256>;",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_M128 {",
      "  // M in (64, 128]",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelTmaWarpSpecializedPingpongFP8FastAccum;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _128, _256>;",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_M256 {",
      "  // M in (128, 256]",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelTmaWarpSpecializedCooperativeFP8FastAccum;",
      "  using EpilogueSchedule =",
      "      typename cutlass::epilogue::TmaWarpSpecializedCooperative;",
      "  using TileShape = Shape<_128, _128, _256>;",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_M512 {",
      "  // M in (256, ]",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelTmaWarpSpecializedCooperativeFP8FastAccum;",
      "  using EpilogueSchedule =",
      "      typename cutlass::epilogue::TmaWarpSpecializedCooperative;",
      "  using TileShape = Shape<_128, _128, _256>;",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_config_default<int8_t, OutType, Epilogue> {",
      "  // For M > 128 and any N",
      "  using KernelSchedule =",
      "      typename cutlass::gemm::KernelTmaWarpSpecializedPingpong;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_128, _128, _128>;",
      "  using ClusterShape = Shape<_2, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<int8_t, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_int8_config_M128 {",
      "  // For M in (64, 128] and any N",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using KernelSchedule =",
      "      typename cutlass::gemm::KernelTmaWarpSpecializedPingpong;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _128, _128>;",
      "  using ClusterShape = Shape<_2, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_int8_config_M64 {",
      "  // For M in (32, 64] and any N",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using KernelSchedule = typename cutlass::gemm::KernelTmaWarpSpecialized;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _64, _256>;",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_int8_config_M32_NBig {",
      "  // For M in [1, 32] and N >= 8192",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using KernelSchedule = typename cutlass::gemm::KernelTmaWarpSpecialized;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _128, _256>;",
      "  using ClusterShape = Shape<_1, _4, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_int8_config_M32_NSmall {",
      "  // For M in [1, 32] and N < 8192",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using KernelSchedule = typename cutlass::gemm::KernelTmaWarpSpecialized;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _64, _256>;",
      "  using ClusterShape = Shape<_1, _8, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_sparse_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                             KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "}  // namespace"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/sparse/cutlass/sparse_scaled_mm_entry.cu",
    "source": [
      "#include <cudaTypedefs.h>",
      "",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <torch/all.h>",
      "",
      "#include \"cutlass_extensions/common.hpp\"",
      "",
      "bool cutlass_sparse_scaled_mm_supported(int64_t cuda_device_capability) {",
      "  // sparse CUTLASS kernels need at least",
      "  //   CUDA 12.2 and SM90 (Hopper)",
      "",
      "#if defined CUDA_VERSION",
      "  return CUDA_VERSION >= 12020 && cuda_device_capability >= 90;",
      "#endif",
      "",
      "  return false;",
      "}",
      "",
      "#if defined ENABLE_SPARSE_SCALED_MM_C3X && ENABLE_SPARSE_SCALED_MM_C3X",
      "void cutlass_scaled_sparse_mm_sm90(torch::Tensor& c, torch::Tensor const& a,",
      "                                   torch::Tensor const& b,",
      "                                   torch::Tensor const& e,",
      "                                   torch::Tensor const& a_scales,",
      "                                   torch::Tensor const& b_scales,",
      "                                   std::optional<torch::Tensor> const& bias);",
      "",
      "using CompressorResult = std::tuple<torch::Tensor, torch::Tensor>;",
      "CompressorResult cutlass_sparse_compress_sm90(torch::Tensor const& a);",
      "#endif",
      "",
      "void cutlass_scaled_sparse_mm(torch::Tensor& c, torch::Tensor const& a,",
      "                              torch::Tensor const& bt_nzs,",
      "                              torch::Tensor const& bt_meta,",
      "                              torch::Tensor const& a_scales,",
      "                              torch::Tensor const& b_scales,",
      "                              std::optional<torch::Tensor> const& bias) {",
      "  // Checks for conformality",
      "  TORCH_CHECK(a.dim() == 2 && bt_nzs.dim() == 2 && c.dim() == 2);",
      "  TORCH_CHECK(c.size(1) == bt_nzs.size(0) && bt_nzs.size(1) * 2 == a.size(1) &&",
      "              a.size(0) == c.size(0));",
      "  TORCH_CHECK(a_scales.numel() == 1 || a_scales.numel() == a.size(0));",
      "  TORCH_CHECK(b_scales.numel() == 1 || b_scales.numel() == bt_nzs.size(0));",
      "",
      "  // Check for strides and alignment",
      "  TORCH_CHECK(a.stride(1) == 1 && bt_nzs.stride(1) == 1 &&",
      "              c.stride(1) == 1);            // Row-major",
      "  TORCH_CHECK(c.stride(0) % 16 == 0);       // 16 Byte Alignment",
      "  TORCH_CHECK(bt_nzs.stride(0) % 16 == 0);  // 16 Byte Alignment",
      "  TORCH_CHECK(a_scales.is_contiguous() && b_scales.is_contiguous());",
      "",
      "  if (bias) {",
      "    TORCH_CHECK(bias->numel() == bt_nzs.size(0) && bias->is_contiguous() &&",
      "                bias->dim() == 1);",
      "  }",
      "",
      "  at::cuda::OptionalCUDAGuard const device_guard(device_of(a));",
      "  int32_t version_num = get_sm_version_num();",
      "",
      "  // Guard against compilation issues for sm90 kernels",
      "#if defined ENABLE_SPARSE_SCALED_MM_C3X && ENABLE_SPARSE_SCALED_MM_C3X",
      "  // We build for 9.0a which is not forward compatible, so restrict this to",
      "  // Hopper only",
      "  if (version_num == 90) {",
      "    cutlass_scaled_sparse_mm_sm90(c, a, bt_nzs, bt_meta, a_scales, b_scales,",
      "                                  bias);",
      "    return;",
      "  }",
      "#endif",
      "",
      "  TORCH_CHECK_NOT_IMPLEMENTED(",
      "      false,",
      "      \"No compiled cutlass_scaled_sparse_mm for a compute capability less than \"",
      "      \"CUDA device capability: \",",
      "      version_num);",
      "}",
      "",
      "std::vector<torch::Tensor> cutlass_sparse_compress(torch::Tensor const& a) {",
      "  // Check for strides and alignment",
      "  TORCH_CHECK(a.stride(1) == 1);      // Row-major",
      "  TORCH_CHECK(a.stride(0) % 8 == 0);  // 8 Byte Alignment for Compression",
      "",
      "  at::cuda::OptionalCUDAGuard const device_guard(device_of(a));",
      "  int32_t version_num = get_sm_version_num();",
      "",
      "  // Guard against compilation issues for sm90 kernels",
      "#if defined ENABLE_SPARSE_SCALED_MM_C3X && ENABLE_SPARSE_SCALED_MM_C3X",
      "  // We build for 9.0a which is not forward compatible, so restrict this to",
      "  // Hopper only",
      "  if (version_num == 90) {",
      "    std::vector<torch::Tensor> result_tensors;",
      "",
      "    auto [a_meta, a_nzs] = cutlass_sparse_compress_sm90(a);",
      "    result_tensors.push_back(std::move(a_nzs));",
      "    result_tensors.push_back(std::move(a_meta));",
      "    return result_tensors;",
      "  }",
      "#endif",
      "",
      "  TORCH_CHECK_NOT_IMPLEMENTED(",
      "      false,",
      "      \"No compiled cutlass_sparse_compress for a compute capability less than \"",
      "      \"CUDA device capability: \",",
      "      version_num);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/mamba/mamba_ssm/selective_scan_fwd.cu",
    "source": [
      "// clang-format off",
      "// adapted from https://github.com/state-spaces/mamba/blob/main/csrc/selective_scan/selective_scan_fwd_kernel.cuh",
      "#include <torch/all.h>",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include \"selective_scan.h\"",
      "",
      "#include <c10/util/BFloat16.h>",
      "#include <c10/util/Half.h>",
      "#ifdef USE_ROCM",
      "    #include <c10/hip/HIPException.h>  // For C10_HIP_CHECK and C10_HIP_KERNEL_LAUNCH_CHECK",
      "#else",
      "    #include <c10/cuda/CUDAException.h>  // For C10_CUDA_CHECK and C10_CUDA_KERNEL_LAUNCH_CHECK",
      "#endif",
      "",
      "#ifndef USE_ROCM",
      "    #include <cub/block/block_load.cuh>",
      "    #include <cub/block/block_store.cuh>",
      "    #include <cub/block/block_scan.cuh>",
      "#else",
      "    #include <hipcub/hipcub.hpp>",
      "    namespace cub = hipcub;",
      "#endif",
      "",
      "#include \"selective_scan.h\"",
      "#include \"static_switch.h\"",
      "",
      "template<int kNThreads_, int kNItems_, int kNRows_, bool kIsEvenLen_,",
      "         bool kIsVariableB_, bool kIsVariableC_,",
      "         bool kHasZ_, bool kVarlen_, typename input_t_, typename weight_t_, typename state_t_>",
      "struct Selective_Scan_fwd_kernel_traits {",
      "    static_assert(kNItems_ % 4 == 0);",
      "    using input_t = input_t_;",
      "    using weight_t = weight_t_;",
      "    using state_t = state_t_;",
      "    static constexpr int kNThreads = kNThreads_;",
      "    // Setting MinBlocksPerMP to be 3 (instead of 2) for 128 threads improves occupancy.",
      "    static constexpr int kMinBlocks = kNThreads < 128 ? 5 : 3;",
      "    static constexpr int kNItems = kNItems_;",
      "    static constexpr int kNRows = kNRows_;",
      "    static constexpr int kNBytes = sizeof(input_t);",
      "    static_assert(kNBytes == 2 || kNBytes == 4);",
      "    static constexpr int kNElts = kNBytes == 4 ? 4 : constexpr_min(8, kNItems);",
      "    static_assert(kNItems % kNElts == 0);",
      "    static constexpr int kNLoads = kNItems / kNElts;",
      "    static constexpr bool kIsEvenLen = kVarlen_ ? false : kIsEvenLen_;",
      "    static constexpr bool kIsVariableB = kIsVariableB_;",
      "    static constexpr bool kIsVariableC = kIsVariableC_;",
      "    static constexpr bool kHasZ = kHasZ_;",
      "    static constexpr bool kVarlen = kVarlen_;",
      "",
      "    static constexpr bool kDirectIO = kVarlen_ ? false : kIsEvenLen && kNLoads == 1;",
      "    static constexpr int kNLoadsIndex = kNItems / 4;",
      "    using vec_t = typename BytesToType<kNBytes * kNElts>::Type;",
      "    using scan_t = float2;",
      "    using BlockLoadT = cub::BlockLoad<input_t, kNThreads, kNItems, cub::BLOCK_LOAD_WARP_TRANSPOSE>;",
      "    using BlockLoadVecT = cub::BlockLoad<vec_t, kNThreads, kNLoads,",
      "        !kDirectIO ? cub::BLOCK_LOAD_WARP_TRANSPOSE : cub::BLOCK_LOAD_DIRECT>;",
      "    using BlockLoadWeightT = cub::BlockLoad<input_t, kNThreads, kNItems , cub::BLOCK_LOAD_WARP_TRANSPOSE>;",
      "    using BlockLoadWeightVecT = cub::BlockLoad<vec_t, kNThreads, kNLoads ,",
      "        !kDirectIO ? cub::BLOCK_LOAD_WARP_TRANSPOSE  : cub::BLOCK_LOAD_DIRECT>;",
      "    using BlockStoreT = cub::BlockStore<input_t, kNThreads, kNItems, cub::BLOCK_STORE_WARP_TRANSPOSE>;",
      "    using BlockStoreVecT = cub::BlockStore<vec_t, kNThreads, kNLoads,",
      "        !kDirectIO ? cub::BLOCK_STORE_WARP_TRANSPOSE : cub::BLOCK_STORE_DIRECT>;",
      "    // using BlockScanT = cub::BlockScan<scan_t, kNThreads, cub::BLOCK_SCAN_RAKING_MEMOIZE>;",
      "    // using BlockScanT = cub::BlockScan<scan_t, kNThreads, cub::BLOCK_SCAN_RAKING>;",
      "    using BlockScanT = cub::BlockScan<scan_t, kNThreads, cub::BLOCK_SCAN_WARP_SCANS>;",
      "    static constexpr int kSmemIOSize = custom_max({sizeof(typename BlockLoadT::TempStorage),",
      "                                                 sizeof(typename BlockLoadVecT::TempStorage),",
      "                                                 (int(kIsVariableB) + int(kIsVariableC)) * sizeof(typename BlockLoadWeightT::TempStorage),",
      "                                                 (int(kIsVariableB) + int(kIsVariableC)) * sizeof(typename BlockLoadWeightVecT::TempStorage),",
      "                                                 sizeof(typename BlockStoreT::TempStorage),",
      "                                                 sizeof(typename BlockStoreVecT::TempStorage)});",
      "    static constexpr int kSmemSize = kSmemIOSize + sizeof(typename BlockScanT::TempStorage);",
      "};",
      "",
      "template<typename Ktraits>",
      "__global__ __launch_bounds__(Ktraits::kNThreads, Ktraits::kMinBlocks)",
      "void selective_scan_fwd_kernel(SSMParamsBase params) {",
      "    constexpr bool kIsVariableB = Ktraits::kIsVariableB;",
      "    constexpr bool kIsVariableC = Ktraits::kIsVariableC;",
      "    constexpr bool kHasZ = Ktraits::kHasZ;",
      "    constexpr bool kVarlen = Ktraits::kVarlen;",
      "    constexpr int kNThreads = Ktraits::kNThreads;",
      "    constexpr int kNItems = Ktraits::kNItems;",
      "    constexpr int kNRows = Ktraits::kNRows;",
      "    constexpr bool kDirectIO = Ktraits::kDirectIO;",
      "    using input_t = typename Ktraits::input_t;",
      "    using weight_t = typename Ktraits::weight_t;",
      "    using scan_t = typename Ktraits::scan_t;",
      "",
      "    // Shared memory.",
      "    extern __shared__ char smem_[];",
      "    // cast to lvalue reference of expected type",
      "    // char *smem_loadstorescan = smem_ + 2 * MAX_DSTATE * sizeof(weight_t);",
      "    // auto& smem_load = reinterpret_cast<typename BlockLoadT::TempStorage&>(smem_ + 2 * MAX_DSTATE * sizeof(weight_t));",
      "    // auto& smem_load = reinterpret_cast<typename BlockLoadT::TempStorage&>(smem_loadstorescan);",
      "    auto& smem_load = reinterpret_cast<typename Ktraits::BlockLoadT::TempStorage&>(smem_);",
      "    auto& smem_load_weight = reinterpret_cast<typename Ktraits::BlockLoadWeightT::TempStorage&>(smem_);",
      "    auto& smem_load_weight1 = *reinterpret_cast<typename Ktraits::BlockLoadWeightT::TempStorage*>(smem_ + sizeof(typename Ktraits::BlockLoadWeightT::TempStorage));",
      "    auto& smem_store = reinterpret_cast<typename Ktraits::BlockStoreT::TempStorage&>(smem_);",
      "    auto& smem_scan = *reinterpret_cast<typename Ktraits::BlockScanT::TempStorage*>(smem_ + Ktraits::kSmemIOSize);",
      "    // weight_t *smem_a = reinterpret_cast<weight_t *>(smem_ + smem_loadstorescan_size);",
      "    // weight_t *smem_bc = reinterpret_cast<weight_t *>(smem_a + MAX_DSTATE);",
      "    scan_t *smem_running_prefix = reinterpret_cast<scan_t *>(smem_ + Ktraits::kSmemSize);",
      "",
      "    const int batch_id = blockIdx.x;",
      "    const int dim_id = blockIdx.y;",
      "    const int group_id = dim_id / (params.dim_ngroups_ratio);",
      "    int seqlen = params.seqlen;",
      "    int sequence_start_index = batch_id;",
      "    if constexpr (kVarlen){",
      "        int *query_start_loc = reinterpret_cast<int *>(params.query_start_loc_ptr);",
      "        sequence_start_index = query_start_loc[batch_id];",
      "        seqlen = query_start_loc[batch_id + 1] - sequence_start_index;",
      "    }",
      "    const bool has_initial_state = params.has_initial_state_ptr == nullptr ? false",
      "        : reinterpret_cast<bool *>(params.has_initial_state_ptr)[batch_id];",
      "",
      "    const int* cache_indices = params.cache_indices_ptr == nullptr ? nullptr",
      "        : reinterpret_cast<int *>(params.cache_indices_ptr);",
      "    const int cache_index = cache_indices == nullptr ? batch_id : cache_indices[batch_id];",
      "    // cache_index == params.pad_slot_id is defined as padding, so we exit early",
      "    if (cache_index == params.pad_slot_id){",
      "        return;",
      "    }",
      "    input_t *u = reinterpret_cast<input_t *>(params.u_ptr) + sequence_start_index * params.u_batch_stride",
      "        + dim_id * kNRows * params.u_d_stride;",
      "    input_t *delta = reinterpret_cast<input_t *>(params.delta_ptr) + sequence_start_index * params.delta_batch_stride",
      "        + dim_id * kNRows * params.delta_d_stride;",
      "    weight_t *A = reinterpret_cast<weight_t *>(params.A_ptr) + dim_id * kNRows * params.A_d_stride;",
      "    weight_t *B = reinterpret_cast<weight_t *>(params.B_ptr) + dim_id * kNRows * params.B_d_stride;",
      "    input_t *Bvar = reinterpret_cast<input_t *>(params.B_ptr) + sequence_start_index * params.B_batch_stride + group_id * params.B_group_stride;",
      "    weight_t *C = reinterpret_cast<weight_t *>(params.C_ptr) + dim_id * kNRows * params.C_d_stride;",
      "    input_t *Cvar = reinterpret_cast<input_t *>(params.C_ptr) + sequence_start_index * params.C_batch_stride + group_id * params.C_group_stride;",
      "    typename Ktraits::state_t *ssm_states = reinterpret_cast<typename Ktraits::state_t *>(params.ssm_states_ptr) + ",
      "    cache_index * params.ssm_states_batch_stride + ",
      "    dim_id * kNRows * params.ssm_states_dim_stride;",
      "    ",
      "    float D_val[kNRows] = {0};",
      "    if (params.D_ptr != nullptr) {",
      "        #pragma unroll",
      "        for (int r = 0; r < kNRows; ++r) {",
      "            D_val[r] = reinterpret_cast<float *>(params.D_ptr)[dim_id * kNRows + r];",
      "        }",
      "    }",
      "    float delta_bias[kNRows] = {0};",
      "    if (params.delta_bias_ptr != nullptr) {",
      "        #pragma unroll",
      "        for (int r = 0; r < kNRows; ++r) {",
      "            delta_bias[r] = reinterpret_cast<float *>(params.delta_bias_ptr)[dim_id * kNRows + r];",
      "        }",
      "    }",
      "",
      "",
      "    // for (int state_idx = threadIdx.x; state_idx < params.dstate; state_idx += blockDim.x) {",
      "    //     smem_a[state_idx] = A[state_idx * params.A_dstate_stride];",
      "    //     smem_bc[state_idx] = B[state_idx * params.B_dstate_stride] * C[state_idx * params.C_dstate_stride];",
      "    // }",
      "",
      "    constexpr int kChunkSize = kNThreads * kNItems;",
      "    const int n_chunks = (seqlen + 2048 - 1) / 2048;",
      "    for (int chunk = 0; chunk < n_chunks; ++chunk) {",
      "        input_t u_vals[kNRows][kNItems], delta_vals_load[kNRows][kNItems];",
      "",
      "        __syncthreads();",
      "        #pragma unroll",
      "        for (int r = 0; r < kNRows; ++r) {",
      "            if constexpr (!kDirectIO) {",
      "                if (r > 0) { __syncthreads(); }",
      "            }",
      "            load_input<Ktraits>(u + r * params.u_d_stride, u_vals[r], smem_load, seqlen - chunk * kChunkSize);",
      "            if constexpr (!kDirectIO) { __syncthreads(); }",
      "            load_input<Ktraits>(delta + r * params.delta_d_stride, delta_vals_load[r], smem_load, seqlen - chunk * kChunkSize);",
      "        }",
      "        u += kChunkSize;",
      "        delta += kChunkSize;",
      "    ",
      "        float delta_vals[kNRows][kNItems], delta_u_vals[kNRows][kNItems], out_vals[kNRows][kNItems];",
      "        #pragma unroll",
      "        for (int r = 0; r < kNRows; ++r) {",
      "            #pragma unroll",
      "            for (int i = 0; i < kNItems; ++i) {",
      "                float u_val = float(u_vals[r][i]);",
      "                delta_vals[r][i] = float(delta_vals_load[r][i]) + delta_bias[r];",
      "                if (params.delta_softplus) {",
      "                    delta_vals[r][i] = delta_vals[r][i] <= 20.f ? log1pf(expf(delta_vals[r][i])) : delta_vals[r][i];",
      "                }",
      "                delta_u_vals[r][i] = delta_vals[r][i] * u_val;",
      "                out_vals[r][i] = D_val[r] * u_val;",
      "            }",
      "        }",
      "",
      "        __syncthreads();",
      "        for (int state_idx = 0; state_idx < params.dstate; ++state_idx) {",
      "            weight_t A_val[kNRows];",
      "            #pragma unroll",
      "            for (int r = 0; r < kNRows; ++r) {",
      "                A_val[r] = A[state_idx * params.A_dstate_stride + r * params.A_d_stride];",
      "                // Multiply the real part of A with LOG2E so we can use exp2f instead of expf.",
      "                constexpr float kLog2e = M_LOG2E;",
      "                A_val[r] *= kLog2e;",
      "            }",
      "            // This variable holds B * C if both B and C are constant across seqlen. If only B varies",
      "            // across seqlen, this holds C. If only C varies across seqlen, this holds B.",
      "            // If both B and C vary, this is unused.",
      "            weight_t BC_val[kNRows];",
      "            weight_t B_vals[kNItems], C_vals[kNItems];",
      "            if constexpr (kIsVariableB) {",
      "                load_weight<Ktraits>(Bvar + state_idx * params.B_dstate_stride, B_vals,",
      "                    smem_load_weight, (seqlen - chunk * kChunkSize) * (1));",
      "                if constexpr (!kIsVariableC) {",
      "                    #pragma unroll",
      "                    for (int r = 0; r < kNRows; ++r) {",
      "                        BC_val[r] = C[state_idx * params.C_dstate_stride + r * params.C_d_stride];",
      "                    }",
      "                }",
      "            }",
      "            if constexpr (kIsVariableC) {",
      "                auto &smem_load_weight_C = !kIsVariableB ? smem_load_weight : smem_load_weight1;",
      "                load_weight<Ktraits>(Cvar + state_idx * params.C_dstate_stride, C_vals,",
      "                    smem_load_weight_C, (seqlen - chunk * kChunkSize) * (1 ));",
      "                if constexpr (!kIsVariableB) {",
      "                    #pragma unroll",
      "                    for (int r = 0; r < kNRows; ++r) {",
      "                        BC_val[r] = B[state_idx * params.B_dstate_stride + r * params.B_d_stride];",
      "                    }",
      "                }",
      "            }",
      "            if constexpr (!kIsVariableB && !kIsVariableC) {",
      "                #pragma unroll",
      "                for (int r = 0; r < kNRows; ++r) {",
      "                    BC_val[r] = B[state_idx * params.B_dstate_stride + r * params.B_d_stride] * C[state_idx * params.C_dstate_stride + r * params.C_d_stride];",
      "                }",
      "            }",
      "",
      "            #pragma unroll",
      "            for (int r = 0; r < kNRows; ++r) {",
      "                if (r > 0) { __syncthreads(); }  // Scan could be using the same smem",
      "                scan_t thread_data[kNItems];",
      "                #pragma unroll",
      "                for (int i = 0; i < kNItems; ++i) {",
      "                    thread_data[i] = make_float2(exp2f(delta_vals[r][i] * A_val[r]),",
      "                                                 !kIsVariableB ? delta_u_vals[r][i] : B_vals[i] * delta_u_vals[r][i]);",
      "                    ",
      "                    if (seqlen % (kNItems * kNThreads) != 0) {  // So that the last state is correct",
      "                        if (threadIdx.x * kNItems + i >= seqlen - chunk * kChunkSize) {",
      "                            thread_data[i] = make_float2(1.f, 0.f);",
      "                        }",
      "                    }",
      "                }",
      "                // Initialize running total",
      "",
      "                scan_t running_prefix = chunk > 0 ? smem_running_prefix[state_idx + r * MAX_DSTATE] : make_float2(1.0, has_initial_state ? float(ssm_states[state_idx * params.ssm_states_dstate_stride]): 0.0);",
      "",
      "                SSMScanPrefixCallbackOp<weight_t> prefix_op(running_prefix);",
      "                typename Ktraits::BlockScanT(smem_scan).InclusiveScan(",
      "                    thread_data, thread_data, SSMScanOp<weight_t>(), prefix_op",
      "                );",
      "                // There's a syncthreads in the scan op, so we don't need to sync here.",
      "                // Unless there's only 1 warp, but then it's the same thread (0) reading and writing.",
      "                if (threadIdx.x == 0) {",
      "                    smem_running_prefix[state_idx] = prefix_op.running_prefix;",
      "                    if (chunk == n_chunks - 1) {",
      "                        ssm_states[state_idx * params.ssm_states_dstate_stride] = typename Ktraits::state_t(prefix_op.running_prefix.y);",
      "                    }",
      "                }",
      "                #pragma unroll",
      "                for (int i = 0; i < kNItems; ++i) {",
      "                    const weight_t C_val = !kIsVariableC",
      "                        ? BC_val[r]",
      "                        : (!kIsVariableB ? BC_val[r] * C_vals[i] : C_vals[i]);",
      "                    out_vals[r][i] += thread_data[i].y * C_val;",
      "                }",
      "            }",
      "        }",
      "        ",
      "        input_t *out = reinterpret_cast<input_t *>(params.out_ptr) + sequence_start_index * params.out_batch_stride",
      "            + dim_id * kNRows * params.out_d_stride + chunk * kChunkSize;",
      "        __syncthreads();",
      "        #pragma unroll",
      "        for (int r = 0; r < kNRows; ++r) {",
      "            if constexpr (!kDirectIO) {",
      "                if (r > 0) { __syncthreads(); }",
      "            }",
      "            store_output<Ktraits>(out + r * params.out_d_stride, out_vals[r], smem_store, seqlen - chunk * kChunkSize);",
      "        }",
      "",
      "        if constexpr (kHasZ) {",
      "            input_t *z = reinterpret_cast<input_t *>(params.z_ptr) + sequence_start_index * params.z_batch_stride",
      "                + dim_id * kNRows * params.z_d_stride + chunk * kChunkSize;",
      "            input_t *out_z = reinterpret_cast<input_t *>(params.out_z_ptr) + sequence_start_index * params.out_z_batch_stride",
      "                + dim_id * kNRows * params.out_z_d_stride + chunk * kChunkSize;",
      "            #pragma unroll",
      "            for (int r = 0; r < kNRows; ++r) {",
      "                input_t z_vals[kNItems];",
      "                __syncthreads();",
      "                load_input<Ktraits>(z + r * params.z_d_stride, z_vals, smem_load, seqlen - chunk * kChunkSize);",
      "                #pragma unroll",
      "                for (int i = 0; i < kNItems; ++i) {",
      "                    float z_val = z_vals[i];",
      "                    out_vals[r][i] *= z_val / (1 + expf(-z_val));",
      "                }",
      "                __syncthreads();",
      "                store_output<Ktraits>(out_z + r * params.out_z_d_stride, out_vals[r], smem_store, seqlen - chunk * kChunkSize);",
      "            }",
      "        }",
      "",
      "        Bvar += kChunkSize * 1;",
      "        Cvar += kChunkSize * 1;",
      "    }",
      "}",
      "",
      "template<int kNThreads, int kNItems, typename input_t, typename weight_t, typename state_t>",
      "void selective_scan_fwd_launch(SSMParamsBase &params, cudaStream_t stream) {",
      "    // Only kNRows == 1 is tested for now, which ofc doesn't differ from previously when we had each block",
      "    // processing 1 row.",
      "    constexpr int kNRows = 1;",
      "    // kIsVariableB, kIsVariableC and kHasZ are all set to True to reduce binary size",
      "    constexpr bool kIsVariableB = true;",
      "    constexpr bool kIsVariableC = true;",
      "    BOOL_SWITCH(params.seqlen % (kNThreads * kNItems) == 0, kIsEvenLen, [&] {",
      "        BOOL_SWITCH(params.z_ptr != nullptr , kHasZ, [&] {",
      "            BOOL_SWITCH(params.query_start_loc_ptr != nullptr , kVarlen, [&] {",
      "                using Ktraits = Selective_Scan_fwd_kernel_traits<kNThreads, kNItems, kNRows, kIsEvenLen, kIsVariableB, kIsVariableC, kHasZ,  kVarlen, input_t, weight_t, state_t>;",
      "                constexpr int kSmemSize = Ktraits::kSmemSize + kNRows * MAX_DSTATE * sizeof(typename Ktraits::scan_t);",
      "                dim3 grid(params.batch, params.dim / kNRows);",
      "                auto kernel = &selective_scan_fwd_kernel<Ktraits>;",
      "                if (kSmemSize >= 48 * 1024) {",
      "#ifdef USE_ROCM",
      "                    C10_HIP_CHECK(hipFuncSetAttribute(",
      "                        reinterpret_cast<const void*>(kernel), hipFuncAttributeMaxDynamicSharedMemorySize, kSmemSize));",
      "#else",
      "                    C10_CUDA_CHECK(cudaFuncSetAttribute(",
      "                        kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, kSmemSize));",
      "#endif",
      "                }",
      "                kernel<<<grid, Ktraits::kNThreads, kSmemSize, stream>>>(params);",
      "                C10_CUDA_KERNEL_LAUNCH_CHECK();",
      "            });",
      "        });",
      "    });",
      "}",
      "",
      "template<typename input_t, typename weight_t, typename state_t>",
      "void selective_scan_fwd_cuda(SSMParamsBase &params, cudaStream_t stream) {",
      "",
      "    #ifndef USE_ROCM",
      "        if (params.seqlen <= 128) {           ",
      "            selective_scan_fwd_launch<32, 4, input_t, weight_t, state_t>(params, stream);",
      "        } else if (params.seqlen <= 256) {",
      "            selective_scan_fwd_launch<32, 8, input_t, weight_t, state_t>(params, stream);",
      "        } else if (params.seqlen <= 512) {",
      "            selective_scan_fwd_launch<32, 16, input_t, weight_t, state_t>(params, stream);",
      "        } else if (params.seqlen <= 1024) {",
      "            selective_scan_fwd_launch<64, 16, input_t, weight_t, state_t>(params, stream);",
      "        } else {",
      "            selective_scan_fwd_launch<128, 16, input_t, weight_t, state_t>(params, stream);",
      "        }",
      "    #else",
      "        if (params.seqlen <= 256) {",
      "            selective_scan_fwd_launch<64, 4, input_t, weight_t, state_t>(params, stream);",
      "        } else if (params.seqlen <= 512) {",
      "            selective_scan_fwd_launch<64, 8, input_t, weight_t, state_t>(params, stream);",
      "        } else if (params.seqlen <= 1024) {",
      "            selective_scan_fwd_launch<64, 16, input_t, weight_t, state_t>(params, stream);",
      "        } else {",
      "            selective_scan_fwd_launch<128, 16, input_t, weight_t, state_t>(params, stream);",
      "        }",
      "    #endif",
      "}",
      "",
      "template void selective_scan_fwd_cuda<at::BFloat16, float, at::BFloat16>(SSMParamsBase &params, cudaStream_t stream);",
      "template void selective_scan_fwd_cuda<at::BFloat16, float, float>(SSMParamsBase &params, cudaStream_t stream);",
      "template void selective_scan_fwd_cuda<at::Half, float, at::Half>(SSMParamsBase &params, cudaStream_t stream);",
      "template void selective_scan_fwd_cuda<at::Half, float, float>(SSMParamsBase &params, cudaStream_t stream);",
      "template void selective_scan_fwd_cuda<float, float, float>(SSMParamsBase &params, cudaStream_t stream);",
      "",
      "#define CHECK_SHAPE(x, ...) TORCH_CHECK(x.sizes() == torch::IntArrayRef({__VA_ARGS__}), #x \" must have shape (\" #__VA_ARGS__ \")\")",
      "",
      "#define DISPATCH_WTYPE_ITYPE_FLOAT_AND_HALF_AND_BF16(ITYPE, STYPE, NAME, ...)       \\",
      "    if (ITYPE == at::ScalarType::Half) {                                            \\",
      "        using input_t = at::Half;                                                   \\",
      "        using weight_t = float;                                                     \\",
      "        if (STYPE == at::ScalarType::Half) {                                        \\",
      "            using state_t = at::Half;                                               \\",
      "            __VA_ARGS__();                                                          \\",
      "        } else if (STYPE == at::ScalarType::Float) {                                \\",
      "            using state_t = float;                                                  \\",
      "            __VA_ARGS__();                                                          \\",
      "        } else {                                                                    \\",
      "            AT_ERROR(#NAME, \" not implemented for state type '\", toString(STYPE), \"'\"); \\",
      "        }                                                                           \\",
      "    } else if (ITYPE == at::ScalarType::BFloat16) {                                 \\",
      "        using input_t = at::BFloat16;                                               \\",
      "        using weight_t = float;                                                     \\",
      "        if (STYPE == at::ScalarType::BFloat16) {                                    \\",
      "            using state_t = at::BFloat16;                                           \\",
      "            __VA_ARGS__();                                                          \\",
      "        } else if (STYPE == at::ScalarType::Float) {                                \\",
      "            using state_t = float;                                                  \\",
      "            __VA_ARGS__();                                                          \\",
      "        } else {                                                                    \\",
      "            AT_ERROR(#NAME, \" not implemented for state type '\", toString(STYPE), \"'\"); \\",
      "        }                                                                           \\",
      "    } else if (ITYPE == at::ScalarType::Float)  {                                   \\",
      "        using input_t = float;                                                      \\",
      "        using weight_t = float;                                                     \\",
      "        using state_t = float;                                                      \\",
      "        __VA_ARGS__();                                                              \\",
      "    } else {                                                                        \\",
      "        AT_ERROR(#NAME, \" not implemented for input type '\", toString(ITYPE), \"'\"); \\",
      "    }",
      "",
      "",
      "template<typename input_t, typename weight_t, typename state_t>",
      "void selective_scan_fwd_cuda(SSMParamsBase &params, cudaStream_t stream);",
      "",
      "void set_ssm_params_fwd(SSMParamsBase &params,",
      "                        // sizes",
      "                        const size_t batch,",
      "                        const size_t dim,",
      "                        const size_t seqlen,",
      "                        const size_t dstate,",
      "                        const size_t n_groups,",
      "                        const bool is_variable_B,",
      "                        const bool is_variable_C,",
      "                        // device pointers",
      "                        const torch::Tensor u,",
      "                        const torch::Tensor delta,",
      "                        const torch::Tensor A,",
      "                        const torch::Tensor B,",
      "                        const torch::Tensor C,",
      "                        const torch::Tensor out,",
      "                        const torch::Tensor z,",
      "                        const torch::Tensor out_z,",
      "                        const std::optional<at::Tensor>& D,",
      "                        const std::optional<at::Tensor>& delta_bias,",
      "                        const torch::Tensor ssm_states,",
      "                        bool has_z, ",
      "                        bool delta_softplus,",
      "                        const std::optional<at::Tensor>& query_start_loc,",
      "                        const std::optional<at::Tensor>& cache_indices,",
      "                        const std::optional<at::Tensor>& has_initial_state,",
      "                        bool varlen,",
      "                        int64_t pad_slot_id) {",
      "",
      "    // Reset the parameters",
      "    memset(&params, 0, sizeof(params));",
      "",
      "    params.batch = batch;",
      "    params.dim = dim;",
      "    params.seqlen = seqlen;",
      "    params.dstate = dstate;",
      "    params.n_groups = n_groups;",
      "    params.dim_ngroups_ratio = dim / n_groups;",
      "    params.pad_slot_id = pad_slot_id;",
      "",
      "    params.delta_softplus = delta_softplus;",
      "",
      "    params.is_variable_B = is_variable_B;",
      "    params.is_variable_C = is_variable_C;",
      "",
      "    // Set the pointers and strides.",
      "    params.u_ptr = u.data_ptr();",
      "    params.delta_ptr = delta.data_ptr();",
      "    params.A_ptr = A.data_ptr();",
      "    params.B_ptr = B.data_ptr();",
      "    params.C_ptr = C.data_ptr();",
      "    params.D_ptr = D.has_value() ? D.value().data_ptr() : nullptr;",
      "    params.delta_bias_ptr = delta_bias.has_value() ? delta_bias.value().data_ptr() : nullptr;",
      "    params.out_ptr = out.data_ptr();",
      "    params.ssm_states_ptr = ssm_states.data_ptr();",
      "    params.z_ptr = has_z ? z.data_ptr() : nullptr;",
      "    params.out_z_ptr = has_z ? out_z.data_ptr() : nullptr;",
      "    params.query_start_loc_ptr = query_start_loc.has_value() ? query_start_loc.value().data_ptr() : nullptr;",
      "    params.cache_indices_ptr = cache_indices.has_value() ? cache_indices.value().data_ptr() : nullptr;",
      "    params.has_initial_state_ptr = has_initial_state.has_value() ? has_initial_state.value().data_ptr() : nullptr;",
      "",
      "",
      "    // All stride are in elements, not bytes.",
      "    params.A_d_stride = A.stride(0);",
      "    params.A_dstate_stride = A.stride(1);",
      "",
      "    if (varlen){",
      "        params.B_batch_stride = B.stride(2);",
      "        params.B_group_stride = B.stride(0);",
      "        params.B_dstate_stride = B.stride(1);",
      "        params.C_batch_stride = C.stride(2);",
      "        params.C_group_stride = C.stride(0);",
      "        params.C_dstate_stride = C.stride(1);",
      "",
      "        params.u_batch_stride = u.stride(1);",
      "        params.u_d_stride = u.stride(0);",
      "        params.delta_batch_stride = delta.stride(1);",
      "        params.delta_d_stride = delta.stride(0);",
      "        if (has_z) {",
      "            params.z_batch_stride = z.stride(1);",
      "            params.z_d_stride = z.stride(0);",
      "            params.out_z_batch_stride = out_z.stride(1);",
      "            params.out_z_d_stride = out_z.stride(0);",
      "        }",
      "        params.out_batch_stride = out.stride(1);",
      "        params.out_d_stride = out.stride(0);",
      "",
      "        params.ssm_states_batch_stride = ssm_states.stride(0);",
      "        params.ssm_states_dim_stride = ssm_states.stride(1);  ",
      "        params.ssm_states_dstate_stride = ssm_states.stride(2);",
      "",
      "    }",
      "    else{",
      "        if (!is_variable_B) {",
      "            params.B_d_stride = B.stride(0);",
      "        } else {",
      "            params.B_batch_stride = B.stride(0);",
      "            params.B_group_stride = B.stride(1);",
      "        }",
      "        params.B_dstate_stride = !is_variable_B ? B.stride(1) : B.stride(2);",
      "        if (!is_variable_C) {",
      "            params.C_d_stride = C.stride(0);",
      "        } else {",
      "            params.C_batch_stride = C.stride(0);",
      "            params.C_group_stride = C.stride(1);",
      "        }",
      "        params.C_dstate_stride = !is_variable_C ? C.stride(1) : C.stride(2);",
      "        params.u_batch_stride = u.stride(0);",
      "        params.u_d_stride = u.stride(1);",
      "        params.delta_batch_stride = delta.stride(0);",
      "        params.delta_d_stride = delta.stride(1);",
      "        if (has_z) {",
      "            params.z_batch_stride = z.stride(0);",
      "            params.z_d_stride = z.stride(1);",
      "            params.out_z_batch_stride = out_z.stride(0);",
      "            params.out_z_d_stride = out_z.stride(1);",
      "        }",
      "        params.out_batch_stride = out.stride(0);",
      "        params.out_d_stride = out.stride(1);",
      "        ",
      "        params.ssm_states_batch_stride = ssm_states.stride(0);",
      "        params.ssm_states_dim_stride = ssm_states.stride(1);  ",
      "        params.ssm_states_dstate_stride = ssm_states.stride(2);",
      "    }",
      "}",
      "",
      "void selective_scan_fwd(const torch::Tensor &u, const torch::Tensor &delta,",
      "                  const torch::Tensor &A, const torch::Tensor &B, const torch::Tensor &C,",
      "                  const std::optional<torch::Tensor> &D_,",
      "                  const std::optional<torch::Tensor> &z_,",
      "                  const std::optional<torch::Tensor> &delta_bias_,",
      "                  bool delta_softplus,",
      "                  const std::optional<torch::Tensor> &query_start_loc,",
      "                  const std::optional<torch::Tensor> &cache_indices,",
      "                  const std::optional<torch::Tensor> &has_initial_state,",
      "                  const torch::Tensor &ssm_states,",
      "                  // used to identify padding entries if cache_indices provided",
      "                  // in case of padding, the kernel will return early",
      "                  int64_t pad_slot_id) {",
      "    auto input_type = u.scalar_type();",
      "    auto weight_type = A.scalar_type();",
      "    TORCH_CHECK(input_type == at::ScalarType::Float || input_type == at::ScalarType::Half || input_type == at::ScalarType::BFloat16);",
      "    TORCH_CHECK(weight_type == at::ScalarType::Float);",
      "",
      "    const bool is_variable_B = B.dim() >= 3;",
      "    const bool is_variable_C = C.dim() >= 3;",
      "",
      "    TORCH_CHECK(delta.scalar_type() == input_type);",
      "    TORCH_CHECK(B.scalar_type() == (!is_variable_B ? weight_type : input_type));",
      "    TORCH_CHECK(C.scalar_type() == (!is_variable_C ? weight_type : input_type));",
      "",
      "    TORCH_CHECK(u.is_cuda());",
      "    TORCH_CHECK(delta.is_cuda());",
      "    TORCH_CHECK(A.is_cuda());",
      "    TORCH_CHECK(B.is_cuda());",
      "    TORCH_CHECK(C.is_cuda());",
      "",
      "    TORCH_CHECK(u.stride(-1) == 1 || u.size(-1) == 1);",
      "    TORCH_CHECK(delta.stride(-1) == 1 || delta.size(-1) == 1);",
      "",
      "    const auto sizes = u.sizes();",
      "    const bool varlen = query_start_loc.has_value();",
      "    const int batch_size = varlen ? query_start_loc.value().sizes()[0] - 1 : sizes[0];",
      "    const int dim = varlen ? sizes[0] : sizes[1];",
      "    const int seqlen = varlen ? sizes[1] : sizes[2];",
      "    const int dstate = A.size(1);",
      "    const int n_groups = varlen ? B.size(0) : B.size(1);",
      "",
      "    TORCH_CHECK(dstate <= 256, \"selective_scan only supports state dimension <= 256\");",
      "",
      "    if (varlen) {",
      "        CHECK_SHAPE(u, dim, seqlen);",
      "        CHECK_SHAPE(delta, dim, seqlen);",
      "    } else {",
      "        CHECK_SHAPE(u, batch_size, dim, seqlen);",
      "        CHECK_SHAPE(delta, batch_size, dim, seqlen);",
      "    }",
      "    CHECK_SHAPE(A, dim, dstate);",
      "    TORCH_CHECK(is_variable_B, \"is_variable_B = False is disabled in favor of reduced binary size\")",
      "    if (varlen) {",
      "        CHECK_SHAPE(B, n_groups, dstate, seqlen);",
      "    } else {",
      "        CHECK_SHAPE(B, batch_size, n_groups, dstate, seqlen); ",
      "    }",
      "    TORCH_CHECK(B.stride(-1) == 1 || B.size(-1) == 1);",
      "",
      "    TORCH_CHECK(is_variable_C, \"is_variable_C = False is disabled in favor of reduced binary size\")",
      "    if (varlen) {",
      "        CHECK_SHAPE(C, n_groups, dstate, seqlen);",
      "    } else {",
      "        CHECK_SHAPE(C, batch_size, n_groups, dstate, seqlen); ",
      "    }",
      "    TORCH_CHECK(C.stride(-1) == 1 || C.size(-1) == 1);",
      "",
      "    if (D_.has_value()) {",
      "        auto D = D_.value();",
      "        TORCH_CHECK(D.scalar_type() == at::ScalarType::Float);",
      "        TORCH_CHECK(D.is_cuda());",
      "        TORCH_CHECK(D.stride(-1) == 1 || D.size(-1) == 1);",
      "        CHECK_SHAPE(D, dim);",
      "    }",
      "",
      "    if (delta_bias_.has_value()) {",
      "        auto delta_bias = delta_bias_.value();",
      "        TORCH_CHECK(delta_bias.scalar_type() == at::ScalarType::Float);",
      "        TORCH_CHECK(delta_bias.is_cuda());",
      "        TORCH_CHECK(delta_bias.stride(-1) == 1 || delta_bias.size(-1) == 1);",
      "        CHECK_SHAPE(delta_bias, dim);",
      "    }",
      "",
      "",
      "    if (has_initial_state.has_value()) {",
      "        auto has_initial_state_ = has_initial_state.value();",
      "        TORCH_CHECK(has_initial_state_.scalar_type() == at::ScalarType::Bool);",
      "        TORCH_CHECK(has_initial_state_.is_cuda());",
      "        CHECK_SHAPE(has_initial_state_, batch_size);",
      "    }",
      "",
      "",
      "    if (query_start_loc.has_value()) {",
      "        auto query_start_loc_ = query_start_loc.value();",
      "        TORCH_CHECK(query_start_loc_.scalar_type() == at::ScalarType::Int);",
      "        TORCH_CHECK(query_start_loc_.is_cuda());",
      "    }",
      "",
      "",
      "    if (cache_indices.has_value()) {",
      "        auto cache_indices_ = cache_indices.value();",
      "        TORCH_CHECK(cache_indices_.scalar_type() == at::ScalarType::Int);",
      "        TORCH_CHECK(cache_indices_.is_cuda());",
      "        CHECK_SHAPE(cache_indices_, batch_size);",
      "    }",
      "   ",
      "",
      "    at::Tensor z, out_z;",
      "    const bool has_z = z_.has_value();",
      "    if (has_z) {",
      "        z = z_.value();",
      "        TORCH_CHECK(z.scalar_type() == input_type);",
      "        TORCH_CHECK(z.is_cuda());",
      "        TORCH_CHECK(z.stride(-1) == 1 || z.size(-1) == 1);",
      "        if (varlen){",
      "            CHECK_SHAPE(z, dim, seqlen);",
      "        } else {",
      "            CHECK_SHAPE(z, batch_size, dim, seqlen);",
      "        }",
      "        ",
      "        out_z = z;",
      "    }",
      "",
      "    // Right now u has BHL layout and delta has HBL layout, and we want out to have HBL layout",
      "    at::Tensor out = delta;",
      "    // ssm_states can now be either the same as input_type or float32",
      "    auto state_type = ssm_states.scalar_type();",
      "    TORCH_CHECK(state_type == input_type || state_type == at::ScalarType::Float);",
      "    TORCH_CHECK(ssm_states.is_cuda());",
      "    TORCH_CHECK(ssm_states.stride(-1) == 1);",
      "",
      "    SSMParamsBase params;",
      "    set_ssm_params_fwd(params, batch_size, dim, seqlen, dstate, n_groups, is_variable_B, is_variable_C,",
      "                       u, delta, A, B, C, out, z, out_z,",
      "                       D_,",
      "                       delta_bias_,",
      "                       ssm_states,",
      "                       has_z,",
      "                       delta_softplus,",
      "                       query_start_loc,",
      "                       cache_indices,",
      "                       has_initial_state,",
      "                       varlen,",
      "                       pad_slot_id",
      "                       );",
      "",
      "    ",
      "    const at::cuda::OptionalCUDAGuard device_guard(device_of(u));",
      "    auto stream = at::cuda::getCurrentCUDAStream().stream();",
      "    DISPATCH_WTYPE_ITYPE_FLOAT_AND_HALF_AND_BF16(u.scalar_type(), ssm_states.scalar_type(), \"selective_scan_fwd\", [&] {",
      "        selective_scan_fwd_cuda<input_t, weight_t, state_t>(params, stream);",
      "    });",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quickreduce/quick_reduce_impl.cuh",
    "source": [
      "#pragma once",
      "",
      "#include <hip/hip_runtime.h>",
      "#include \"base.h\"",
      "",
      "namespace quickreduce {",
      "",
      "struct CodecBase {",
      "  const int thread;",
      "  const int rank;",
      "  const int group_leader;",
      "  __quickreduce_device_inline__ CodecBase(int thread, int rank)",
      "      : thread(thread),",
      "        rank(rank),",
      "        group_leader((threadIdx.x / kThreadGroupSize) * kThreadGroupSize) {",
      "    set_fp16_ovfl(true);",
      "  }",
      "};",
      "",
      "// Default full precision codec.",
      "template <typename T, int world_size>",
      "struct CodecFP : public CodecBase {",
      "  static constexpr int kWorldSize = world_size;",
      "  static constexpr int kRankAtoms = kAtoms / kWorldSize;",
      "",
      "  // Codec tile size process by this workgroup.",
      "  // Each thread processes atoms of f16x8_t (16B).",
      "  static constexpr int kRankTransmittedTileSize =",
      "      kBlockSize * kRankAtoms * sizeof(int32x4_t);",
      "  static_assert(kRankTransmittedTileSize % 16 == 0,",
      "                \"kRankTransmittedTileSize must be 16B aligned.\");",
      "",
      "  // Total tile size for the collective communication.",
      "  static constexpr int kTransmittedTileSize =",
      "      kRankTransmittedTileSize * kWorldSize;",
      "",
      "  __quickreduce_device_inline__ CodecFP(int thread, int rank)",
      "      : CodecBase(thread, rank) {}",
      "",
      "  __quickreduce_device_inline__ void send(int32x4_t* __restrict__ send_buffer,",
      "                                          const int32x4_t* __restrict__ data) {",
      "    for (int i = 0; i < kRankAtoms; i++) {",
      "      __builtin_nontemporal_store(data[i], send_buffer + thread);",
      "      send_buffer += kAtomStride;",
      "    }",
      "  }",
      "",
      "  __quickreduce_device_inline__ void recv(int32x4_t** __restrict__ recv_buffer,",
      "                                          int32x4_t* __restrict__ data) {",
      "    for (int i = 0; i < kRankAtoms; i++) {",
      "      data[i] = __builtin_nontemporal_load(*recv_buffer + thread);",
      "      *recv_buffer += kAtomStride;",
      "    }",
      "  }",
      "};",
      "",
      "// Int4 symmetric quantization codec.",
      "// We quantize the FP16 data to block-scaled Int4 in blocks of 4 *",
      "// kThreadGroupSize.",
      "template <typename T, int world_size>",
      "struct CodecQ4 : public CodecBase {",
      "  static constexpr int kWorldSize = world_size;",
      "",
      "  // Codec tile size process by this workgroup.",
      "  // Each threads processes a fragment of fp16x8_t (16B),",
      "  // into a int4x8_t (4B) and a fp16 scale shared among 32 values.",
      "  static constexpr int kRankAtoms = kAtoms / kWorldSize;",
      "  static constexpr int kRankTileStride = 1152;",
      "  static constexpr int kRankTileScaleOffset = 1024;",
      "  static constexpr int kRankTransmittedTileSize = kRankTileStride * kRankAtoms;",
      "  static_assert(kRankTransmittedTileSize % 16 == 0,",
      "                \"kRankTransmittedTileSize must be 16B aligned.\");",
      "",
      "  static constexpr int kRankBufferTileStride =",
      "      kRankTileStride / sizeof(int32x4_t);",
      "",
      "  // Total tile size for the collective communication.",
      "  static constexpr int kTransmittedTileSize =",
      "      kRankTransmittedTileSize * kWorldSize;",
      "",
      "  // Constants configuration",
      "",
      "  // {-1/8.0h, -1/8.0h}, f16x2_t",
      "  static constexpr int kScaleFactor =",
      "      std::is_same<T, half>::value ? 0xB000B000 : 0xBE00BE00;",
      "",
      "  // {1e-7, 1e-7}, f16x2_t",
      "  static constexpr int kScaleEpsilon =",
      "      std::is_same<T, half>::value ? 0x00010001 : 0x33D733D7;",
      "",
      "  // {-8, -8}, f16x2_t",
      "  static constexpr int kRangeMin =",
      "      std::is_same<T, half>::value ? 0xC800C800 : 0xC100C100;",
      "",
      "  // {+7, +7}, f16x2_t",
      "  static constexpr int kRangeMax =",
      "      std::is_same<T, half>::value ? 0x47004700 : 0x40E040E0;",
      "",
      "  // {+8, +8}, int16x2_t",
      "  static constexpr int kRangeBias = 0x00080008;",
      "",
      "  __quickreduce_device_inline__ CodecQ4(int thread, int rank)",
      "      : CodecBase(thread, rank) {}",
      "",
      "  __quickreduce_device_inline__ void send(int32x4_t* __restrict__ send_buffer,",
      "                                          const int32x4_t* __restrict__ data) {",
      "    for (int k = 0; k < kRankAtoms; k++) {",
      "      int32x4_t const atom = data[k];",
      "",
      "      // Compute the absolute maximum of the atom in the thread group",
      "      // In 2 blocks of values, upper/lower halves of the f16x2_t",
      "      int wblockmax = group_abs_max<T>(atom);",
      "",
      "      // Derive scales",
      "      int decoding_scale;",
      "      int encoding_scale;",
      "      decoding_scale = packed_mul<T>(wblockmax, kScaleFactor);",
      "      encoding_scale = packed_add<T>(decoding_scale, kScaleEpsilon);",
      "      encoding_scale = packed_rcp<T>(encoding_scale);",
      "",
      "      // Apply scales to get quantized values",
      "      int32x4_t w;",
      "      for (int i = 0; i < 4; i++) {",
      "        w[i] = packed_mul<T>(atom[i], encoding_scale);",
      "        w[i] = packed_max<T>(w[i], kRangeMin);",
      "        w[i] = packed_min<T>(w[i], kRangeMax);",
      "      }",
      "",
      "      // Convert from f16x2_t to uint16x2_t",
      "      int32x4_t q;",
      "      {",
      "        int16_t* qi = reinterpret_cast<int16_t*>(&q);",
      "        T* wh = reinterpret_cast<T*>(&w);",
      "        for (int i = 0; i < 8; i++) qi[i] = (int16_t)rintf(T2float_cast(wh[i]));",
      "",
      "        for (int i = 0; i < 4; i++) {",
      "          q[i] = packed_add<int16_t>(q[i], kRangeBias);",
      "        }",
      "      }",
      "",
      "      // Pack 8 x q4 into int32_t",
      "      int qw = q[0] | (q[1] << 4) | (q[2] << 8) | (q[3] << 12);",
      "",
      "      // Write quantized atom to send_buffer",
      "      // note: only the group leader stores the scale",
      "      uint8_t* atom_ptr =",
      "          reinterpret_cast<uint8_t*>(send_buffer + k * kRankBufferTileStride);",
      "      int32_t* qw_ptr = reinterpret_cast<int32_t*>(atom_ptr) + thread;",
      "      int* qs_ptr = reinterpret_cast<int*>(atom_ptr + kRankTileScaleOffset) +",
      "                    (thread / 8);",
      "",
      "      __builtin_nontemporal_store(qw, qw_ptr);",
      "      if (threadIdx.x == group_leader) {",
      "        __builtin_nontemporal_store(decoding_scale, qs_ptr);",
      "      }",
      "    }",
      "  }",
      "",
      "  __quickreduce_device_inline__ void recv(int32x4_t** __restrict__ recv_buffer,",
      "                                          int32x4_t* __restrict__ data) {",
      "    for (int k = 0; k < kRankAtoms; k++) {",
      "      // Directly read quantized atom from recv_buffer",
      "      uint8_t* atom_ptr = reinterpret_cast<uint8_t*>(*recv_buffer);",
      "      int32_t* qw_ptr = reinterpret_cast<int32_t*>(atom_ptr) + thread;",
      "      int* qs_ptr = reinterpret_cast<int*>(atom_ptr + kRankTileScaleOffset) +",
      "                    (thread / 8);",
      "",
      "      int32_t qw = __builtin_nontemporal_load(qw_ptr);",
      "      int qs = __builtin_nontemporal_load(qs_ptr);",
      "",
      "      *recv_buffer += kRankBufferTileStride;",
      "",
      "      // Unpack q4 into f16x8_t",
      "      int32x4_t w;",
      "      {",
      "        static constexpr uint kMask000F = 0x000F000F;",
      "        static constexpr uint kHalf2_1024 =",
      "            0x64006400;  // {1024.0, 1024.0}, fp16x2_t",
      "        static uint constexpr kHalf2_1032 =",
      "            0xE408E408;  // {-1032.0, -1032.0}, fp16x2_t",
      "",
      "        for (int i = 0; i < 4; i++) {",
      "          if constexpr (std::is_same<T, half>::value) {",
      "            int32_t q4 = ((qw >> (i * 4)) & kMask000F) | kHalf2_1024;",
      "            w[i] = packed_add<half>(q4, kHalf2_1032);",
      "          } else {",
      "            int32_t int16_2 = (qw >> (i * 4)) & kMask000F;",
      "            int16_t low = static_cast<int16_t>(int16_2 & 0xFFFF);",
      "            int16_t high = static_cast<int16_t>((int16_2 >> 16) & 0xFFFF);",
      "            nv_bfloat16 bf_low = __float2bfloat16(static_cast<float>(low));",
      "            nv_bfloat16 bf_high = __float2bfloat16(static_cast<float>(high));",
      "            nv_bfloat162 bf2 = __halves2bfloat162(bf_low, bf_high);",
      "            int32_t packed_bf16 = *reinterpret_cast<int32_t*>(&bf2);",
      "            w[i] = packed_add<nv_bfloat16>(packed_bf16, kRangeMin);",
      "          }",
      "        }",
      "      }",
      "",
      "      // Apply decoding scales",
      "      for (int i = 0; i < 4; i++) {",
      "        w[i] = packed_mul<T>(w[i], qs);",
      "      }",
      "",
      "      data[k] = w;",
      "    }",
      "  }",
      "};",
      "",
      "// Int6 symmetric quantization codec.",
      "// We quantize the FP16 data to block-scaled Int6 in blocks of 4 *",
      "// kThreadGroupSize.",
      "template <typename T, int world_size>",
      "struct CodecQ6 : public CodecBase {",
      "  static constexpr int kWorldSize = world_size;",
      "",
      "  // Codec tile size process by this workgroup.",
      "  // Each threads processes a fragment of fp16x8_t (16B),",
      "  // into a int6x8_t (4B + 2B) and a fp16 scale shared among 32 values.",
      "  static constexpr int kRankAtoms = kAtoms / kWorldSize;",
      "  static constexpr int kRankTileStride = 1664;",
      "  static constexpr int kRankTileQ2Offset = 1024;",
      "  static constexpr int kRankTileScaleOffset = 1536;",
      "  static constexpr int kRankTransmittedTileSize = kRankTileStride * kRankAtoms;",
      "  static_assert(kRankTransmittedTileSize % 16 == 0,",
      "                \"kRankTransmittedTileSize must be 16B aligned.\");",
      "",
      "  static constexpr int kRankBufferTileStride =",
      "      kRankTileStride / sizeof(int32x4_t);",
      "",
      "  // Total tile size for the collective communication.",
      "  static constexpr int kTransmittedTileSize =",
      "      kRankTransmittedTileSize * kWorldSize;",
      "",
      "  // Constants configuration",
      "",
      "  // {-1/32.0h, -1/32.0h}, fp16x2_t",
      "  static constexpr int kScaleFactor =",
      "      std::is_same<T, half>::value ? 0xA800A800 : 0xBD00BD00;",
      "",
      "  // {1e-7, 1e-7}, fp16x2_t",
      "  static constexpr int kScaleEpsilon =",
      "      std::is_same<T, half>::value ? 0x00010001 : 0x33D733D7;",
      "",
      "  // {-32, -32}, fp16x2_t",
      "  static constexpr int kRangeMin =",
      "      std::is_same<T, half>::value ? 0xD000D000 : 0xC200C200;",
      "",
      "  // {+31, +31}, fp16x2_t",
      "  static constexpr int kRangeMax =",
      "      std::is_same<T, half>::value ? 0x4FC04FC0 : 0x41F841F8;",
      "",
      "  // {+32, +32}, int16x2_t",
      "  static constexpr int kRangeBias = 0x00200020;",
      "",
      "  __quickreduce_device_inline__ CodecQ6(int thread, int rank)",
      "      : CodecBase(thread, rank) {}",
      "",
      "  __quickreduce_device_inline__ void send(int32x4_t* __restrict__ send_buffer,",
      "                                          const int32x4_t* __restrict__ data) {",
      "    for (int k = 0; k < kRankAtoms; k++) {",
      "      int32x4_t const atom = data[k];",
      "",
      "      // Compute the absolute maximum of the atom in the thread group",
      "      // In 2 blocks of values, upper/lower halves of the f16x2_t",
      "      int wblockmax = group_abs_max<T>(atom);",
      "",
      "      // Derive scales",
      "      int decoding_scale;",
      "      int encoding_scale;",
      "      decoding_scale = packed_mul<T>(wblockmax, kScaleFactor);",
      "      encoding_scale = packed_add<T>(decoding_scale, kScaleEpsilon);",
      "      encoding_scale = packed_rcp<T>(encoding_scale);",
      "",
      "      // Apply scales to get quantized values",
      "      int32x4_t w;",
      "      for (int i = 0; i < 4; i++) {",
      "        w[i] = packed_mul<T>(atom[i], encoding_scale);",
      "        w[i] = packed_max<T>(w[i], kRangeMin);",
      "        w[i] = packed_min<T>(w[i], kRangeMax);",
      "      }",
      "",
      "      // Convert from f16x2_t to uint16x2_t",
      "      int32x4_t q;",
      "      {",
      "        int16_t* qi = reinterpret_cast<int16_t*>(&q);",
      "        T* wh = reinterpret_cast<T*>(&w);",
      "        for (int i = 0; i < 8; i++) qi[i] = (int16_t)rintf(T2float_cast(wh[i]));",
      "",
      "        for (int i = 0; i < 4; i++) {",
      "          q[i] = packed_add<int16_t>(q[i], kRangeBias);",
      "        }",
      "      }",
      "",
      "      // Pack 8 x q6 into int32_t + int16_t",
      "      uint32_t q4w;",
      "      uint16_t q2w = 0;",
      "      q4w = (q[0] & 0x000F000F) | ((q[1] & 0x000F000F) << 4) |",
      "            ((q[2] & 0x000F000F) << 8) | ((q[3] & 0x000F000F) << 12);",
      "      {",
      "        int16_t* tw = reinterpret_cast<int16_t*>(&q);",
      "#pragma unroll",
      "        for (int i = 0; i < 8; i++) {",
      "          q2w |= (tw[i] >> 4) << (i * 2);",
      "        }",
      "      }",
      "      // Write quantized atom to send_buffer",
      "      // note: only the group leader stores the scale",
      "      uint8_t* atom_ptr =",
      "          reinterpret_cast<uint8_t*>(send_buffer + k * kRankBufferTileStride);",
      "      uint32_t* q4w_ptr = reinterpret_cast<uint32_t*>(atom_ptr) + thread;",
      "      uint16_t* q2w_ptr =",
      "          reinterpret_cast<uint16_t*>(atom_ptr + kRankTileQ2Offset) + thread;",
      "      int* qs_ptr = reinterpret_cast<int*>(atom_ptr + kRankTileScaleOffset) +",
      "                    (thread / 8);",
      "",
      "      __builtin_nontemporal_store(q4w, q4w_ptr);",
      "      __builtin_nontemporal_store(q2w, q2w_ptr);",
      "      if (threadIdx.x == group_leader) {",
      "        __builtin_nontemporal_store(decoding_scale, qs_ptr);",
      "      }",
      "    }",
      "  }",
      "",
      "  __quickreduce_device_inline__ void recv(int32x4_t** __restrict__ recv_buffer,",
      "                                          int32x4_t* __restrict__ data) {",
      "    for (int k = 0; k < kRankAtoms; k++) {",
      "      // Directly read quantized atom from recv_buffer",
      "      uint8_t* atom_ptr = reinterpret_cast<uint8_t*>(*recv_buffer);",
      "      uint32_t* q4w_ptr = reinterpret_cast<uint32_t*>(atom_ptr) + thread;",
      "      uint16_t* q2w_ptr =",
      "          reinterpret_cast<uint16_t*>(atom_ptr + kRankTileQ2Offset) + thread;",
      "      int* qs_ptr = reinterpret_cast<int*>(atom_ptr + kRankTileScaleOffset) +",
      "                    (thread / 8);",
      "",
      "      uint32_t q4w = __builtin_nontemporal_load(q4w_ptr);",
      "      uint16_t q2w = __builtin_nontemporal_load(q2w_ptr);",
      "      int qs = __builtin_nontemporal_load(qs_ptr);",
      "",
      "      *recv_buffer += kRankBufferTileStride;",
      "",
      "      // Unpack q6 into fp16x8_t",
      "      int32x4_t w;",
      "      {",
      "        static uint constexpr kMask000F = 0x000F000F;",
      "        static uint constexpr kHalf2_1024 =",
      "            0x64006400;  // {1024.0, 1024.0}, fp16x2_t",
      "        static uint constexpr kHalf2_1056 =",
      "            0xE420E420;  // {-1056.0, -1056.0}, fp16x2_t",
      "",
      "#pragma unroll",
      "        for (int i = 0; i < 4; i++) {",
      "          int32_t q4 = q4w & kMask000F;",
      "          int32_t q2 = (q2w & 0x3) | ((q2w & 0xC) << 14);",
      "          q4w >>= 4;",
      "          q2w >>= 4;",
      "          if constexpr (std::is_same<T, half>::value) {",
      "            int32_t q6 = q4 | (q2 << 4) | kHalf2_1024;",
      "            asm volatile(\"v_pk_add_f16 %0, %1, %2\"",
      "                         : \"=v\"(w[i])",
      "                         : \"v\"(q6), \"v\"(kHalf2_1056));",
      "          } else {",
      "            int32_t int16_2 = q4 | (q2 << 4);",
      "            int16_t low = static_cast<int16_t>(int16_2 & 0xFFFF);",
      "            int16_t high = static_cast<int16_t>((int16_2 >> 16) & 0xFFFF);",
      "",
      "            nv_bfloat16 bf_low = __float2bfloat16(static_cast<float>(low));",
      "            nv_bfloat16 bf_high = __float2bfloat16(static_cast<float>(high));",
      "            nv_bfloat162 bf2 = __halves2bfloat162(bf_low, bf_high);",
      "            int32_t packed_bf16 = *reinterpret_cast<int32_t*>(&bf2);",
      "            w[i] = packed_add<nv_bfloat16>(packed_bf16, kRangeMin);",
      "          }",
      "        }",
      "      }",
      "",
      "      // Apply decoding scales",
      "      for (int i = 0; i < 4; i++) {",
      "        w[i] = packed_mul<T>(w[i], qs);",
      "      }",
      "",
      "      // That's pretty much it...",
      "      data[k] = w;",
      "    }",
      "  }",
      "};",
      "",
      "// Int8 symmetric quantization codec.",
      "// We quantize the FP16 data to block-scaled Int8 in blocks of 4 *",
      "// kThreadGroupSize.",
      "template <typename T, int world_size>",
      "struct CodecQ8 : public CodecBase {",
      "  static constexpr int kWorldSize = world_size;",
      "",
      "  // Codec tile size process by this workgroup.",
      "  // Each threads processes a fragment of f16x8_t (16B),",
      "  // into a int8x8_t (8B) and a f16 scale shared among 32 values.",
      "  static constexpr int kRankAtoms = kAtoms / kWorldSize;",
      "  static constexpr int kRankTileStride = 2176;",
      "  static constexpr int kRankTileScaleOffset = 2048;",
      "  static constexpr int kRankTransmittedTileSize = kRankTileStride * kRankAtoms;",
      "  static_assert(kRankTransmittedTileSize % 16 == 0,",
      "                \"kRankTileSize must be 16B aligned.\");",
      "",
      "  static constexpr int kRankBufferTileStride =",
      "      kRankTileStride / sizeof(int32x4_t);",
      "",
      "  // Total tile size for the collective communication.",
      "  static constexpr int kTransmittedTileSize =",
      "      kRankTransmittedTileSize * kWorldSize;",
      "",
      "  // Constants configuration",
      "",
      "  // {-1/128.0h, -1/128.0h}, f16x2_t",
      "  static constexpr int kScaleFactor =",
      "      std::is_same<T, half>::value ? 0xA000A000 : 0xBC00BC00;",
      "",
      "  // {1e-7, 1e-7}, f16x2_t",
      "  static constexpr int kScaleEpsilon =",
      "      std::is_same<T, half>::value ? 0x00010001 : 0x33D733D7;",
      "",
      "  // {-128, -128}, f16x2_t",
      "  static constexpr int kRangeMin =",
      "      std::is_same<T, half>::value ? 0xD800D800 : 0xC300C300;",
      "  // {+127, +127}, f16x2_t",
      "  static constexpr int kRangeMax =",
      "      std::is_same<T, half>::value ? 0x57F057F0 : 0x42FE42FE;",
      "",
      "  // {+128, +128}, int16x2_t",
      "  static constexpr int kRangeBias = 0x00800080;",
      "",
      "  __quickreduce_device_inline__ CodecQ8(int thread, int rank)",
      "      : CodecBase(thread, rank) {}",
      "",
      "  __quickreduce_device_inline__ void send(int32x4_t* __restrict__ send_buffer,",
      "                                          int32x4_t const* __restrict__ data) {",
      "    for (int k = 0; k < kRankAtoms; k++) {",
      "      int32x4_t const atom = data[k];",
      "      // Compute the absolute maximum of the atom in the thread group",
      "      // In 2 blocks of values, upper/lower halves of the f16x2_t",
      "      int wblockmax = group_abs_max<T>(atom);",
      "",
      "      // Derive scales",
      "      int decoding_scale;",
      "      int encoding_scale;",
      "      decoding_scale = packed_mul<T>(wblockmax, kScaleFactor);",
      "      encoding_scale = packed_add<T>(decoding_scale, kScaleEpsilon);",
      "      encoding_scale = packed_rcp<T>(encoding_scale);",
      "",
      "      // Apply scales to get quantized values",
      "      int32x4_t w;",
      "      for (int i = 0; i < 4; i++) {",
      "        w[i] = packed_mul<T>(atom[i], encoding_scale);",
      "        w[i] = packed_max<T>(w[i], kRangeMin);",
      "        w[i] = packed_min<T>(w[i], kRangeMax);",
      "      }",
      "",
      "      // Convert from f16x2_t to uint16x2_t",
      "      int32x4_t q;",
      "      {",
      "        int16_t* qi = reinterpret_cast<int16_t*>(&q);",
      "        T* wh = reinterpret_cast<T*>(&w);",
      "        for (int i = 0; i < 8; i++) qi[i] = (int16_t)rintf(T2float_cast(wh[i]));",
      "",
      "        for (int i = 0; i < 4; i++) {",
      "          q[i] = packed_add<int16_t>(q[i], kRangeBias);",
      "        }",
      "      }",
      "",
      "      // Pack 8 x q8 into int32x2_t",
      "      int32x2_t qw;",
      "      qw[0] = q[0] | (q[1] << 8);",
      "      qw[1] = q[2] | (q[3] << 8);",
      "",
      "      // Write quantized atom to send_buffer",
      "      // note: only the group leader stores the scale",
      "      uint8_t* atom_ptr =",
      "          reinterpret_cast<uint8_t*>(send_buffer + k * kRankBufferTileStride);",
      "      int32x2_t* qw_ptr = reinterpret_cast<int32x2_t*>(atom_ptr) + thread;",
      "      int* qs_ptr = reinterpret_cast<int*>(atom_ptr + kRankTileScaleOffset) +",
      "                    (thread / 8);",
      "",
      "      __builtin_nontemporal_store(qw, qw_ptr);",
      "      if (threadIdx.x == group_leader) {",
      "        __builtin_nontemporal_store(decoding_scale, qs_ptr);",
      "      }",
      "    }",
      "  }",
      "",
      "  __quickreduce_device_inline__ void recv(int32x4_t** __restrict__ recv_buffer,",
      "                                          int32x4_t* __restrict__ data) {",
      "    for (int k = 0; k < kRankAtoms; k++) {",
      "      // Directly read quantized atom from recv_buffer",
      "      uint8_t* atom_ptr = reinterpret_cast<uint8_t*>(*recv_buffer);",
      "      int32x2_t* qw_ptr = reinterpret_cast<int32x2_t*>(atom_ptr) + thread;",
      "      int* qs_ptr = reinterpret_cast<int*>(atom_ptr + kRankTileScaleOffset) +",
      "                    (thread / 8);",
      "",
      "      int32x2_t qw = __builtin_nontemporal_load(qw_ptr);",
      "      int qs = __builtin_nontemporal_load(qs_ptr);",
      "",
      "      *recv_buffer += kRankBufferTileStride;",
      "",
      "      // Unpack q8 into fp16x8_t",
      "      int32x4_t w;",
      "      {",
      "        static uint constexpr kMask00FF = 0x00FF00FF;",
      "",
      "        // {1024.0, 1024.0}, fp16x2_t",
      "        static uint constexpr kHalf2_1024 = 0x64006400;",
      "",
      "        // {-1152.0, -1152.0}, fp16x2_t",
      "        static uint constexpr kHalf2_1152 = 0xE480E480;",
      "",
      "#pragma unroll",
      "        for (int i = 0; i < 4; i++) {",
      "          if constexpr (std::is_same<T, half>::value) {",
      "            int32_t q8 =",
      "                ((qw[i / 2] >> ((i % 2) * 8)) & kMask00FF) | kHalf2_1024;",
      "            w[i] = packed_add<half>(q8, kHalf2_1152);",
      "          } else {",
      "            int32_t int16_2 = (qw[i / 2] >> ((i % 2) * 8)) & kMask00FF;",
      "            int16_t low = static_cast<int16_t>(int16_2 & 0xFFFF);",
      "            int16_t high = static_cast<int16_t>((int16_2 >> 16) & 0xFFFF);",
      "            nv_bfloat16 bf_low = __float2bfloat16(static_cast<float>(low));",
      "            nv_bfloat16 bf_high = __float2bfloat16(static_cast<float>(high));",
      "            nv_bfloat162 bf2 = __halves2bfloat162(bf_low, bf_high);",
      "            int32_t packed_bf16 = *reinterpret_cast<int32_t*>(&bf2);",
      "            w[i] = packed_add<nv_bfloat16>(packed_bf16, kRangeMin);",
      "          }",
      "        }",
      "      }",
      "",
      "      // Apply decoding scales",
      "      for (int i = 0; i < 4; i++) {",
      "        w[i] = packed_mul<T>(w[i], qs);",
      "      }",
      "",
      "      data[k] = w;",
      "    }",
      "  }",
      "};",
      "",
      "// Twoshot All Reduce",
      "template <typename T, class Codec, bool cast_bf2half>",
      "struct AllReduceTwoshot {",
      "  static_assert(sizeof(T) == 2);",
      "",
      "  static constexpr int kWorldSize = Codec::kWorldSize;",
      "",
      "  __device__ static void run(",
      "      T const* __restrict__ input, T* __restrict__ output,",
      "      uint32_t const N,                    // number of elements",
      "      int const block,                     // block index",
      "      int const rank,                      // rank index",
      "      uint8_t** __restrict__ buffer_list,  // communication buffers",
      "      uint32_t const data_offset,          // offset to start of the data buffer",
      "      uint32_t flag_color) {",
      "    // Topology",
      "    int thread = threadIdx.x + threadIdx.y * kWavefront;",
      "    uint8_t* rank_buffer = buffer_list[rank];",
      "    Codec codec(thread, rank);",
      "    int block_id = blockIdx.x;",
      "    int grid_size = gridDim.x;",
      "    // --------------------------------------------------------",
      "    // Read input into registers",
      "    int32x4_t tA[kAtoms];",
      "",
      "    BufferResource src_buffer(const_cast<T*>(input), N * sizeof(T));",
      "    uint32_t src_offset = block * kTileSize + thread * sizeof(int32x4_t);",
      "",
      "    for (int i = 0; i < kAtoms; i++) {",
      "      tA[i] = buffer_load_dwordx4(src_buffer.descriptor, src_offset, 0, 0);",
      "      src_offset += kAtomStride * sizeof(int32x4_t);",
      "      if constexpr (cast_bf2half) {",
      "        const nv_bfloat162* bf_buf =",
      "            reinterpret_cast<const nv_bfloat162*>(&tA[i]);",
      "        half2 half_buf[4];",
      "#pragma unroll",
      "        for (int j = 0; j < 4; ++j) {",
      "          float2 f = __bfloat1622float2(bf_buf[j]);",
      "          half_buf[j] = __float22half2_rn(f);",
      "        }",
      "        tA[i] = *reinterpret_cast<const int32x4_t*>(half_buf);",
      "      }",
      "    }",
      "",
      "    // --------------------------------------------------------",
      "    // Phase-1A: Write segment data into the communication buffer of the target",
      "    // rank responsible for this segment.",
      "    uint32_t comm_data0_offset =",
      "        data_offset + block_id * Codec::kTransmittedTileSize;",
      "    uint32_t comm_data1_offset =",
      "        grid_size * Codec::kTransmittedTileSize + comm_data0_offset;",
      "",
      "    uint32_t comm_flags0_offset = block_id * (kWorldSize * sizeof(uint32_t));",
      "    uint32_t comm_flags1_offset =",
      "        grid_size * (kWorldSize * sizeof(uint32_t)) + comm_flags0_offset;",
      "",
      "    for (int r = 0; r < kWorldSize; r++) {",
      "      int32x4_t* send_buffer =",
      "          reinterpret_cast<int32x4_t*>(buffer_list[r] + comm_data0_offset +",
      "                                       rank * Codec::kRankTransmittedTileSize);",
      "      codec.send(send_buffer, &tA[r * Codec::kRankAtoms]);",
      "    }",
      "",
      "    __syncthreads();",
      "    if (thread < kWorldSize) {",
      "      int r = thread;",
      "      uint32_t* flag_ptr = reinterpret_cast<uint32_t*>(",
      "          buffer_list[r] + comm_flags0_offset + rank * sizeof(uint32_t));",
      "      set_sync_flag(flag_ptr, flag_color);",
      "    }",
      "    // --------------------------------------------------------",
      "    // Phase-1B: Reduce the segment data from the communication buffers.",
      "    int32x4_t tR[Codec::kRankAtoms] = {};",
      "    {",
      "      // Read the data from the communication buffer.",
      "      int32x4_t* recv_buffer =",
      "          reinterpret_cast<int32x4_t*>(rank_buffer + comm_data0_offset);",
      "      uint32_t* flag_ptr =",
      "          reinterpret_cast<uint32_t*>(rank_buffer + comm_flags0_offset);",
      "",
      "      for (int r = 0; r < kWorldSize; r++) {",
      "        // Wait for the flags to be set.",
      "        if (thread == 0) {",
      "          wait_sync_flag(&flag_ptr[r], flag_color);",
      "        }",
      "        __syncthreads();",
      "",
      "        // note: we reuse tA as temp buffer here",
      "        codec.recv(&recv_buffer, tA);",
      "",
      "        for (int i = 0; i < Codec::kRankAtoms; i++) {",
      "          packed_assign_add<T>(&tR[i], &tA[i]);",
      "        }",
      "      }",
      "    }",
      "",
      "    // Phase-2: Write the reduced segment to every other rank",
      "    for (int r = 0; r < kWorldSize; r++) {",
      "      int32x4_t* send_buffer =",
      "          reinterpret_cast<int32x4_t*>(buffer_list[r] + comm_data1_offset +",
      "                                       rank * Codec::kRankTransmittedTileSize);",
      "      codec.send(send_buffer, tR);",
      "    }",
      "",
      "    __syncthreads();",
      "    if (thread < kWorldSize) {",
      "      int r = thread;",
      "      uint32_t* flag_ptr = reinterpret_cast<uint32_t*>(",
      "          buffer_list[r] + comm_flags1_offset + rank * sizeof(uint32_t));",
      "      set_sync_flag(flag_ptr, flag_color);",
      "    }",
      "",
      "    // Phase-2: Read the gather segments from the rank's communication buffer.",
      "    {",
      "      // Read the data from the communication buffer.",
      "      int32x4_t* recv_buffer =",
      "          reinterpret_cast<int32x4_t*>(rank_buffer + comm_data1_offset);",
      "      uint32_t* flag_ptr =",
      "          reinterpret_cast<uint32_t*>(rank_buffer + comm_flags1_offset);",
      "",
      "      for (int r = 0; r < kWorldSize; r++) {",
      "        // Wait for the flags to be set.",
      "        if (thread == 0) {",
      "          wait_sync_flag(&flag_ptr[r], flag_color);",
      "        }",
      "        __syncthreads();",
      "",
      "        // Gather all reduced and final rank segments into tA.",
      "        codec.recv(&recv_buffer, &tA[r * Codec::kRankAtoms]);",
      "      }",
      "    }",
      "",
      "    // --------------------------------------------------------",
      "    // Write the result to output.",
      "    BufferResource dst_buffer(output, N * sizeof(T));",
      "    uint32_t dst_offset = block * kTileSize + thread * sizeof(int32x4_t);",
      "",
      "    for (int i = 0; i < kAtoms; i++) {",
      "      if constexpr (cast_bf2half) {",
      "        const half2* half_buf = reinterpret_cast<const half2*>(&tA[i]);",
      "        nv_bfloat162 bf16_buf[4];",
      "#pragma unroll",
      "        for (int j = 0; j < 4; ++j) {",
      "          float2 f = __half22float2(half_buf[j]);",
      "          bf16_buf[j] = __float22bfloat162_rn(f);",
      "        }",
      "        buffer_store_dwordx4(*reinterpret_cast<const int32x4_t*>(bf16_buf),",
      "                             dst_buffer.descriptor, dst_offset, 0, 0);",
      "      } else {",
      "        buffer_store_dwordx4(tA[i], dst_buffer.descriptor, dst_offset, 0, 0);",
      "      }",
      "      dst_offset += kAtomStride * sizeof(int32x4_t);",
      "    }",
      "  }",
      "};",
      "",
      "}  // namespace quickreduce"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/moe/moe_align_sum_kernels.cu",
    "source": [
      "#include <torch/all.h>",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <cub/cub.cuh>",
      "",
      "#include <ATen/ATen.h>",
      "#include <ATen/cuda/Atomic.cuh>",
      "",
      "#include \"../cuda_compat.h\"",
      "#include \"../dispatch_utils.h\"",
      "",
      "#define CEILDIV(x, y) (((x) + (y) - 1) / (y))",
      "",
      "namespace vllm {",
      "namespace moe {",
      "",
      "template <typename scalar_t>",
      "__global__ void moe_align_block_size_kernel(",
      "    const scalar_t* __restrict__ topk_ids,",
      "    int32_t* __restrict__ sorted_token_ids, int32_t* __restrict__ expert_ids,",
      "    int32_t* __restrict__ total_tokens_post_pad, int32_t num_experts,",
      "    int32_t padded_num_experts, int32_t experts_per_warp, int32_t block_size,",
      "    size_t numel, int32_t* __restrict__ cumsum, int32_t max_num_tokens_padded) {",
      "  extern __shared__ int32_t shared_counts[];",
      "",
      "  // Initialize sorted_token_ids with numel",
      "  for (size_t it = threadIdx.x; it < max_num_tokens_padded; it += blockDim.x) {",
      "    sorted_token_ids[it] = numel;",
      "  }",
      "",
      "  const int warp_id = threadIdx.x / WARP_SIZE;",
      "  const int my_expert_start = warp_id * experts_per_warp;",
      "",
      "  for (int i = 0; i < experts_per_warp; ++i) {",
      "    if (my_expert_start + i < padded_num_experts) {",
      "      shared_counts[warp_id * experts_per_warp + i] = 0;",
      "    }",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  const size_t tid = threadIdx.x;",
      "  const size_t stride = blockDim.x;",
      "",
      "  for (size_t i = tid; i < numel; i += stride) {",
      "    int expert_id = topk_ids[i];",
      "    int warp_idx = expert_id / experts_per_warp;",
      "    int expert_offset = expert_id % experts_per_warp;",
      "    atomicAdd(&shared_counts[warp_idx * experts_per_warp + expert_offset], 1);",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  // Compute prefix sum over token counts per expert",
      "  using BlockScan = cub::BlockScan<int32_t, 1024>;",
      "  __shared__ typename BlockScan::TempStorage temp_storage;",
      "",
      "  int expert_count = 0;",
      "  int expert_id = threadIdx.x;",
      "  if (expert_id < num_experts) {",
      "    int warp_idx = expert_id / experts_per_warp;",
      "    int expert_offset = expert_id % experts_per_warp;",
      "    expert_count = shared_counts[warp_idx * experts_per_warp + expert_offset];",
      "    expert_count = CEILDIV(expert_count, block_size) * block_size;",
      "  }",
      "",
      "  int cumsum_val;",
      "  BlockScan(temp_storage).ExclusiveSum(expert_count, cumsum_val);",
      "  if (expert_id <= num_experts) {",
      "    cumsum[expert_id] = cumsum_val;",
      "  }",
      "",
      "  if (expert_id == num_experts) {",
      "    *total_tokens_post_pad = cumsum_val;",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  if (threadIdx.x < num_experts) {",
      "    for (int i = cumsum[threadIdx.x]; i < cumsum[threadIdx.x + 1];",
      "         i += block_size) {",
      "      expert_ids[i / block_size] = threadIdx.x;",
      "    }",
      "  }",
      "",
      "  // Fill remaining expert_ids with 0",
      "  const size_t fill_start_idx = cumsum[num_experts] / block_size + threadIdx.x;",
      "  const size_t expert_ids_size = CEILDIV(max_num_tokens_padded, block_size);",
      "  for (size_t i = fill_start_idx; i < expert_ids_size; i += blockDim.x) {",
      "    expert_ids[i] = 0;",
      "  }",
      "}",
      "",
      "template <typename scalar_t>",
      "__global__ void count_and_sort_expert_tokens_kernel(",
      "    const scalar_t* __restrict__ topk_ids,",
      "    int32_t* __restrict__ sorted_token_ids, int32_t* __restrict__ cumsum_buffer,",
      "    size_t numel) {",
      "  const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;",
      "  const size_t stride = blockDim.x * gridDim.x;",
      "",
      "  for (size_t i = tid; i < numel; i += stride) {",
      "    int32_t expert_id = topk_ids[i];",
      "    int32_t rank_post_pad = atomicAdd(&cumsum_buffer[expert_id], 1);",
      "    sorted_token_ids[rank_post_pad] = i;",
      "  }",
      "}",
      "",
      "template <typename scalar_t, int TOPK>",
      "__global__ void moe_sum_kernel(",
      "    scalar_t* __restrict__ out,          // [..., d]",
      "    const scalar_t* __restrict__ input,  // [..., topk, d]",
      "    const int d) {",
      "  const int64_t token_idx = blockIdx.x;",
      "  for (int64_t idx = threadIdx.x; idx < d; idx += blockDim.x) {",
      "    scalar_t x = 0.0;",
      "#pragma unroll",
      "    for (int k = 0; k < TOPK; ++k) {",
      "      x += VLLM_LDG(&input[token_idx * TOPK * d + k * d + idx]);",
      "    }",
      "    out[token_idx * d + idx] = x;",
      "  }",
      "}",
      "",
      "template <typename scalar_t>",
      "__global__ void moe_align_block_size_small_batch_expert_kernel(",
      "    const scalar_t* __restrict__ topk_ids,",
      "    int32_t* __restrict__ sorted_token_ids, int32_t* __restrict__ expert_ids,",
      "    int32_t* __restrict__ total_tokens_post_pad, int32_t num_experts,",
      "    int32_t block_size, size_t numel, int32_t max_num_tokens_padded) {",
      "  // Initialize sorted_token_ids with numel",
      "  for (size_t it = threadIdx.x; it < max_num_tokens_padded; it += blockDim.x) {",
      "    sorted_token_ids[it] = numel;",
      "  }",
      "",
      "  const size_t tid = threadIdx.x;",
      "  const size_t stride = blockDim.x;",
      "",
      "  extern __shared__ int32_t shared_mem[];",
      "  int32_t* cumsum = shared_mem;",
      "  int32_t* tokens_cnts = (int32_t*)(shared_mem + num_experts + 1);",
      "",
      "  for (int i = 0; i < num_experts; ++i) {",
      "    tokens_cnts[(threadIdx.x + 1) * num_experts + i] = 0;",
      "  }",
      "",
      "  for (size_t i = tid; i < numel; i += stride) {",
      "    ++tokens_cnts[(threadIdx.x + 1) * num_experts + topk_ids[i]];",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  if (threadIdx.x < num_experts) {",
      "    tokens_cnts[threadIdx.x] = 0;",
      "    for (int i = 1; i <= blockDim.x; ++i) {",
      "      tokens_cnts[i * num_experts + threadIdx.x] +=",
      "          tokens_cnts[(i - 1) * num_experts + threadIdx.x];",
      "    }",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  if (threadIdx.x == 0) {",
      "    cumsum[0] = 0;",
      "    for (int i = 1; i <= num_experts; ++i) {",
      "      cumsum[i] =",
      "          cumsum[i - 1] +",
      "          CEILDIV(tokens_cnts[blockDim.x * num_experts + i - 1], block_size) *",
      "              block_size;",
      "    }",
      "    *total_tokens_post_pad = static_cast<int32_t>(cumsum[num_experts]);",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  if (threadIdx.x < num_experts) {",
      "    for (int i = cumsum[threadIdx.x]; i < cumsum[threadIdx.x + 1];",
      "         i += block_size) {",
      "      expert_ids[i / block_size] = threadIdx.x;",
      "    }",
      "  }",
      "",
      "  // Fill remaining expert_ids with 0",
      "  const size_t fill_start_idx = cumsum[num_experts] / block_size + threadIdx.x;",
      "  const size_t expert_ids_size = CEILDIV(max_num_tokens_padded, block_size);",
      "  for (size_t i = fill_start_idx; i < expert_ids_size; i += blockDim.x) {",
      "    expert_ids[i] = 0;",
      "  }",
      "",
      "  for (size_t i = tid; i < numel; i += stride) {",
      "    int32_t expert_id = topk_ids[i];",
      "    int32_t rank_post_pad =",
      "        tokens_cnts[threadIdx.x * num_experts + expert_id] + cumsum[expert_id];",
      "    sorted_token_ids[rank_post_pad] = i;",
      "    ++tokens_cnts[threadIdx.x * num_experts + expert_id];",
      "  }",
      "}",
      "",
      "}  // namespace moe",
      "}  // namespace vllm",
      "",
      "// taken from",
      "// https://github.com/sgl-project/sglang/blob/8b5f83ed3b7d2a49ad5c5cd5aa61c5d502f47dbc",
      "void moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,",
      "                          int64_t block_size, torch::Tensor sorted_token_ids,",
      "                          torch::Tensor experts_ids,",
      "                          torch::Tensor num_tokens_post_pad) {",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "",
      "  int64_t padded_num_experts =",
      "      ((num_experts + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE;",
      "  int experts_per_warp = WARP_SIZE;",
      "  int threads = 1024;",
      "  threads = ((threads + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE;",
      "",
      "  // BlockScan uses 1024 threads and assigns one thread per expert.",
      "  TORCH_CHECK(padded_num_experts < 1024,",
      "              \"padded_num_experts must be less than 1024\");",
      "",
      "  VLLM_DISPATCH_INTEGRAL_AND_UNSIGNED_TYPES(",
      "      topk_ids.scalar_type(), \"moe_align_block_size_kernel\", [&] {",
      "        // calc needed amount of shared mem for `cumsum` tensors",
      "        auto options_int =",
      "            torch::TensorOptions().dtype(torch::kInt).device(topk_ids.device());",
      "        torch::Tensor cumsum_buffer =",
      "            torch::empty({num_experts + 1}, options_int);",
      "        bool small_batch_expert_mode =",
      "            (topk_ids.numel() < 1024) && (num_experts <= 64);",
      "",
      "        if (small_batch_expert_mode) {",
      "          const int32_t threads = max((int32_t)num_experts, WARP_SIZE);",
      "          const int32_t shared_mem_size =",
      "              ((threads + 1) * num_experts + (num_experts + 1)) *",
      "              sizeof(int32_t);",
      "",
      "          auto small_batch_expert_kernel =",
      "              vllm::moe::moe_align_block_size_small_batch_expert_kernel<",
      "                  scalar_t>;",
      "          small_batch_expert_kernel<<<1, threads, shared_mem_size, stream>>>(",
      "              topk_ids.data_ptr<scalar_t>(),",
      "              sorted_token_ids.data_ptr<int32_t>(),",
      "              experts_ids.data_ptr<int32_t>(),",
      "              num_tokens_post_pad.data_ptr<int32_t>(), num_experts, block_size,",
      "              topk_ids.numel(), sorted_token_ids.size(0));",
      "        } else {",
      "          auto align_kernel = vllm::moe::moe_align_block_size_kernel<scalar_t>;",
      "",
      "          size_t num_warps = CEILDIV(padded_num_experts, experts_per_warp);",
      "          size_t shared_mem_size =",
      "              num_warps * experts_per_warp * sizeof(int32_t);",
      "",
      "          align_kernel<<<1, threads, shared_mem_size, stream>>>(",
      "              topk_ids.data_ptr<scalar_t>(),",
      "              sorted_token_ids.data_ptr<int32_t>(),",
      "              experts_ids.data_ptr<int32_t>(),",
      "              num_tokens_post_pad.data_ptr<int32_t>(), num_experts,",
      "              padded_num_experts, experts_per_warp, block_size,",
      "              topk_ids.numel(), cumsum_buffer.data_ptr<int32_t>(),",
      "              sorted_token_ids.size(0));",
      "",
      "          const int block_threads = std::min(256, (int)threads);",
      "          const int num_blocks =",
      "              (topk_ids.numel() + block_threads - 1) / block_threads;",
      "          const int max_blocks = 65535;",
      "          const int actual_blocks = std::min(num_blocks, max_blocks);",
      "",
      "          auto sort_kernel =",
      "              vllm::moe::count_and_sort_expert_tokens_kernel<scalar_t>;",
      "          sort_kernel<<<actual_blocks, block_threads, 0, stream>>>(",
      "              topk_ids.data_ptr<scalar_t>(),",
      "              sorted_token_ids.data_ptr<int32_t>(),",
      "              cumsum_buffer.data_ptr<int32_t>(), topk_ids.numel());",
      "        }",
      "      });",
      "}",
      "",
      "void moe_sum(torch::Tensor& input,   // [num_tokens, topk, hidden_size]",
      "             torch::Tensor& output)  // [num_tokens, hidden_size]",
      "{",
      "  const int hidden_size = input.size(-1);",
      "  const auto num_tokens = output.numel() / hidden_size;",
      "  const int topk = input.size(1);",
      "",
      "  dim3 grid(num_tokens);",
      "  dim3 block(std::min(hidden_size, 1024));",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(output));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "",
      "  switch (topk) {",
      "    case 2:",
      "      VLLM_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"moe_sum_kernel\", [&] {",
      "        vllm::moe::moe_sum_kernel<scalar_t, 2><<<grid, block, 0, stream>>>(",
      "            output.data_ptr<scalar_t>(), input.data_ptr<scalar_t>(),",
      "            hidden_size);",
      "      });",
      "      break;",
      "",
      "    case 3:",
      "      VLLM_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"moe_sum_kernel\", [&] {",
      "        vllm::moe::moe_sum_kernel<scalar_t, 3><<<grid, block, 0, stream>>>(",
      "            output.data_ptr<scalar_t>(), input.data_ptr<scalar_t>(),",
      "            hidden_size);",
      "      });",
      "      break;",
      "",
      "    case 4:",
      "      VLLM_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"moe_sum_kernel\", [&] {",
      "        vllm::moe::moe_sum_kernel<scalar_t, 4><<<grid, block, 0, stream>>>(",
      "            output.data_ptr<scalar_t>(), input.data_ptr<scalar_t>(),",
      "            hidden_size);",
      "      });",
      "      break;",
      "",
      "    default:",
      "      at::sum_out(output, input, 1);",
      "      break;",
      "  }",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/moe/moe_permute_unpermute_op.cu",
    "source": [
      "#include <c10/core/ScalarType.h>",
      "#include <torch/all.h>",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include \"permute_unpermute_kernels/moe_permute_unpermute_kernel.h\"",
      "#include \"permute_unpermute_kernels/dispatch.h\"",
      "#include \"core/registration.h\"",
      "",
      "// moe_permute kernels require at least CUDA 12.0",
      "#if defined(CUDA_VERSION) && (CUDA_VERSION >= 12000)",
      "",
      "void moe_permute(",
      "    const torch::Tensor& input,                      // [n_token, hidden]",
      "    const torch::Tensor& topk_ids,                   // [n_token, topk]",
      "    const torch::Tensor& token_expert_indices,       // [n_token, topk]",
      "    const std::optional<torch::Tensor>& expert_map,  // [n_expert]",
      "    int64_t n_expert, int64_t n_local_expert, int64_t topk,",
      "    const std::optional<int64_t>& align_block_size,",
      "    torch::Tensor& permuted_input,             // [permuted_size, hidden]",
      "    torch::Tensor& expert_first_token_offset,  // [n_local_expert + 1]",
      "    torch::Tensor& inv_permuted_idx,           // [n_token, topk]",
      "    torch::Tensor& permuted_idx,               // [permute_size]",
      "    torch::Tensor& m_indices) {                // [align_expand_m]",
      "  TORCH_CHECK(expert_first_token_offset.scalar_type() == at::ScalarType::Long,",
      "              \"expert_first_token_offset must be int64\");",
      "  TORCH_CHECK(topk_ids.scalar_type() == at::ScalarType::Int,",
      "              \"topk_ids must be int32\");",
      "  TORCH_CHECK(token_expert_indices.scalar_type() == at::ScalarType::Int,",
      "              \"token_expert_indices must be int32\");",
      "  TORCH_CHECK(inv_permuted_idx.scalar_type() == at::ScalarType::Int,",
      "              \"inv_permuted_idx must be int32\");",
      "  TORCH_CHECK(expert_first_token_offset.size(0) == n_local_expert + 1,",
      "              \"expert_first_token_offset shape != n_local_expert+1\")",
      "  TORCH_CHECK(inv_permuted_idx.sizes() == token_expert_indices.sizes(),",
      "              \"token_expert_indices shape must be same as inv_permuted_idx\");",
      "  auto n_token = input.sizes()[0];",
      "  auto n_hidden = input.sizes()[1];",
      "  auto align_block_size_value =",
      "      align_block_size.has_value() ? align_block_size.value() : -1;",
      "  auto stream = at::cuda::getCurrentCUDAStream().stream();",
      "  const long sorter_size =",
      "      CubKeyValueSorter::getWorkspaceSize(n_token * topk, n_expert);",
      "  auto sort_workspace = torch::empty(",
      "      {sorter_size},",
      "      torch::dtype(torch::kInt8).device(torch::kCUDA).requires_grad(false));",
      "  auto copy_topk_ids = topk_ids.clone();  // copy topk_ids for preprocess",
      "  auto permuted_experts_id = torch::empty_like(topk_ids);",
      "  auto sorted_row_idx = torch::empty_like(inv_permuted_idx);",
      "",
      "  CubKeyValueSorter sorter{};",
      "  int64_t* valid_num_ptr = nullptr;",
      "  // pre-process kernel for expert-parallelism:",
      "  // no local expert id plus \"n_expert\" offset for priority to local expert",
      "  // map local expert id [n, .., n+n_local_expert-1] to [0, n_local_expert -1]",
      "  // For example, 4 expert with ep_size=2. ep_rank=1 owns global expert id",
      "  // [2,3] with expert_map[-1, -1, 0, 1], preprocess_topk_id  process topk_ids",
      "  // and map global expert id [2, 3] to local_expert id [0, 1] and map global",
      "  // expert id [0, 1] ( not in ep rank=1)  to [4, 5] by plus n_expert. This map",
      "  // operation is to make local expert high priority in following sort topk_ids",
      "  // and scan local expert_first_token_offset for each ep rank for next group",
      "  // gemm.",
      "  if (expert_map.has_value()) {",
      "    const int* expert_map_ptr = get_ptr<int>(expert_map.value());",
      "    valid_num_ptr =",
      "        get_ptr<int64_t>(expert_first_token_offset) + n_local_expert;",
      "    preprocessTopkIdLauncher(get_ptr<int>(copy_topk_ids), n_token * topk,",
      "                             expert_map_ptr, n_expert, stream);",
      "  }",
      "  // expert sort topk expert id and scan expert id get expert_first_token_offset",
      "  sortAndScanExpert(",
      "      get_ptr<int>(copy_topk_ids), get_ptr<int>(token_expert_indices),",
      "      get_ptr<int>(permuted_experts_id), get_ptr<int>(sorted_row_idx),",
      "      get_ptr<int64_t>(expert_first_token_offset), n_token, n_expert,",
      "      n_local_expert, topk, sorter, get_ptr<int>(sort_workspace), stream);",
      "",
      "  // dispatch expandInputRowsKernelLauncher",
      "  MOE_DISPATCH(input.scalar_type(), [&] {",
      "    expandInputRowsKernelLauncher<scalar_t>(",
      "        get_ptr<scalar_t>(input), get_ptr<scalar_t>(permuted_input),",
      "        get_ptr<int>(permuted_experts_id), get_ptr<int>(sorted_row_idx),",
      "        get_ptr<int>(inv_permuted_idx), get_ptr<int>(permuted_idx),",
      "        get_ptr<int64_t>(expert_first_token_offset), n_token, valid_num_ptr,",
      "        n_hidden, topk, n_local_expert, align_block_size_value, stream);",
      "  });",
      "",
      "  // get m_indices and update expert_first_token_offset with align block",
      "  // this is only required for DeepGemm and not required for CUTLASS group gemm",
      "  if (align_block_size.has_value()) {",
      "    auto align_expert_first_token_offset =",
      "        torch::zeros_like(expert_first_token_offset);",
      "    getMIndices(get_ptr<int64_t>(expert_first_token_offset),",
      "                get_ptr<int64_t>(align_expert_first_token_offset),",
      "                get_ptr<int>(m_indices), n_local_expert, align_block_size_value,",
      "                stream);",
      "    expert_first_token_offset.copy_(align_expert_first_token_offset);",
      "  }",
      "}",
      "",
      "void moe_unpermute(",
      "    const torch::Tensor& permuted_hidden_states,  // [n_token * topk, hidden]",
      "    const torch::Tensor& topk_weights,            // [n_token, topk]",
      "    const torch::Tensor& inv_permuted_idx,        // [n_token, topk]",
      "    const std::optional<torch::Tensor>&",
      "        expert_first_token_offset,  // [n_local_expert+1]",
      "    int64_t topk,",
      "    torch::Tensor& hidden_states  // [n_token, hidden]",
      ") {",
      "  TORCH_CHECK(",
      "      permuted_hidden_states.scalar_type() == hidden_states.scalar_type(),",
      "      \"permuted_hidden_states dtype must be same as hidden_states\");",
      "  auto n_token = hidden_states.size(0);",
      "  auto n_hidden = hidden_states.size(1);",
      "  auto stream = at::cuda::getCurrentCUDAStream().stream();",
      "",
      "  int64_t const* valid_ptr = nullptr;",
      "  if (expert_first_token_offset.has_value()) {",
      "    int n_local_expert = expert_first_token_offset.value().size(0) - 1;",
      "    valid_ptr =",
      "        get_ptr<int64_t>(expert_first_token_offset.value()) + n_local_expert;",
      "  }",
      "",
      "  MOE_DISPATCH(hidden_states.scalar_type(), [&] {",
      "    finalizeMoeRoutingKernelLauncher<scalar_t, scalar_t>(",
      "        get_ptr<scalar_t>(permuted_hidden_states),",
      "        get_ptr<scalar_t>(hidden_states), get_ptr<float>(topk_weights),",
      "        get_ptr<int>(inv_permuted_idx), n_token, n_hidden, topk, valid_ptr,",
      "        stream);",
      "  });",
      "}",
      "",
      "template <typename T>",
      "__global__ void shuffleInputRowsKernel(const T* input,",
      "                                       const int32_t* dst2src_map, T* output,",
      "                                       int64_t num_src_rows,",
      "                                       int64_t num_dst_rows, int64_t num_cols) {",
      "  int64_t dest_row_idx = blockIdx.x;",
      "  int64_t const source_row_idx = dst2src_map[dest_row_idx];",
      "",
      "  if (blockIdx.x < num_dst_rows) {",
      "    // Load 128-bits per thread",
      "    constexpr int64_t ELEM_PER_THREAD = 128 / sizeof(T) / 8;",
      "    using DataElem = cutlass::Array<T, ELEM_PER_THREAD>;",
      "",
      "    // Duplicate and permute rows",
      "    auto const* source_row_ptr =",
      "        reinterpret_cast<DataElem const*>(input + source_row_idx * num_cols);",
      "    auto* dest_row_ptr =",
      "        reinterpret_cast<DataElem*>(output + dest_row_idx * num_cols);",
      "",
      "    int64_t const start_offset = threadIdx.x;",
      "    int64_t const stride = blockDim.x;",
      "    int64_t const num_elems_in_col = num_cols / ELEM_PER_THREAD;",
      "",
      "    for (int elem_index = start_offset; elem_index < num_elems_in_col;",
      "         elem_index += stride) {",
      "      dest_row_ptr[elem_index] = source_row_ptr[elem_index];",
      "    }",
      "  }",
      "}",
      "",
      "void shuffle_rows(const torch::Tensor& input_tensor,",
      "                  const torch::Tensor& dst2src_map,",
      "                  torch::Tensor& output_tensor) {",
      "  TORCH_CHECK(input_tensor.scalar_type() == output_tensor.scalar_type(),",
      "              \"Input and output tensors must have the same data type\");",
      "",
      "  auto stream = at::cuda::getCurrentCUDAStream().stream();",
      "  int64_t const blocks = output_tensor.size(0);",
      "  int64_t const threads = 256;",
      "  int64_t const num_dest_rows = output_tensor.size(0);",
      "  int64_t const num_src_rows = input_tensor.size(0);",
      "  int64_t const num_cols = input_tensor.size(1);",
      "",
      "  TORCH_CHECK(!(num_cols % (128 / sizeof(input_tensor.scalar_type()) / 8)),",
      "              \"num_cols must be divisible by 128 / \"",
      "              \"sizeof(input_tensor.scalar_type()) / 8\");",
      "",
      "  MOE_DISPATCH(input_tensor.scalar_type(), [&] {",
      "    shuffleInputRowsKernel<scalar_t><<<blocks, threads, 0, stream>>>(",
      "        reinterpret_cast<scalar_t*>(input_tensor.data_ptr()),",
      "        dst2src_map.data_ptr<int32_t>(),",
      "        reinterpret_cast<scalar_t*>(output_tensor.data_ptr()), num_src_rows,",
      "        num_dest_rows, num_cols);",
      "  });",
      "}",
      "",
      "#else",
      "",
      "void moe_permute(const torch::Tensor& input, const torch::Tensor& topk_weights,",
      "                 torch::Tensor& topk_ids,",
      "                 const torch::Tensor& token_expert_indices,",
      "                 const std::optional<torch::Tensor>& expert_map,",
      "                 int64_t n_expert, int64_t n_local_expert, int64_t topk,",
      "                 const std::optional<int64_t>& align_block_size,",
      "                 torch::Tensor& permuted_input,",
      "                 torch::Tensor& expert_first_token_offset,",
      "                 torch::Tensor& src_row_id2dst_row_id_map,",
      "                 torch::Tensor& m_indices) {",
      "  TORCH_CHECK(false, \"moe_permute is not supported on CUDA < 12.0\");",
      "}",
      "",
      "void moe_unpermute(",
      "    const torch::Tensor& permuted_hidden_states,",
      "    const torch::Tensor& topk_weights, const torch::Tensor& inv_permuted_idx,",
      "    const std::optional<torch::Tensor>& expert_first_token_offset, int64_t topk,",
      "    torch::Tensor& hidden_states) {",
      "  TORCH_CHECK(false, \"moe_unpermute is not supported on CUDA < 12.0\");",
      "}",
      "",
      "#endif",
      "",
      "bool moe_permute_unpermute_supported() {",
      "#if defined(CUDA_VERSION) && (CUDA_VERSION >= 12000)",
      "  return true;",
      "#else",
      "  return false;",
      "#endif",
      "}",
      "",
      "TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, CUDA, m) {",
      "  m.impl(\"moe_permute\", &moe_permute);",
      "  m.impl(\"moe_unpermute\", &moe_unpermute);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/moe/grouped_topk_kernels.cu",
    "source": [
      "/*",
      " * Adapted from",
      " * https://github.com/NVIDIA/TensorRT-LLM/blob/v0.21.0/cpp/tensorrt_llm/kernels/noAuxTcKernels.cu",
      " * Copyright (c) 2025, The vLLM team.",
      " * SPDX-FileCopyrightText: Copyright (c) 1993-2024 NVIDIA CORPORATION &",
      " * AFFILIATES. All rights reserved. SPDX-License-Identifier: Apache-2.0",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " * http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "#include <c10/cuda/CUDAStream.h>",
      "#include <torch/all.h>",
      "#include <cuda_fp16.h>",
      "#include <cuda_bf16.h>",
      "#include <cooperative_groups.h>",
      "#include <cooperative_groups/reduce.h>",
      "namespace cg = cooperative_groups;",
      "",
      "namespace vllm {",
      "namespace moe {",
      "",
      "constexpr float kNegInfinity = INFINITY * -1;",
      "constexpr unsigned FULL_WARP_MASK = 0xffffffff;",
      "constexpr int32_t WARP_SIZE = 32;",
      "constexpr int32_t BLOCK_SIZE = 512;",
      "constexpr int32_t NUM_WARPS_PER_BLOCK = BLOCK_SIZE / WARP_SIZE;",
      "",
      "namespace warp_topk {",
      "",
      "template <int size, typename T>",
      "__host__ __device__ constexpr T round_up_to_multiple_of(T len) {",
      "  if (len == 0) {",
      "    return 0;",
      "  }",
      "  return ((len - 1) / size + 1) * size;",
      "}",
      "",
      "template <typename T>",
      "constexpr __host__ __device__ bool isPowerOf2(T v) {",
      "  return (v && !(v & (v - 1)));",
      "}",
      "",
      "template <bool greater, typename T>",
      "__forceinline__ __device__ bool is_better_than(T val, T baseline) {",
      "  return (val > baseline && greater) || (val < baseline && !greater);",
      "}",
      "",
      "template <bool greater, typename T, typename idxT>",
      "__forceinline__ __device__ bool is_better_than(T val, T baseline, idxT index,",
      "                                               idxT baseline_index) {",
      "  bool res = (val > baseline && greater) || (val < baseline && !greater);",
      "  if (val == baseline) {",
      "    res = (index < baseline_index && greater) ||",
      "          (index < baseline_index && !greater);",
      "  }",
      "  return res;",
      "}",
      "",
      "template <typename T, typename idxT>",
      "int calc_smem_size_for_block_wide(int num_of_warp, int64_t k) {",
      "  int64_t cache_topk = (sizeof(T) + sizeof(idxT)) * num_of_warp * k;",
      "  int64_t n = std::max<int>(num_of_warp / 2 * k, num_of_warp * WARP_SIZE);",
      "  return max(cache_topk,",
      "             round_up_to_multiple_of<256>(n * sizeof(T)) + n * sizeof(idxT));",
      "}",
      "",
      "template <int size, bool ascending, bool reverse, typename T, typename idxT,",
      "          bool is_stable>",
      "struct BitonicMerge {",
      "  // input should be a bitonic sequence, and sort it to be a monotonic sequence",
      "  __device__ static void merge(T* __restrict__ val_arr,",
      "                               idxT* __restrict__ idx_arr) {",
      "    static_assert(isPowerOf2(size));",
      "    static_assert(size >= 2 * WARP_SIZE);",
      "    constexpr int arr_len = size / WARP_SIZE;",
      "",
      "    constexpr int stride = arr_len / 2;",
      "    for (int i = 0; i < stride; ++i) {",
      "      int const other_i = i + stride;",
      "      T& val = val_arr[i];",
      "      T& other_val = val_arr[other_i];",
      "      bool is_better;",
      "      if constexpr (is_stable) {",
      "        is_better = is_better_than<ascending>(val, other_val, idx_arr[i],",
      "                                              idx_arr[other_i]);",
      "      } else {",
      "        is_better = is_better_than<ascending>(val, other_val);",
      "      }",
      "",
      "      if (is_better) {",
      "        T tmp = val;",
      "        val = other_val;",
      "        other_val = tmp;",
      "",
      "        idxT tmp2 = idx_arr[i];",
      "        idx_arr[i] = idx_arr[other_i];",
      "        idx_arr[other_i] = tmp2;",
      "      }",
      "    }",
      "",
      "    BitonicMerge<size / 2, ascending, reverse, T, idxT, is_stable>::merge(",
      "        val_arr, idx_arr);",
      "    BitonicMerge<size / 2, ascending, reverse, T, idxT, is_stable>::merge(",
      "        val_arr + arr_len / 2, idx_arr + arr_len / 2);",
      "  }",
      "};",
      "",
      "template <int size, bool ascending, typename T, typename idxT, bool is_stable>",
      "struct BitonicSort {",
      "  __device__ static void sort(T* __restrict__ val_arr,",
      "                              idxT* __restrict__ idx_arr) {",
      "    static_assert(isPowerOf2(size));",
      "    static_assert(size >= 2 * WARP_SIZE);",
      "    constexpr int arr_len = size / WARP_SIZE;",
      "",
      "    BitonicSort<size / 2, true, T, idxT, is_stable>::sort(val_arr, idx_arr);",
      "    BitonicSort<size / 2, false, T, idxT, is_stable>::sort(",
      "        val_arr + arr_len / 2, idx_arr + arr_len / 2);",
      "    BitonicMerge<size, ascending, ascending, T, idxT, is_stable>::merge(",
      "        val_arr, idx_arr);",
      "  }",
      "};",
      "",
      "template <bool ascending, typename T, typename idxT, bool is_stable>",
      "struct BitonicSort<32, ascending, T, idxT, is_stable> {",
      "  __device__ static void sort(T* __restrict__ val_arr,",
      "                              idxT* __restrict__ idx_arr) {",
      "    int const lane = threadIdx.x % WARP_SIZE;",
      "",
      "    // ascending doesn't matter before merging since all we need is a bitonic",
      "    // sequence",
      "    for (int stage = 0; stage < 4; ++stage) {",
      "      for (int stride = (1 << stage); stride > 0; stride /= 2) {",
      "        bool reverse = (lane >> stage) & 2;",
      "        bool is_second = lane & stride;",
      "",
      "        T other = __shfl_xor_sync(FULL_WARP_MASK, *val_arr, stride);",
      "        idxT other_idx = __shfl_xor_sync(FULL_WARP_MASK, *idx_arr, stride);",
      "",
      "        bool is_better;",
      "        if constexpr (is_stable) {",
      "          if constexpr (ascending) {",
      "            is_better = ((*val_arr > other) ||",
      "                         ((*val_arr == other) && (*idx_arr < other_idx))) !=",
      "                        (reverse != is_second);",
      "          } else {",
      "            is_better = ((*val_arr > other) ||",
      "                         ((*val_arr == other) && (*idx_arr > other_idx))) !=",
      "                        (reverse != is_second);",
      "          }",
      "        } else {",
      "          is_better = (*val_arr != other &&",
      "                       (*val_arr > other) != (reverse != is_second));",
      "        }",
      "        if (is_better) {",
      "          *val_arr = other;",
      "          *idx_arr = other_idx;",
      "        }",
      "      }",
      "    }",
      "",
      "    BitonicMerge<32, ascending, ascending, T, idxT, is_stable>::merge(val_arr,",
      "                                                                      idx_arr);",
      "  }",
      "};",
      "",
      "template <bool ascending, bool reverse, typename T, typename idxT,",
      "          bool is_stable>",
      "struct BitonicMerge<32, ascending, reverse, T, idxT, is_stable> {",
      "  __device__ static void merge(T* __restrict__ val_arr,",
      "                               idxT* __restrict__ idx_arr) {",
      "    int const lane = threadIdx.x % WARP_SIZE;",
      "    for (int stride = WARP_SIZE / 2; stride > 0; stride /= 2) {",
      "      bool is_second = lane & stride;",
      "      T& val = *val_arr;",
      "      T other = __shfl_xor_sync(FULL_WARP_MASK, val, stride);",
      "      idxT& idx = *idx_arr;",
      "      idxT other_idx = __shfl_xor_sync(FULL_WARP_MASK, idx, stride);",
      "",
      "      bool is_better;",
      "      if constexpr (is_stable) {",
      "        if constexpr (ascending) {",
      "          is_better = ((*val_arr > other) ||",
      "                       ((*val_arr == other) && (*idx_arr < other_idx))) ==",
      "                      (reverse != is_second);  // for min",
      "        } else {",
      "          is_better = ((*val_arr > other) ||",
      "                       ((*val_arr == other) && (*idx_arr > other_idx))) ==",
      "                      (reverse != is_second);  // for max",
      "        }",
      "      } else {",
      "        is_better =",
      "            (val != other && ((val > other) == (ascending != is_second)));",
      "      }",
      "",
      "      if (is_better) {",
      "        val = other;",
      "        idx = other_idx;",
      "      }",
      "    }",
      "  }",
      "};",
      "",
      "template <int capacity, bool greater, typename T, typename idxT, bool is_stable>",
      "class WarpSort {",
      " public:",
      "  __device__ WarpSort(idxT k, T dummy)",
      "      : lane_(threadIdx.x % WARP_SIZE), k_(k), dummy_(dummy) {",
      "    static_assert(capacity >= WARP_SIZE && isPowerOf2(capacity));",
      "",
      "    for (int i = 0; i < max_arr_len_; ++i) {",
      "      val_arr_[i] = dummy_;",
      "      idx_arr_[i] = 0;",
      "    }",
      "  }",
      "",
      "  // load and merge k sorted values",
      "  __device__ void load_sorted(T const* __restrict__ in,",
      "                              idxT const* __restrict__ in_idx, idxT start) {",
      "    idxT idx = start + WARP_SIZE - 1 - lane_;",
      "    for (int i = max_arr_len_ - 1; i >= 0; --i, idx += WARP_SIZE) {",
      "      if (idx < start + k_) {",
      "        T t = in[idx];",
      "        bool is_better;",
      "        if constexpr (is_stable) {",
      "          is_better =",
      "              is_better_than<greater>(t, val_arr_[i], in_idx[idx], idx_arr_[i]);",
      "        } else {",
      "          is_better = is_better_than<greater>(t, val_arr_[i]);",
      "        }",
      "        if (is_better) {",
      "          val_arr_[i] = t;",
      "          idx_arr_[i] = in_idx[idx];",
      "        }",
      "      }",
      "    }",
      "",
      "    BitonicMerge<capacity, greater, !greater, T, idxT, is_stable>::merge(",
      "        val_arr_, idx_arr_);",
      "  }",
      "",
      "  __device__ void dump(T* __restrict__ out, idxT* __restrict__ out_idx) const {",
      "    for (int i = 0; i < max_arr_len_; ++i) {",
      "      idxT out_i = i * WARP_SIZE + lane_;",
      "      if (out_i < k_) {",
      "        out[out_i] = val_arr_[i];",
      "        out_idx[out_i] = idx_arr_[i];",
      "      }",
      "    }",
      "  }",
      "",
      "  __device__ void dumpIdx(idxT* __restrict__ out_idx) const {",
      "    for (int i = 0; i < max_arr_len_; ++i) {",
      "      idxT out_i = i * WARP_SIZE + lane_;",
      "      if (out_i < k_) {",
      "        out_idx[out_i] = idx_arr_[i];",
      "      }",
      "    }",
      "  }",
      "",
      " protected:",
      "  static constexpr int max_arr_len_ = capacity / WARP_SIZE;",
      "",
      "  T val_arr_[max_arr_len_];",
      "  idxT idx_arr_[max_arr_len_];",
      "",
      "  int const lane_;",
      "  idxT const k_;",
      "  T const dummy_;",
      "",
      "};  // end class WarpSort",
      "",
      "template <int capacity, bool greater, typename T, typename idxT, bool is_stable>",
      "class WarpSelect : public WarpSort<capacity, greater, T, idxT, is_stable> {",
      " public:",
      "  __device__ WarpSelect(idxT k, T dummy)",
      "      : WarpSort<capacity, greater, T, idxT, is_stable>(k, dummy),",
      "        k_th_(dummy),",
      "        k_th_lane_((k - 1) % WARP_SIZE) {",
      "    extern __shared__ char smem_buf[];  // extern __shared__ T smem_buf[];",
      "",
      "    int const num_of_warp = blockDim.x / WARP_SIZE;",
      "    int const warp_id = threadIdx.x / WARP_SIZE;",
      "    val_smem_ = reinterpret_cast<T*>(smem_buf);",
      "    val_smem_ += warp_id * WARP_SIZE;",
      "    idx_smem_ = reinterpret_cast<idxT*>(",
      "        smem_buf +",
      "        round_up_to_multiple_of<256>(num_of_warp * sizeof(T) * WARP_SIZE));",
      "    idx_smem_ += warp_id * WARP_SIZE;",
      "  }",
      "",
      "  __device__ void add(T const* in, idxT start, idxT end) {",
      "    idxT const end_for_fullwarp =",
      "        round_up_to_multiple_of<WARP_SIZE>(end - start) + start;",
      "    for (idxT i = start + lane_; i < end_for_fullwarp; i += WARP_SIZE) {",
      "      T val = (i < end) ? in[i] : dummy_;",
      "      add(val, i);",
      "    }",
      "  }",
      "",
      "  __device__ void add(T val, idxT idx) {",
      "    bool do_add;",
      "    if constexpr (is_stable) {",
      "      do_add = is_better_than<greater>(val, k_th_, idx, k_th_idx_);",
      "    } else {",
      "      do_add = is_better_than<greater>(val, k_th_);",
      "    }",
      "",
      "    uint32_t mask = __ballot_sync(FULL_WARP_MASK, do_add);",
      "    if (mask == 0) {",
      "      return;",
      "    }",
      "",
      "    int pos = smem_buf_len_ + __popc(mask & ((0x1u << lane_) - 1));",
      "    if (do_add && pos < WARP_SIZE) {",
      "      val_smem_[pos] = val;",
      "      idx_smem_[pos] = idx;",
      "      do_add = false;",
      "    }",
      "    smem_buf_len_ += __popc(mask);",
      "    if (smem_buf_len_ >= WARP_SIZE) {",
      "      __syncwarp();",
      "      merge_buf_(val_smem_[lane_], idx_smem_[lane_]);",
      "      smem_buf_len_ -= WARP_SIZE;",
      "    }",
      "    if (do_add) {",
      "      pos -= WARP_SIZE;",
      "      val_smem_[pos] = val;",
      "      idx_smem_[pos] = idx;",
      "    }",
      "    __syncwarp();",
      "  }",
      "",
      "  __device__ void done() {",
      "    if (smem_buf_len_) {",
      "      T val = (lane_ < smem_buf_len_) ? val_smem_[lane_] : dummy_;",
      "      idxT idx = (lane_ < smem_buf_len_) ? idx_smem_[lane_] : 0;",
      "      merge_buf_(val, idx);",
      "    }",
      "",
      "    // after done(), smem is used for merging results among warps",
      "    __syncthreads();",
      "  }",
      "",
      " private:",
      "  __device__ void set_k_th_() {",
      "    k_th_ = __shfl_sync(FULL_WARP_MASK, val_arr_[max_arr_len_ - 1], k_th_lane_);",
      "    if constexpr (is_stable) {",
      "      k_th_idx_ =",
      "          __shfl_sync(FULL_WARP_MASK, idx_arr_[max_arr_len_ - 1], k_th_lane_);",
      "    }",
      "  }",
      "",
      "  __device__ void merge_buf_(T val, idxT idx) {",
      "    BitonicSort<WARP_SIZE, greater, T, idxT, is_stable>::sort(&val, &idx);",
      "",
      "    T& old = val_arr_[max_arr_len_ - 1];",
      "",
      "    bool is_better;",
      "    if constexpr (is_stable) {",
      "      is_better =",
      "          is_better_than<greater>(val, old, idx, idx_arr_[max_arr_len_ - 1]);",
      "    } else {",
      "      is_better = is_better_than<greater>(val, old);",
      "    }",
      "",
      "    if (is_better) {",
      "      old = val;",
      "      idx_arr_[max_arr_len_ - 1] = idx;",
      "    }",
      "",
      "    BitonicMerge<capacity, greater, !greater, T, idxT, is_stable>::merge(",
      "        val_arr_, idx_arr_);",
      "",
      "    set_k_th_();",
      "  }",
      "",
      "  using WarpSort<capacity, greater, T, idxT, is_stable>::max_arr_len_;",
      "  using WarpSort<capacity, greater, T, idxT, is_stable>::val_arr_;",
      "  using WarpSort<capacity, greater, T, idxT, is_stable>::idx_arr_;",
      "  using WarpSort<capacity, greater, T, idxT, is_stable>::lane_;",
      "  using WarpSort<capacity, greater, T, idxT, is_stable>::k_;",
      "  using WarpSort<capacity, greater, T, idxT, is_stable>::dummy_;",
      "",
      "  T* val_smem_;",
      "  idxT* idx_smem_;",
      "  int smem_buf_len_ = 0;",
      "",
      "  T k_th_;",
      "  idxT k_th_idx_;",
      "  int const k_th_lane_;",
      "};  // end class WarpSelect",
      "}  // namespace warp_topk",
      "",
      "template <typename T_OUT, typename T_IN>",
      "__device__ inline T_OUT cuda_cast(T_IN val) {",
      "  return val;",
      "}",
      "",
      "template <>",
      "__device__ inline float cuda_cast<float, __nv_bfloat16>(__nv_bfloat16 val) {",
      "  return __bfloat162float(val);",
      "}",
      "",
      "template <typename T>",
      "__device__ void topk_with_k2(T* output, T const* input,",
      "                             cg::thread_block_tile<32> const& tile,",
      "                             int32_t const lane_id,",
      "                             int const num_experts_per_group) {",
      "  // Get the top2 per thread",
      "  T largest = -INFINITY;",
      "  T second_largest = -INFINITY;",
      "",
      "  if (num_experts_per_group > WARP_SIZE) {",
      "    for (int i = lane_id; i < num_experts_per_group; i += WARP_SIZE) {",
      "      T value = input[i];",
      "      if (value > largest) {",
      "        second_largest = largest;",
      "        largest = value;",
      "      } else if (value > second_largest) {",
      "        second_largest = value;",
      "      }",
      "    }",
      "  } else {",
      "    for (int i = lane_id; i < num_experts_per_group; i += WARP_SIZE) {",
      "      largest = input[i];",
      "    }",
      "  }",
      "",
      "  __syncwarp();  // Ensure all threads have valid data before reduction",
      "  // Get the top2 warpwise",
      "  T max1 = cg::reduce(tile, largest, cg::greater<T>());",
      "",
      "  T max2 = max1;",
      "  bool equal_to_max1 = (max1 == largest);",
      "",
      "  int count_max1 = __popc(__ballot_sync(FULL_WARP_MASK, equal_to_max1));",
      "",
      "  if (count_max1 == 1) {",
      "    largest = (largest == max1) ? second_largest : largest;",
      "    max2 = cg::reduce(tile, largest, cg::greater<T>());",
      "  }",
      "",
      "  if (lane_id == 0) {",
      "    *output = max1 + max2;",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__global__ void topk_with_k2_kernel(T* output, T* input,",
      "                                    int64_t const num_tokens,",
      "                                    int64_t const num_cases,",
      "                                    int64_t const n_group,",
      "                                    int64_t const num_experts_per_group) {",
      "  int32_t warp_id = threadIdx.x / WARP_SIZE;",
      "  int32_t lane_id = threadIdx.x % WARP_SIZE;",
      "",
      "  int32_t case_id = blockIdx.x * NUM_WARPS_PER_BLOCK + warp_id;",
      "  if (case_id < num_cases) {",
      "    input += case_id * num_experts_per_group;",
      "    output += case_id;",
      "",
      "    cg::thread_block block = cg::this_thread_block();",
      "    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(block);",
      "",
      "#if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 900))",
      "    asm volatile(\"griddepcontrol.wait;\");",
      "#endif",
      "    topk_with_k2(output, input, tile, lane_id, num_experts_per_group);",
      "  }",
      "#if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 900))",
      "  asm volatile(\"griddepcontrol.launch_dependents;\");",
      "#endif",
      "}",
      "",
      "template <typename T, typename IdxT>",
      "__global__ void group_idx_and_topk_idx_kernel(",
      "    T* scores, T const* group_scores, T* topk_values, IdxT* topk_indices,",
      "    T* scores_with_bias, int64_t const num_tokens, int64_t const n_group,",
      "    int64_t const topk_group, int64_t const topk, int64_t const num_experts,",
      "    int64_t const num_experts_per_group, bool renormalize,",
      "    double routed_scaling_factor) {",
      "  int32_t warp_id = threadIdx.x / WARP_SIZE;",
      "  int32_t lane_id = threadIdx.x % WARP_SIZE;",
      "  int32_t case_id =",
      "      blockIdx.x * NUM_WARPS_PER_BLOCK + warp_id;  // one per token",
      "  scores_with_bias += case_id * num_experts;",
      "  scores += case_id * num_experts;",
      "  group_scores += case_id * n_group;",
      "  topk_values += case_id * topk;",
      "  topk_indices += case_id * topk;",
      "",
      "  int32_t align_num_experts_per_group =",
      "      warp_topk::round_up_to_multiple_of<WARP_SIZE>(num_experts_per_group);",
      "",
      "  cg::thread_block block = cg::this_thread_block();",
      "  cg::thread_block_tile<32> tile = cg::tiled_partition<32>(block);",
      "",
      "  extern __shared__ char smem_buf[];  // NOTE: reuse the shared memory here to",
      "                                      // store the target topk idx",
      "  int32_t* s_topk_idx = reinterpret_cast<int32_t*>(smem_buf);",
      "  T* s_topk_value =",
      "      reinterpret_cast<T*>(s_topk_idx + NUM_WARPS_PER_BLOCK * topk) +",
      "      warp_id * topk;",
      "  s_topk_idx += warp_id * topk;",
      "",
      "  T value = kNegInfinity;",
      "  T topk_group_value = kNegInfinity;",
      "  int32_t num_equalto_topkth_group;",
      "",
      "#if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 900))",
      "  asm volatile(\"griddepcontrol.wait;\");  // I think all prolog can be put before",
      "                                         // acqbulk because it's ptr arithmetic",
      "#endif",
      "",
      "  if (case_id < num_tokens) {",
      "    // calculate group_idx",
      "    int32_t target_num_min = WARP_SIZE - n_group + topk_group;",
      "    if (lane_id < n_group &&",
      "        (isfinite(cuda_cast<float, T>(",
      "            group_scores[lane_id]))))  // The check is necessary to avoid",
      "                                       // abnormal input",
      "    {",
      "      value = group_scores[lane_id];",
      "    }",
      "",
      "    int count_equal_to_top_value = WARP_SIZE - n_group;",
      "    int pre_count_equal_to_top_value = 0;",
      "    // Use loop to find the largset top_group",
      "    while (count_equal_to_top_value < target_num_min) {",
      "      __syncwarp();  // Ensure all threads have valid data before reduction",
      "      topk_group_value = cg::reduce(tile, value, cg::greater<T>());",
      "      if (value == topk_group_value) {",
      "        value = kNegInfinity;",
      "      }",
      "      pre_count_equal_to_top_value = count_equal_to_top_value;",
      "      count_equal_to_top_value = __popc(__ballot_sync(",
      "          FULL_WARP_MASK, (value == cuda_cast<T, float>(kNegInfinity))));",
      "    }",
      "    num_equalto_topkth_group = target_num_min - pre_count_equal_to_top_value;",
      "  }",
      "  __syncthreads();",
      "",
      "  warp_topk::WarpSelect</*capability*/ WARP_SIZE, /*greater*/ true, T, int32_t,",
      "                        /* is_stable */ true>",
      "      queue((int32_t)topk, -INFINITY);",
      "",
      "  int count_equalto_topkth_group = 0;",
      "  bool if_proceed_next_topk =",
      "      (topk_group_value != cuda_cast<T, float>(kNegInfinity));",
      "  if (case_id < num_tokens && if_proceed_next_topk) {",
      "    for (int i_group = 0; i_group < n_group; i_group++) {",
      "      if ((group_scores[i_group] > topk_group_value) ||",
      "          ((group_scores[i_group] == topk_group_value) &&",
      "           (count_equalto_topkth_group < num_equalto_topkth_group))) {",
      "        int32_t offset = i_group * num_experts_per_group;",
      "        for (int32_t i = lane_id; i < align_num_experts_per_group;",
      "             i += WARP_SIZE) {",
      "          T candidates =",
      "              (i < num_experts_per_group) && isfinite(cuda_cast<float, T>(",
      "                                                 scores_with_bias[offset + i]))",
      "                  ? scores_with_bias[offset + i]",
      "                  : cuda_cast<T, float>(kNegInfinity);",
      "          queue.add(candidates, offset + i);",
      "        }",
      "        if (group_scores[i_group] == topk_group_value) {",
      "          count_equalto_topkth_group++;",
      "        }",
      "      }",
      "    }",
      "    queue.done();",
      "    __syncwarp();",
      "    // Get the topk_idx",
      "    queue.dumpIdx(s_topk_idx);",
      "    __syncwarp();",
      "  }",
      "",
      "  // Load the valid score value",
      "  // Calculate the summation",
      "  float topk_sum = 1e-20;",
      "  if (case_id < num_tokens && if_proceed_next_topk) {",
      "    for (int i = lane_id;",
      "         i < warp_topk::round_up_to_multiple_of<WARP_SIZE>(topk);",
      "         i += WARP_SIZE) {",
      "      T value =",
      "          i < topk",
      "              ? scores[s_topk_idx[i]]",
      "              : cuda_cast<T, float>(0.0f);  // Load the valid value of expert",
      "      if (i < topk) {",
      "        s_topk_value[i] = value;",
      "      }",
      "      topk_sum += reduce(tile, cuda_cast<float, T>(value), cg::plus<float>());",
      "    }",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  if (case_id < num_tokens) {",
      "    if (if_proceed_next_topk) {",
      "      for (int i = lane_id; i < topk; i += WARP_SIZE) {",
      "        float value;",
      "        if (renormalize) {",
      "          value = cuda_cast<float, T>(s_topk_value[i]) / topk_sum *",
      "                  routed_scaling_factor;",
      "        } else {",
      "          value = cuda_cast<float, T>(s_topk_value[i]) * routed_scaling_factor;",
      "        }",
      "        topk_indices[i] = s_topk_idx[i];",
      "        topk_values[i] = cuda_cast<T, float>(value);",
      "      }",
      "    } else {",
      "      for (int i = lane_id; i < topk; i += WARP_SIZE) {",
      "        topk_indices[i] = i;",
      "        topk_values[i] = cuda_cast<T, float>(1.0f / topk);",
      "      }",
      "    }",
      "    // Note: when if_proceed_next_topk==false, choose the first 8 experts as the",
      "    // default result.",
      "  }",
      "#if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 900))",
      "  asm volatile(\"griddepcontrol.launch_dependents;\");",
      "#endif",
      "}",
      "",
      "template <typename T, typename IdxT>",
      "void invokeNoAuxTc(T* scores, T* group_scores, T* topk_values,",
      "                   IdxT* topk_indices, T* scores_with_bias,",
      "                   int64_t const num_tokens, int64_t const num_experts,",
      "                   int64_t const n_group, int64_t const topk_group,",
      "                   int64_t const topk, bool const renormalize,",
      "                   double const routed_scaling_factor, bool enable_pdl = false,",
      "                   cudaStream_t const stream = 0) {",
      "  int64_t num_cases = num_tokens * n_group;",
      "  int64_t topk_with_k2_num_blocks = (num_cases - 1) / NUM_WARPS_PER_BLOCK + 1;",
      "  auto* kernel_instance1 = &topk_with_k2_kernel<T>;",
      "  cudaLaunchConfig_t config;",
      "  config.gridDim = topk_with_k2_num_blocks;",
      "  config.blockDim = BLOCK_SIZE;",
      "  config.dynamicSmemBytes = 0;",
      "  config.stream = stream;",
      "  cudaLaunchAttribute attrs[1];",
      "  attrs[0].id = cudaLaunchAttributeProgrammaticStreamSerialization;",
      "  attrs[0].val.programmaticStreamSerializationAllowed = enable_pdl;",
      "  config.numAttrs = 1;",
      "  config.attrs = attrs;",
      "  cudaLaunchKernelEx(&config, kernel_instance1, group_scores, scores_with_bias,",
      "                     num_tokens, num_cases, n_group, num_experts / n_group);",
      "",
      "  int64_t topk_with_k_group_num_blocks =",
      "      (num_tokens - 1) / NUM_WARPS_PER_BLOCK + 1;",
      "  size_t dynamic_smem_in_bytes =",
      "      warp_topk::calc_smem_size_for_block_wide<T, int32_t>(NUM_WARPS_PER_BLOCK,",
      "                                                           topk);",
      "  auto* kernel_instance2 = &group_idx_and_topk_idx_kernel<T, IdxT>;",
      "  config.gridDim = topk_with_k_group_num_blocks;",
      "  config.blockDim = BLOCK_SIZE;",
      "  config.dynamicSmemBytes = dynamic_smem_in_bytes;",
      "  config.stream = stream;",
      "  attrs[0].id = cudaLaunchAttributeProgrammaticStreamSerialization;",
      "  attrs[0].val.programmaticStreamSerializationAllowed = enable_pdl;",
      "  config.numAttrs = 1;",
      "  config.attrs = attrs;",
      "  cudaLaunchKernelEx(&config, kernel_instance2, scores, group_scores,",
      "                     topk_values, topk_indices, scores_with_bias, num_tokens,",
      "                     n_group, topk_group, topk, num_experts,",
      "                     num_experts / n_group, renormalize, routed_scaling_factor);",
      "}",
      "",
      "#define INSTANTIATE_NOAUX_TC(T, IdxT)                                       \\",
      "  template void invokeNoAuxTc<T, IdxT>(                                     \\",
      "      T * scores, T * group_scores, T * topk_values, IdxT * topk_indices,   \\",
      "      T * scores_with_bias, int64_t const num_tokens,                       \\",
      "      int64_t const num_experts, int64_t const n_group,                     \\",
      "      int64_t const topk_group, int64_t const topk, bool const renormalize, \\",
      "      double const routed_scaling_factor, bool enable_pdl,                  \\",
      "      cudaStream_t const stream);",
      "",
      "INSTANTIATE_NOAUX_TC(float, int32_t);",
      "INSTANTIATE_NOAUX_TC(half, int32_t);",
      "INSTANTIATE_NOAUX_TC(__nv_bfloat16, int32_t);",
      "}  // end namespace moe",
      "}  // namespace vllm",
      "",
      "std::tuple<torch::Tensor, torch::Tensor> grouped_topk(",
      "    torch::Tensor const& scores, torch::Tensor const& scores_with_bias,",
      "    int64_t n_group, int64_t topk_group, int64_t topk, bool renormalize,",
      "    double routed_scaling_factor) {",
      "  auto data_type = scores_with_bias.scalar_type();",
      "  auto input_size = scores_with_bias.sizes();",
      "  int64_t num_tokens = input_size[0];",
      "  int64_t num_experts = input_size[1];",
      "  TORCH_CHECK(input_size.size() == 2, \"scores_with_bias must be a 2D Tensor\");",
      "  TORCH_CHECK(num_experts % n_group == 0,",
      "              \"num_experts should be divisible by n_group\");",
      "  TORCH_CHECK(n_group <= 32,",
      "              \"n_group should be smaller than or equal to 32 for now\");",
      "  TORCH_CHECK(topk <= 32, \"topk should be smaller than or equal to 32 for now\");",
      "",
      "  torch::Tensor group_scores = torch::empty(",
      "      {num_tokens, n_group}, torch::dtype(data_type).device(torch::kCUDA));",
      "  torch::Tensor topk_values = torch::empty(",
      "      {num_tokens, topk}, torch::dtype(data_type).device(torch::kCUDA));",
      "  torch::Tensor topk_indices = torch::empty(",
      "      {num_tokens, topk}, torch::dtype(torch::kInt32).device(torch::kCUDA));",
      "",
      "  auto stream = c10::cuda::getCurrentCUDAStream(scores_with_bias.get_device());",
      "",
      "  switch (data_type) {",
      "    case torch::kFloat16:",
      "      // Handle Float16",
      "      vllm::moe::invokeNoAuxTc<half, int32_t>(",
      "          reinterpret_cast<half*>(scores.mutable_data_ptr()),",
      "          reinterpret_cast<half*>(group_scores.mutable_data_ptr()),",
      "          reinterpret_cast<half*>(topk_values.mutable_data_ptr()),",
      "          reinterpret_cast<int32_t*>(topk_indices.mutable_data_ptr()),",
      "          reinterpret_cast<half*>(scores_with_bias.data_ptr()), num_tokens,",
      "          num_experts, n_group, topk_group, topk, renormalize,",
      "          routed_scaling_factor, false, stream);",
      "      break;",
      "    case torch::kFloat32:",
      "      // Handle Float32",
      "      vllm::moe::invokeNoAuxTc<float, int32_t>(",
      "          reinterpret_cast<float*>(scores.mutable_data_ptr()),",
      "          reinterpret_cast<float*>(group_scores.mutable_data_ptr()),",
      "          reinterpret_cast<float*>(topk_values.mutable_data_ptr()),",
      "          reinterpret_cast<int32_t*>(topk_indices.mutable_data_ptr()),",
      "          reinterpret_cast<float*>(scores_with_bias.data_ptr()), num_tokens,",
      "          num_experts, n_group, topk_group, topk, renormalize,",
      "          routed_scaling_factor, false, stream);",
      "      break;",
      "    case torch::kBFloat16:",
      "      // Handle BFloat16",
      "      vllm::moe::invokeNoAuxTc<__nv_bfloat16, int32_t>(",
      "          reinterpret_cast<__nv_bfloat16*>(scores.mutable_data_ptr()),",
      "          reinterpret_cast<__nv_bfloat16*>(group_scores.mutable_data_ptr()),",
      "          reinterpret_cast<__nv_bfloat16*>(topk_values.mutable_data_ptr()),",
      "          reinterpret_cast<int32_t*>(topk_indices.mutable_data_ptr()),",
      "          reinterpret_cast<__nv_bfloat16*>(scores_with_bias.data_ptr()),",
      "          num_tokens, num_experts, n_group, topk_group, topk, renormalize,",
      "          routed_scaling_factor, false, stream);",
      "      break;",
      "    default:",
      "      // Handle other data types",
      "      throw std::invalid_argument(",
      "          \"Invalid dtype, only supports float16, float32, and bfloat16\");",
      "      break;",
      "  }",
      "  return {topk_values, topk_indices};",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/moe/moe_wna16.cu",
    "source": [
      "",
      "#include <torch/all.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <cuda_runtime.h>",
      "",
      "#include <cuda_fp16.h>",
      "#include <cuda_bf16.h>",
      "#include \"moe_wna16_utils.h\"",
      "",
      "#define DIVIDE(x, size) (((x) + (size) - 1) / (size))",
      "",
      "template <typename scalar_t, int bit, int GROUPS>",
      "__global__ void moe_wna16_gemm_kernel(",
      "    const scalar_t* __restrict__ input, scalar_t* __restrict__ output,",
      "    const uint32_t* __restrict__ qweight, const scalar_t* __restrict__ scales,",
      "    const uint32_t* __restrict__ qzeros,",
      "",
      "    const float* __restrict__ topk_weights,",
      "    const int32_t* __restrict__ sorted_token_ids,",
      "    const int32_t* __restrict__ expert_ids,",
      "    const int32_t* __restrict__ num_tokens_post_pad,",
      "",
      "    uint16_t num_experts, uint16_t group_size, uint16_t top_k, uint32_t size_m,",
      "    uint32_t size_n, uint32_t size_k, uint16_t BLOCK_SIZE_M,",
      "    uint16_t BLOCK_SIZE_N, uint16_t BLOCK_SIZE_K, bool has_zp,",
      "    bool mul_topk_weight) {",
      "#if !defined(__CUDA_ARCH__) || __CUDA_ARCH__ < 800",
      "  if constexpr (std::is_same<scalar_t, nv_bfloat16>::value) {",
      "    return;",
      "  } else {",
      "#endif",
      "",
      "    using Dtype = ScalarType<scalar_t>;",
      "    using scalar_t2 = typename ScalarType<scalar_t>::scalar_t2;",
      "",
      "    if (blockIdx.x * BLOCK_SIZE_M >= num_tokens_post_pad[0]) return;",
      "",
      "    const int32_t offset_n = blockIdx.y * BLOCK_SIZE_N + threadIdx.x;",
      "    const int32_t offset_k = blockIdx.z * BLOCK_SIZE_K;",
      "",
      "    const int32_t expert_id = expert_ids[blockIdx.x];",
      "",
      "    int32_t num_valid_tokens = 0;",
      "    extern __shared__ uint16_t block_input_tmp[];",
      "    scalar_t* block_input = reinterpret_cast<scalar_t*>(block_input_tmp);",
      "    scalar_t2* block_input_half2 = reinterpret_cast<scalar_t2*>(block_input);",
      "",
      "    // load BLOCK_SIZE_M * BLOCK_SIZE_K into shared memory",
      "    for (int m = 0; m < BLOCK_SIZE_M; m++) {",
      "      const int32_t offset_m = blockIdx.x * BLOCK_SIZE_M + m;",
      "      const int32_t token_index = sorted_token_ids[offset_m];",
      "      if (token_index / top_k >= size_m) break;",
      "",
      "      num_valid_tokens = m + 1;",
      "",
      "      if (expert_id != -1) {",
      "        int k_per_thread = DIVIDE(BLOCK_SIZE_K, BLOCK_SIZE_N);",
      "        for (int i = 0; i < k_per_thread; i++) {",
      "          int k = BLOCK_SIZE_N * i + threadIdx.x;",
      "          if (k >= BLOCK_SIZE_K) break;",
      "          if (offset_k + k >= size_k) break;",
      "",
      "          // load input to shared memory",
      "          // use a special layout to fit the layout of dequanted-weight",
      "          int origin_k;",
      "          if constexpr (bit == 4) {",
      "            // [0, 4, 1, 5, 2, 6, 3, 7]",
      "            int8_t order = (threadIdx.x % 2) * 4 + ((threadIdx.x % 8) / 2);",
      "            origin_k = BLOCK_SIZE_N * i + threadIdx.x / 8 * 8 + order;",
      "          } else {",
      "            // [0, 2, 1, 3]",
      "            int8_t order = (threadIdx.x % 2) * 2 + ((threadIdx.x % 4) / 2);",
      "            origin_k = BLOCK_SIZE_N * i + threadIdx.x / 4 * 4 + order;",
      "          }",
      "",
      "          origin_k += token_index / top_k * size_k + blockIdx.z * BLOCK_SIZE_K;",
      "          block_input[m * BLOCK_SIZE_K + k] = input[origin_k];",
      "        }",
      "      }",
      "    }",
      "",
      "    if (expert_id == -1) return;",
      "    __syncthreads();",
      "    if (threadIdx.x >= BLOCK_SIZE_N || offset_n >= size_n) return;",
      "",
      "    float res[64];  // assume BLOCK_SIZE_M <= 64",
      "    scalar_t2 res2;",
      "    scalar_t2 scale_f2;",
      "    scalar_t2 qzero_f2;",
      "",
      "    // note that (size_n * size_k * expert_id) may greater than 2 ** 31",
      "    constexpr int8_t pack_factor = 32 / bit;",
      "    const uint64_t expert_offset = ((uint64_t)size_n) * size_k * expert_id;",
      "    const uint32_t* expert_qweight = qweight + expert_offset / pack_factor;",
      "    const scalar_t* expert_scales = scales + expert_offset / group_size;",
      "    const uint32_t* expert_qzeros =",
      "        qzeros + expert_offset / group_size / pack_factor;",
      "",
      "    // load 4*int32 one time: 4 int32 = 128 bit = 1 float4",
      "    // weight would be loaded in loop",
      "    uint32_t expert_qweight_tmp[4];",
      "    float4* expert_qweight_tmp_float4 =",
      "        reinterpret_cast<float4*>(expert_qweight_tmp);",
      "",
      "    // load all required scales one time",
      "    scalar_t expert_scales_groups[GROUPS];",
      "    int scales_offset_tmp =",
      "        (offset_n * size_k + offset_k) / group_size / GROUPS;",
      "    if constexpr (GROUPS == 1) {",
      "      *expert_scales_groups = expert_scales[scales_offset_tmp];",
      "    } else if constexpr (GROUPS == 2) {",
      "      float* expert_scales_groups_tmp =",
      "          reinterpret_cast<float*>(expert_scales_groups);",
      "      *expert_scales_groups_tmp =",
      "          reinterpret_cast<const float*>(expert_scales)[scales_offset_tmp];",
      "    } else if constexpr (GROUPS == 4) {",
      "      float2* expert_scales_groups_tmp =",
      "          reinterpret_cast<float2*>(expert_scales_groups);",
      "      *expert_scales_groups_tmp =",
      "          reinterpret_cast<const float2*>(expert_scales)[scales_offset_tmp];",
      "    } else if constexpr (GROUPS == 8) {",
      "      float4* expert_scales_groups_tmp =",
      "          reinterpret_cast<float4*>(expert_scales_groups);",
      "      *expert_scales_groups_tmp =",
      "          reinterpret_cast<const float4*>(expert_scales)[scales_offset_tmp];",
      "    }",
      "",
      "    // load all required qzeros one time",
      "    uint8_t expert_qzeros_groups[GROUPS];",
      "    if (!has_zp) {",
      "      if constexpr (bit == 4) {",
      "        qzero_f2 = Dtype::num2num2(Dtype::int2num(8));",
      "      } else {",
      "        qzero_f2 = Dtype::num2num2(Dtype::int2num(128));",
      "      }",
      "    } else {",
      "      int qzeros_offset_tmp =",
      "          (offset_n / (8 / bit)) * (size_k / group_size / GROUPS) +",
      "          offset_k / group_size / GROUPS;",
      "      if constexpr (GROUPS == 1) {",
      "        uint8_t* expert_qzeros_groups_tmp =",
      "            reinterpret_cast<uint8_t*>(expert_qzeros_groups);",
      "        *expert_qzeros_groups_tmp =",
      "            reinterpret_cast<const uint8_t*>(expert_qzeros)[qzeros_offset_tmp];",
      "      } else if constexpr (GROUPS == 2) {",
      "        uint16_t* expert_qzeros_groups_tmp =",
      "            reinterpret_cast<uint16_t*>(expert_qzeros_groups);",
      "        *expert_qzeros_groups_tmp =",
      "            reinterpret_cast<const uint16_t*>(expert_qzeros)[qzeros_offset_tmp];",
      "      } else if constexpr (GROUPS == 4) {",
      "        uint32_t* expert_qzeros_groups_tmp =",
      "            reinterpret_cast<uint32_t*>(expert_qzeros_groups);",
      "        *expert_qzeros_groups_tmp =",
      "            reinterpret_cast<const uint32_t*>(expert_qzeros)[qzeros_offset_tmp];",
      "      } else if constexpr (GROUPS == 8) {",
      "        uint64_t* expert_qzeros_groups_tmp =",
      "            reinterpret_cast<uint64_t*>(expert_qzeros_groups);",
      "        *expert_qzeros_groups_tmp =",
      "            reinterpret_cast<const uint64_t*>(expert_qzeros)[qzeros_offset_tmp];",
      "      }",
      "    }",
      "",
      "    for (int tmp_k = 0; tmp_k < BLOCK_SIZE_K / pack_factor; tmp_k++) {",
      "      int k = offset_k + tmp_k * pack_factor;",
      "      if (k >= size_k) break;",
      "      const int32_t weight_offset = offset_n * size_k + k;",
      "",
      "      if (tmp_k % 4 == 0) {",
      "        *expert_qweight_tmp_float4 = reinterpret_cast<const float4*>(",
      "            expert_qweight)[weight_offset / pack_factor / 4];",
      "      }",
      "",
      "      if (tmp_k % (group_size / pack_factor) == 0) {",
      "        scalar_t scale_f =",
      "            expert_scales_groups[tmp_k / (group_size / pack_factor)];",
      "        scale_f2 = Dtype::num2num2(scale_f);",
      "",
      "        if (has_zp) {",
      "          uint8_t qzero =",
      "              expert_qzeros_groups[tmp_k / (group_size / pack_factor)];",
      "          if constexpr (bit == 4) {",
      "            qzero = (qzero >> ((threadIdx.x % 2) * 4)) & 0xF;",
      "          }",
      "          qzero_f2 = Dtype::num2num2(Dtype::int2num(qzero));",
      "        }",
      "      }",
      "",
      "      scalar_t2 weight_half2[16 / bit];",
      "      dequant<scalar_t2, bit>(expert_qweight_tmp[tmp_k % 4], weight_half2);",
      "",
      "      for (int m = 0; m < num_valid_tokens; m++) {",
      "        res2 = {};",
      "",
      "#pragma unroll",
      "        for (int i = 0; i < 16 / bit; i++) {",
      "          int32_t offset_input = m * BLOCK_SIZE_K / 2 + tmp_k * (16 / bit) + i;",
      "          res2 = __hfma2(__hmul2(__hsub2(weight_half2[i], qzero_f2), scale_f2),",
      "                         block_input_half2[offset_input], res2);",
      "        }",
      "",
      "        if (tmp_k == 0) {",
      "          res[m] = Dtype::num2float(res2.x) + Dtype::num2float(res2.y);",
      "        } else {",
      "          res[m] += Dtype::num2float(res2.x) + Dtype::num2float(res2.y);",
      "        }",
      "      }",
      "    }",
      "",
      "    for (int m = 0; m < num_valid_tokens; ++m) {",
      "      const int32_t token_index =",
      "          sorted_token_ids[blockIdx.x * BLOCK_SIZE_M + m];",
      "      if (mul_topk_weight) {",
      "        res[m] *= topk_weights[token_index];",
      "      }",
      "      atomicAdd(&output[token_index * size_n + offset_n],",
      "                Dtype::float2num(res[m]));",
      "    }",
      "",
      "#if !defined(__CUDA_ARCH__) || __CUDA_ARCH__ < 800",
      "  }",
      "#endif",
      "}",
      "",
      "template <typename scalar_t>",
      "void run_moe_wna16_gemm(const scalar_t* input, scalar_t* output,",
      "                        const uint32_t* b_qweight, const scalar_t* b_scales,",
      "                        const uint32_t* b_qzeros, const float* topk_weights,",
      "                        const int32_t* sorted_token_ids,",
      "                        const int32_t* expert_ids,",
      "                        const int32_t* num_tokens_post_pad, int num_experts,",
      "                        int group_size, int num_token_blocks, int top_k,",
      "                        int size_m, int size_n, int size_k, int BLOCK_SIZE_M,",
      "                        int BLOCK_SIZE_N, int BLOCK_SIZE_K, int bit,",
      "                        bool has_zp, bool mul_topk_weight) {",
      "  dim3 blockDim, gridDim;",
      "  blockDim.x = BLOCK_SIZE_N;",
      "  blockDim.y = 1;",
      "  blockDim.z = 1;",
      "  gridDim.x = num_token_blocks;",
      "  gridDim.y = DIVIDE(size_n, BLOCK_SIZE_N);",
      "  gridDim.z = DIVIDE(size_k, BLOCK_SIZE_K);",
      "",
      "  auto kernel = moe_wna16_gemm_kernel<scalar_t, 4, 1>;",
      "  if (bit == 4) {",
      "    if (BLOCK_SIZE_K / group_size == 2) {",
      "      kernel = moe_wna16_gemm_kernel<scalar_t, 4, 2>;",
      "    } else if (BLOCK_SIZE_K / group_size == 4) {",
      "      kernel = moe_wna16_gemm_kernel<scalar_t, 4, 4>;",
      "    } else if (BLOCK_SIZE_K / group_size == 8) {",
      "      kernel = moe_wna16_gemm_kernel<scalar_t, 4, 8>;",
      "    }",
      "  } else {",
      "    if (BLOCK_SIZE_K / group_size == 1) {",
      "      kernel = moe_wna16_gemm_kernel<scalar_t, 8, 1>;",
      "    } else if (BLOCK_SIZE_K / group_size == 2) {",
      "      kernel = moe_wna16_gemm_kernel<scalar_t, 8, 2>;",
      "    } else if (BLOCK_SIZE_K / group_size == 4) {",
      "      kernel = moe_wna16_gemm_kernel<scalar_t, 8, 4>;",
      "    } else if (BLOCK_SIZE_K / group_size == 8) {",
      "      kernel = moe_wna16_gemm_kernel<scalar_t, 8, 8>;",
      "    }",
      "  }",
      "",
      "  const int shared_mem_size = BLOCK_SIZE_M * BLOCK_SIZE_K * 2;",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  kernel<<<gridDim, blockDim, shared_mem_size, stream>>>(",
      "      input, output, b_qweight, b_scales, b_qzeros, topk_weights,",
      "      sorted_token_ids, expert_ids, num_tokens_post_pad, num_experts,",
      "      group_size, top_k, size_m, size_n, size_k, BLOCK_SIZE_M, BLOCK_SIZE_N,",
      "      BLOCK_SIZE_K, has_zp, mul_topk_weight);",
      "}",
      "",
      "torch::Tensor moe_wna16_gemm(torch::Tensor input, torch::Tensor output,",
      "                             torch::Tensor b_qweight, torch::Tensor b_scales,",
      "                             std::optional<torch::Tensor> b_qzeros,",
      "                             std::optional<torch::Tensor> topk_weights,",
      "                             torch::Tensor sorted_token_ids,",
      "                             torch::Tensor expert_ids,",
      "                             torch::Tensor num_tokens_post_pad, int64_t top_k,",
      "                             int64_t BLOCK_SIZE_M, int64_t BLOCK_SIZE_N,",
      "                             int64_t BLOCK_SIZE_K, int64_t bit) {",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));",
      "  output.zero_();",
      "",
      "  const int num_experts = b_qweight.size(0);",
      "  const int size_m = input.size(0);",
      "  const int size_n = b_qweight.size(1);",
      "  const int size_k = input.size(1);",
      "  const int group_size = size_k / b_scales.size(2);",
      "",
      "  int64_t EM = sorted_token_ids.size(0);",
      "  if (size_m <= BLOCK_SIZE_M) {",
      "    EM = min(EM, size_m * BLOCK_SIZE_M * top_k);",
      "  }",
      "  const int num_token_blocks = (EM + BLOCK_SIZE_M - 1) / BLOCK_SIZE_M;",
      "",
      "  const uint32_t* b_qzeros_ptr;",
      "  if (b_qzeros.has_value())",
      "    b_qzeros_ptr = (const uint32_t*)b_qzeros.value().data_ptr<uint8_t>();",
      "  const float* topk_weights_ptr = nullptr;",
      "  if (topk_weights.has_value())",
      "    topk_weights_ptr = (const float*)topk_weights.value().data_ptr<float>();",
      "",
      "  int groups_per_block_row = BLOCK_SIZE_K / group_size;",
      "  TORCH_CHECK(bit == 4 || bit == 8, \"bit must be 4 or 8\");",
      "  TORCH_CHECK(size_k % BLOCK_SIZE_K == 0,",
      "              \"size_k must divisible by BLOCK_SIZE_K\");",
      "  TORCH_CHECK(BLOCK_SIZE_K % group_size == 0,",
      "              \"BLOCK_SIZE_K must divisible by group_size\");",
      "  TORCH_CHECK(BLOCK_SIZE_M <= 64, \"BLOCK_SIZE_M must less or equal to 64\");",
      "  TORCH_CHECK(groups_per_block_row == 1 || groups_per_block_row == 2 ||",
      "                  groups_per_block_row == 4 || groups_per_block_row == 8,",
      "              \"BLOCK_SIZE_K // group_size must be one of [1, 2, 4, 8]\");",
      "",
      "  if (input.scalar_type() == at::ScalarType::Half) {",
      "    run_moe_wna16_gemm<half>(",
      "        (const half*)input.data_ptr<at::Half>(),",
      "        (half*)output.data_ptr<at::Half>(),",
      "        (const uint32_t*)b_qweight.data_ptr<uint8_t>(),",
      "        (const half*)b_scales.data_ptr<at::Half>(), b_qzeros_ptr,",
      "        topk_weights_ptr, sorted_token_ids.data_ptr<int32_t>(),",
      "        expert_ids.data_ptr<int32_t>(), num_tokens_post_pad.data_ptr<int32_t>(),",
      "        num_experts, group_size, num_token_blocks, top_k, size_m, size_n,",
      "        size_k, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, bit,",
      "        b_qzeros.has_value(), topk_weights.has_value());",
      "  } else if (input.scalar_type() == at::ScalarType::BFloat16) {",
      "    run_moe_wna16_gemm<nv_bfloat16>(",
      "        (const nv_bfloat16*)input.data_ptr<at::BFloat16>(),",
      "        (nv_bfloat16*)output.data_ptr<at::BFloat16>(),",
      "        (const uint32_t*)b_qweight.data_ptr<uint8_t>(),",
      "        (const nv_bfloat16*)b_scales.data_ptr<at::BFloat16>(), b_qzeros_ptr,",
      "        topk_weights_ptr, sorted_token_ids.data_ptr<int32_t>(),",
      "        expert_ids.data_ptr<int32_t>(), num_tokens_post_pad.data_ptr<int32_t>(),",
      "        num_experts, group_size, num_token_blocks, top_k, size_m, size_n,",
      "        size_k, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, bit,",
      "        b_qzeros.has_value(), topk_weights.has_value());",
      "  } else {",
      "    TORCH_CHECK(false, \"moe_wna16_gemm only supports bfloat16 and float16\");",
      "  }",
      "  return output;",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/moe/topk_softmax_kernels.cu",
    "source": [
      "/*",
      " * Adapted from https://github.com/NVIDIA/TensorRT-LLM/blob/v0.7.1/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.cu",
      " * Copyright (c) 2024, The vLLM team.",
      " * SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.",
      " * SPDX-License-Identifier: Apache-2.0",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " * http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "#include <torch/all.h>",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include \"../cuda_compat.h\"",
      "",
      "#ifndef USE_ROCM",
      "    #include <cub/util_type.cuh>",
      "    #include <cub/cub.cuh>",
      "    #include <cuda/std/functional>",
      "    using AddOp = cuda::std::plus<float>;",
      "#else",
      "    #include <hipcub/util_type.hpp>",
      "    #include <hipcub/hipcub.hpp>",
      "    using AddOp = cub::Sum; ",
      "#endif",
      "",
      "#define MAX(a, b) ((a) > (b) ? (a) : (b))",
      "#define MIN(a, b) ((a) < (b) ? (a) : (b))",
      "",
      "namespace vllm {",
      "namespace moe {",
      "",
      "/// Aligned array type",
      "template <",
      "    typename T,",
      "    /// Number of elements in the array",
      "    int N,",
      "    /// Alignment requirement in bytes",
      "    int Alignment = sizeof(T) * N",
      ">",
      "class alignas(Alignment) AlignedArray {",
      "    float data[N];",
      "};",
      "",
      "// ====================== Softmax things ===============================",
      "// We have our own implementation of softmax here so we can support transposing the output",
      "// in the softmax kernel when we extend this module to support expert-choice routing.",
      "template <int TPB>",
      "__launch_bounds__(TPB) __global__",
      "    void moeSoftmax(const float* input, const bool* finished, float* output, const int num_cols)",
      "{",
      "    using BlockReduce = cub::BlockReduce<float, TPB>;",
      "    __shared__ typename BlockReduce::TempStorage tmpStorage;",
      "",
      "    __shared__ float normalizing_factor;",
      "    __shared__ float float_max;",
      "",
      "    const int thread_row_offset = blockIdx.x * num_cols;",
      "",
      "    float threadData(-FLT_MAX);",
      "",
      "    // Don't touch finished rows.",
      "    if ((finished != nullptr) && finished[blockIdx.x])",
      "    {",
      "        return;",
      "    }",
      "",
      "    for (int ii = threadIdx.x; ii < num_cols; ii += TPB)",
      "    {",
      "        const int idx = thread_row_offset + ii;",
      "        threadData = max(static_cast<float>(input[idx]), threadData);",
      "    }",
      "",
      "    const float maxElem = BlockReduce(tmpStorage).Reduce(threadData, cub::Max());",
      "    if (threadIdx.x == 0)",
      "    {",
      "        float_max = maxElem;",
      "    }",
      "    __syncthreads();",
      "",
      "    threadData = 0;",
      "",
      "    for (int ii = threadIdx.x; ii < num_cols; ii += TPB)",
      "    {",
      "        const int idx = thread_row_offset + ii;",
      "        threadData += exp((static_cast<float>(input[idx]) - float_max));",
      "    }",
      "",
      "    const auto Z = BlockReduce(tmpStorage).Reduce(threadData, AddOp());",
      "",
      "    if (threadIdx.x == 0)",
      "    {",
      "        normalizing_factor = 1.f / Z;",
      "    }",
      "    __syncthreads();",
      "",
      "    for (int ii = threadIdx.x; ii < num_cols; ii += TPB)",
      "    {",
      "        const int idx = thread_row_offset + ii;",
      "        const float val = exp((static_cast<float>(input[idx]) - float_max)) * normalizing_factor;",
      "        output[idx] = val;",
      "    }",
      "}",
      "",
      "template <int TPB, typename IndType>",
      "__launch_bounds__(TPB) __global__ void moeTopK(",
      "    const float* inputs_after_softmax,",
      "    const bool* finished,",
      "    float* output,",
      "    IndType* indices,",
      "    int* source_rows,",
      "    const int num_experts,",
      "    const int k,",
      "    const int start_expert,",
      "    const int end_expert)",
      "{",
      "",
      "    using cub_kvp = cub::KeyValuePair<int, float>;",
      "    using BlockReduce = cub::BlockReduce<cub_kvp, TPB>;",
      "    __shared__ typename BlockReduce::TempStorage tmpStorage;",
      "",
      "    cub_kvp thread_kvp;",
      "    cub::ArgMax arg_max;",
      "",
      "    const int num_rows = gridDim.x;",
      "    const int block_row = blockIdx.x;",
      "",
      "    const bool row_is_active = finished ? !finished[block_row] : true;",
      "    const int thread_read_offset = blockIdx.x * num_experts;",
      "    for (int k_idx = 0; k_idx < k; ++k_idx)",
      "    {",
      "        thread_kvp.key = 0;",
      "        thread_kvp.value = -1.f; // This is OK because inputs are probabilities",
      "",
      "        cub_kvp inp_kvp;",
      "        for (int expert = threadIdx.x; expert < num_experts; expert += TPB)",
      "        {",
      "            const int idx = thread_read_offset + expert;",
      "            inp_kvp.key = expert;",
      "            inp_kvp.value = inputs_after_softmax[idx];",
      "",
      "            for (int prior_k = 0; prior_k < k_idx; ++prior_k)",
      "            {",
      "                const int prior_winning_expert = indices[k * block_row + prior_k];",
      "",
      "                if (prior_winning_expert == expert)",
      "                {",
      "                    inp_kvp = thread_kvp;",
      "                }",
      "            }",
      "",
      "            thread_kvp = arg_max(inp_kvp, thread_kvp);",
      "        }",
      "",
      "        const cub_kvp result_kvp = BlockReduce(tmpStorage).Reduce(thread_kvp, arg_max);",
      "        if (threadIdx.x == 0)",
      "        {",
      "            // Ignore experts the node isn't responsible for with expert parallelism",
      "            const int expert = result_kvp.key;",
      "            const bool node_uses_expert = expert >= start_expert && expert < end_expert;",
      "            const bool should_process_row = row_is_active && node_uses_expert;",
      "",
      "            const int idx = k * block_row + k_idx;",
      "            output[idx] = result_kvp.value;",
      "            indices[idx] = should_process_row ? (expert - start_expert) : num_experts;",
      "            assert(indices[idx] >= 0);",
      "            source_rows[idx] = k_idx * num_rows + block_row;",
      "        }",
      "        __syncthreads();",
      "    }",
      "}",
      "",
      "// ====================== TopK softmax things ===============================",
      "",
      "/*",
      "  A Top-K gating softmax written to exploit when the number of experts in the MoE layers",
      "  are a small power of 2. This allows us to cleanly share the rows among the threads in",
      "  a single warp and eliminate communication between warps (so no need to use shared mem).",
      "",
      "  It fuses the softmax, max and argmax into a single kernel.",
      "",
      "  Limitations:",
      "  1) This implementation is optimized for when the number of experts is a small power of 2.",
      "     Additionally it also supports when number of experts is multiple of 64 which is still",
      "     faster than the computing softmax and topK separately (only tested on CUDA yet).",
      "  2) This implementation assumes k is small, but will work for any k.",
      "*/",
      "",
      "template <int VPT, int NUM_EXPERTS, int WARPS_PER_CTA, int BYTES_PER_LDG, int WARP_SIZE_PARAM, typename IndType>",
      "__launch_bounds__(WARPS_PER_CTA* WARP_SIZE_PARAM) __global__",
      "    void topkGatingSoftmax(const float* input, const bool* finished, float* output, const int num_rows, IndType* indices,",
      "        int* source_rows, const int k, const int start_expert, const int end_expert)",
      "{",
      "    // We begin by enforcing compile time assertions and setting up compile time constants.",
      "    static_assert(BYTES_PER_LDG == (BYTES_PER_LDG & -BYTES_PER_LDG), \"BYTES_PER_LDG must be power of 2\");",
      "    static_assert(BYTES_PER_LDG <= 16, \"BYTES_PER_LDG must be leq 16\");",
      "",
      "    // Number of bytes each thread pulls in per load",
      "    static constexpr int ELTS_PER_LDG = BYTES_PER_LDG / sizeof(float);",
      "    static constexpr int ELTS_PER_ROW = NUM_EXPERTS;",
      "    static constexpr int THREADS_PER_ROW = ELTS_PER_ROW / VPT;",
      "    static constexpr int LDG_PER_THREAD = VPT / ELTS_PER_LDG;",
      "",
      "    // Restrictions based on previous section.",
      "    static_assert(VPT % ELTS_PER_LDG == 0, \"The elements per thread must be a multiple of the elements per ldg\");",
      "    static_assert(WARP_SIZE_PARAM % THREADS_PER_ROW == 0, \"The threads per row must cleanly divide the threads per warp\");",
      "    static_assert(THREADS_PER_ROW == (THREADS_PER_ROW & -THREADS_PER_ROW), \"THREADS_PER_ROW must be power of 2\");",
      "    static_assert(THREADS_PER_ROW <= WARP_SIZE_PARAM, \"THREADS_PER_ROW can be at most warp size\");",
      "",
      "    // We have NUM_EXPERTS elements per row. We specialize for small #experts",
      "    static constexpr int ELTS_PER_WARP = WARP_SIZE_PARAM * VPT;",
      "    static constexpr int ROWS_PER_WARP = ELTS_PER_WARP / ELTS_PER_ROW;",
      "    static constexpr int ROWS_PER_CTA = WARPS_PER_CTA * ROWS_PER_WARP;",
      "",
      "    // Restrictions for previous section.",
      "    static_assert(ELTS_PER_WARP % ELTS_PER_ROW == 0, \"The elts per row must cleanly divide the total elt per warp\");",
      "",
      "    // ===================== From this point, we finally start computing run-time variables. ========================",
      "",
      "    // Compute CTA and warp rows. We pack multiple rows into a single warp, and a block contains WARPS_PER_CTA warps.",
      "    // This, each block processes a chunk of rows. We start by computing the start row for each block.",
      "    const int cta_base_row = blockIdx.x * ROWS_PER_CTA;",
      "",
      "    // Now, using the base row per thread block, we compute the base row per warp.",
      "    const int warp_base_row = cta_base_row + threadIdx.y * ROWS_PER_WARP;",
      "",
      "    // The threads in a warp are split into sub-groups that will work on a row.",
      "    // We compute row offset for each thread sub-group",
      "    const int thread_row_in_warp = threadIdx.x / THREADS_PER_ROW;",
      "    const int thread_row = warp_base_row + thread_row_in_warp;",
      "",
      "    // Threads with indices out of bounds should early exit here.",
      "    if (thread_row >= num_rows)",
      "    {",
      "        return;",
      "    }",
      "    const bool row_is_active = finished ? !finished[thread_row] : true;",
      "",
      "    // We finally start setting up the read pointers for each thread. First, each thread jumps to the start of the",
      "    // row it will read.",
      "    const float* thread_row_ptr = input + thread_row * ELTS_PER_ROW;",
      "",
      "    // Now, we compute the group each thread belong to in order to determine the first column to start loads.",
      "    const int thread_group_idx = threadIdx.x % THREADS_PER_ROW;",
      "    const int first_elt_read_by_thread = thread_group_idx * ELTS_PER_LDG;",
      "    const float* thread_read_ptr = thread_row_ptr + first_elt_read_by_thread;",
      "",
      "    // Determine the pointer type to use to read in the data depending on the BYTES_PER_LDG template param. In theory,",
      "    // this can support all powers of 2 up to 16.",
      "    // NOTE(woosuk): The original implementation uses CUTLASS aligned array here.",
      "    // We defined our own aligned array and use it here to avoid the dependency on CUTLASS.",
      "    using AccessType = AlignedArray<float, ELTS_PER_LDG>;",
      "",
      "    // Finally, we pull in the data from global mem",
      "    float row_chunk[VPT];",
      "    AccessType* row_chunk_vec_ptr = reinterpret_cast<AccessType*>(&row_chunk);",
      "    const AccessType* vec_thread_read_ptr = reinterpret_cast<const AccessType*>(thread_read_ptr);",
      "#pragma unroll",
      "    for (int ii = 0; ii < LDG_PER_THREAD; ++ii)",
      "    {",
      "        row_chunk_vec_ptr[ii] = vec_thread_read_ptr[ii * THREADS_PER_ROW];",
      "    }",
      "",
      "    // First, we perform a max reduce within the thread. We can do the max in fp16 safely (I think) and just",
      "    // convert to float afterwards for the exp + sum reduction.",
      "    float thread_max = row_chunk[0];",
      "#pragma unroll",
      "    for (int ii = 1; ii < VPT; ++ii)",
      "    {",
      "        thread_max = max(thread_max, row_chunk[ii]);",
      "    }",
      "",
      "// Now, we find the max within the thread group and distribute among the threads. We use a butterfly reduce.",
      "#pragma unroll",
      "    for (int mask = THREADS_PER_ROW / 2; mask > 0; mask /= 2)",
      "    {",
      "        thread_max = max(thread_max, VLLM_SHFL_XOR_SYNC_WIDTH(thread_max, mask, THREADS_PER_ROW));",
      "    }",
      "",
      "    // From this point, thread max in all the threads have the max within the row.",
      "    // Now, we subtract the max from each element in the thread and take the exp. We also compute the thread local sum.",
      "    float row_sum = 0;",
      "#pragma unroll",
      "    for (int ii = 0; ii < VPT; ++ii)",
      "    {",
      "        row_chunk[ii] = expf(row_chunk[ii] - thread_max);",
      "        row_sum += row_chunk[ii];",
      "    }",
      "",
      "// Now, we perform the sum reduce within each thread group. Similar to the max reduce, we use a bufferfly pattern.",
      "#pragma unroll",
      "    for (int mask = THREADS_PER_ROW / 2; mask > 0; mask /= 2)",
      "    {",
      "        row_sum += VLLM_SHFL_XOR_SYNC_WIDTH(row_sum, mask, THREADS_PER_ROW);",
      "    }",
      "",
      "    // From this point, all threads have the max and the sum for their rows in the thread_max and thread_sum variables",
      "    // respectively. Finally, we can scale the rows for the softmax. Technically, for top-k gating we don't need to",
      "    // compute the entire softmax row. We can likely look at the maxes and only compute for the top-k values in the row.",
      "    // However, this kernel will likely not be a bottle neck and it seems better to closer match torch and find the",
      "    // argmax after computing the softmax.",
      "    const float reciprocal_row_sum = 1.f / row_sum;",
      "",
      "#pragma unroll",
      "    for (int ii = 0; ii < VPT; ++ii)",
      "    {",
      "        row_chunk[ii] = row_chunk[ii] * reciprocal_row_sum;",
      "    }",
      "",
      "    // Now, softmax_res contains the softmax of the row chunk. Now, I want to find the topk elements in each row, along",
      "    // with the max index.",
      "    int start_col = first_elt_read_by_thread;",
      "    static constexpr int COLS_PER_GROUP_LDG = ELTS_PER_LDG * THREADS_PER_ROW;",
      "",
      "    for (int k_idx = 0; k_idx < k; ++k_idx)",
      "    {",
      "        // First, each thread does the local argmax",
      "        float max_val = row_chunk[0];",
      "        int expert = start_col;",
      "#pragma unroll",
      "        for (int ldg = 0, col = start_col; ldg < LDG_PER_THREAD; ++ldg, col += COLS_PER_GROUP_LDG)",
      "        {",
      "#pragma unroll",
      "            for (int ii = 0; ii < ELTS_PER_LDG; ++ii)",
      "            {",
      "                float val = row_chunk[ldg * ELTS_PER_LDG + ii];",
      "",
      "                // No check on the experts here since columns with the smallest index are processed first and only",
      "                // updated if > (not >=)",
      "                if (val > max_val)",
      "                {",
      "                    max_val = val;",
      "                    expert = col + ii;",
      "                }",
      "            }",
      "        }",
      "",
      "// Now, we perform the argmax reduce. We use the butterfly pattern so threads reach consensus about the max.",
      "// This will be useful for K > 1 so that the threads can agree on \"who\" had the max value. That thread can",
      "// then blank out their max with -inf and the warp can run more iterations...",
      "#pragma unroll",
      "        for (int mask = THREADS_PER_ROW / 2; mask > 0; mask /= 2)",
      "        {",
      "            float other_max = VLLM_SHFL_XOR_SYNC_WIDTH(max_val, mask, THREADS_PER_ROW);",
      "            int other_expert = VLLM_SHFL_XOR_SYNC_WIDTH(expert, mask, THREADS_PER_ROW);",
      "",
      "            // We want lower indices to \"win\" in every thread so we break ties this way",
      "            if (other_max > max_val || (other_max == max_val && other_expert < expert))",
      "            {",
      "                max_val = other_max;",
      "                expert = other_expert;",
      "            }",
      "        }",
      "",
      "        // Write the max for this k iteration to global memory.",
      "        if (thread_group_idx == 0)",
      "        {",
      "            // Add a guard to ignore experts not included by this node",
      "            const bool node_uses_expert = expert >= start_expert && expert < end_expert;",
      "            const bool should_process_row = row_is_active && node_uses_expert;",
      "",
      "            // The lead thread from each sub-group will write out the final results to global memory. (This will be a",
      "            // single) thread per row of the input/output matrices.",
      "            const int idx = k * thread_row + k_idx;",
      "            output[idx] = max_val;",
      "            indices[idx] = should_process_row ? (expert - start_expert) : NUM_EXPERTS;",
      "            source_rows[idx] = k_idx * num_rows + thread_row;",
      "        }",
      "",
      "        // Finally, we clear the value in the thread with the current max if there is another iteration to run.",
      "        if (k_idx + 1 < k)",
      "        {",
      "            const int ldg_group_for_expert = expert / COLS_PER_GROUP_LDG;",
      "            const int thread_to_clear_in_group = (expert / ELTS_PER_LDG) % THREADS_PER_ROW;",
      "",
      "            // Only the thread in the group which produced the max will reset the \"winning\" value to -inf.",
      "            if (thread_group_idx == thread_to_clear_in_group)",
      "            {",
      "                const int offset_for_expert = expert % ELTS_PER_LDG;",
      "                // Safe to set to any negative value since row_chunk values must be between 0 and 1.",
      "                row_chunk[ldg_group_for_expert * ELTS_PER_LDG + offset_for_expert] = -10000.f;",
      "            }",
      "        }",
      "    }",
      "}",
      "",
      "namespace detail",
      "{",
      "// Constructs some constants needed to partition the work across threads at compile time.",
      "template <int EXPERTS, int BYTES_PER_LDG, int WARP_SIZE_PARAM>",
      "struct TopkConstants",
      "{",
      "    static constexpr int ELTS_PER_LDG = BYTES_PER_LDG / sizeof(float);",
      "    static_assert(EXPERTS / (ELTS_PER_LDG * WARP_SIZE_PARAM) == 0 || EXPERTS % (ELTS_PER_LDG * WARP_SIZE_PARAM) == 0, \"\");",
      "    static constexpr int VECs_PER_THREAD = MAX(1, EXPERTS / (ELTS_PER_LDG * WARP_SIZE_PARAM));",
      "    static constexpr int VPT = VECs_PER_THREAD * ELTS_PER_LDG;",
      "    static constexpr int THREADS_PER_ROW = EXPERTS / VPT;",
      "    static const int ROWS_PER_WARP = WARP_SIZE_PARAM / THREADS_PER_ROW;",
      "};",
      "} // namespace detail",
      "",
      "template <int EXPERTS, int WARPS_PER_TB, int WARP_SIZE_PARAM, int MAX_BYTES_PER_LDG, typename IndType>",
      "void topkGatingSoftmaxLauncherHelper(const float* input, const bool* finished, float* output, IndType* indices,",
      "    int* source_row, const int num_rows, const int k, const int start_expert, const int end_expert, cudaStream_t stream)",
      "{",
      "    static constexpr int BYTES_PER_LDG = MIN(MAX_BYTES_PER_LDG, sizeof(float) * EXPERTS);",
      "    using Constants = detail::TopkConstants<EXPERTS, BYTES_PER_LDG, WARP_SIZE_PARAM>;",
      "    static constexpr int VPT = Constants::VPT;",
      "    static constexpr int ROWS_PER_WARP = Constants::ROWS_PER_WARP;",
      "    const int num_warps = (num_rows + ROWS_PER_WARP - 1) / ROWS_PER_WARP;",
      "    const int num_blocks = (num_warps + WARPS_PER_TB - 1) / WARPS_PER_TB;",
      "",
      "    dim3 block_dim(WARP_SIZE_PARAM, WARPS_PER_TB);",
      "    topkGatingSoftmax<VPT, EXPERTS, WARPS_PER_TB, BYTES_PER_LDG, WARP_SIZE_PARAM><<<num_blocks, block_dim, 0, stream>>>(",
      "        input, finished, output, num_rows, indices, source_row, k, start_expert, end_expert);",
      "}",
      "",
      "#ifndef USE_ROCM",
      "#define LAUNCH_SOFTMAX(NUM_EXPERTS, WARPS_PER_TB, MAX_BYTES)                          \\",
      "    static_assert(WARP_SIZE == 32,                                                    \\",
      "                  \"Unsupported warp size. Only 32 is supported for CUDA\");            \\",
      "    topkGatingSoftmaxLauncherHelper<NUM_EXPERTS, WARPS_PER_TB, WARP_SIZE, MAX_BYTES>( \\",
      "        gating_output, nullptr, topk_weights, topk_indices,                           \\",
      "        token_expert_indices, num_tokens, topk, 0, num_experts, stream);",
      "#else",
      "#define LAUNCH_SOFTMAX(NUM_EXPERTS, WARPS_PER_TB, MAX_BYTES)                             \\",
      "    if (WARP_SIZE == 64) {                                                               \\",
      "        topkGatingSoftmaxLauncherHelper<NUM_EXPERTS, WARPS_PER_TB, 64, MAX_BYTES>(       \\",
      "            gating_output, nullptr, topk_weights, topk_indices,                          \\",
      "            token_expert_indices, num_tokens, topk, 0, num_experts, stream);             \\",
      "    } else if (WARP_SIZE == 32) {                                                        \\",
      "        topkGatingSoftmaxLauncherHelper<NUM_EXPERTS, WARPS_PER_TB, 32, MAX_BYTES>(       \\",
      "            gating_output, nullptr, topk_weights, topk_indices,                          \\",
      "            token_expert_indices, num_tokens, topk, 0, num_experts, stream);             \\",
      "    } else {                                                                             \\",
      "        assert(false && \"Unsupported warp size. Only 32 and 64 are supported for ROCm\"); \\",
      "    }",
      "#endif",
      "",
      "template <typename IndType>",
      "void topkGatingSoftmaxKernelLauncher(",
      "    const float* gating_output,",
      "    float* topk_weights,",
      "    IndType* topk_indices,",
      "    int* token_expert_indices,",
      "    float* softmax_workspace,",
      "    const int num_tokens,",
      "    const int num_experts,",
      "    const int topk,",
      "    cudaStream_t stream) {",
      "    static constexpr int WARPS_PER_TB = 4;",
      "    static constexpr int BYTES_PER_LDG_POWER_OF_2 = 16;",
      "#ifndef USE_ROCM",
      "    static constexpr int BYTES_PER_LDG_MULTIPLE_64 = 8;",
      "#endif",
      "    switch (num_experts) {",
      "        case 1:",
      "            LAUNCH_SOFTMAX(1, WARPS_PER_TB, BYTES_PER_LDG_POWER_OF_2);",
      "            break;",
      "        case 2:",
      "            LAUNCH_SOFTMAX(2, WARPS_PER_TB, BYTES_PER_LDG_POWER_OF_2);",
      "            break;",
      "        case 4:",
      "            LAUNCH_SOFTMAX(4, WARPS_PER_TB, BYTES_PER_LDG_POWER_OF_2);",
      "            break;",
      "        case 8:",
      "            LAUNCH_SOFTMAX(8, WARPS_PER_TB, BYTES_PER_LDG_POWER_OF_2);",
      "            break;",
      "        case 16:",
      "            LAUNCH_SOFTMAX(16, WARPS_PER_TB, BYTES_PER_LDG_POWER_OF_2);",
      "            break;",
      "        case 32:",
      "            LAUNCH_SOFTMAX(32, WARPS_PER_TB, BYTES_PER_LDG_POWER_OF_2);",
      "            break;",
      "        case 64:",
      "            LAUNCH_SOFTMAX(64, WARPS_PER_TB, BYTES_PER_LDG_POWER_OF_2);",
      "            break;",
      "        case 128:",
      "            LAUNCH_SOFTMAX(128, WARPS_PER_TB, BYTES_PER_LDG_POWER_OF_2);",
      "            break;",
      "        case 256:",
      "            LAUNCH_SOFTMAX(256, WARPS_PER_TB, BYTES_PER_LDG_POWER_OF_2);",
      "            break;",
      "        case 512:",
      "            LAUNCH_SOFTMAX(512, WARPS_PER_TB, BYTES_PER_LDG_POWER_OF_2);",
      "            break;",
      "        // (CUDA only) support multiples of 64 when num_experts is not power of 2.",
      "        // ROCm uses WARP_SIZE 64 so 8 bytes loading won't fit for some of num_experts,",
      "        // alternatively we can test 4 bytes loading and enable it in future.",
      "#ifndef USE_ROCM",
      "        case 192:",
      "            LAUNCH_SOFTMAX(192, WARPS_PER_TB, BYTES_PER_LDG_MULTIPLE_64);",
      "            break;",
      "        case 320:",
      "            LAUNCH_SOFTMAX(320, WARPS_PER_TB, BYTES_PER_LDG_MULTIPLE_64);",
      "            break;",
      "        case 384:",
      "            LAUNCH_SOFTMAX(384, WARPS_PER_TB, BYTES_PER_LDG_MULTIPLE_64);",
      "            break;",
      "        case 448:",
      "            LAUNCH_SOFTMAX(448, WARPS_PER_TB, BYTES_PER_LDG_MULTIPLE_64);",
      "            break;",
      "        case 576:",
      "            LAUNCH_SOFTMAX(576, WARPS_PER_TB, BYTES_PER_LDG_MULTIPLE_64);",
      "            break;",
      "#endif",
      "        default: {",
      "            TORCH_CHECK(softmax_workspace != nullptr,",
      "                \"softmax_workspace must be provided for num_experts that are not a power of 2 or multiple of 64.\");",
      "            static constexpr int TPB = 256;",
      "            moeSoftmax<TPB><<<num_tokens, TPB, 0, stream>>>(",
      "                gating_output, nullptr, softmax_workspace, num_experts);",
      "            moeTopK<TPB><<<num_tokens, TPB, 0, stream>>>(",
      "                softmax_workspace, nullptr, topk_weights, topk_indices, token_expert_indices,",
      "                num_experts, topk, 0, num_experts);",
      "        }",
      "    }",
      "}",
      "",
      "} // namespace moe",
      "} // namespace vllm",
      "",
      "void topk_softmax(",
      "    torch::Tensor& topk_weights,                // [num_tokens, topk]",
      "    torch::Tensor& topk_indices,                // [num_tokens, topk]",
      "    torch::Tensor& token_expert_indices,        // [num_tokens, topk]",
      "    torch::Tensor& gating_output)               // [num_tokens, num_experts]",
      "{",
      "    const int num_experts = gating_output.size(-1);",
      "    const auto num_tokens = gating_output.numel() / num_experts;",
      "    const int topk = topk_weights.size(-1);",
      "",
      "    const bool is_pow_2 = (num_experts != 0) && ((num_experts & (num_experts - 1)) == 0);",
      "    const bool needs_workspace = !is_pow_2 || num_experts > 256;",
      "    const int64_t workspace_size = needs_workspace ? num_tokens * num_experts : 0;",
      "",
      "    const at::cuda::OptionalCUDAGuard device_guard(device_of(gating_output));",
      "    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "    torch::Tensor softmax_workspace = torch::empty({workspace_size}, gating_output.options());",
      "",
      "    if(topk_indices.scalar_type() == at::ScalarType::Int)",
      "    {",
      "        vllm::moe::topkGatingSoftmaxKernelLauncher(",
      "            gating_output.data_ptr<float>(),",
      "            topk_weights.data_ptr<float>(),",
      "            topk_indices.data_ptr<int>(),",
      "            token_expert_indices.data_ptr<int>(),",
      "            softmax_workspace.data_ptr<float>(),",
      "            num_tokens,",
      "            num_experts,",
      "            topk,",
      "            stream);",
      "    }",
      "    else if (topk_indices.scalar_type() == at::ScalarType::UInt32)",
      "    {",
      "        vllm::moe::topkGatingSoftmaxKernelLauncher(",
      "            gating_output.data_ptr<float>(),",
      "            topk_weights.data_ptr<float>(),",
      "            topk_indices.data_ptr<uint32_t>(),",
      "            token_expert_indices.data_ptr<int>(),",
      "            softmax_workspace.data_ptr<float>(),",
      "            num_tokens,",
      "            num_experts,",
      "            topk,",
      "            stream);",
      "    }",
      "    else {",
      "        TORCH_CHECK(topk_indices.scalar_type() == at::ScalarType::Long);",
      "        vllm::moe::topkGatingSoftmaxKernelLauncher(",
      "            gating_output.data_ptr<float>(),",
      "            topk_weights.data_ptr<float>(),",
      "            topk_indices.data_ptr<int64_t>(),",
      "            token_expert_indices.data_ptr<int>(),",
      "            softmax_workspace.data_ptr<float>(),",
      "            num_tokens,",
      "            num_experts,",
      "            topk,",
      "            stream);",
      "    }",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.cu",
    "source": [
      "",
      "#include \"moe_permute_unpermute_kernel.h\"",
      "",
      "// moe_permute kernels require at least CUDA 12.0",
      "#if defined(CUDA_VERSION) && (CUDA_VERSION >= 12000)",
      "",
      "// CubKeyValueSorter definition begin",
      "CubKeyValueSorter::CubKeyValueSorter()",
      "    : num_experts_(0), num_bits_(sizeof(int) * 8) {}",
      "",
      "int CubKeyValueSorter::expertsToBits(int num_experts) {",
      "  // Max value we represent is V = num_experts + (num_experts - 1) = 2 *",
      "  // num_experts - 1 The maximum number of bits is therefore floor(log2(V)) + 1",
      "  return static_cast<int>(log2(2 * num_experts - 1)) + 1;",
      "}",
      "",
      "CubKeyValueSorter::CubKeyValueSorter(int const num_experts)",
      "    : num_experts_(num_experts), num_bits_(expertsToBits(num_experts)) {}",
      "",
      "void CubKeyValueSorter::updateNumExperts(int const num_experts) {",
      "  num_experts_ = num_experts;",
      "  num_bits_ = expertsToBits(num_experts);",
      "}",
      "",
      "size_t CubKeyValueSorter::getWorkspaceSize(size_t const num_key_value_pairs,",
      "                                           int const num_experts) {",
      "  int num_bits = expertsToBits(num_experts);",
      "  size_t required_storage = 0;",
      "  int* null_int = nullptr;",
      "  cub::DeviceRadixSort::SortPairs(nullptr, required_storage, null_int, null_int,",
      "                                  null_int, null_int, num_key_value_pairs, 0,",
      "                                  num_bits);",
      "",
      "  //   when num_key_value_pairs, num_experts, num_bits, required_storage = 64,",
      "  //   4, 3, 0 The required_storage seems to vary between 0 and 1 for the same",
      "  //   inputs",
      "  if (required_storage == 0) {",
      "    required_storage = 1;",
      "  }",
      "  return required_storage;",
      "}",
      "",
      "void CubKeyValueSorter::run(void* workspace, size_t const workspace_size,",
      "                            int const* keys_in, int* keys_out,",
      "                            int const* values_in, int* values_out,",
      "                            size_t const num_key_value_pairs,",
      "                            cudaStream_t stream) {",
      "  size_t expected_ws_size = getWorkspaceSize(num_key_value_pairs, num_experts_);",
      "  size_t actual_ws_size = workspace_size;",
      "",
      "  TORCH_CHECK(expected_ws_size <= workspace_size,",
      "              \"[CubKeyValueSorter::run] The allocated workspace is too small \"",
      "              \"to run this problem.\");",
      "  cub::DeviceRadixSort::SortPairs(workspace, actual_ws_size, keys_in, keys_out,",
      "                                  values_in, values_out, num_key_value_pairs, 0,",
      "                                  num_bits_, stream);",
      "}",
      "// CubKeyValueSorter definition end",
      "",
      "static inline size_t pad_to_multiple_of_16(size_t const& input) {",
      "  static constexpr int ALIGNMENT = 16;",
      "  return ALIGNMENT * ((input + ALIGNMENT - 1) / ALIGNMENT);",
      "}",
      "template <class T>",
      "__device__ inline int64_t findTotalEltsLessThanTarget(T const* sorted_indices,",
      "                                                      int64_t const arr_length,",
      "                                                      T const target) {",
      "  int64_t low = 0, high = arr_length - 1, target_location = -1;",
      "  while (low <= high) {",
      "    int64_t mid = (low + high) / 2;",
      "",
      "    if (sorted_indices[mid] >= target) {",
      "      high = mid - 1;",
      "    } else {",
      "      low = mid + 1;",
      "      target_location = mid;",
      "    }",
      "  }",
      "  return target_location + 1;",
      "}",
      "",
      "// Calculates the start offset of the tokens for a given expert. The last",
      "// element is the total number of valid tokens",
      "__global__ void computeExpertFirstTokenOffsetKernel(",
      "    int const* sorted_experts, int64_t const sorted_experts_len,",
      "    int const num_experts, int64_t* expert_first_token_offset) {",
      "  // First, compute the global tid. We only need 1 thread per expert.",
      "  int const expert = blockIdx.x * blockDim.x + threadIdx.x;",
      "",
      "  // Note that expert goes [0, num_experts] (inclusive) because we want a count",
      "  // for the total number of active tokens at the end of the scan.",
      "  if (expert >= num_experts + 1) {",
      "    return;",
      "  }",
      "  expert_first_token_offset[expert] =",
      "      findTotalEltsLessThanTarget(sorted_experts, sorted_experts_len, expert);",
      "}",
      "",
      "void computeExpertFirstTokenOffset(int const* sorted_indices,",
      "                                   int const total_indices,",
      "                                   int const num_experts,",
      "                                   int64_t* expert_first_token_offset,",
      "                                   cudaStream_t stream) {",
      "  int const num_entries = num_experts + 1;",
      "  int const threads = std::min(1024, num_entries);",
      "  int const blocks = (num_entries + threads - 1) / threads;",
      "",
      "  computeExpertFirstTokenOffsetKernel<<<blocks, threads, 0, stream>>>(",
      "      sorted_indices, total_indices, num_experts, expert_first_token_offset);",
      "}",
      "",
      "void sortAndScanExpert(int* expert_for_source_row, const int* source_rows,",
      "                       int* permuted_experts, int* permuted_rows,",
      "                       int64_t* expert_first_token_offset, int num_rows,",
      "                       int num_experts, int num_experts_per_node, int k,",
      "                       CubKeyValueSorter& sorter, void* sorter_ws,",
      "                       cudaStream_t stream) {",
      "  int64_t const expanded_num_rows = static_cast<int64_t>(k) * num_rows;",
      "  // We need to use the full num_experts because that is the sentinel value used",
      "  // by topk for disabled experts",
      "  sorter.updateNumExperts(num_experts);",
      "  size_t const sorter_ws_size_bytes = pad_to_multiple_of_16(",
      "      sorter.getWorkspaceSize(expanded_num_rows, num_experts));",
      "  sorter.run((void*)sorter_ws, sorter_ws_size_bytes, expert_for_source_row,",
      "             permuted_experts, source_rows, permuted_rows, expanded_num_rows,",
      "             stream);",
      "  computeExpertFirstTokenOffset(permuted_experts, expanded_num_rows,",
      "                                num_experts_per_node, expert_first_token_offset,",
      "                                stream);",
      "}",
      "",
      "__global__ void preprocessTopkIdKernel(int* topk_id_ptr, int size,",
      "                                       const int* expert_map_ptr,",
      "                                       int num_experts) {",
      "  auto tidx = threadIdx.x;",
      "  auto bidx = blockIdx.x;",
      "  auto offset = bidx * blockDim.x;",
      "  auto bound = min(offset + blockDim.x, size);",
      "  extern __shared__ int smem_expert_map[];",
      "  // store expert_map in smem",
      "  for (int i = tidx; i < num_experts; i += blockDim.x) {",
      "    smem_expert_map[i] = expert_map_ptr[i];",
      "  }",
      "  __syncthreads();",
      "",
      "  // query global expert id in expert map.",
      "  // if global expert id = -1 in exert map, plus n_expert",
      "  // else set global expert id = exert map[global expert id]",
      "  if (offset + tidx < bound) {",
      "    auto topk_id = topk_id_ptr[offset + tidx];",
      "    auto local_expert_idx = smem_expert_map[topk_id];",
      "    if (local_expert_idx == -1) {",
      "      topk_id += num_experts;",
      "    } else {",
      "      topk_id = local_expert_idx;",
      "    }",
      "    __syncwarp();",
      "    topk_id_ptr[offset + tidx] = topk_id;",
      "  }",
      "}",
      "void preprocessTopkIdLauncher(int* topk_id_ptr, int size,",
      "                              const int* expert_map_ptr, int num_experts,",
      "                              cudaStream_t stream) {",
      "  int block = std::min(size, 1024);",
      "  int grid = (size + block - 1) / block;",
      "  int smem_size = (num_experts) * sizeof(int);",
      "  preprocessTopkIdKernel<<<grid, block, smem_size, stream>>>(",
      "      topk_id_ptr, size, expert_map_ptr, num_experts);",
      "}",
      "",
      "template <bool ALIGN_BLOCK_SIZE>",
      "__global__ void getMIndicesKernel(int64_t* expert_first_token_offset,",
      "                                  int64_t* align_expert_first_token_offset,",
      "                                  int* m_indices, const int num_local_expert,",
      "                                  const int align_block_size) {",
      "  int eidx = blockIdx.x;",
      "  int tidx = threadIdx.x;",
      "  extern __shared__ int64_t smem_expert_first_token_offset[];",
      "  for (int i = tidx; i <= num_local_expert; i += blockDim.x) {",
      "    smem_expert_first_token_offset[i] = __ldg(expert_first_token_offset + i);",
      "  }",
      "  __syncthreads();",
      "  auto last_token_offset = smem_expert_first_token_offset[eidx + 1];",
      "  auto first_token_offset = smem_expert_first_token_offset[eidx];",
      "  int n_token_in_expert = last_token_offset - first_token_offset;",
      "",
      "  if constexpr (ALIGN_BLOCK_SIZE) {",
      "    n_token_in_expert = (n_token_in_expert + align_block_size - 1) /",
      "                        align_block_size * align_block_size;",
      "    // round up to ALIGN_BLOCK_SIZE",
      "    int64_t accumulate_align_offset = 0;",
      "    for (int i = 1; i <= eidx + 1; i++) {",
      "      int n_token = smem_expert_first_token_offset[i] -",
      "                    smem_expert_first_token_offset[i - 1];",
      "      accumulate_align_offset =",
      "          accumulate_align_offset + (n_token + align_block_size - 1) /",
      "                                        align_block_size * align_block_size;",
      "      if (i == eidx) {",
      "        first_token_offset = accumulate_align_offset;",
      "      }",
      "      // last block store align_expert_first_token_offset",
      "      if (eidx == num_local_expert - 1 && threadIdx.x == 0) {",
      "        align_expert_first_token_offset[i] = accumulate_align_offset;",
      "      }",
      "    }",
      "  }",
      "  for (int idx = tidx; idx < n_token_in_expert; idx += blockDim.x) {",
      "    // update m_indice with expert id",
      "    m_indices[first_token_offset + idx] = eidx;",
      "  }",
      "}",
      "",
      "void getMIndices(int64_t* expert_first_token_offset,",
      "                 int64_t* align_expert_first_token_offset, int* m_indices,",
      "                 int num_local_expert, const int align_block_size,",
      "                 cudaStream_t stream) {",
      "  int block = 256;",
      "  int grid = num_local_expert;",
      "  int smem_size = sizeof(int64_t) * (num_local_expert + 1);",
      "  if (align_block_size == -1) {",
      "    getMIndicesKernel<false><<<grid, block, smem_size, stream>>>(",
      "        expert_first_token_offset, align_expert_first_token_offset, m_indices,",
      "        num_local_expert, align_block_size);",
      "  } else {",
      "    getMIndicesKernel<true><<<grid, block, smem_size, stream>>>(",
      "        expert_first_token_offset, align_expert_first_token_offset, m_indices,",
      "        num_local_expert, align_block_size);",
      "  }",
      "}",
      "",
      "#endif"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/moe/marlin_moe_wna16/ops.cu",
    "source": [
      "/*",
      " * Modified by Neural Magic",
      " * Copyright (C) Marlin.2024 Elias Frantar",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *         http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "",
      "/*",
      " * Adapted from https://github.com/IST-DASLab/marlin",
      " */",
      "",
      "#ifndef MARLIN_NAMESPACE_NAME",
      "  #define MARLIN_NAMESPACE_NAME marlin_moe_wna16",
      "#endif",
      "",
      "#include \"kernel.h\"",
      "#include \"core/registration.h\"",
      "",
      "#define STATIC_ASSERT_SCALAR_TYPE_VALID(scalar_t)               \\",
      "  static_assert(std::is_same<scalar_t, half>::value ||          \\",
      "                    std::is_same<scalar_t, nv_bfloat16>::value, \\",
      "                \"only float16 and bfloat16 is supported\");",
      "",
      "namespace MARLIN_NAMESPACE_NAME {",
      "",
      "__global__ void MarlinDefault(MARLIN_KERNEL_PARAMS){};",
      "",
      "using MarlinFuncPtr = void (*)(MARLIN_KERNEL_PARAMS);",
      "",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "",
      "template <int moe_block_size>",
      "__global__ void permute_cols_kernel(",
      "    int4 const* __restrict__ a_int4_ptr, int const* __restrict__ perm_int_ptr,",
      "    int4* __restrict__ out_int4_ptr,",
      "    const int32_t* __restrict__ sorted_token_ids_ptr,",
      "    const int32_t* __restrict__ expert_ids_ptr,",
      "    const int32_t* __restrict__ num_tokens_past_padded_ptr, int size_m,",
      "    int size_k, int top_k) {};",
      "",
      "}  // namespace marlin",
      "",
      "torch::Tensor moe_wna16_marlin_gemm(",
      "    torch::Tensor& a, std::optional<torch::Tensor> c_or_none,",
      "    torch::Tensor& b_q_weight,",
      "    std::optional<torch::Tensor> const& b_bias_or_none, torch::Tensor& b_scales,",
      "    std::optional<torch::Tensor> const& b_zeros_or_none,",
      "    std::optional<torch::Tensor> const& g_idx_or_none,",
      "    std::optional<torch::Tensor> const& perm_or_none, torch::Tensor& workspace,",
      "    torch::Tensor& sorted_token_ids, torch::Tensor& expert_ids,",
      "    torch::Tensor& num_tokens_past_padded, torch::Tensor& topk_weights,",
      "    int64_t moe_block_size, int64_t top_k, bool mul_topk_weights, bool is_ep,",
      "    vllm::ScalarTypeId const& b_q_type_id, int64_t size_m, int64_t size_n,",
      "    int64_t size_k, bool is_k_full, bool use_atomic_add, bool use_fp32_reduce,",
      "    bool is_zp_float) {",
      "  TORCH_CHECK_NOT_IMPLEMENTED(false,",
      "                              \"marlin_gemm(..) requires CUDA_ARCH >= 8.0\");",
      "  return torch::empty({1, 1});",
      "}",
      "",
      "#else",
      "",
      "// For a given \"a\" of size [M,K] performs a permutation of the K columns based",
      "// on the given \"perm\" indices.",
      "template <int moe_block_size>",
      "__global__ void permute_cols_kernel(",
      "    int4 const* __restrict__ a_int4_ptr, int const* __restrict__ perm_int_ptr,",
      "    int4* __restrict__ out_int4_ptr,",
      "    const int32_t* __restrict__ sorted_token_ids_ptr,",
      "    const int32_t* __restrict__ expert_ids_ptr,",
      "    const int32_t* __restrict__ num_tokens_past_padded_ptr, int size_m,",
      "    int size_k, int top_k) {",
      "  int num_tokens_past_padded = num_tokens_past_padded_ptr[0];",
      "  int num_moe_blocks = div_ceil(num_tokens_past_padded, moe_block_size);",
      "  int32_t block_sorted_ids[moe_block_size];",
      "  int block_num_valid_tokens = 0;",
      "  int64_t old_expert_id = 0;",
      "  int64_t expert_id = 0;",
      "  int row_stride = size_k * sizeof(half) / 16;",
      "",
      "  auto read_moe_block_data = [&](int block_id) {",
      "    block_num_valid_tokens = moe_block_size;",
      "    int4* tmp_block_sorted_ids = reinterpret_cast<int4*>(block_sorted_ids);",
      "    for (int i = 0; i < moe_block_size / 4; i++) {",
      "      tmp_block_sorted_ids[i] =",
      "          ((int4*)sorted_token_ids_ptr)[block_id * moe_block_size / 4 + i];",
      "    }",
      "    for (int i = 0; i < moe_block_size; i++) {",
      "      if (block_sorted_ids[i] >= size_m * top_k) {",
      "        block_num_valid_tokens = i;",
      "        break;",
      "      };",
      "    }",
      "  };",
      "",
      "  auto permute_row = [&](int row) {",
      "    int iters = size_k / default_threads;",
      "    int rest = size_k % default_threads;",
      "",
      "    int in_offset = (row / top_k) * row_stride;",
      "    int out_offset = row * row_stride;",
      "",
      "    half const* a_row_half =",
      "        reinterpret_cast<half const*>(a_int4_ptr + in_offset);",
      "    half* out_half = reinterpret_cast<half*>(out_int4_ptr + out_offset);",
      "",
      "    int base_k = 0;",
      "",
      "    for (int i = 0; i < iters; i++) {",
      "      auto cur_k = base_k + threadIdx.x;",
      "      int src_pos = perm_int_ptr[cur_k];",
      "",
      "      out_half[cur_k] = a_row_half[src_pos];",
      "",
      "      base_k += default_threads;",
      "    }",
      "",
      "    if (rest) {",
      "      if (threadIdx.x < rest) {",
      "        auto cur_k = base_k + threadIdx.x;",
      "        int src_pos = perm_int_ptr[cur_k];",
      "",
      "        out_half[cur_k] = a_row_half[src_pos];",
      "      }",
      "    }",
      "  };",
      "",
      "  for (int index = blockIdx.x; index < num_moe_blocks; index += gridDim.x) {",
      "    old_expert_id = expert_id;",
      "    int tmp_expert_id = expert_ids_ptr[index];",
      "    if (tmp_expert_id == -1) continue;",
      "    expert_id = tmp_expert_id;",
      "    perm_int_ptr += (expert_id - old_expert_id) * size_k;",
      "    read_moe_block_data(index);",
      "",
      "    for (int i = 0; i < block_num_valid_tokens; i++)",
      "      permute_row(block_sorted_ids[i]);",
      "  }",
      "}",
      "",
      "typedef struct {",
      "  int thread_k;",
      "  int thread_n;",
      "  int num_threads;",
      "} thread_config_t;",
      "",
      "thread_config_t small_batch_thread_configs[] = {",
      "    // Ordered by priority",
      "",
      "    // thread_k, thread_n, num_threads",
      "    {128, 128, 256},",
      "    {64, 128, 128}};",
      "",
      "thread_config_t large_batch_thread_configs[] = {",
      "    // Ordered by priority",
      "",
      "    // thread_k, thread_n, num_threads",
      "    {64, 256, 256},",
      "    {64, 128, 128}};",
      "",
      "typedef struct {",
      "  int blocks_per_sm;",
      "  thread_config_t tb_cfg;",
      "} exec_config_t;",
      "",
      "int get_scales_cache_size(thread_config_t const& th_config, int prob_m,",
      "                          int prob_n, int prob_k, int num_bits, int group_size,",
      "                          bool has_act_order, bool is_k_full) {",
      "  bool cache_scales_chunk = has_act_order && !is_k_full;",
      "",
      "  int tb_n = th_config.thread_n;",
      "  int tb_k = th_config.thread_k;",
      "",
      "  // Get max scale groups per thread-block",
      "  int tb_groups;",
      "  if (group_size == -1) {",
      "    tb_groups = 1;",
      "  } else if (group_size == 0) {",
      "    tb_groups = div_ceil(tb_k, 32);  // Worst case is 32 group size",
      "  } else {",
      "    tb_groups = div_ceil(tb_k, group_size);",
      "  }",
      "",
      "  if (cache_scales_chunk) {",
      "    int load_groups =",
      "        tb_groups * pipe_stages * 2;     // Chunk size is 2x pipeline over dim K",
      "    load_groups = max(load_groups, 32);  // We load at least 32 scale groups",
      "    return load_groups * tb_n * 2;",
      "  } else {",
      "    int tb_scales = tb_groups * tb_n * 2;",
      "",
      "    return tb_scales * pipe_stages;",
      "  }",
      "}",
      "",
      "int get_kernel_cache_size(thread_config_t const& th_config, bool m_block_size_8,",
      "                          int thread_m_blocks, int prob_m, int prob_n,",
      "                          int prob_k, int num_bits, int group_size,",
      "                          bool has_act_order, bool is_k_full, int has_zp,",
      "                          int is_zp_float) {",
      "  int pack_factor = 32 / num_bits;",
      "",
      "  // Get B size",
      "  int tb_k = th_config.thread_k;",
      "  int tb_n = th_config.thread_n;",
      "  int tb_m = thread_m_blocks * 16;",
      "",
      "  // shm size for block_sorted_ids/rd_block_sorted_ids/block_topk_weights",
      "  // both of them requires tb_m * 4 bytes (tb_m * int32 or tb_m * float32)",
      "  int sh_block_meta_size = tb_m * 4;",
      "  int sh_a_size = pipe_stages * (tb_m * tb_k) * 2;",
      "  int sh_b_size = pipe_stages * (tb_k * tb_n / pack_factor) * 4;",
      "  int sh_red_size = tb_m * (tb_n + 8) * 2;",
      "  int sh_bias_size = tb_n * 2;",
      "  int tmp_size =",
      "      (sh_b_size > sh_red_size ? sh_red_size : sh_b_size) + sh_bias_size;",
      "  tmp_size = max(max(sh_b_size, sh_red_size), tmp_size);",
      "",
      "  int sh_s_size =",
      "      get_scales_cache_size(th_config, prob_m, prob_n, prob_k, num_bits,",
      "                            group_size, has_act_order, is_k_full);",
      "  int sh_g_idx_size = has_act_order && !is_k_full ? pipe_stages * tb_k / 4 : 0;",
      "  int sh_zp_size = 0;",
      "  if (has_zp) {",
      "    if (is_zp_float)",
      "      sh_zp_size = sh_s_size;",
      "    else if (num_bits == 4)",
      "      sh_zp_size = sh_s_size / 4;",
      "    else if (num_bits == 8)",
      "      sh_zp_size = sh_s_size / 2;",
      "  }",
      "",
      "  int total_size = tmp_size + sh_a_size + sh_s_size + sh_zp_size +",
      "                   sh_g_idx_size + sh_block_meta_size;",
      "",
      "  return total_size;",
      "}",
      "",
      "bool is_valid_config(thread_config_t const& th_config, bool m_block_size_8,",
      "                     int thread_m_blocks, int prob_m, int prob_n, int prob_k,",
      "                     int num_bits, int group_size, bool has_act_order,",
      "                     bool is_k_full, int has_zp, int is_zp_float,",
      "                     int max_shared_mem) {",
      "  // Sanity",
      "  if (th_config.thread_k == -1 || th_config.thread_n == -1 ||",
      "      th_config.num_threads == -1) {",
      "    return false;",
      "  }",
      "",
      "  // Verify K/N are divisible by thread K/N",
      "  if (prob_k % th_config.thread_k != 0 || prob_n % th_config.thread_n != 0) {",
      "    return false;",
      "  }",
      "",
      "  // Verify min for thread K/N",
      "  if (th_config.thread_n < min_thread_n || th_config.thread_k < min_thread_k) {",
      "    return false;",
      "  }",
      "",
      "  // num_threads must be at least 128 (= 4 warps)",
      "  if (th_config.num_threads < 128) {",
      "    return false;",
      "  }",
      "",
      "  // Check that pipeline fits into cache",
      "  int cache_size = get_kernel_cache_size(",
      "      th_config, m_block_size_8, thread_m_blocks, prob_m, prob_n, prob_k,",
      "      num_bits, group_size, has_act_order, is_k_full, has_zp, is_zp_float);",
      "  return cache_size + 512 <= max_shared_mem;",
      "}",
      "",
      "  #define _GET_IF(W_TYPE, THREAD_M_BLOCKS, THREAD_N_BLOCKS, THREAD_K_BLOCKS,   \\",
      "                  M_BLOCK_SIZE_8, GROUP_BLOCKS, NUM_THREADS, IS_ZP_FLOAT)      \\",
      "    else if (q_type == W_TYPE && thread_m_blocks == THREAD_M_BLOCKS &&         \\",
      "             thread_n_blocks == THREAD_N_BLOCKS &&                             \\",
      "             thread_k_blocks == THREAD_K_BLOCKS &&                             \\",
      "             m_block_size_8 == M_BLOCK_SIZE_8 &&                               \\",
      "             group_blocks == GROUP_BLOCKS && num_threads == NUM_THREADS &&     \\",
      "             is_zp_float == IS_ZP_FLOAT) {                                     \\",
      "      constexpr auto S_TYPE =                                                  \\",
      "          W_TYPE == vllm::kFE2M1f                                              \\",
      "              ? (GROUP_BLOCKS == 1 ? vllm::kFE4M3fn : vllm::kFE8M0fnu)         \\",
      "              : (std::is_same<scalar_t, half>::value ? vllm::kFloat16          \\",
      "                                                     : vllm::kBFloat16);       \\",
      "      kernel = Marlin<scalar_t, W_TYPE.id(), S_TYPE.id(), NUM_THREADS,         \\",
      "                      THREAD_M_BLOCKS, THREAD_N_BLOCKS, THREAD_K_BLOCKS,       \\",
      "                      M_BLOCK_SIZE_8, pipe_stages, GROUP_BLOCKS, IS_ZP_FLOAT>; \\",
      "    }",
      "",
      "  // COMMON: cases for (group_blocks in [-1, 2, 4, 8] and is_zp_float == false)",
      "  //         this is the most common cases",
      "  // BIGGROUP: cases for big group size (group_blocks in [-1, 8])",
      "  // FZP: cases for float-zero-point (is_zp_float = true)",
      "  // ACT: cases for act order case (group_blocks == 0)",
      "  // FP4: cases for nvfp4(e2m1) (group_blocks == 1)",
      "  #define COMMON_GET_IF_M1(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)       \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, -1, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 2, NUM_THREADS, false)   \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 4, NUM_THREADS, false)   \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 8, NUM_THREADS, false)   \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)",
      "",
      "  #define COMMON_GET_IF_M234(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)     \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)  \\",
      "                                                                          \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)  \\",
      "                                                                          \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)",
      "",
      "  #define COMMON_GET_IF(W_TYPE)            \\",
      "    COMMON_GET_IF_M1(W_TYPE, 8, 8, 256)    \\",
      "    COMMON_GET_IF_M1(W_TYPE, 8, 4, 128)    \\",
      "    COMMON_GET_IF_M234(W_TYPE, 16, 4, 256) \\",
      "    COMMON_GET_IF_M234(W_TYPE, 8, 4, 128)",
      "",
      "  #define BIGGROUP_GET_IF_M1(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)     \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, -1, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 8, NUM_THREADS, false)   \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)",
      "",
      "  #define BIGGROUP_GET_IF_M234(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)   \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)",
      "",
      "  #define BIGGROUP_GET_IF(W_TYPE)            \\",
      "    BIGGROUP_GET_IF_M1(W_TYPE, 8, 8, 256)    \\",
      "    BIGGROUP_GET_IF_M1(W_TYPE, 8, 4, 128)    \\",
      "    BIGGROUP_GET_IF_M234(W_TYPE, 16, 4, 256) \\",
      "    BIGGROUP_GET_IF_M234(W_TYPE, 8, 4, 128)",
      "",
      "  #define NVFP4_GET_IF_M1(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)      \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 1, NUM_THREADS, false)",
      "",
      "  #define NVFP4_GET_IF_M234(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)     \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 1, NUM_THREADS, false)",
      "",
      "  #define NVFP4_GET_IF(W_TYPE)            \\",
      "    NVFP4_GET_IF_M1(W_TYPE, 8, 8, 256)    \\",
      "    NVFP4_GET_IF_M1(W_TYPE, 8, 4, 128)    \\",
      "    NVFP4_GET_IF_M234(W_TYPE, 16, 4, 256) \\",
      "    NVFP4_GET_IF_M234(W_TYPE, 8, 4, 128)",
      "",
      "  #define MXFP4_GET_IF_M1(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)      \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 2, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false)",
      "",
      "  #define MXFP4_GET_IF_M234(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)     \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false)",
      "",
      "  #define MXFP4_GET_IF(W_TYPE)            \\",
      "    MXFP4_GET_IF_M1(W_TYPE, 8, 8, 256)    \\",
      "    MXFP4_GET_IF_M1(W_TYPE, 8, 4, 128)    \\",
      "    MXFP4_GET_IF_M234(W_TYPE, 16, 4, 256) \\",
      "    MXFP4_GET_IF_M234(W_TYPE, 8, 4, 128)",
      "",
      "  // We currently have 4-bit models only with group_blocks == 4",
      "  #define FZP_GET_IF_M1(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)       \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 4, NUM_THREADS, true) \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, true)",
      "",
      "  #define FZP_GET_IF_M234(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)      \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, true) \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, true) \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, true)",
      "",
      "  #define FZP_GET_IF(W_TYPE)            \\",
      "    FZP_GET_IF_M1(W_TYPE, 8, 8, 256)    \\",
      "    FZP_GET_IF_M1(W_TYPE, 8, 4, 128)    \\",
      "    FZP_GET_IF_M234(W_TYPE, 16, 4, 256) \\",
      "    FZP_GET_IF_M234(W_TYPE, 8, 4, 128)",
      "",
      "  // We currently have 4-bit models only with group_blocks == 4",
      "  #define ACT_GET_IF_M1(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)        \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 0, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 0, NUM_THREADS, false)",
      "",
      "  #define ACT_GET_IF_M234(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)       \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 0, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 0, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 0, NUM_THREADS, false)",
      "",
      "  #define ACT_GET_IF(W_TYPE)            \\",
      "    ACT_GET_IF_M1(W_TYPE, 8, 8, 256)    \\",
      "    ACT_GET_IF_M1(W_TYPE, 8, 4, 128)    \\",
      "    ACT_GET_IF_M234(W_TYPE, 16, 4, 256) \\",
      "    ACT_GET_IF_M234(W_TYPE, 8, 4, 128)",
      "",
      "template <typename scalar_t>",
      "MarlinFuncPtr get_marlin_kernel(const vllm::ScalarType q_type,",
      "                                int thread_m_blocks, int thread_n_blocks,",
      "                                int thread_k_blocks, bool m_block_size_8,",
      "                                bool has_act_order, bool has_zp,",
      "                                int group_blocks, int num_threads,",
      "                                bool is_zp_float) {",
      "  int num_bits = q_type.size_bits();",
      "  auto kernel = MarlinDefault;",
      "  if (false) {",
      "  }",
      "",
      "  COMMON_GET_IF(vllm::kU4)",
      "  COMMON_GET_IF(vllm::kU4B8)",
      "  COMMON_GET_IF(vllm::kU8B128)",
      "",
      "  NVFP4_GET_IF(vllm::kFE2M1f)",
      "",
      "  BIGGROUP_GET_IF(vllm::kFE4M3fn)",
      "",
      "  ACT_GET_IF(vllm::kU4B8)",
      "  ACT_GET_IF(vllm::kU8B128)",
      "  if (std::is_same<scalar_t, nv_bfloat16>::value) {",
      "    if (false) {",
      "    }",
      "    MXFP4_GET_IF(vllm::kFE2M1f)",
      "  }",
      "",
      "  return kernel;",
      "}",
      "",
      "template <typename scalar_t>",
      "exec_config_t determine_exec_config(const vllm::ScalarType& q_type, int prob_m,",
      "                                    int prob_n, int prob_k, int thread_m_blocks,",
      "                                    bool m_block_size_8, int num_bits,",
      "                                    int group_size, bool has_act_order,",
      "                                    bool is_k_full, bool has_zp,",
      "                                    bool is_zp_float, int max_shared_mem) {",
      "  exec_config_t exec_cfg = exec_config_t{1, thread_config_t{-1, -1, -1}};",
      "  thread_config_t* thread_configs = thread_m_blocks > 1",
      "                                        ? large_batch_thread_configs",
      "                                        : small_batch_thread_configs;",
      "  int thread_configs_size =",
      "      thread_m_blocks > 1",
      "          ? sizeof(large_batch_thread_configs) / sizeof(thread_config_t)",
      "          : sizeof(small_batch_thread_configs) / sizeof(thread_config_t);",
      "",
      "  int count = 0;",
      "  constexpr int device_max_reg_size = 255 * 1024;",
      "  for (int i = 0; i < thread_configs_size; i++) {",
      "    thread_config_t th_config = thread_configs[i];",
      "",
      "    if (!is_valid_config(th_config, m_block_size_8, thread_m_blocks, prob_m,",
      "                         prob_n, prob_k, num_bits, group_size, has_act_order,",
      "                         is_k_full, has_zp, is_zp_float, max_shared_mem)) {",
      "      continue;",
      "    }",
      "",
      "    int cache_size = get_kernel_cache_size(",
      "        th_config, m_block_size_8, thread_m_blocks, prob_m, prob_n, prob_k,",
      "        num_bits, group_size, has_act_order, is_k_full, has_zp, is_zp_float);",
      "",
      "    int group_blocks = 0;",
      "    if (!has_act_order) {",
      "      group_blocks = group_size == -1 ? -1 : (group_size / 16);",
      "    }",
      "",
      "    auto kernel = get_marlin_kernel<scalar_t>(",
      "        q_type, thread_m_blocks, th_config.thread_n / 16,",
      "        th_config.thread_k / 16, m_block_size_8, has_act_order, has_zp,",
      "        group_blocks, th_config.num_threads, is_zp_float);",
      "",
      "    if (kernel == MarlinDefault) continue;",
      "",
      "    if (thread_m_blocks > 1) {",
      "      exec_cfg = {1, th_config};",
      "      break;",
      "    } else {",
      "      cudaFuncAttributes attr;",
      "      cudaFuncGetAttributes(&attr, kernel);",
      "      int reg_size = max(attr.numRegs, 1) * th_config.num_threads * 4;",
      "      int allow_count = min(device_max_reg_size / reg_size,",
      "                            max_shared_mem / (cache_size + 1024));",
      "      allow_count = max(min(allow_count, 4), 1);",
      "      if (allow_count > count) {",
      "        count = allow_count;",
      "        exec_cfg = {count, th_config};",
      "      };",
      "    }",
      "  }",
      "",
      "  return exec_cfg;",
      "}",
      "",
      "template <typename scalar_t>",
      "void marlin_mm(const void* A, const void* B, void* C, void* C_tmp, void* b_bias,",
      "               void* s, void* s2, void* zp, void* g_idx, void* perm,",
      "               void* a_tmp, void* sorted_token_ids, void* expert_ids,",
      "               void* num_tokens_past_padded, void* topk_weights,",
      "               int moe_block_size, int top_k, bool mul_topk_weights, bool is_ep,",
      "               int prob_m, int prob_n, int prob_k, void* workspace,",
      "               vllm::ScalarType const& q_type, bool has_bias,",
      "               bool has_act_order, bool is_k_full, bool has_zp, int num_groups,",
      "               int group_size, int dev, cudaStream_t stream, int thread_k,",
      "               int thread_n, int sms, bool use_atomic_add, bool use_fp32_reduce,",
      "               bool is_zp_float) {",
      "  int thread_m_blocks = div_ceil(moe_block_size, 16);",
      "  bool m_block_size_8 = moe_block_size == 8;",
      "",
      "  if (has_zp) {",
      "    TORCH_CHECK(",
      "        q_type == vllm::kU4 || q_type == vllm::kU8,",
      "        \"q_type must be u4 or u8 when has_zp = True. Got = \", q_type.str());",
      "  } else {",
      "    TORCH_CHECK(",
      "        q_type == vllm::kU4B8 || q_type == vllm::kU8B128 ||",
      "            q_type == vllm::kFE4M3fn || q_type == vllm::kFE2M1f,",
      "        \"q_type must be uint4b8, uint8b128, float8_e4m3fn or float4_e2m1f when \"",
      "        \"has_zp = False. Got = \",",
      "        q_type.str());",
      "  }",
      "",
      "  TORCH_CHECK(prob_m > 0 && prob_n > 0 && prob_k > 0, \"Invalid MNK = [\", prob_m,",
      "              \", \", prob_n, \", \", prob_k, \"]\");",
      "",
      "  int group_blocks = 0;",
      "  if (has_act_order) {",
      "    if (is_k_full) {",
      "      TORCH_CHECK(group_size != -1);",
      "      group_blocks = group_size / 16;",
      "      TORCH_CHECK(prob_k % group_blocks == 0, \"prob_k = \", prob_k,",
      "                  \" is not divisible by group_blocks = \", group_blocks);",
      "    } else {",
      "      TORCH_CHECK(group_size == 0);",
      "      group_blocks = 0;",
      "    }",
      "  } else {",
      "    if (group_size == -1) {",
      "      group_blocks = -1;",
      "    } else {",
      "      group_blocks = group_size / 16;",
      "      TORCH_CHECK(prob_k % group_blocks == 0, \"prob_k = \", prob_k,",
      "                  \" is not divisible by group_blocks = \", group_blocks);",
      "    }",
      "  }",
      "",
      "  int num_bits = q_type.size_bits();",
      "  const int4* A_ptr = (const int4*)A;",
      "  const int4* B_ptr = (const int4*)B;",
      "  int4* C_ptr = (int4*)C;",
      "  int4* C_tmp_ptr = (int4*)C_tmp;",
      "  const int4* bias_ptr = (const int4*)b_bias;",
      "  const int4* s_ptr = (const int4*)s;",
      "  const uint16_t* s2_ptr = (const uint16_t*)s2;",
      "  const int4* zp_ptr = (const int4*)zp;",
      "  const int* g_idx_ptr = (const int*)g_idx;",
      "  const int* perm_ptr = (const int*)perm;",
      "  int4* a_tmp_ptr = (int4*)a_tmp;",
      "  const int32_t* sorted_token_ids_ptr = (const int32_t*)sorted_token_ids;",
      "  const int32_t* expert_ids_ptr = (const int32_t*)expert_ids;",
      "  const int32_t* num_tokens_past_padded_ptr =",
      "      (const int32_t*)num_tokens_past_padded;",
      "  const float* topk_weights_ptr = (const float*)topk_weights;",
      "  int* locks = (int*)workspace;",
      "",
      "  if (has_act_order) {",
      "    // Permute A columns",
      "    auto kernel = permute_cols_kernel<8>;",
      "    if (moe_block_size == 8) {",
      "    } else if (moe_block_size == 16)",
      "      kernel = permute_cols_kernel<16>;",
      "    else if (moe_block_size == 32)",
      "      kernel = permute_cols_kernel<32>;",
      "    else if (moe_block_size == 48)",
      "      kernel = permute_cols_kernel<48>;",
      "    else if (moe_block_size == 64)",
      "      kernel = permute_cols_kernel<64>;",
      "    else",
      "      TORCH_CHECK(false, \"unsupported moe_block_size \", moe_block_size);",
      "",
      "    // avoid \">>>\" being formatted to \"> > >\"",
      "    // clang-format off",
      "    kernel<<<sms, default_threads, 0, stream>>>(",
      "        A_ptr, perm_ptr, a_tmp_ptr, sorted_token_ids_ptr, expert_ids_ptr,",
      "        num_tokens_past_padded_ptr, prob_m, prob_k, top_k);",
      "    // clang-format on",
      "    A_ptr = a_tmp_ptr;",
      "    prob_m = prob_m * top_k;",
      "    top_k = 1;",
      "",
      "    // If we have a full K, then we can run the non-act-order version of Marlin",
      "    // (since the weight rows are reordered by increasing group ids, and by",
      "    // having a full K, we have full original groups)",
      "    if (is_k_full) has_act_order = false;",
      "  }",
      "",
      "  int max_shared_mem = 0;",
      "  cudaDeviceGetAttribute(&max_shared_mem,",
      "                         cudaDevAttrMaxSharedMemoryPerBlockOptin, dev);",
      "  TORCH_CHECK(max_shared_mem > 0);",
      "",
      "  // Set thread config",
      "  exec_config_t exec_cfg;",
      "  thread_config_t thread_tfg;",
      "  if (thread_k != -1 && thread_n != -1) {",
      "    thread_tfg = thread_config_t{thread_k, thread_n, default_threads};",
      "    exec_cfg = exec_config_t{1, thread_tfg};",
      "    TORCH_CHECK(prob_n % thread_n == 0, \"prob_n = \", prob_n,",
      "                \" is not divisible by thread_n = \", thread_n);",
      "    TORCH_CHECK(prob_k % thread_k == 0, \"prob_k = \", prob_k,",
      "                \" is not divisible by thread_k = \", thread_k);",
      "  } else {",
      "    // Auto config",
      "    exec_cfg = determine_exec_config<scalar_t>(",
      "        q_type, prob_m, prob_n, prob_k, thread_m_blocks, m_block_size_8,",
      "        num_bits, group_size, has_act_order, is_k_full, has_zp, is_zp_float,",
      "        max_shared_mem);",
      "    thread_tfg = exec_cfg.tb_cfg;",
      "  }",
      "",
      "  int num_threads = thread_tfg.num_threads;",
      "  thread_k = thread_tfg.thread_k;",
      "  thread_n = thread_tfg.thread_n;",
      "  int blocks = sms * exec_cfg.blocks_per_sm;",
      "  if (exec_cfg.blocks_per_sm > 1)",
      "    max_shared_mem = max_shared_mem / exec_cfg.blocks_per_sm - 1024;",
      "",
      "  int thread_k_blocks = thread_k / 16;",
      "  int thread_n_blocks = thread_n / 16;",
      "",
      "  TORCH_CHECK(",
      "      is_valid_config(thread_tfg, m_block_size_8, thread_m_blocks, prob_m,",
      "                      prob_n, prob_k, num_bits, group_size, has_act_order,",
      "                      is_k_full, has_zp, is_zp_float, max_shared_mem),",
      "      \"Invalid thread config: thread_m_blocks = \", thread_m_blocks,",
      "      \", thread_k = \", thread_tfg.thread_k,",
      "      \", thread_n = \", thread_tfg.thread_n,",
      "      \", num_threads = \", thread_tfg.num_threads, \" for MKN = [\", prob_m, \", \",",
      "      prob_k, \", \", prob_n, \"] and num_bits = \", num_bits,",
      "      \", group_size = \", group_size, \", has_act_order = \", has_act_order,",
      "      \", is_k_full = \", is_k_full, \", has_zp = \", has_zp,",
      "      \", is_zp_float = \", is_zp_float, \", max_shared_mem = \", max_shared_mem);",
      "",
      "  auto kernel = get_marlin_kernel<scalar_t>(",
      "      q_type, thread_m_blocks, thread_n_blocks, thread_k_blocks, m_block_size_8,",
      "      has_act_order, has_zp, group_blocks, num_threads, is_zp_float);",
      "",
      "  if (kernel == MarlinDefault) {",
      "    TORCH_CHECK(false, \"Unsupported shapes: MNK = [\", prob_m, \", \", prob_n,",
      "                \", \", prob_k, \"]\", \", has_act_order = \", has_act_order,",
      "                \", num_groups = \", num_groups, \", group_size = \", group_size,",
      "                \", thread_m_blocks = \", thread_m_blocks,",
      "                \", thread_n_blocks = \", thread_n_blocks,",
      "                \", thread_k_blocks = \", thread_k_blocks,",
      "                \", num_bits = \", num_bits);",
      "  }",
      "",
      "  cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize,",
      "                       max_shared_mem);",
      "  // avoid \">>>\" being formatted to \"> > >\"",
      "  // clang-format off",
      "  kernel<<<blocks, num_threads, max_shared_mem, stream>>>(",
      "      A_ptr, B_ptr, C_ptr, C_tmp_ptr, bias_ptr, s_ptr, s2_ptr, zp_ptr, g_idx_ptr,",
      "      sorted_token_ids_ptr, expert_ids_ptr, num_tokens_past_padded_ptr,",
      "      topk_weights_ptr, top_k, mul_topk_weights, is_ep, num_groups, prob_m,",
      "      prob_n, prob_k, locks, has_bias, use_atomic_add, use_fp32_reduce, max_shared_mem);",
      "  // clang-format on",
      "}",
      "",
      "}  // namespace MARLIN_NAMESPACE_NAME",
      "",
      "torch::Tensor moe_wna16_marlin_gemm(",
      "    torch::Tensor& a, std::optional<torch::Tensor> const& c_or_none,",
      "    torch::Tensor& b_q_weight,",
      "    std::optional<torch::Tensor> const& b_bias_or_none, torch::Tensor& b_scales,",
      "    std::optional<torch::Tensor> const& global_scale_or_none,",
      "    std::optional<torch::Tensor> const& b_zeros_or_none,",
      "    std::optional<torch::Tensor> const& g_idx_or_none,",
      "    std::optional<torch::Tensor> const& perm_or_none, torch::Tensor& workspace,",
      "    torch::Tensor& sorted_token_ids, torch::Tensor& expert_ids,",
      "    torch::Tensor& num_tokens_past_padded, torch::Tensor& topk_weights,",
      "    int64_t moe_block_size, int64_t top_k, bool mul_topk_weights, bool is_ep,",
      "    vllm::ScalarTypeId const& b_q_type_id, int64_t size_m, int64_t size_n,",
      "    int64_t size_k, bool is_k_full, bool use_atomic_add, bool use_fp32_reduce,",
      "    bool is_zp_float) {",
      "  vllm::ScalarType const b_q_type = vllm::ScalarType::from_id(b_q_type_id);",
      "  int pack_factor = 32 / b_q_type.size_bits();",
      "",
      "  if (moe_block_size != 8) {",
      "    TORCH_CHECK(moe_block_size % 16 == 0,",
      "                \"unsupported moe_block_size=\", moe_block_size);",
      "    TORCH_CHECK(moe_block_size >= 16 && moe_block_size <= 64,",
      "                \"unsupported moe_block_size=\", moe_block_size);",
      "  }",
      "",
      "  // Verify A",
      "  TORCH_CHECK(a.size(0) == size_m, \"Shape mismatch: a.size(0) = \", a.size(0),",
      "              \", size_m = \", size_m);",
      "  TORCH_CHECK(a.size(1) == size_k, \"Shape mismatch: a.size(1) = \", a.size(1),",
      "              \", size_k = \", size_k);",
      "",
      "  // Verify B",
      "  TORCH_CHECK(",
      "      size_k % MARLIN_NAMESPACE_NAME::tile_size == 0, \"size_k = \", size_k,",
      "      \" is not divisible by tile_size = \", MARLIN_NAMESPACE_NAME::tile_size);",
      "  TORCH_CHECK((size_k / MARLIN_NAMESPACE_NAME::tile_size) == b_q_weight.size(1),",
      "              \"Shape mismatch: b_q_weight.size(1) = \", b_q_weight.size(1),",
      "              \", size_k = \", size_k,",
      "              \", tile_size = \", MARLIN_NAMESPACE_NAME::tile_size);",
      "  TORCH_CHECK(",
      "      b_q_weight.size(2) % MARLIN_NAMESPACE_NAME::tile_size == 0,",
      "      \"b_q_weight.size(2) = \", b_q_weight.size(2),",
      "      \" is not divisible by tile_size = \", MARLIN_NAMESPACE_NAME::tile_size);",
      "  int actual_size_n =",
      "      (b_q_weight.size(2) / MARLIN_NAMESPACE_NAME::tile_size) * pack_factor;",
      "  TORCH_CHECK(size_n == actual_size_n, \"size_n = \", size_n,",
      "              \", actual_size_n = \", actual_size_n);",
      "",
      "  // Verify device and strides",
      "  TORCH_CHECK(a.device().is_cuda(), \"A is not on GPU\");",
      "  TORCH_CHECK(a.is_contiguous(), \"A is not contiguous\");",
      "",
      "  TORCH_CHECK(b_q_weight.device().is_cuda(), \"b_q_weight is not on GPU\");",
      "  TORCH_CHECK(b_q_weight.is_contiguous(), \"b_q_weight is not contiguous\");",
      "",
      "  TORCH_CHECK(b_scales.device().is_cuda(), \"b_scales is not on GPU\");",
      "  TORCH_CHECK(b_scales.is_contiguous(), \"b_scales is not contiguous\");",
      "",
      "  // thread_k: `k` size of a thread_tile in `weights` (can usually be left as",
      "  // auto -1)",
      "  int thread_k = -1;",
      "  // thread_n: `n` size of a thread_tile in `weights` (can usually be left as",
      "  // auto -1)",
      "  int thread_n = -1;",
      "  // sms: number of SMs to use for the kernel",
      "  int sms = -1;",
      "  cudaDeviceGetAttribute(&sms, cudaDevAttrMultiProcessorCount, a.get_device());",
      "",
      "  // Alloc buffers",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(a));",
      "  auto options = torch::TensorOptions().dtype(a.dtype()).device(a.device());",
      "  torch::Tensor c;",
      "  if (c_or_none.has_value()) {",
      "    c = c_or_none.value();",
      "    TORCH_CHECK(c.device().is_cuda(), \"c is not on GPU\");",
      "    TORCH_CHECK(c.is_contiguous(), \"c is not contiguous\");",
      "    TORCH_CHECK(c.size(0) == size_m * top_k,",
      "                \"Shape mismatch: c.size(0) = \", c.size(0),",
      "                \", size_m * topk = \", size_m * top_k);",
      "    TORCH_CHECK(c.size(1) == size_n, \"Shape mismatch: c.size(1) = \", c.size(1),",
      "                \", size_n = \", size_n);",
      "  } else {",
      "    c = torch::empty({size_m * top_k, size_n}, options);",
      "  }",
      "",
      "  // Alloc C tmp buffer that is going to be used for the global reduce",
      "  torch::Tensor c_tmp;",
      "  auto options_fp32 =",
      "      torch::TensorOptions().dtype(at::kFloat).device(a.device());",
      "  if (use_fp32_reduce && !use_atomic_add) {",
      "    // max num of threadblocks is sms * 4",
      "    long max_c_tmp_size = min(",
      "        (long)size_n * sorted_token_ids.size(0),",
      "        (long)sms * 4 * moe_block_size * MARLIN_NAMESPACE_NAME::max_thread_n);",
      "    if (moe_block_size == 8) max_c_tmp_size *= 2;",
      "    c_tmp = torch::empty({max_c_tmp_size}, options_fp32);",
      "  } else {",
      "    c_tmp = torch::empty({0}, options_fp32);",
      "  }",
      "",
      "  // Detect groupsize and act_order",
      "  int num_groups = -1;",
      "  int group_size = -1;",
      "",
      "  int rank = b_scales.sizes().size();",
      "  TORCH_CHECK(rank == 3, \"b_scales rank = \", rank, \" is not 3\");",
      "  TORCH_CHECK(b_scales.size(2) == size_n, \"b_scales dim 2 = \", b_scales.size(2),",
      "              \" is not size_n = \", size_n);",
      "  num_groups = b_scales.size(1);",
      "",
      "  torch::Tensor g_idx, perm, a_tmp;",
      "  if (g_idx_or_none.has_value() && perm_or_none.has_value()) {",
      "    g_idx = g_idx_or_none.value();",
      "    perm = perm_or_none.value();",
      "",
      "    TORCH_CHECK(g_idx.device().is_cuda(), \"g_idx is not on GPU\");",
      "    TORCH_CHECK(g_idx.is_contiguous(), \"g_idx is not contiguous\");",
      "    TORCH_CHECK(perm.device().is_cuda(), \"perm is not on GPU\");",
      "    TORCH_CHECK(perm.is_contiguous(), \"perm is not contiguous\");",
      "",
      "    // Verify g_idx and perm",
      "    TORCH_CHECK((g_idx.size(-1) == 0 && perm.size(-1) == 0) ||",
      "                    (g_idx.size(-1) == size_k && perm.size(-1) == size_k),",
      "                \"Unexpected g_idx.size(-1) = \", g_idx.size(-1),",
      "                \" and perm.size(-1) = \", perm.size(-1),",
      "                \", where size_k = \", size_k);",
      "  } else {",
      "    g_idx = torch::empty({0}, options);",
      "    perm = torch::empty({0}, options);",
      "    a_tmp = torch::empty({0}, options);",
      "  }",
      "  bool has_act_order = g_idx.size(-1) > 0 && perm.size(-1) > 0;",
      "",
      "  if (has_act_order) {",
      "    a_tmp = torch::empty({size_m * top_k, size_k}, options);",
      "    if (is_k_full) {",
      "      TORCH_CHECK(num_groups > 1, \"For act_order, num_groups must be > 1\");",
      "      TORCH_CHECK(size_k % num_groups == 0, \"size_k = \", size_k,",
      "                  \", is not divisible by num_groups = \", num_groups);",
      "      group_size = size_k / num_groups;",
      "    } else {",
      "      group_size = 0;",
      "    }",
      "",
      "  } else {",
      "    a_tmp = torch::empty({0}, options);",
      "    if (num_groups > 1) {",
      "      TORCH_CHECK(",
      "          size_k % num_groups == 0, \"size_k = \", size_k,",
      "          \", is not divisible by b_scales.size(1) = \", b_scales.size(1));",
      "      group_size = size_k / num_groups;",
      "    } else {",
      "      group_size = -1;",
      "    }",
      "  }",
      "",
      "  torch::Tensor global_scale;",
      "  if (global_scale_or_none.has_value()) {",
      "    global_scale = global_scale_or_none.value();",
      "    TORCH_CHECK(b_q_type == vllm::kFE2M1f && group_size == 16,",
      "                \"global_scale can only be used for nvfp4 format.\");",
      "  } else {",
      "    global_scale = torch::empty({0}, options);",
      "    TORCH_CHECK(!(b_q_type == vllm::kFE2M1f && group_size == 16),",
      "                \"the global_scale parameter must be passed for nvfp4 format.\");",
      "  }",
      "",
      "  bool has_bias = b_bias_or_none.has_value();",
      "  torch::Tensor b_bias;",
      "  if (has_bias) {",
      "    b_bias = b_bias_or_none.value();",
      "    TORCH_CHECK(b_bias.device().is_cuda(), \"b_bias is not on GPU\");",
      "    TORCH_CHECK(b_bias.is_contiguous(), \"b_bias is not contiguous\");",
      "    TORCH_CHECK(b_bias.size(1) == size_n, \"b_bias.size(0) != size_n\");",
      "    TORCH_CHECK(b_bias.stride(1) == 1, \"b_bias.stride(1) != 1\");",
      "  } else {",
      "    b_bias = torch::empty({0}, options);",
      "  }",
      "",
      "  torch::Tensor b_zeros;",
      "  if (b_zeros_or_none.has_value()) {",
      "    b_zeros = b_zeros_or_none.value();",
      "    TORCH_CHECK(b_zeros.device().is_cuda(), \"b_zeros is not on GPU\");",
      "    TORCH_CHECK(b_zeros.is_contiguous(), \"b_zeros is not contiguous\");",
      "  } else {",
      "    b_zeros = torch::empty({0}, options);",
      "  }",
      "  bool has_zp = b_zeros.size(-1) > 0;",
      "  if (has_zp) {",
      "    TORCH_CHECK(",
      "        b_q_type == vllm::kU4 || b_q_type == vllm::kU8,",
      "        \"b_q_type must be u4 or u8 when has_zp = True. Got = \", b_q_type.str());",
      "  } else {",
      "    TORCH_CHECK(b_q_type == vllm::kU4B8 || b_q_type == vllm::kU8B128 ||",
      "                    b_q_type == vllm::kFE4M3fn || b_q_type == vllm::kFE2M1f,",
      "                \"b_q_type must be uint4b8, uint8b128, float8_e4m3fn or \"",
      "                \"float4_e2m1f when \"",
      "                \"has_zp = False. Got = \",",
      "                b_q_type.str());",
      "  }",
      "",
      "  if (has_zp && is_zp_float) {",
      "    TORCH_CHECK(a.scalar_type() == at::ScalarType::Half,",
      "                \"Computation type must be float16 (half) when using float zero \"",
      "                \"points.\");",
      "  }",
      "",
      "  // Verify b_zeros",
      "  if (has_zp) {",
      "    int rank = b_zeros.sizes().size();",
      "    TORCH_CHECK(rank == 3, \"b_zeros rank = \", rank, \" is not 3\");",
      "    if (is_zp_float) {",
      "      TORCH_CHECK(b_zeros.size(2) == size_n,",
      "                  \"b_zeros dim 2 = \", b_zeros.size(2),",
      "                  \" is not size_n = \", size_n);",
      "      TORCH_CHECK(num_groups == b_zeros.size(1),",
      "                  \"b_zeros dim 1 = \", b_zeros.size(1),",
      "                  \" is not num_groups = \", num_groups);",
      "      TORCH_CHECK(num_groups != -1, \"num_groups must be != -1\");",
      "    } else {",
      "      TORCH_CHECK(b_zeros.size(1) == num_groups,",
      "                  \"b_zeros dim 1 = \", b_zeros.size(1),",
      "                  \" is not num_groups = \", num_groups);",
      "      TORCH_CHECK(b_zeros.size(2) == size_n / pack_factor,",
      "                  \"b_zeros dim 2 = \", b_zeros.size(2),",
      "                  \" is not size_n / pack_factor = \", size_n / pack_factor);",
      "    }",
      "  }",
      "",
      "  // Verify workspace size",
      "  TORCH_CHECK(size_n % MARLIN_NAMESPACE_NAME::min_thread_n == 0,",
      "              \"size_n = \", size_n, \", is not divisible by min_thread_n = \",",
      "              MARLIN_NAMESPACE_NAME::min_thread_n);",
      "",
      "  int max_n_tiles = size_n / MARLIN_NAMESPACE_NAME::min_thread_n;",
      "  int min_workspace_size = min(",
      "      max_n_tiles * (int)(sorted_token_ids.size(0) / moe_block_size), sms * 4);",
      "  TORCH_CHECK(workspace.numel() >= min_workspace_size,",
      "              \"workspace.numel = \", workspace.numel(),",
      "              \" is below min_workspace_size = \", min_workspace_size);",
      "",
      "  int dev = a.get_device();",
      "  if (a.scalar_type() == at::ScalarType::Half) {",
      "    void* scales_ptr;",
      "    if (b_q_type == vllm::kFE2M1f) {",
      "      if (group_size == 16)",
      "        scales_ptr = b_scales.data_ptr<at::Float8_e4m3fn>();",
      "      else if (group_size == 32)",
      "        scales_ptr = b_scales.data_ptr<at::Float8_e8m0fnu>();",
      "      else",
      "        TORCH_CHECK(false,",
      "                    \"float4_e2m1f only supports group_size == 16 (NVFP4) \",",
      "                    \"and group_size == 32 (MXFP4)\");",
      "    } else {",
      "      scales_ptr = b_scales.data_ptr<at::Half>();",
      "    }",
      "",
      "    MARLIN_NAMESPACE_NAME::marlin_mm<half>(",
      "        a.data_ptr<at::Half>(), b_q_weight.data_ptr(), c.data_ptr<at::Half>(),",
      "        c_tmp.data_ptr<float>(), b_bias.data_ptr<at::Half>(), scales_ptr,",
      "        global_scale.data_ptr<at::Half>(), b_zeros.data_ptr(), g_idx.data_ptr(),",
      "        perm.data_ptr(), a_tmp.data_ptr<at::Half>(),",
      "        sorted_token_ids.data_ptr(), expert_ids.data_ptr(),",
      "        num_tokens_past_padded.data_ptr(), topk_weights.data_ptr(),",
      "        moe_block_size, top_k, mul_topk_weights, is_ep, size_m, size_n, size_k,",
      "        workspace.data_ptr(), b_q_type, has_bias, has_act_order, is_k_full,",
      "        has_zp, num_groups, group_size, dev,",
      "        at::cuda::getCurrentCUDAStream(dev), thread_k, thread_n, sms,",
      "        use_atomic_add, use_fp32_reduce, is_zp_float);",
      "  } else if (a.scalar_type() == at::ScalarType::BFloat16) {",
      "    void* scales_ptr;",
      "    if (b_q_type == vllm::kFE2M1f) {",
      "      if (group_size == 16)",
      "        scales_ptr = b_scales.data_ptr<at::Float8_e4m3fn>();",
      "      else if (group_size == 32)",
      "        scales_ptr = b_scales.data_ptr<at::Float8_e8m0fnu>();",
      "      else",
      "        TORCH_CHECK(false,",
      "                    \"float4_e2m1f only supports group_size == 16 (NVFP4) \",",
      "                    \"and group_size == 32 (MXFP4)\");",
      "    } else {",
      "      scales_ptr = b_scales.data_ptr<at::BFloat16>();",
      "    }",
      "",
      "    MARLIN_NAMESPACE_NAME::marlin_mm<nv_bfloat16>(",
      "        a.data_ptr<at::BFloat16>(), b_q_weight.data_ptr(),",
      "        c.data_ptr<at::BFloat16>(), c_tmp.data_ptr<float>(),",
      "        b_bias.data_ptr<at::BFloat16>(), scales_ptr,",
      "        global_scale.data_ptr<at::BFloat16>(), b_zeros.data_ptr(),",
      "        g_idx.data_ptr(), perm.data_ptr(), a_tmp.data_ptr<at::BFloat16>(),",
      "        sorted_token_ids.data_ptr(), expert_ids.data_ptr(),",
      "        num_tokens_past_padded.data_ptr(), topk_weights.data_ptr(),",
      "        moe_block_size, top_k, mul_topk_weights, is_ep, size_m, size_n, size_k,",
      "        workspace.data_ptr(), b_q_type, has_bias, has_act_order, is_k_full,",
      "        has_zp, num_groups, group_size, dev,",
      "        at::cuda::getCurrentCUDAStream(dev), thread_k, thread_n, sms,",
      "        use_atomic_add, use_fp32_reduce, is_zp_float);",
      "  } else {",
      "    TORCH_CHECK(false,",
      "                \"moe_wna16_marlin_gemm only supports bfloat16 and float16\");",
      "  }",
      "",
      "  return c;",
      "}",
      "",
      "#endif",
      "",
      "TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, CUDA, m) {",
      "  m.impl(\"moe_wna16_marlin_gemm\", &moe_wna16_marlin_gemm);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/rocm/attention.cu",
    "source": [
      "/*",
      " * Copyright (c) 2024, The vLLM team.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "",
      "#include <torch/all.h>",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <hip/hip_fp8.h>",
      "#include <hip/hip_bf16.h>",
      "#include \"../cuda_compat.h\"",
      "",
      "#include <algorithm>",
      "#include \"../attention/dtype_fp8.cuh\"",
      "#include \"../quantization/fp8/amd/quant_utils.cuh\"",
      "",
      "#if defined(__HIPCC__) && \\",
      "    (defined(__gfx90a__) || defined(__gfx942__) || defined(__gfx950__))",
      "  #define __HIP__GFX9__",
      "#endif",
      "",
      "#if defined(__HIPCC__) && (defined(__gfx1100__) || defined(__gfx1101__))",
      "  #define __HIP__GFX11__",
      "#endif",
      "",
      "#if defined(__HIPCC__) && (defined(__gfx1200__) || defined(__gfx1201__))",
      "  #define __HIP__GFX12__",
      "#endif",
      "",
      "#if defined(NDEBUG)",
      "  #undef NDEBUG",
      "  #include <assert.h>",
      "  #define UNREACHABLE_CODE assert(false);",
      "  #define NDEBUG",
      "#else",
      "  #define UNREACHABLE_CODE assert(false);",
      "#endif",
      "",
      "#define MAX(a, b) ((a) > (b) ? (a) : (b))",
      "#define MIN(a, b) ((a) < (b) ? (a) : (b))",
      "#define DIVIDE_ROUND_UP(a, b) (((a) + (b) - 1) / (b))",
      "",
      "#if defined(__HIP__GFX9__)",
      "",
      "  #define GCN_MFMA_INSTR1 __builtin_amdgcn_mfma_f32_16x16x4f32",
      "  #define GCN_MFMA_INSTR __builtin_amdgcn_mfma_f32_4x4x4f16",
      "",
      "using floatx4 = __attribute__((__vector_size__(4 * sizeof(float)))) float;",
      "using float16x4 =",
      "    __attribute__((__vector_size__(4 * sizeof(_Float16)))) _Float16;",
      "typedef float16x4 _Half4;",
      "using float16x2 =",
      "    __attribute__((__vector_size__(2 * sizeof(_Float16)))) _Float16;",
      "typedef float16x2 _Half2;",
      "typedef struct _Half8 {",
      "  _Half4 xy[2];",
      "} _Half8;",
      "",
      "using bit16_t = uint16_t;",
      "using bit16x4 = __attribute__((__vector_size__(4 * sizeof(uint16_t)))) uint16_t;",
      "typedef bit16x4 _B16x4;",
      "typedef struct _B16x8 {",
      "  _B16x4 xy[2];",
      "} _B16x8;",
      "",
      "using _B8x8 = uint2;",
      "using _B8x4 = int32_t;  // used in builtins",
      "using bit8_t = uint8_t;",
      "",
      "typedef struct _B8x16 {",
      "  _B8x8 xy[2];",
      "} _B8x16;",
      "",
      "template <typename T, int absz, int cbid, int blgp>",
      "__device__ __forceinline__ floatx4 gcn_mfma4x4x4_instr(const _B16x4& inpA,",
      "                                                       const _B16x4& inpB,",
      "                                                       const floatx4& inpC) {",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    return __builtin_amdgcn_mfma_f32_4x4x4f16(inpA, inpB, inpC, absz, cbid,",
      "                                              blgp);",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    return __builtin_amdgcn_mfma_f32_4x4x4bf16_1k(inpA, inpB, inpC, absz, cbid,",
      "                                                  blgp);",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "template <typename T, int absz, int cbid, int blgp>",
      "__device__ __forceinline__ floatx4 gcn_mfma16x16x16_instr(const _B16x4& inpA,",
      "                                                          const _B16x4& inpB,",
      "                                                          const floatx4& inpC) {",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    return __builtin_amdgcn_mfma_f32_16x16x16f16(inpA, inpB, inpC, absz, cbid,",
      "                                                 blgp);",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    return __builtin_amdgcn_mfma_f32_16x16x16bf16_1k(inpA, inpB, inpC, absz,",
      "                                                     cbid, blgp);",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ float to_float(const T& inp) {",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    return (float)inp;",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    return __bfloat162float(inp);",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ T from_float(const float& inp) {",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    return (_Float16)inp;",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    return __float2bfloat16(inp);",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ _B16x4 from_floatx4(const floatx4& inp) {",
      "  _B16x4 ret;",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    union h2cvt {",
      "      __half2 h2[2];",
      "      _B16x4 b16x4;",
      "    } u;",
      "    u.h2[0] = __float22half2_rn(make_float2(inp[0], inp[1]));",
      "    u.h2[1] = __float22half2_rn(make_float2(inp[2], inp[3]));",
      "    return u.b16x4;",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    for (int i = 0; i < 4; i++) {",
      "      union fcvt {",
      "        uint32_t u32;",
      "        float f32;",
      "      } u;",
      "      u.f32 = inp[i];",
      "      u.u32 += 0x7fff + ((u.u32 >> 16) & 1);  // BF16 RNE with no nan/inf check",
      "      ret[i] = uint16_t(u.u32 >> 16);",
      "    }",
      "    return ret;",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ _B16x4 addx4(const _B16x4& inp1,",
      "                                        const _B16x4& inp2) {",
      "  _B16x4 ret;",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    union h2cvt {",
      "      _B16x4 b16x4;",
      "      __half2 h2[2];",
      "    } u1, u2, s;",
      "    u1.b16x4 = inp1;",
      "    u2.b16x4 = inp2;",
      "    s.h2[0] = u1.h2[0] + u2.h2[0];",
      "    s.h2[1] = u1.h2[1] + u2.h2[1];",
      "    return s.b16x4;",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    for (int i = 0; i < 4; i++) {",
      "      union fcvt {",
      "        float f32;",
      "        uint32_t i32;",
      "      } u1, u2, s;",
      "      u1.i32 = uint32_t(inp1[i]) << 16;",
      "      u2.i32 = uint32_t(inp2[i]) << 16;",
      "      s.f32 = u1.f32 + u2.f32;",
      "      ret[i] = uint16_t(s.i32 >> 16);",
      "    }",
      "    return ret;",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "__device__ __forceinline__ floatx4 to_float_fp8x4(const _B8x4& inp) {",
      "  // From MI300+ platforms, we have v_cvt_pk_f32_fp8 instruction",
      "  // to convert 2 packed fp8 to 2 packed fp32 values.",
      "  // However, in MI200 platforms, we only have v_cvt_f32_fp8",
      "  // to convert fp8 values individually. So we added",
      "  // #else case for fewer instructions (# inst=2) in MI300+,",
      "  // and fallback to",
      "  // #if case for other platforms (# inst=4).",
      "  #if defined(__gfx90a__)",
      "  float4 f32x4 = vllm::fp8::vec_conversion<float4, uint32_t>(",
      "      *reinterpret_cast<const uint32_t*>(&inp));",
      "  return *reinterpret_cast<floatx4*>(&f32x4);",
      "  #else  // MI3xx+ optimized builtins",
      "  const auto f0 = __builtin_amdgcn_cvt_pk_f32_fp8(inp, false);",
      "  const auto f1 = __builtin_amdgcn_cvt_pk_f32_fp8(inp, true);",
      "  floatx4 ret;",
      "  ret[0] = f0[0];",
      "  ret[1] = f0[1];",
      "  ret[2] = f1[0];",
      "  ret[3] = f1[1];",
      "  return ret;",
      "  #endif",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ _B16x4 from_floatx4_rtz(const floatx4& inp) {",
      "  _B16x4 ret;",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    union h2cvt {",
      "      _Half2 h2[2];",
      "      _B16x4 b16x4;",
      "    } u;",
      "    u.h2[0] = __builtin_amdgcn_cvt_pkrtz(inp[0], inp[1]);",
      "    u.h2[1] = __builtin_amdgcn_cvt_pkrtz(inp[2], inp[3]);",
      "    return u.b16x4;",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    for (int i = 0; i < 4; i++) {",
      "      union fcvt {",
      "        uint32_t i32;",
      "        float f32;",
      "      } u;",
      "      u.f32 = inp[i];",
      "      ret[i] = uint16_t(u.i32 >> 16);",
      "    }",
      "    return ret;",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ _B16x8 convert_b8x8_custom(const _B8x8 input) {",
      "  union {",
      "    _B8x8 b8x8;",
      "    _B8x4 b8x4[2];",
      "  } tmp;",
      "  tmp.b8x8 = input;",
      "  _B16x8 ret;",
      "  for (int i = 0; i < 2; i++) {",
      "    ret.xy[i] = from_floatx4_rtz<T>(to_float_fp8x4(tmp.b8x4[i]));",
      "  }",
      "  return ret;",
      "}",
      "",
      "// grid (num_seqs, num_partitions,num_kv_heads)",
      "// block (256)",
      "// clang-format off",
      "template <typename scalar_t, typename cache_t,",
      "          vllm::Fp8KVCacheDataType KV_DTYPE, typename OUTT, int BLOCK_SIZE,",
      "          int HEAD_SIZE, int NUM_THREADS, bool ALIBI_ENABLED, int GQA_RATIO>",
      "__global__",
      "__launch_bounds__(NUM_THREADS, 5) void paged_attention_ll4mi_QKV_mfma16_kernel(",
      "    const scalar_t* __restrict__ q,         // [num_seqs, num_heads, head_size]",
      "    const cache_t* __restrict__ k_cache,    // [num_blocks, num_kv_heads, head_size/x, block_size, x]",
      "    const cache_t* __restrict__ v_cache,    // [num_blocks, num_kv_heads, head_size, block_size]",
      "    const int num_kv_heads,   ",
      "    const float scale,    ",
      "    const int* __restrict__ block_tables,   // [num_seqs, max_num_blocks_per_seq]",
      "    const int* __restrict__ seq_lens,   // [num_seqs]",
      "    const int* __restrict__ query_start_loc_ptr,   // [num_seqs]",
      "    const int max_num_blocks_per_seq,",
      "    const float* __restrict__ alibi_slopes, // [num_heads]",
      "    const int q_stride,",
      "    const int kv_block_stride,",
      "    const int kv_head_stride,",
      "    float* __restrict__ exp_sums,           // [num_seqs, num_heads, max_num_partitions]",
      "    float* __restrict__ max_logits,         // [num_seqs, num_heads, max_num_partitions]",
      "    scalar_t* __restrict__ out,             // [num_seqs, num_heads, max_num_partitions, head_size]",
      "    OUTT* __restrict__ final_out,           // [num_seqs, num_heads, head_size]",
      "    int max_ctx_blocks, const float* k_scale, const float* v_scale) {",
      "  // clang-format on",
      "  constexpr int NWARPS = NUM_THREADS / WARP_SIZE;",
      "  const auto warpid = threadIdx.x / WARP_SIZE;",
      "  const auto laneid = threadIdx.x % WARP_SIZE;",
      "  const int lane4id = laneid % 4;",
      "  const int lane16id = laneid % 16;",
      "  const int rowid = laneid / 16;",
      "",
      "  const auto seq_idx = blockIdx.x;",
      "  // NOTE queries with sequence len > 1 are prefills and taken care by another",
      "  // kernel.",
      "  if (query_start_loc_ptr != nullptr &&",
      "      (query_start_loc_ptr[seq_idx + 1] - query_start_loc_ptr[seq_idx]) != 1) {",
      "    return;",
      "  }",
      "",
      "  const auto partition_idx = blockIdx.y;",
      "",
      "  constexpr int T_PAR_SIZE = 256;  // token partition size set to 256",
      "",
      "  const auto max_num_partitions = gridDim.y;",
      "",
      "  const int seq_len = seq_lens[seq_idx];",
      "",
      "  const int partition_start_token_idx =",
      "      partition_idx * T_PAR_SIZE;  // partition_size;",
      "  // exit if partition is out of context for seq",
      "  if (partition_start_token_idx >= seq_len) {",
      "    return;",
      "  }",
      "",
      "  constexpr int GQA_RATIO4 = DIVIDE_ROUND_UP(GQA_RATIO, 4);",
      "",
      "  // shared_logits is used for multiple purposes",
      "  __shared__ _B16x4 shared_logits[NWARPS][4][16][4];",
      "",
      "  // for QK mfma16x16, layout is QHead/Tokenx16 across every 16 lanes, 16 Bytes",
      "  // HeadElements in each lane, 4x16B HeadElements across 4 rows of warp",
      "  constexpr int ROWS_PER_WARP =",
      "      WARP_SIZE / 16;  // rows refers to 16 lanes; refer DDP (Data Parallel",
      "                       // Processing) terminology",
      "  constexpr int CONTIGUOUS_KV_ELEMS_16B_LOAD =",
      "      16 / sizeof(cache_t);  // 8 for 16 bit cache type, 16 for 8 bit types",
      "  constexpr int QKHE_PER_FETCH =",
      "      CONTIGUOUS_KV_ELEMS_16B_LOAD *",
      "      ROWS_PER_WARP;  // each fetch across a warp fetches these many elements",
      "  constexpr int QK_SIZE_RATIO =",
      "      sizeof(scalar_t) /",
      "      sizeof(cache_t);  // 1 for 16bit types, 2 for 8bit types",
      "  constexpr int QKHELOOP = HEAD_SIZE / QKHE_PER_FETCH;  // 4xQKHE_16B across",
      "                                                        // warp",
      "",
      "  _B16x8 Qlocal[QKHELOOP]",
      "               [QK_SIZE_RATIO];  // note that 16 contiguous elements of Q should",
      "                                 // be fetched per lane for 8 bit cache types :",
      "                                 // QK_SIZE_RATIO changes for this",
      "",
      "  constexpr int CONTIGUOUS_SCALAR_ELEMS_16B = 16 / sizeof(scalar_t);",
      "",
      "  constexpr int TOKENS_PER_WARP =",
      "      T_PAR_SIZE /",
      "      NWARPS;  // sub partition of tokens per warp for qk calculation",
      "  constexpr int TLOOP =",
      "      TOKENS_PER_WARP /",
      "      16;  // each mfma16x16x16 instruction processes 16 tokens",
      "",
      "  // can be interpreted as B8x16 for 8 bit types",
      "  _B16x8 Klocal[TLOOP][QKHELOOP];",
      "",
      "  const auto wg_start_head_idx = blockIdx.z * GQA_RATIO;",
      "  const auto wg_start_kv_head_idx = blockIdx.z;",
      "  const auto total_num_heads = gridDim.z * GQA_RATIO;",
      "",
      "  // for QK mfma, tokens in multiples of TOKENS_PER_WARP are spread across warps",
      "  // each mfma takes QH16xT16x16HE across warp",
      "  // repeat mfmas across QKHELOOP dimension",
      "  // output layout from QKmfma : QH16xT4x4 16 qheads across 16 lanes, 16 tokens",
      "  // across 4 rows x 4 tokens per lane",
      "",
      "  const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);",
      "  const int last_seq_block = num_seq_blocks - 1;",
      "",
      "  const int* block_table_seq = block_tables + seq_idx * max_num_blocks_per_seq;",
      "",
      "  int kphysical_block_number[TLOOP];",
      "",
      "  // fetch k physical block numbers",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    const int klocal_token_idx =",
      "        TOKENS_PER_WARP * warpid + token_depth * 16 + lane16id;",
      "    const int kglobal_token_idx = partition_start_token_idx + klocal_token_idx;",
      "    const int kblock_idx = (kglobal_token_idx < seq_len)",
      "                               ? kglobal_token_idx / BLOCK_SIZE",
      "                               : last_seq_block;",
      "    kphysical_block_number[token_depth] = block_table_seq[kblock_idx];",
      "  }",
      "",
      "  // fetch Q in shared across warps and then write to registers",
      "  const int local_qhead_idx = 4 * warpid + rowid;",
      "  const int global_qhead_idx = wg_start_head_idx + local_qhead_idx;",
      "  const int64_t query_start_off = static_cast<int64_t>(",
      "      query_start_loc_ptr ? query_start_loc_ptr[seq_idx] : seq_idx);",
      "  const scalar_t* q_ptr =",
      "      q + query_start_off * q_stride + global_qhead_idx * HEAD_SIZE;",
      "",
      "  const int qhead_element = lane16id * CONTIGUOUS_SCALAR_ELEMS_16B;",
      "  if ((local_qhead_idx < GQA_RATIO) && (qhead_element < HEAD_SIZE)) {",
      "    const scalar_t* q_fetch_ptr = q_ptr + qhead_element;",
      "    const _B16x8* q_fetch_ptr_16B =",
      "        reinterpret_cast<const _B16x8*>(q_fetch_ptr);",
      "    _B16x8 tmp = *q_fetch_ptr_16B;",
      "    if constexpr (KV_DTYPE == vllm::Fp8KVCacheDataType::kAuto) {",
      "      const int offset1 =",
      "          lane16id /",
      "          4;  // 16 contiguous chunks of head elems are spread across 4x4lanes",
      "      shared_logits[offset1][lane4id][local_qhead_idx][0] = tmp.xy[0];",
      "      shared_logits[offset1][lane4id][local_qhead_idx][1] = tmp.xy[1];",
      "    } else {",
      "      for (int i = 0; i < 2; i++) {",
      "        const int head_elem = lane16id * 2 + i;  // element id in _B16x4 terms",
      "        const int offset3 = head_elem % 4;",
      "        const int offset2 = (head_elem / 4) % 4;",
      "        const int offset1 = head_elem / 4 / 4;",
      "        shared_logits[offset1][offset2][local_qhead_idx][offset3] = tmp.xy[i];",
      "      }",
      "    }",
      "  }",
      "  __syncthreads();",
      "  for (int qkhe_depth = 0; qkhe_depth < QKHELOOP; qkhe_depth++) {",
      "    for (int qkratio = 0; qkratio < QK_SIZE_RATIO; qkratio++) {",
      "      for (int i = 0; i < 2; i++) {",
      "        Qlocal[qkhe_depth][qkratio].xy[i] =",
      "            shared_logits[qkhe_depth][rowid][lane16id % GQA_RATIO]",
      "                         [2 * qkratio + i];",
      "      }",
      "    }",
      "  }",
      "",
      "  constexpr int KX =",
      "      16 / sizeof(cache_t);  // vLLM defines x as 16 Bytes of kv cache elements",
      "  const cache_t* k_ptr = k_cache + wg_start_kv_head_idx * kv_head_stride;",
      "",
      "  const int row_head_elem = rowid * CONTIGUOUS_KV_ELEMS_16B_LOAD;",
      "  // fetch K values",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    const int64_t kblock_number =",
      "        static_cast<int64_t>(kphysical_block_number[token_depth]);",
      "    const cache_t* k_ptr2 = k_ptr + kblock_number * kv_block_stride;",
      "    const int klocal_token_idx =",
      "        TOKENS_PER_WARP * warpid + token_depth * 16 + lane16id;",
      "    const int kphysical_block_offset = klocal_token_idx % BLOCK_SIZE;",
      "    const cache_t* k_ptr3 = k_ptr2 + kphysical_block_offset * KX;",
      "",
      "    for (int qkhe_depth = 0; qkhe_depth < QKHELOOP; qkhe_depth++) {",
      "      const int head_elem = row_head_elem + qkhe_depth * QKHE_PER_FETCH;",
      "      const int offset1 = head_elem / KX;",
      "      const int offset2 = head_elem % KX;",
      "      const cache_t* k_fetch_ptr = k_ptr3 + offset1 * BLOCK_SIZE * KX + offset2;",
      "      const _B16x8* k_fetch_ptr_16B =",
      "          reinterpret_cast<const _B16x8*>(k_fetch_ptr);",
      "      Klocal[token_depth][qkhe_depth] = *k_fetch_ptr_16B;",
      "    }",
      "  }",
      "",
      "  float alibi_slope;",
      "  if constexpr (ALIBI_ENABLED) {",
      "    const int alibi_head_idx = wg_start_head_idx + lane16id;",
      "    alibi_slope = (lane16id < GQA_RATIO) ? alibi_slopes[alibi_head_idx] : 0.f;",
      "  }",
      "",
      "  constexpr int VTOKENS_PER_LANE =",
      "      TOKENS_PER_WARP / ROWS_PER_WARP;  // 64/4 = 16 contiguous vtokens per lane",
      "  constexpr int VBLOCKS_PER_LANE =",
      "      1;  // assumes block size >=16, each lane can correspond to 1 block only",
      "  constexpr int VTLOOP = NWARPS;  // corresponds to tokens across warps",
      "  constexpr int VTLANELOOP = DIVIDE_ROUND_UP(",
      "      VTOKENS_PER_LANE,",
      "      CONTIGUOUS_KV_ELEMS_16B_LOAD);  // optimized for 16B fetches; assumes",
      "                                      // minimum block size is 16",
      "  constexpr int VHELOOP = HEAD_SIZE / 16 / NWARPS;",
      "",
      "  int vphysical_block_number[VTLOOP][VBLOCKS_PER_LANE];",
      "",
      "  // fetch v physical block numbers",
      "  for (int vtoken_depth = 0; vtoken_depth < VTLOOP; vtoken_depth++) {",
      "    for (int vblock_depth = 0; vblock_depth < VBLOCKS_PER_LANE;",
      "         vblock_depth++) {",
      "      const int vlocal_token_idx =",
      "          vtoken_depth * VTOKENS_PER_LANE * ROWS_PER_WARP +",
      "          rowid * VTOKENS_PER_LANE + vblock_depth * BLOCK_SIZE;",
      "      // Safe to use an int32_t here assuming we are working with < 2 billion",
      "      // tokens",
      "      const int vglobal_token_idx =",
      "          partition_start_token_idx + vlocal_token_idx;",
      "      const int vblock_idx = (vglobal_token_idx < seq_len)",
      "                                 ? vglobal_token_idx / BLOCK_SIZE",
      "                                 : last_seq_block;",
      "      vphysical_block_number[vtoken_depth][vblock_depth] =",
      "          block_table_seq[vblock_idx];",
      "    }",
      "  }",
      "",
      "  _B16x8 Vlocal[VTLOOP][VHELOOP][VTLANELOOP];  // this could be B8x16 too",
      "",
      "  const cache_t* v_ptr = v_cache + wg_start_kv_head_idx * kv_head_stride +",
      "                         ((rowid * VTOKENS_PER_LANE) % BLOCK_SIZE);",
      "",
      "  // v fetches are 16head elems across lanes x 16 tokens per lane",
      "  for (int vhe_depth = 0; vhe_depth < VHELOOP; vhe_depth++) {",
      "    const int vhead_elem = vhe_depth * NWARPS * 16 + warpid * 16 + lane16id;",
      "    const cache_t* v_ptr2 = v_ptr + vhead_elem * BLOCK_SIZE;",
      "",
      "    for (int vtoken_depth = 0; vtoken_depth < VTLOOP; vtoken_depth++) {",
      "      for (int vfetch_depth = 0; vfetch_depth < VTLANELOOP; vfetch_depth++) {",
      "        const int vblock_depth = 0;",
      "        const int64_t vblock_number = static_cast<int64_t>(",
      "            vphysical_block_number[vtoken_depth][vblock_depth]);",
      "        const cache_t* v_ptr3 = v_ptr2 + (vblock_number * kv_block_stride);",
      "",
      "        const cache_t* v_fetch_ptr =",
      "            v_ptr3 + vfetch_depth * CONTIGUOUS_KV_ELEMS_16B_LOAD;",
      "        const _B16x8* v_fetch_ptr_16B =",
      "            reinterpret_cast<const _B16x8*>(v_fetch_ptr);",
      "        Vlocal[vtoken_depth][vhe_depth][vfetch_depth] = *v_fetch_ptr_16B;",
      "      }",
      "    }",
      "  }",
      "",
      "  // calculate post qk mfma scale",
      "  float scale2 = scale;",
      "  if constexpr (KV_DTYPE != vllm::Fp8KVCacheDataType::kAuto) {",
      "    // multiply by k_scale if fp8 kv cache",
      "    scale2 *= *k_scale;",
      "  }",
      "",
      "  floatx4 d_out[TLOOP];",
      "  // qk mfma",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    d_out[token_depth] = {0};",
      "    for (int qkhe_depth = 0; qkhe_depth < QKHELOOP; qkhe_depth++) {",
      "      if constexpr (KV_DTYPE == vllm::Fp8KVCacheDataType::kAuto) {",
      "        for (int qkratio = 0; qkratio < QK_SIZE_RATIO; qkratio++) {",
      "          for (int i = 0; i < 2; i++) {",
      "            d_out[token_depth] = gcn_mfma16x16x16_instr<scalar_t, 0, 0, 0>(",
      "                Klocal[token_depth][qkhe_depth].xy[i],",
      "                Qlocal[qkhe_depth][qkratio].xy[i], d_out[token_depth]);",
      "          }",
      "        }",
      "      } else {  // kv cache dtype fp8",
      "        auto Ktmp = Klocal[token_depth][qkhe_depth];",
      "        _B8x16 Ktmp8x16 = *reinterpret_cast<_B8x16*>(&Ktmp);",
      "        for (int qkratio = 0; qkratio < QK_SIZE_RATIO; qkratio++) {",
      "          _B8x8 Ktmp8x8 = Ktmp8x16.xy[qkratio];",
      "          _B16x8 Klocaltmp = convert_b8x8_custom<scalar_t>(Ktmp8x8);",
      "          for (int i = 0; i < 2; i++) {",
      "            d_out[token_depth] = gcn_mfma16x16x16_instr<scalar_t, 0, 0, 0>(",
      "                Klocaltmp.xy[i], Qlocal[qkhe_depth][qkratio].xy[i],",
      "                d_out[token_depth]);",
      "          }",
      "        }",
      "      }",
      "    }",
      "    d_out[token_depth] *= scale2;",
      "  }",
      "",
      "  const int qkout_token_idx =",
      "      partition_start_token_idx + TOKENS_PER_WARP * warpid + rowid * 4;",
      "",
      "  // apply alibi",
      "  if constexpr (ALIBI_ENABLED) {",
      "    for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "      const int local_token_idx = qkout_token_idx + token_depth * 16;",
      "      const int alibi_offset = local_token_idx - seq_len + 1;",
      "      for (int i = 0; i < 4; i++) {",
      "        d_out[token_depth][i] += alibi_slope * (alibi_offset + i);",
      "      }",
      "    }",
      "  }",
      "",
      "  // calculate qk_max and exp_sum per warp and write to shared memory",
      "  float qk_max = -FLT_MAX;",
      "  float exp_sum = 0.0f;",
      "",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    const int local_token_idx = qkout_token_idx + token_depth * 16;",
      "    for (int i = 0; i < 4; i++) {",
      "      const float tmp =",
      "          (local_token_idx + i < seq_len) ? d_out[token_depth][i] : -FLT_MAX;",
      "      qk_max = fmaxf(qk_max, tmp);",
      "    }",
      "  }",
      "",
      "  for (int mask = WARP_SIZE / 2; mask >= 16; mask /= 2) {",
      "    qk_max = fmaxf(qk_max, __shfl_xor(qk_max, mask));",
      "  }",
      "",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    const int local_token_idx = qkout_token_idx + token_depth * 16;",
      "    for (int i = 0; i < 4; i++) {",
      "      const float tmp = (local_token_idx + i < seq_len)",
      "                            ? __expf(d_out[token_depth][i] - qk_max)",
      "                            : 0.0f;",
      "      d_out[token_depth][i] = tmp;",
      "      exp_sum += tmp;",
      "    }",
      "  }",
      "",
      "  for (int mask = WARP_SIZE / 2; mask >= 16; mask /= 2) {",
      "    exp_sum += __shfl_xor(exp_sum, mask);",
      "  }",
      "",
      "  __syncthreads();  // sync before writing to shared mem",
      "",
      "  float* shared_mem = reinterpret_cast<float*>(shared_logits);",
      "  if (laneid < 16) {",
      "    const int qk_max_offset = warpid * 16 + lane16id;",
      "    shared_mem[qk_max_offset] = qk_max;",
      "    const int exp_sum_offset = NWARPS * 16 + qk_max_offset;",
      "    shared_mem[exp_sum_offset] = exp_sum;",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  // calculate partition qk_max and exp_sum",
      "  float partition_qk_max = -FLT_MAX;",
      "  float warp_qk_max_exp[NWARPS];",
      "  float partition_exp_sum = 0.0f;",
      "",
      "  for (int w = 0; w < NWARPS; w++) {",
      "    warp_qk_max_exp[w] = shared_mem[w * 16 + lane16id];",
      "    partition_qk_max = fmaxf(partition_qk_max, warp_qk_max_exp[w]);",
      "  }",
      "",
      "  for (int w = 0; w < NWARPS; w++) {",
      "    warp_qk_max_exp[w] = __expf(warp_qk_max_exp[w] - partition_qk_max);",
      "    partition_exp_sum +=",
      "        shared_mem[NWARPS * 16 + w * 16 + lane16id] * warp_qk_max_exp[w];",
      "  }",
      "",
      "  const float inv_sum_scale =",
      "      __fdividef(1.f, partition_exp_sum + 1e-6f) * warp_qk_max_exp[warpid];",
      "",
      "  __syncthreads();",
      "",
      "  // disable rtz conversion due to its impact on accuracy.",
      "  constexpr bool LOGITS_RTZ_CONVERSION = false;",
      "",
      "  // write logits to shared mem",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    d_out[token_depth] *= inv_sum_scale;",
      "    if constexpr (LOGITS_RTZ_CONVERSION) {",
      "      // use rtz conversion for better performance, with negligible impact on",
      "      // accuracy",
      "      shared_logits[warpid][token_depth][lane16id][rowid] =",
      "          from_floatx4_rtz<scalar_t>(d_out[token_depth]);",
      "    } else {",
      "      shared_logits[warpid][token_depth][lane16id][rowid] =",
      "          from_floatx4<scalar_t>(d_out[token_depth]);",
      "    }",
      "  }",
      "",
      "  // write out partition max_logits and exp_sum",
      "  if (threadIdx.x < GQA_RATIO) {",
      "    const int qhead_idx = lane16id;",
      "    const int64_t offset = static_cast<int64_t>(seq_idx) *",
      "                               static_cast<int64_t>(total_num_heads) *",
      "                               static_cast<int64_t>(max_num_partitions) +",
      "                           (static_cast<int64_t>(wg_start_head_idx) +",
      "                            static_cast<int64_t>(qhead_idx)) *",
      "                               static_cast<int64_t>(max_num_partitions) +",
      "                           static_cast<int64_t>(partition_idx);",
      "    max_logits[offset] = partition_qk_max;",
      "    exp_sums[offset] = partition_exp_sum;",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  constexpr int ELEMS8_ELEMS4_RATIO = 8 / 4;",
      "  constexpr int ELEMS16_ELEMS8_RATIO = 16 / 8;",
      "",
      "  _B16x4 outelems[VHELOOP];",
      "  // Softmax V mfma",
      "  // v layout: 16he across lanes x 16 tokens per lane",
      "  for (int vhe_depth = 0; vhe_depth < VHELOOP; vhe_depth++) {",
      "    floatx4 tmp_out = {0};",
      "",
      "    for (int vtoken_depth = 0; vtoken_depth < VTLOOP; vtoken_depth++) {",
      "      if constexpr (KV_DTYPE == vllm::Fp8KVCacheDataType::kAuto) {",
      "        for (int vfetch_depth = 0; vfetch_depth < VTLANELOOP; vfetch_depth++) {",
      "          for (int i = 0; i < ELEMS8_ELEMS4_RATIO; i++) {",
      "            const int offset = rowid * VTLANELOOP * ELEMS8_ELEMS4_RATIO +",
      "                               vfetch_depth * ELEMS8_ELEMS4_RATIO + i;",
      "            const int offset1 = offset % ROWS_PER_WARP;",
      "            const int offset2 = offset / ROWS_PER_WARP;",
      "            // output format is 16 qheads across 16 lanes, 16 head elems spread",
      "            // across 4 rows",
      "            tmp_out = gcn_mfma16x16x16_instr<scalar_t, 0, 0, 0>(",
      "                Vlocal[vtoken_depth][vhe_depth][vfetch_depth].xy[i],",
      "                shared_logits[vtoken_depth][offset2][lane16id][offset1],",
      "                tmp_out);",
      "          }",
      "        }",
      "        // KV cache fp8",
      "      } else {",
      "        for (int vfetch_depth = 0; vfetch_depth < VTLANELOOP; vfetch_depth++) {",
      "          _B16x8 Vtmp = Vlocal[vtoken_depth][vhe_depth][vfetch_depth];",
      "          // reinterpret V format as 16 elements of 8bits",
      "          _B8x16 Vtmp8x16 = *reinterpret_cast<_B8x16*>(&Vtmp);",
      "          for (int j = 0; j < ELEMS16_ELEMS8_RATIO; j++) {",
      "            _B8x8 Vtmp8x8 = Vtmp8x16.xy[j];",
      "            _B16x8 Vlocaltmp = convert_b8x8_custom<scalar_t>(Vtmp8x8);",
      "            for (int i = 0; i < ELEMS8_ELEMS4_RATIO; i++) {",
      "              const int offset =",
      "                  rowid * ELEMS16_ELEMS8_RATIO * ELEMS8_ELEMS4_RATIO +",
      "                  j * ELEMS8_ELEMS4_RATIO + i;",
      "              const int offset1 = offset % ROWS_PER_WARP;",
      "              const int offset2 = offset / ROWS_PER_WARP;",
      "              // output format is 16 qheads across 16 lanes, 16 head elems",
      "              // spread across 4 rows",
      "              tmp_out = gcn_mfma16x16x16_instr<scalar_t, 0, 0, 0>(",
      "                  Vlocaltmp.xy[i],",
      "                  shared_logits[vtoken_depth][offset2][lane16id][offset1],",
      "                  tmp_out);",
      "            }",
      "          }",
      "        }",
      "      }",
      "    }",
      "    // apply post Softmax V mfma v_scale",
      "    if constexpr (KV_DTYPE != vllm::Fp8KVCacheDataType::kAuto) {",
      "      tmp_out *= *v_scale;",
      "    }",
      "    outelems[vhe_depth] = from_floatx4<scalar_t>(tmp_out);",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  // store Softmax-V mfma output to shared mem",
      "  for (int vhe_depth = 0; vhe_depth < VHELOOP; vhe_depth++) {",
      "    // lane16 id head dimension; rowid head element dimension",
      "    shared_logits[warpid][vhe_depth][lane16id][rowid] = outelems[vhe_depth];",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  // write to tmp_out with coalesced writes after reading from shared mem",
      "  if (warpid == 0) {",
      "    _B16x8 vout[GQA_RATIO4];",
      "    // each lane writes out 16Bytes of tmp_out along head elem dimension",
      "    const int head_elem_idx = lane16id * 8;",
      "    if (head_elem_idx < HEAD_SIZE) {",
      "      for (int h = 0; h < GQA_RATIO4; h++) {",
      "        const int local_head_idx = 4 * h + rowid;",
      "        const int offset1 = (head_elem_idx / 16) % 4;",
      "        const int offset2 = head_elem_idx / 16 / NWARPS;",
      "        const int offset3 = (head_elem_idx / 4) % 4;",
      "        for (int i = 0; i < 2; i++) {",
      "          vout[h].xy[i] =",
      "              shared_logits[offset1][offset2][local_head_idx][offset3 + i];",
      "        }",
      "      }",
      "",
      "      const int64_t hsz_maxp_mult =",
      "          static_cast<int64_t>(HEAD_SIZE * max_num_partitions);",
      "      scalar_t* out_ptr = out + seq_idx * total_num_heads * hsz_maxp_mult +",
      "                          partition_idx * HEAD_SIZE;",
      "      for (int h = 0; h < GQA_RATIO4; h++) {",
      "        const int local_head_idx = 4 * h + rowid;",
      "        if (local_head_idx < GQA_RATIO) {",
      "          const int64_t out_head_idx =",
      "              static_cast<int64_t>(wg_start_head_idx + local_head_idx);",
      "          scalar_t* out_ptr2 = out_ptr + out_head_idx * hsz_maxp_mult;",
      "          scalar_t* out_ptr3 = out_ptr2 + head_elem_idx;",
      "          _B16x8* out_ptr_B16x8 = reinterpret_cast<_B16x8*>(out_ptr3);",
      "          *out_ptr_B16x8 = vout[h];",
      "        }",
      "      }",
      "    }",
      "  }",
      "}",
      "",
      "// grid (num_seqs, num_partitions, num_kv_heads)",
      "// block (256 : partition size)",
      "// each WG handles 1 partition per sequence",
      "// clang-format off",
      "template <typename scalar_t, typename cache_t,",
      "          vllm::Fp8KVCacheDataType KV_DTYPE, typename OUTT, int BLOCK_SIZE,",
      "          int HEAD_SIZE, int NUM_THREADS, bool ALIBI_ENABLED,",
      "          int GQA_RATIO>",
      "__global__",
      "__launch_bounds__(NUM_THREADS) void paged_attention_ll4mi_QKV_mfma4_kernel(",
      "    const scalar_t* __restrict__ q,         // [num_seqs, num_heads, head_size]",
      "    const cache_t* __restrict__ k_cache,    // [num_blocks, num_kv_heads, head_size/x, block_size, x]",
      "    const cache_t* __restrict__ v_cache,    // [num_blocks, num_kv_heads, head_size, block_size]",
      "    const int num_kv_heads,",
      "    const float scale,",
      "    const int* __restrict__ block_tables,   // [num_seqs, max_num_blocks_per_seq]",
      "    const int* __restrict__ seq_lens,   // [num_seqs]",
      "    const int* __restrict__ query_start_loc_ptr,   // [num_seqs]",
      "    const int max_num_blocks_per_seq,",
      "    const float* __restrict__ alibi_slopes, // [num_heads]",
      "    const int q_stride,",
      "    const int kv_block_stride,",
      "    const int kv_head_stride,",
      "    float* __restrict__ exp_sums,           // [num_seqs, num_heads, max_num_partitions]",
      "    float* __restrict__ max_logits,         // [num_seqs, num_heads, max_num_partitions]",
      "    scalar_t* __restrict__ out,             // [num_seqs, num_heads, max_num_partitions, head_size]",
      "    OUTT* __restrict__ final_out,           // [num_seqs, num_heads, head_size]",
      "    int max_ctx_blocks, const float* k_scale, const float* v_scale) {",
      "  // clang-format on",
      "  constexpr int NWARPS = NUM_THREADS / WARP_SIZE;",
      "  const auto warpid = threadIdx.x / WARP_SIZE;",
      "  const auto laneid = threadIdx.x % WARP_SIZE;",
      "  const int lane4id = laneid % 4;",
      "",
      "  const auto seq_idx = blockIdx.x;",
      "  // NOTE queries with sequence len > 1 are prefills and taken care by another",
      "  // kernel.",
      "  if (query_start_loc_ptr != nullptr &&",
      "      (query_start_loc_ptr[seq_idx + 1] - query_start_loc_ptr[seq_idx] != 1)) {",
      "    return;",
      "  }",
      "  const auto partition_idx = blockIdx.y;",
      "  const auto partition_size = blockDim.x;",
      "  const auto max_num_partitions = gridDim.y;",
      "",
      "  const int seq_len = seq_lens[seq_idx];",
      "  const int partition_start_token_idx = partition_idx * partition_size;",
      "  // exit if partition is out of context for seq",
      "  if (partition_start_token_idx >= seq_len) {",
      "    return;",
      "  }",
      "  // every 4 lanes fetch 4 different qheads",
      "  // qhloop = num loops over qhead dimension",
      "  constexpr int QHLOOP = DIVIDE_ROUND_UP(GQA_RATIO, 4);",
      "  constexpr int GQA_RATIO4 = 4 * QHLOOP;",
      "  __shared__ float shared_qk_max[NWARPS][GQA_RATIO4 + 1];",
      "  __shared__ float shared_exp_sum[NWARPS][GQA_RATIO4 + 1];",
      "  _B16x8 Qlocal[QHLOOP];",
      "  constexpr int x = 16 / sizeof(scalar_t);",
      "  // kheloop = num loops over head_size for 16Bytes of Q/dequantized K elements",
      "  constexpr int KHELOOP = HEAD_SIZE / x;",
      "  _B16x8 Klocal[KHELOOP];",
      "  _B8x8 Klocalb8[KHELOOP];",
      "  // for SoftMax-V Gemm, V head_size dimension is distributed across warp",
      "  // vheloop = num loops to cover v head size dimension",
      "  constexpr int VHELOOP = HEAD_SIZE / WARP_SIZE;",
      "  // softmax out has warp_size tokens across warp",
      "  // vtloop = num loops to cover warp_size(64) tokens with 16Bytes of",
      "  // dequantized V elements",
      "  constexpr int VTLOOP = WARP_SIZE / 8;",
      "  // num vblocks to cover warp_size(64) v elements",
      "  constexpr int VBLOCKS = 8 * VTLOOP / BLOCK_SIZE;",
      "  int vphysical_blocks[VBLOCKS];",
      "  _B16x8 Vlocal[VHELOOP][VTLOOP];",
      "  _B8x8 Vlocalb8[VHELOOP][VTLOOP];",
      "  floatx4 d_out[QHLOOP];",
      "  float qk_max[QHLOOP];",
      "",
      "  __shared__ _B16x4 vout_shared[QHLOOP][VHELOOP][WARP_SIZE][NWARPS + 1];",
      "",
      "  for (int h = 0; h < QHLOOP; h++) {",
      "    d_out[h] = {0};",
      "    qk_max[h] = -FLT_MAX;",
      "  }",
      "",
      "  const auto wg_start_head_idx = blockIdx.z * GQA_RATIO;",
      "  const auto wg_start_kv_head_idx = blockIdx.z;",
      "",
      "  const int warp_start_token_idx =",
      "      partition_start_token_idx + warpid * WARP_SIZE;",
      "",
      "  if (warp_start_token_idx >= seq_len) {  // warp out of context",
      "  #pragma unroll",
      "    for (int h = 0; h < GQA_RATIO4; h++) {",
      "      shared_qk_max[warpid][h] = -FLT_MAX;",
      "      shared_exp_sum[warpid][h] = 0.0f;",
      "    }",
      "  } else {  // warp within context",
      "",
      "    const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);",
      "    const int last_seq_block = num_seq_blocks - 1;",
      "",
      "    const int* block_table = block_tables + seq_idx * max_num_blocks_per_seq;",
      "    // token id within partition",
      "    const auto local_token_idx = threadIdx.x;",
      "    // token id within sequence",
      "    const int global_token_idx = partition_start_token_idx + local_token_idx;",
      "",
      "    // fetch block number for k",
      "    const int block_idx = (global_token_idx < seq_len)",
      "                              ? global_token_idx / BLOCK_SIZE",
      "                              : last_seq_block;",
      "",
      "    // fetch k physical block number",
      "    //  int32 physical_block_number leads to overflow when multiplied with",
      "    //  kv_block_stride",
      "    const int64_t physical_block_number =",
      "        static_cast<int64_t>(block_table[block_idx]);",
      "",
      "    // fetch vphysical block numbers up front",
      "    const int warp_start_block_idx = warp_start_token_idx / BLOCK_SIZE;",
      "    for (int b = 0; b < VBLOCKS; b++) {",
      "      const int vblock_idx = warp_start_block_idx + b;",
      "      const int vblock_idx_ctx =",
      "          (vblock_idx <= last_seq_block) ? vblock_idx : last_seq_block;",
      "      vphysical_blocks[b] = block_table[vblock_idx_ctx];",
      "    }",
      "",
      "    // fetch q elements",
      "    // every 4 lanes fetch 8 elems, so warp fetches 8*16 = 128 elemsc",
      "    const int64_t query_start_off = static_cast<int64_t>(",
      "        query_start_loc_ptr ? query_start_loc_ptr[seq_idx] : seq_idx);",
      "    const scalar_t* q_ptr =",
      "        q + query_start_off * q_stride + wg_start_head_idx * HEAD_SIZE;",
      "    const _B16x8* q_ptrh8 = reinterpret_cast<const _B16x8*>(q_ptr);",
      "    const int qhead_elemh8 = laneid / 4;",
      "",
      "    for (int h = 0; h < QHLOOP - 1; h++) {",
      "      const int qhead_idx = h * 4 + lane4id;",
      "      Qlocal[h] = q_ptrh8[qhead_idx * HEAD_SIZE / 8 + qhead_elemh8];",
      "    }",
      "    const int final_qhead_idx = 4 * (QHLOOP - 1) + lane4id;",
      "    if (final_qhead_idx < GQA_RATIO) {",
      "      Qlocal[QHLOOP - 1] =",
      "          q_ptrh8[final_qhead_idx * HEAD_SIZE / 8 + qhead_elemh8];",
      "    } else {",
      "      Qlocal[QHLOOP - 1].xy[0] = {0};",
      "      Qlocal[QHLOOP - 1].xy[1] = {0};",
      "    }",
      "",
      "    // fetch k elements",
      "    const cache_t* k_ptr = k_cache + physical_block_number * kv_block_stride +",
      "                           wg_start_kv_head_idx * kv_head_stride;",
      "",
      "    // physical_block_offset is already cast in terms of _B16x8",
      "    const int physical_block_offset = local_token_idx % BLOCK_SIZE;",
      "",
      "    // each K fetch is for 8 elements of cache_t which are later dequantized to",
      "    // scalar_t for fp8",
      "    if constexpr (KV_DTYPE == vllm::Fp8KVCacheDataType::kAuto) {",
      "      const _B16x8* k_ptrh8 = reinterpret_cast<const _B16x8*>(k_ptr);",
      "      for (int d = 0; d < KHELOOP; d++) {",
      "        Klocal[d] = k_ptrh8[d * BLOCK_SIZE + physical_block_offset];",
      "      }",
      "    } else {",
      "      // vllm defines X as 16 Bytes of elements of cache_t",
      "      constexpr int X = 16 / sizeof(cache_t);",
      "      const cache_t* k_ptr2 = k_ptr + physical_block_offset * X;",
      "      for (int d = 0; d < KHELOOP; d++) {",
      "        const int head_elem = d * 8;",
      "        const int offset1 = head_elem / X;",
      "        const int offset2 = head_elem % X;",
      "        const cache_t* k_ptr3 = k_ptr2 + offset1 * BLOCK_SIZE * X + offset2;",
      "        Klocalb8[d] = *reinterpret_cast<const _B8x8*>(k_ptr3);",
      "      }",
      "    }",
      "",
      "    // optional alibi fetch",
      "    float alibi_slope[QHLOOP];",
      "    if constexpr (ALIBI_ENABLED) {",
      "      for (int h = 0; h < QHLOOP; h++) {",
      "        const int qhead_idx = h * 4 + lane4id;",
      "        alibi_slope[h] = (qhead_idx < GQA_RATIO)",
      "                             ? alibi_slopes[wg_start_head_idx + qhead_idx]",
      "                             : 0.f;",
      "      }",
      "    }",
      "",
      "    const cache_t* v_ptr = v_cache + wg_start_kv_head_idx * kv_head_stride;",
      "    // fetch vcache in kv cache auto case",
      "    if constexpr (KV_DTYPE == vllm::Fp8KVCacheDataType::kAuto) {",
      "      const _B16x8* v_ptrh8 = reinterpret_cast<const _B16x8*>(v_ptr);",
      "      // iterate over each v block",
      "      for (int b = 0; b < VBLOCKS; b++) {",
      "        // int32 physical_block_number leads to overflow when multiplied with",
      "        // kv_block_stride",
      "        const int64_t vphysical_block_number =",
      "            static_cast<int64_t>(vphysical_blocks[b]);",
      "        const _B16x8* v_ptrh8b =",
      "            v_ptrh8 + (vphysical_block_number * kv_block_stride) / 8;",
      "        // iterate over each head elem (within head_size)",
      "        for (int h = 0; h < VHELOOP; h++) {",
      "          const int head_size_elem = h * WARP_SIZE + laneid;",
      "          const _B16x8* v_ptrh8be = v_ptrh8b + head_size_elem * BLOCK_SIZE / 8;",
      "          // iterate over all velems within block",
      "          for (int d = 0; d < BLOCK_SIZE / 8; d++) {",
      "            Vlocal[h][b * BLOCK_SIZE / 8 + d] = v_ptrh8be[d];",
      "          }",
      "        }",
      "      }",
      "    }  // if constexpr (KV_DTYPE == vllm::Fp8KVCacheDataType::kAuto)",
      "    // fetch vcache in fp8 case",
      "    else {  // if constexpr (KV_DTYPE != vllm::Fp8KVCacheDataType::kAuto)",
      "      const _B8x8* v_ptrh8 = reinterpret_cast<const _B8x8*>(v_ptr);",
      "      // iterate over each v block",
      "      for (int b = 0; b < VBLOCKS; b++) {",
      "        // int32 physical_block_number leads to overflow when multiplied with",
      "        // kv_block_stride",
      "        const int64_t vphysical_block_number =",
      "            static_cast<int64_t>(vphysical_blocks[b]);",
      "        const _B8x8* v_ptrh8b =",
      "            v_ptrh8 + (vphysical_block_number * kv_block_stride) / 8;",
      "        // iterate over each head elem (within head_size)",
      "        for (int h = 0; h < VHELOOP; h++) {",
      "          const int head_size_elem = h * WARP_SIZE + laneid;",
      "          const _B8x8* v_ptrh8be = v_ptrh8b + head_size_elem * BLOCK_SIZE / 8;",
      "          // iterate over all velems within block",
      "          for (int d = 0; d < BLOCK_SIZE / 8; d++) {",
      "            Vlocalb8[h][b * BLOCK_SIZE / 8 + d] = v_ptrh8be[d];",
      "          }",
      "        }",
      "      }",
      "    }",
      "",
      "  #define QK_mfma(x)                                             \\",
      "    if constexpr (KV_DTYPE != vllm::Fp8KVCacheDataType::kAuto) { \\",
      "      Klocal[x] = convert_b8x8_custom<scalar_t>(Klocalb8[x]);    \\",
      "    }                                                            \\",
      "    for (int h = 0; h < QHLOOP; h++) {                           \\",
      "      d_out[h] = gcn_mfma4x4x4_instr<scalar_t, 4, x, 0>(         \\",
      "          Qlocal[h].xy[0], Klocal[x].xy[0], d_out[h]);           \\",
      "      d_out[h] = gcn_mfma4x4x4_instr<scalar_t, 4, x, 0>(         \\",
      "          Qlocal[h].xy[1], Klocal[x].xy[1], d_out[h]);           \\",
      "    }",
      "    // QK mfma with Q mfma block broadcast",
      "    // Q values across head_size dimension stored across lanes",
      "    // K values across head_size dimension are stored depthwise within lane",
      "    // Q broadcast with absz, cbid of mfma instruction",
      "    QK_mfma(0);",
      "    QK_mfma(1);",
      "    QK_mfma(2);",
      "    QK_mfma(3);",
      "    QK_mfma(4);",
      "    QK_mfma(5);",
      "    QK_mfma(6);",
      "    QK_mfma(7);",
      "    // below only needed for head size 128",
      "    if constexpr (KHELOOP > 8) {",
      "      QK_mfma(8);",
      "      QK_mfma(9);",
      "      QK_mfma(10);",
      "      QK_mfma(11);",
      "      QK_mfma(12);",
      "      QK_mfma(13);",
      "      QK_mfma(14);",
      "      QK_mfma(15);",
      "    }",
      "  #undef QK_mfma",
      "",
      "    float scale2 = scale;",
      "    if constexpr (KV_DTYPE != vllm::Fp8KVCacheDataType::kAuto) {",
      "      // post mfma scaling for fp8",
      "      scale2 *= *k_scale;",
      "    }",
      "",
      "    for (int h = 0; h < QHLOOP; h++) {",
      "      d_out[h] *= scale2;",
      "    }",
      "",
      "    // transpose d_out so that 4 token ids are in each lane, and 4 heads are",
      "    // across 4 lanes",
      "    for (int h = 0; h < QHLOOP; h++) {",
      "      floatx4 tmp = {0};",
      "      for (int i = 0; i < 4; i++) {",
      "        const float B = (lane4id == i) ? 1.0f : 0.0f;",
      "        tmp = __builtin_amdgcn_mfma_f32_4x4x1f32(d_out[h][i], B, tmp, 0, 0, 0);",
      "      }",
      "      d_out[h] = tmp;",
      "    }",
      "",
      "    const int lane4_token_idx = 4 * (global_token_idx >> 2);",
      "",
      "    if constexpr (ALIBI_ENABLED) {",
      "      const int alibi_offset = lane4_token_idx - seq_len + 1;",
      "      for (int h = 0; h < QHLOOP; h++) {",
      "        for (int i = 0; i < 4; i++) {",
      "          d_out[h][i] += alibi_slope[h] * (alibi_offset + i);",
      "        }",
      "      }",
      "    }",
      "",
      "    const int bpermute_mask = 4 * (16 * ((laneid >> 2) % 4) + lane4id);",
      "",
      "    for (int h = 0; h < QHLOOP; h++) {",
      "      qk_max[h] = -FLT_MAX;",
      "      for (int i = 0; i < 4; i++) {",
      "        qk_max[h] = (lane4_token_idx + i < seq_len)",
      "                        ? fmaxf(qk_max[h], d_out[h][i])",
      "                        : qk_max[h];",
      "      }",
      "",
      "      // for (int mask = WARP_SIZE / 2; mask >= 4; mask /= 2) {",
      "      //   qk_max[h] = fmaxf(qk_max[h], __shfl_xor(qk_max[h], mask));",
      "      // }",
      "      // faster version of above code with dpp",
      "      asm(\"v_nop\\n v_nop\\n v_max_f32_dpp %0, %1, %2 row_ror:4\"",
      "          : \"=v\"(qk_max[h])",
      "          : \"v\"(qk_max[h]), \"v\"(qk_max[h]));",
      "      asm(\"v_nop\\n v_nop\\n v_max_f32_dpp %0, %1, %2 row_ror:8\"",
      "          : \"=v\"(qk_max[h])",
      "          : \"v\"(qk_max[h]), \"v\"(qk_max[h]));",
      "",
      "      auto tmp = __builtin_amdgcn_ds_bpermute(",
      "          bpermute_mask, *reinterpret_cast<int*>(&qk_max[h]));",
      "      qk_max[h] = *reinterpret_cast<float*>(&tmp);",
      "      asm(\"v_nop\\n v_nop\\n v_max_f32_dpp %0, %1, %2 row_ror:4\"",
      "          : \"=v\"(qk_max[h])",
      "          : \"v\"(qk_max[h]), \"v\"(qk_max[h]));",
      "      asm(\"v_nop\\n v_nop\\n v_max_f32_dpp %0, %1, %2 row_ror:8\"",
      "          : \"=v\"(qk_max[h])",
      "          : \"v\"(qk_max[h]), \"v\"(qk_max[h]));",
      "    }",
      "",
      "    float exp_sum[QHLOOP];",
      "    for (int h = 0; h < QHLOOP; h++) {",
      "      exp_sum[h] = 0.0f;",
      "      for (int i = 0; i < 4; i++) {",
      "        d_out[h][i] = (lane4_token_idx + i < seq_len)",
      "                          ? __expf(d_out[h][i] - qk_max[h])",
      "                          : 0.0f;",
      "        exp_sum[h] += d_out[h][i];",
      "      }",
      "      // for (int mask = WARP_SIZE / 2; mask >= 4; mask /= 2) {",
      "      //   exp_sum[h] += __shfl_xor(exp_sum[h], mask);",
      "      // }",
      "      // faster version of above code with dpp",
      "      asm(\"v_nop\\n v_nop\\n v_add_f32_dpp %0, %1, %2 row_ror:4\"",
      "          : \"=v\"(exp_sum[h])",
      "          : \"v\"(exp_sum[h]), \"v\"(exp_sum[h]));",
      "      asm(\"v_nop\\n v_nop\\n v_add_f32_dpp %0, %1, %2 row_ror:8\"",
      "          : \"=v\"(exp_sum[h])",
      "          : \"v\"(exp_sum[h]), \"v\"(exp_sum[h]));",
      "",
      "      auto tmp = __builtin_amdgcn_ds_bpermute(",
      "          bpermute_mask, *reinterpret_cast<int*>(&exp_sum[h]));",
      "      exp_sum[h] = *reinterpret_cast<float*>(&tmp);",
      "      asm(\"v_nop\\n v_nop\\n v_add_f32_dpp %0, %1, %2 row_ror:4\"",
      "          : \"=v\"(exp_sum[h])",
      "          : \"v\"(exp_sum[h]), \"v\"(exp_sum[h]));",
      "      asm(\"v_nop\\n v_nop\\n v_add_f32_dpp %0, %1, %2 row_ror:8\"",
      "          : \"=v\"(exp_sum[h])",
      "          : \"v\"(exp_sum[h]), \"v\"(exp_sum[h]));",
      "    }",
      "",
      "    if (laneid < 4) {",
      "      for (int h = 0; h < QHLOOP; h++) {",
      "        const int head_idx = 4 * h + lane4id;",
      "        shared_qk_max[warpid][head_idx] = qk_max[h];",
      "        shared_exp_sum[warpid][head_idx] = exp_sum[h];",
      "      }",
      "    }",
      "  }  // warp within context",
      "",
      "  __syncthreads();",
      "",
      "  const auto num_heads = gridDim.z * GQA_RATIO;",
      "  float* max_logits_ptr =",
      "      max_logits + seq_idx * num_heads * max_num_partitions + partition_idx;",
      "  float* exp_sums_ptr =",
      "      exp_sums + seq_idx * num_heads * max_num_partitions + partition_idx;",
      "  // calculate qk_max and exp_sums for partition",
      "  for (int h = 0; h < QHLOOP; h++) {",
      "    float global_qk_max = -FLT_MAX;",
      "    float warp_qk_max[NWARPS];",
      "    const int head_idx = 4 * h + lane4id;",
      "    for (int w = 0; w < NWARPS; w++) {",
      "      warp_qk_max[w] = shared_qk_max[w][head_idx];",
      "      global_qk_max = fmaxf(global_qk_max, warp_qk_max[w]);",
      "    }",
      "    float global_exp_sum = 0.0f;",
      "    for (int w = 0; w < NWARPS; w++) {",
      "      global_exp_sum +=",
      "          shared_exp_sum[w][head_idx] * __expf(warp_qk_max[w] - global_qk_max);",
      "    }",
      "    if (head_idx < GQA_RATIO) {",
      "      max_logits_ptr[(wg_start_head_idx + head_idx) * max_num_partitions] =",
      "          global_qk_max;",
      "      exp_sums_ptr[(wg_start_head_idx + head_idx) * max_num_partitions] =",
      "          global_exp_sum;",
      "    }",
      "    const float global_inv_sum_scale = __fdividef(1.f, global_exp_sum + 1e-6f) *",
      "                                       __expf(qk_max[h] - global_qk_max);",
      "    d_out[h] *= global_inv_sum_scale;",
      "  }",
      "  constexpr bool LOGITS_RTZ_CONVERSION = false;",
      "  // logits[h] -> every 4 lanes hold 4 heads, each lane holds 4 tokens, there",
      "  // are 4x16 tokens across warp",
      "  _B16x4 logits[QHLOOP];",
      "  for (int h = 0; h < QHLOOP; h++) {",
      "    if constexpr (LOGITS_RTZ_CONVERSION) {",
      "      // use rtz for faster performance with no perceivable accuracy loss",
      "      logits[h] = from_floatx4_rtz<scalar_t>(d_out[h]);",
      "    } else {",
      "      logits[h] = from_floatx4<scalar_t>(d_out[h]);",
      "    }",
      "  }",
      "",
      "  if (warp_start_token_idx >= seq_len) {  // warp out of context",
      "    for (int qh = 0; qh < QHLOOP; qh++) {",
      "      for (int vh = 0; vh < VHELOOP; vh++) {",
      "        vout_shared[qh][vh][laneid][warpid] = {0};",
      "      }",
      "    }",
      "  } else {  // warp in context",
      "  #define SV_mfma(x)                                                  \\",
      "    if constexpr (KV_DTYPE != vllm::Fp8KVCacheDataType::kAuto) {      \\",
      "      Vlocal[vh][x] = convert_b8x8_custom<scalar_t>(Vlocalb8[vh][x]); \\",
      "    }                                                                 \\",
      "    for (int qh = 0; qh < QHLOOP; qh++) {                             \\",
      "      acc[qh] = gcn_mfma4x4x4_instr<scalar_t, 4, 2 * x, 0>(           \\",
      "          logits[qh], Vlocal[vh][x].xy[0], acc[qh]);                  \\",
      "      acc[qh] = gcn_mfma4x4x4_instr<scalar_t, 4, 2 * x + 1, 0>(       \\",
      "          logits[qh], Vlocal[vh][x].xy[1], acc[qh]);                  \\",
      "    }",
      "",
      "    for (int vh = 0; vh < VHELOOP; vh++) {",
      "      floatx4 acc[QHLOOP];",
      "      for (int qh = 0; qh < QHLOOP; qh++) {",
      "        acc[qh] = {0};",
      "      }",
      "      // SoftMax-V calculation",
      "      // logits -> token dimension is distributed across lanes",
      "      // Vlocal -> token dimension is depthwise within lane",
      "      // uses mfma instruction block broadcast for logits",
      "      SV_mfma(0);",
      "      SV_mfma(1);",
      "      SV_mfma(2);",
      "      SV_mfma(3);",
      "      SV_mfma(4);",
      "      SV_mfma(5);",
      "      SV_mfma(6);",
      "      SV_mfma(7);",
      "",
      "      for (int qh = 0; qh < QHLOOP; qh++) {",
      "        if constexpr (KV_DTYPE != vllm::Fp8KVCacheDataType::kAuto) {",
      "          // post mfma v scale for fp8",
      "          acc[qh] *= *v_scale;",
      "        }",
      "        vout_shared[qh][vh][laneid][warpid] = from_floatx4<scalar_t>(acc[qh]);",
      "      }",
      "    }",
      "",
      "  #undef SV_mfma",
      "  }  // warp in context",
      "",
      "  __syncthreads();",
      "",
      "  // final write to tmp_out after vout accumulation",
      "  if (warpid == 0) {",
      "    _B16x4 vout[QHLOOP][VHELOOP];",
      "    // iterate across heads",
      "    for (int qh = 0; qh < QHLOOP; qh++) {",
      "      // iterate over each v head elem (within head_size)",
      "      for (int vh = 0; vh < VHELOOP; vh++) {",
      "        vout[qh][vh] = {0};",
      "        for (int w = 0; w < NWARPS; w++) {",
      "          vout[qh][vh] =",
      "              addx4<scalar_t>(vout[qh][vh], vout_shared[qh][vh][laneid][w]);",
      "        }",
      "      }",
      "    }",
      "",
      "    scalar_t* out_ptr = out +",
      "                        seq_idx * num_heads * max_num_partitions * HEAD_SIZE +",
      "                        partition_idx * HEAD_SIZE;",
      "    const int out_num_partitions = max_num_partitions;",
      "    bit16_t* out_ptr_b16 = reinterpret_cast<bit16_t*>(out_ptr);",
      "    for (int qh = 0; qh < QHLOOP; qh++) {",
      "      for (int vh = 0; vh < VHELOOP; vh++) {",
      "        const int head_size_elem = vh * WARP_SIZE + laneid;",
      "        for (int i = 0; i < 4; i++) {",
      "          const int head_idx = 4 * qh + i;",
      "          if (head_idx < GQA_RATIO) {",
      "            out_ptr_b16[(wg_start_head_idx + head_idx) * out_num_partitions *",
      "                            HEAD_SIZE +",
      "                        head_size_elem] = vout[qh][vh][i];",
      "          }",
      "        }",
      "      }",
      "    }",
      "  }  // warpid == 0",
      "}",
      "",
      "// Grid: (num_heads, num_seqs).",
      "template <typename scalar_t, typename OUTT, int HEAD_SIZE, int NUM_THREADS,",
      "          int PARTITION_SIZE, int NPAR_LOOPS>",
      "__global__",
      "__launch_bounds__(NUM_THREADS) void paged_attention_ll4mi_reduce_kernel(",
      "    OUTT* __restrict__ out,                // [num_seqs, num_heads, head_size]",
      "    const float* __restrict__ exp_sums,    // [num_seqs, num_heads,",
      "                                           // max_num_partitions]",
      "    const float* __restrict__ max_logits,  // [num_seqs, num_heads,",
      "                                           // max_num_partitions]",
      "    const scalar_t* __restrict__ tmp_out,  // [num_seqs, num_heads,",
      "                                           // max_num_partitions, head_size]",
      "    const int* __restrict__ seq_lens,      // [num_seqs]",
      "    const int* __restrict__ query_start_loc_ptr,  // [num_seqs]",
      "    const int max_num_partitions, const float* __restrict__ fp8_out_scale_ptr) {",
      "  const auto num_heads = gridDim.x;",
      "  const auto head_idx = blockIdx.x;",
      "  const auto seq_idx = blockIdx.y;",
      "",
      "  // NOTE queries with sequence len > 1 are prefills and taken care by another",
      "  // kernel.",
      "  if (query_start_loc_ptr != nullptr &&",
      "      (query_start_loc_ptr[seq_idx + 1] - query_start_loc_ptr[seq_idx] != 1)) {",
      "    return;",
      "  }",
      "",
      "  const int seq_len = seq_lens[seq_idx];",
      "  const int num_partitions = DIVIDE_ROUND_UP(seq_len, PARTITION_SIZE);",
      "  const auto warpid = threadIdx.x / WARP_SIZE;",
      "",
      "  __shared__ float shared_global_exp_sum;",
      "  // max num partitions supported is warp_size * NPAR_LOOPS",
      "  __shared__ float shared_exp_sums[NPAR_LOOPS * WARP_SIZE];",
      "",
      "  if (warpid == 0) {",
      "    const float* max_logits_ptr = max_logits +",
      "                                  seq_idx * num_heads * max_num_partitions +",
      "                                  head_idx * max_num_partitions;",
      "",
      "    // valid partition is the last valid partition in case threadid > num",
      "    // partitions",
      "    int valid_partition[NPAR_LOOPS];",
      "    float reg_max_logit[NPAR_LOOPS];",
      "    const int last_valid_partition = num_partitions - 1;",
      "",
      "  #pragma unroll",
      "    for (int i = 0; i < NPAR_LOOPS; i++) {",
      "      const auto partition_no = i * WARP_SIZE + threadIdx.x;",
      "      valid_partition[i] =",
      "          (partition_no < num_partitions) ? partition_no : last_valid_partition;",
      "    }",
      "  #pragma unroll",
      "    for (int i = 0; i < NPAR_LOOPS; i++) {",
      "      reg_max_logit[i] = max_logits_ptr[valid_partition[i]];",
      "    }",
      "    float max_logit = reg_max_logit[0];",
      "  #pragma unroll",
      "    for (int i = 1; i < NPAR_LOOPS; i++) {",
      "      max_logit = fmaxf(max_logit, reg_max_logit[i]);",
      "    }",
      "",
      "  #pragma unroll",
      "    for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {",
      "      max_logit = fmaxf(max_logit, __shfl_xor(max_logit, mask));",
      "    }",
      "",
      "    const float* exp_sums_ptr = exp_sums +",
      "                                seq_idx * num_heads * max_num_partitions +",
      "                                head_idx * max_num_partitions;",
      "",
      "    float rescaled_exp_sum[NPAR_LOOPS];",
      "  #pragma unroll",
      "    for (int i = 0; i < NPAR_LOOPS; i++) {",
      "      rescaled_exp_sum[i] = exp_sums_ptr[valid_partition[i]];",
      "    }",
      "  #pragma unroll",
      "    for (int i = 0; i < NPAR_LOOPS; i++) {",
      "      const auto partition_no = i * WARP_SIZE + threadIdx.x;",
      "      rescaled_exp_sum[i] *= (partition_no < num_partitions)",
      "                                 ? expf(reg_max_logit[i] - max_logit)",
      "                                 : 0.0f;",
      "    }",
      "    float global_exp_sum = rescaled_exp_sum[0];",
      "  #pragma unroll",
      "    for (int i = 1; i < NPAR_LOOPS; i++) {",
      "      global_exp_sum += rescaled_exp_sum[i];",
      "    }",
      "  #pragma unroll",
      "    for (int i = 0; i < NPAR_LOOPS; i++) {",
      "      const auto partition_no = i * WARP_SIZE + threadIdx.x;",
      "      shared_exp_sums[partition_no] = rescaled_exp_sum[i];",
      "    }",
      "",
      "  #pragma unroll",
      "    for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {",
      "      global_exp_sum += __shfl_xor(global_exp_sum, mask);",
      "    }",
      "    if (threadIdx.x == 0) {",
      "      shared_global_exp_sum = global_exp_sum;",
      "    }",
      "  }  // warpid == 0",
      "  const scalar_t* tmp_out_ptr =",
      "      tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +",
      "      head_idx * max_num_partitions * HEAD_SIZE + threadIdx.x;",
      "  constexpr int MAX_NPAR = 64;",
      "  scalar_t tmps[MAX_NPAR];",
      "  const float dzero = 0.0f;",
      "  #pragma unroll",
      "  for (int j = 0; j < MAX_NPAR; j++) {",
      "    tmps[j] = from_float<scalar_t>(dzero);",
      "  }",
      "  const int last_partition_offset = (num_partitions - 1) * HEAD_SIZE;",
      "  const int num_partition_offset = (num_partitions)*HEAD_SIZE;",
      "  int idx = 0;",
      "",
      "  constexpr int JCHUNK = 16;",
      "",
      "  #pragma unroll",
      "  for (int j = 0; j < JCHUNK * HEAD_SIZE; j += HEAD_SIZE) {",
      "    // lastj is last valid partition",
      "    const int lastj_offset =",
      "        (j < num_partition_offset) ? j : last_partition_offset;",
      "    tmps[idx] = tmp_out_ptr[lastj_offset];",
      "    idx++;",
      "  }",
      "  __syncthreads();",
      "",
      "  if (num_partitions > JCHUNK) {",
      "  #pragma unroll",
      "    for (int j = JCHUNK * HEAD_SIZE; j < 2 * JCHUNK * HEAD_SIZE;",
      "         j += HEAD_SIZE) {",
      "      const int lastj_offset =",
      "          (j < num_partition_offset) ? j : last_partition_offset;",
      "      tmps[idx] = tmp_out_ptr[lastj_offset];",
      "      idx++;",
      "    }",
      "",
      "    if (num_partitions > 2 * JCHUNK) {",
      "  #pragma unroll",
      "      for (int j = 2 * JCHUNK * HEAD_SIZE; j < MAX_NPAR * HEAD_SIZE;",
      "           j += HEAD_SIZE) {",
      "        const int lastj_offset =",
      "            (j < num_partition_offset) ? j : last_partition_offset;",
      "        tmps[idx] = tmp_out_ptr[lastj_offset];",
      "        idx++;",
      "      }",
      "    }",
      "  }  // num_partitions > JCHUNK",
      "",
      "  // Aggregate tmp_out to out.",
      "  float acc = 0.0f;",
      "  #pragma unroll",
      "  for (int j = 0; j < JCHUNK; j++) {",
      "    acc += to_float<scalar_t>(tmps[j]) * shared_exp_sums[j];",
      "  }",
      "  if (num_partitions > JCHUNK) {",
      "  #pragma unroll",
      "    for (int j = JCHUNK; j < 2 * JCHUNK; j++) {",
      "      acc += to_float<scalar_t>(tmps[j]) * shared_exp_sums[j];",
      "    }",
      "    if (num_partitions > 2 * JCHUNK) {",
      "  #pragma unroll",
      "      for (int j = 2 * JCHUNK; j < MAX_NPAR; j++) {",
      "        acc += to_float<scalar_t>(tmps[j]) * shared_exp_sums[j];",
      "      }",
      "    }",
      "  }",
      "",
      "  for (int p = 1; p < NPAR_LOOPS; p++) {",
      "    if (num_partitions > p * MAX_NPAR) {",
      "      idx = 0;",
      "  #pragma unroll",
      "      for (int j = p * MAX_NPAR * HEAD_SIZE; j < (p + 1) * MAX_NPAR * HEAD_SIZE;",
      "           j += HEAD_SIZE) {",
      "        // lastj is last valid partition",
      "        const int lastj_offset =",
      "            (j < num_partition_offset) ? j : last_partition_offset;",
      "        tmps[idx] = tmp_out_ptr[lastj_offset];",
      "        idx++;",
      "      }",
      "",
      "  #pragma unroll",
      "      for (int j = 0; j < MAX_NPAR; j++) {",
      "        acc += to_float<scalar_t>(tmps[j]) * shared_exp_sums[j + p * MAX_NPAR];",
      "      }",
      "    }",
      "  }",
      "",
      "  const float inv_global_exp_sum =",
      "      __fdividef(1.0f, shared_global_exp_sum + 1e-6f);",
      "  const float out_scale =",
      "      (fp8_out_scale_ptr != nullptr) ? 1.0f / (*fp8_out_scale_ptr) : 1.0f;",
      "  acc *= inv_global_exp_sum;",
      "  acc *= out_scale;",
      "  const int64_t query_start_off = static_cast<int64_t>(",
      "      query_start_loc_ptr ? query_start_loc_ptr[seq_idx] : seq_idx);",
      "  OUTT* out_ptr = out + query_start_off * num_heads * HEAD_SIZE +",
      "                  static_cast<int64_t>(head_idx) * HEAD_SIZE;",
      "  if constexpr (std::is_same<OUTT, bit8_t>::value) {",
      "    out_ptr[threadIdx.x] =",
      "        __hip_cvt_float_to_fp8(acc, vllm::fp8::fp8_type::__default_saturation,",
      "                               vllm::fp8::fp8_type::__default_interpret);",
      "  } else {",
      "    out_ptr[threadIdx.x] = from_float<scalar_t>(acc);",
      "  }",
      "}",
      "",
      "#elif defined(__HIP__GFX11__)",
      "",
      "using floatx8 = __attribute__((__vector_size__(8 * sizeof(float)))) float;",
      "",
      "using bit16_t = uint16_t;",
      "using bit16x4 = __attribute__((__vector_size__(4 * sizeof(uint16_t)))) uint16_t;",
      "typedef bit16x4 _B16x4;",
      "",
      "using bit16x8 = __attribute__((__vector_size__(8 * sizeof(uint16_t)))) uint16_t;",
      "union b16x8_u {",
      "  bit16x8 u16x8;",
      "  _B16x4 xy[2];",
      "};",
      "typedef b16x8_u _B16x8;",
      "",
      "using bit16x16 =",
      "    __attribute__((__vector_size__(16 * sizeof(uint16_t)))) uint16_t;",
      "union b16x16_u {",
      "  bit16x16 u16x16;",
      "  _B16x8 xy[2];",
      "};",
      "typedef b16x16_u _B16x16;",
      "",
      "using _B8x8 = uint2;",
      "using bit8_t = uint8_t;",
      "",
      "typedef struct _B8x16 {",
      "  _B8x8 xy[2];",
      "} _B8x16;",
      "",
      "template <typename T, int absz, int cbid, int blgp>",
      "__device__ __forceinline__ floatx8 gcn_wmma16x16x16_instr(const bit16x16& inpA,",
      "                                                          const bit16x16& inpB,",
      "                                                          const floatx8& inpC) {",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    return __builtin_amdgcn_wmma_f32_16x16x16_f16_w32(inpA, inpB, inpC);",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    return __builtin_amdgcn_wmma_f32_16x16x16_bf16_w32(inpA, inpB, inpC);",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ float to_float(const T& inp) {",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    return (float)inp;",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    return __bfloat162float(inp);",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ T from_float(const float& inp) {",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    return (_Float16)inp;",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    return __float2bfloat16(inp);",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ _B16x8 from_floatx8(const floatx8& inp) {",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    union h2cvt {",
      "      __half2 h2[4];",
      "      _B16x8 b16x8;",
      "    } u;",
      "    u.h2[0] = __float22half2_rn(make_float2(inp[0], inp[1]));",
      "    u.h2[1] = __float22half2_rn(make_float2(inp[2], inp[3]));",
      "    u.h2[2] = __float22half2_rn(make_float2(inp[4], inp[5]));",
      "    u.h2[3] = __float22half2_rn(make_float2(inp[6], inp[7]));",
      "    return u.b16x8;",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    union b2cvt {",
      "      __hip_bfloat162 b2[4];",
      "      _B16x8 b16x8;",
      "    } u;",
      "",
      "    u.b2[0] = __float22bfloat162_rn(make_float2(inp[0], inp[1]));",
      "    u.b2[1] = __float22bfloat162_rn(make_float2(inp[2], inp[3]));",
      "    u.b2[2] = __float22bfloat162_rn(make_float2(inp[4], inp[5]));",
      "    u.b2[3] = __float22bfloat162_rn(make_float2(inp[6], inp[7]));",
      "",
      "    return u.b16x8;",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "// clang-format off",
      "template <typename scalar_t, typename cache_t,",
      "          vllm::Fp8KVCacheDataType KV_DTYPE, typename OUTT, int BLOCK_SIZE,",
      "          int HEAD_SIZE, int NUM_THREADS, bool ALIBI_ENABLED, int GQA_RATIO>",
      "__global__",
      "__launch_bounds__(NUM_THREADS, 3) void paged_attention_ll4mi_QKV_mfma16_kernel(",
      "    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]",
      "    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,",
      "                                          // head_size/x, block_size, x]",
      "    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,",
      "                                          // head_size, block_size]",
      "    const int num_kv_heads, const float scale,",
      "    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]",
      "    const int* __restrict__ seq_lens,  // [num_seqs]",
      "    const int* __restrict__ query_start_loc_ptr,   // [num_seqs]",
      "    const int max_num_blocks_per_seq,",
      "    const float* __restrict__ alibi_slopes,  // [num_heads]",
      "    const int q_stride, const int kv_block_stride, const int kv_head_stride,",
      "    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]",
      "    float* __restrict__ max_logits,  // [num_seqs, num_heads,",
      "                                     // max_num_partitions]",
      "    scalar_t* __restrict__ out,    // [num_seqs, num_heads, max_num_partitions,",
      "                                   // head_size]",
      "    OUTT* __restrict__ final_out,  // [num_seqs, num_heads, head_size]",
      "    int max_ctx_blocks, const float* k_scale, const float* v_scale) {",
      "  // clang-format on",
      "  constexpr int NWARPS = NUM_THREADS / WARP_SIZE;  // 8 warps on gfx11",
      "  const int warpid = threadIdx.x / WARP_SIZE;",
      "  const int laneid = threadIdx.x % WARP_SIZE;",
      "  const int lane2id = laneid % 2;",
      "  const int lane16id = laneid % 16;",
      "  const int rowid = laneid / 16;",
      "",
      "  const int seq_idx = blockIdx.x;",
      "  // NOTE queries with sequence len > 1 are prefills and taken care by another",
      "  // kernel.",
      "  if (query_start_loc_ptr != nullptr &&",
      "      (query_start_loc_ptr[seq_idx + 1] - query_start_loc_ptr[seq_idx]) != 1) {",
      "    return;",
      "  }",
      "",
      "  const int partition_idx = blockIdx.y;",
      "",
      "  constexpr int T_PAR_SIZE = 256;  // token partition size set to 256",
      "",
      "  const int max_num_partitions = gridDim.y;",
      "",
      "  const int seq_len = seq_lens[seq_idx];  // length of a seq",
      "",
      "  const int partition_start_token_idx = partition_idx * T_PAR_SIZE;",
      "  // exit if partition is out of context for seq",
      "  if (partition_start_token_idx >= seq_len) {",
      "    return;",
      "  }",
      "",
      "  constexpr int GQA_RATIO2 = DIVIDE_ROUND_UP(GQA_RATIO, 2);",
      "",
      "  __shared__ float shared_qk_max[NWARPS][16 + 1];",
      "  __shared__ float shared_exp_sum[NWARPS][16 + 1];",
      "  // shared_logits is used for multiple purposes",
      "  __shared__ _B16x16 shared_logits[NWARPS][2][16][2];",
      "",
      "  // for QK wmma16x16, layout is QHead/Tokenx16 across every 16 lanes,",
      "  // 32 Bytes HeadElements in each lane, 2x16B HeadElements across a row of warp",
      "  constexpr int ROWS_PER_WARP =",
      "      WARP_SIZE / 16 / 2;  // rows refers to 16 lanes; refer dpp terminology",
      "  constexpr int CONTIGUOUS_KV_ELEMS_16B_LOAD =",
      "      16 / sizeof(cache_t);  // 8 for 16 bit cache type, 16 for 8 bit types",
      "  constexpr int QKHE_PER_FETCH =",
      "      CONTIGUOUS_KV_ELEMS_16B_LOAD *",
      "      ROWS_PER_WARP;  // each fetch across a warp fetches these many elements",
      "  constexpr int QKHELOOP = HEAD_SIZE / QKHE_PER_FETCH;  // 2xQKHE_16B across",
      "                                                        // warp",
      "",
      "  _B16x16 Qlocal[QKHELOOP / 2];  // note that 16 contiguous elements of Q should",
      "                                 // be fetched per lane for 16 bit cache types",
      "",
      "  constexpr int CONTIGUOUS_SCALAR_ELEMS_16B = 16 / sizeof(scalar_t);",
      "",
      "  constexpr int TOKENS_PER_WARP =",
      "      T_PAR_SIZE /",
      "      NWARPS;  // sub partition of tokens per warp for qk calculation",
      "  constexpr int TLOOP =",
      "      TOKENS_PER_WARP /",
      "      16;  // each wmma16x16x16 instruction processes 16 tokens",
      "",
      "  _B16x16 Klocal[TLOOP]",
      "                [QKHELOOP / 2];  // can be interpreted as B8x16 for 8 bit types",
      "",
      "  const int wg_start_head_idx = blockIdx.z * GQA_RATIO;",
      "  const int wg_start_kv_head_idx = blockIdx.z;",
      "  const int total_num_heads = gridDim.z * GQA_RATIO;",
      "",
      "  // for QK wmma, tokens in multiples of TOKENS_PER_WARP are spread across warps",
      "  // each wmma takes QH16xT16x16HE across warp",
      "  // repeat wmma across QKHELOOP dimension",
      "  // output layout from QKwmma : QH16xT8x2 16 qheads across 16 lanes, 16 tokens",
      "  // across 2 rows x 8 tokens per lane",
      "",
      "  const int64_t query_start_off = static_cast<int64_t>(",
      "      query_start_loc_ptr ? query_start_loc_ptr[seq_idx] : seq_idx);",
      "",
      "  if (GQA_RATIO == 1) {",
      "    const int local_qhead_idx = lane16id % GQA_RATIO;",
      "    const int global_qhead_idx = wg_start_head_idx + local_qhead_idx;",
      "    const scalar_t* q_ptr =",
      "        q + query_start_off * q_stride + global_qhead_idx * HEAD_SIZE;",
      "    if (lane16id < GQA_RATIO) {",
      "  #pragma unroll",
      "      for (int qkhe_depth = 0; qkhe_depth < QKHELOOP / 2; qkhe_depth++) {",
      "        const scalar_t* q_fetch_ptr = q_ptr + qkhe_depth * QKHE_PER_FETCH * 2;",
      "        const _B16x16* q_fetch_ptr_32B =",
      "            reinterpret_cast<const _B16x16*>(q_fetch_ptr);",
      "        Qlocal[qkhe_depth] = *q_fetch_ptr_32B;",
      "      }",
      "    }",
      "  } else {",
      "    // fetch Q in shared across warps and then write to registers",
      "    const int local_qhead_idx = 2 * warpid + rowid;",
      "    const int global_qhead_idx = wg_start_head_idx + local_qhead_idx;",
      "    const scalar_t* q_ptr =",
      "        q + query_start_off * q_stride + global_qhead_idx * HEAD_SIZE;",
      "",
      "    const int qhead_element = lane16id * CONTIGUOUS_SCALAR_ELEMS_16B;",
      "    if ((local_qhead_idx < GQA_RATIO) && (qhead_element < HEAD_SIZE)) {",
      "      const scalar_t* q_fetch_ptr = q_ptr + qhead_element;",
      "      const _B16x8* q_fetch_ptr_16B =",
      "          reinterpret_cast<const _B16x8*>(q_fetch_ptr);",
      "      _B16x8 tmp = *q_fetch_ptr_16B;",
      "",
      "      const int offset1 =",
      "          lane16id /",
      "          2;  // 16 contiguous chunks of head elems are spread across 8x2lanes",
      "      shared_logits[offset1][lane2id][local_qhead_idx][0].xy[0] = tmp;",
      "    }",
      "",
      "    __syncthreads();",
      "",
      "  #pragma unroll",
      "    for (int qkhe_depth = 0; qkhe_depth < QKHELOOP / 2; qkhe_depth++) {",
      "      Qlocal[qkhe_depth].xy[0] =",
      "          shared_logits[qkhe_depth][0][lane16id % GQA_RATIO][0].xy[0];",
      "      Qlocal[qkhe_depth].xy[1] =",
      "          shared_logits[qkhe_depth][1][lane16id % GQA_RATIO][0].xy[0];",
      "    }",
      "  }",
      "",
      "  const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);",
      "  const int last_seq_block = num_seq_blocks - 1;",
      "",
      "  const int* block_table_seq = block_tables + seq_idx * max_num_blocks_per_seq;",
      "",
      "  int kphysical_block_number[TLOOP];",
      "",
      "  // fetch k physical block numbers",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    const int klocal_token_idx =",
      "        TOKENS_PER_WARP * warpid + token_depth * 16 + lane16id;",
      "    const int kglobal_token_idx = partition_start_token_idx + klocal_token_idx;",
      "    const int kblock_idx = (kglobal_token_idx < seq_len)",
      "                               ? kglobal_token_idx / BLOCK_SIZE",
      "                               : last_seq_block;",
      "    kphysical_block_number[token_depth] = block_table_seq[kblock_idx];",
      "  }",
      "",
      "  constexpr int KX = 16 / sizeof(cache_t);",
      "  const cache_t* k_ptr = k_cache + wg_start_kv_head_idx * kv_head_stride;",
      "",
      "  const int row_head_elem = 0;",
      "",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    const int64_t kblock_number =",
      "        static_cast<int64_t>(kphysical_block_number[token_depth]);",
      "    const cache_t* k_ptr2 = k_ptr + kblock_number * kv_block_stride;",
      "    const int klocal_token_idx =",
      "        TOKENS_PER_WARP * warpid + token_depth * 16 + lane16id;",
      "    const int kphysical_block_offset = klocal_token_idx % BLOCK_SIZE;",
      "    const cache_t* k_ptr3 = k_ptr2 + kphysical_block_offset * KX;",
      "",
      "    for (int qkhe_depth = 0; qkhe_depth < QKHELOOP; qkhe_depth++) {",
      "      const int head_elem = row_head_elem + qkhe_depth * QKHE_PER_FETCH;",
      "      const int offset1 = head_elem / KX;",
      "      const int offset2 = head_elem % KX;",
      "      const cache_t* k_fetch_ptr = k_ptr3 + offset1 * BLOCK_SIZE * KX + offset2;",
      "      const _B16x8* k_fetch_ptr_16B =",
      "          reinterpret_cast<const _B16x8*>(k_fetch_ptr);",
      "      Klocal[token_depth][qkhe_depth / 2].xy[qkhe_depth % 2] = *k_fetch_ptr_16B;",
      "    }",
      "  }",
      "",
      "  constexpr int VTOKENS_PER_LANE =",
      "      TOKENS_PER_WARP / ROWS_PER_WARP;  // 32/1 = 32 vtokens per lane",
      "  constexpr int VBLOCKS_PER_LANE = 2;   // assumes block size >=16",
      "  constexpr int VTLOOP = NWARPS;        // corresponds to tokens across warps",
      "  constexpr int VTLANELOOP = DIVIDE_ROUND_UP(",
      "      VTOKENS_PER_LANE,",
      "      CONTIGUOUS_KV_ELEMS_16B_LOAD);  // optimized for 16B fetches; assumes",
      "                                      // minimum block size is 16",
      "  constexpr int VHELOOP = DIVIDE_ROUND_UP(",
      "      (HEAD_SIZE / 16), NWARPS);  // head_size distributed across warps; each",
      "                                  // wmma instr works on 16 head elements",
      "",
      "  int vphysical_block_number[VTLOOP][VBLOCKS_PER_LANE];",
      "",
      "  // fetch v physical block numbers",
      "  for (int vtoken_depth = 0; vtoken_depth < VTLOOP; vtoken_depth++) {",
      "    for (int vblock_depth = 0; vblock_depth < VBLOCKS_PER_LANE;",
      "         vblock_depth++) {",
      "      const int vlocal_token_idx =",
      "          vtoken_depth * VTOKENS_PER_LANE * ROWS_PER_WARP +",
      "          vblock_depth * BLOCK_SIZE;",
      "      const int vglobal_token_idx =",
      "          partition_start_token_idx + vlocal_token_idx;",
      "      const int vblock_idx = (vglobal_token_idx < seq_len)",
      "                                 ? vglobal_token_idx / BLOCK_SIZE",
      "                                 : last_seq_block;",
      "      vphysical_block_number[vtoken_depth][vblock_depth] =",
      "          block_table_seq[vblock_idx];",
      "    }",
      "  }",
      "",
      "  _B16x16 Vlocal[VTLOOP][VHELOOP]",
      "                [VTLANELOOP / 2];  // this can be interpreted as B8x16 too",
      "",
      "  const cache_t* v_ptr = v_cache + wg_start_kv_head_idx * kv_head_stride;",
      "  // v fetches are 16head elems across lanes x (16x2) tokens per lane",
      "  for (int vhe_depth = 0; vhe_depth < VHELOOP; vhe_depth++) {",
      "    const int vhead_elem = vhe_depth * NWARPS * 16 + warpid * 16 + lane16id;",
      "    const cache_t* v_ptr2 = v_ptr + vhead_elem * BLOCK_SIZE;",
      "",
      "    for (int vtoken_depth = 0; vtoken_depth < VTLOOP; vtoken_depth++) {",
      "      for (int vfetch_depth = 0; vfetch_depth < VTLANELOOP; vfetch_depth++) {",
      "        const int64_t vblock_number = static_cast<int64_t>(",
      "            vphysical_block_number[vtoken_depth]",
      "                                  [vfetch_depth / VBLOCKS_PER_LANE]);",
      "        const cache_t* v_ptr3 = v_ptr2 + (vblock_number * kv_block_stride);",
      "",
      "        const cache_t* v_fetch_ptr =",
      "            v_ptr3 +",
      "            (vfetch_depth % VBLOCKS_PER_LANE) * CONTIGUOUS_KV_ELEMS_16B_LOAD;",
      "        const _B16x8* v_fetch_ptr_16B =",
      "            reinterpret_cast<const _B16x8*>(v_fetch_ptr);",
      "        Vlocal[vtoken_depth][vhe_depth][vfetch_depth / 2].xy[vfetch_depth % 2] =",
      "            *v_fetch_ptr_16B;",
      "      }",
      "    }",
      "  }",
      "",
      "  floatx8 dout[TLOOP];",
      "  // qk wmma",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    dout[token_depth] = {0};",
      "    for (int qkhe_depth = 0; qkhe_depth < QKHELOOP / 2; qkhe_depth++) {",
      "      dout[token_depth] = gcn_wmma16x16x16_instr<scalar_t, 0, 0, 0>(",
      "          Klocal[token_depth][qkhe_depth].u16x16, Qlocal[qkhe_depth].u16x16,",
      "          dout[token_depth]);",
      "    }",
      "    dout[token_depth] *= scale;",
      "  }",
      "",
      "  // calculate qk_max and exp_sum per warp and write to shared memory",
      "  float qk_max = -FLT_MAX;",
      "  float exp_sum = 0.0f;",
      "  const int qkout_token_idx =",
      "      partition_start_token_idx + TOKENS_PER_WARP * warpid + rowid;",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    const int local_token_idx = qkout_token_idx + token_depth * 16;",
      "    for (int i = 0; i < 8; i++) {",
      "      const float tmp =",
      "          (local_token_idx + 2 * i < seq_len) ? dout[token_depth][i] : -FLT_MAX;",
      "      qk_max = fmaxf(qk_max, tmp);",
      "    }",
      "  }",
      "",
      "  qk_max = fmaxf(qk_max, __shfl_xor(qk_max, 16));",
      "",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    const int local_token_idx = qkout_token_idx + token_depth * 16;",
      "    for (int i = 0; i < 8; i++) {",
      "      const float tmp = (local_token_idx + 2 * i < seq_len)",
      "                            ? __expf(dout[token_depth][i] - qk_max)",
      "                            : 0.0f;",
      "      dout[token_depth][i] = tmp;",
      "      exp_sum += tmp;",
      "    }",
      "  }",
      "",
      "  exp_sum += __shfl_xor(exp_sum, 16);",
      "",
      "  __syncthreads();",
      "",
      "  if (laneid < 16) {",
      "    shared_qk_max[warpid][lane16id] = qk_max;",
      "    shared_exp_sum[warpid][lane16id] = exp_sum;",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  // calculate partition qk_max and exp_sum",
      "  float partition_qk_max = -FLT_MAX;",
      "  float warp_qk_max_exp[NWARPS];",
      "  float partition_exp_sum = 0.0f;",
      "",
      "  #pragma unroll",
      "  for (int w = 0; w < NWARPS; w++) {",
      "    warp_qk_max_exp[w] = shared_qk_max[w][lane16id];",
      "    partition_qk_max = fmaxf(partition_qk_max, warp_qk_max_exp[w]);",
      "  }",
      "",
      "  for (int w = 0; w < NWARPS; w++) {",
      "    warp_qk_max_exp[w] = __expf(warp_qk_max_exp[w] - partition_qk_max);",
      "    partition_exp_sum += shared_exp_sum[w][lane16id] * warp_qk_max_exp[w];",
      "  }",
      "",
      "  const float inv_sum_scale =",
      "      __fdividef(1.f, partition_exp_sum + 1e-6f) * warp_qk_max_exp[warpid];",
      "",
      "  __syncthreads();",
      "",
      "  // write logits to shared mem",
      "  #pragma unroll",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    dout[token_depth] *= inv_sum_scale;",
      "    shared_logits[warpid][token_depth][lane16id][0].xy[rowid] =",
      "        from_floatx8<scalar_t>(dout[token_depth]);",
      "  }",
      "  __syncthreads();",
      "",
      "  _B16x8 swp_buf[TLOOP][2];",
      "  #pragma unroll",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    swp_buf[token_depth][0] =",
      "        shared_logits[warpid][token_depth][lane16id][0].xy[0];",
      "    swp_buf[token_depth][1] =",
      "        shared_logits[warpid][token_depth][lane16id][0].xy[1];",
      "  }",
      "",
      "  #pragma unroll",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "  #pragma unroll",
      "    for (int i = 0; i < 8; i++) {",
      "      shared_logits[warpid][token_depth][lane16id][0].xy[rowid].u16x8[i] =",
      "          swp_buf[token_depth][i % 2].u16x8[4 * rowid + (i / 2)];",
      "    }",
      "  }",
      "",
      "  // write out partition max_logits and exp_sum",
      "  if (threadIdx.x < GQA_RATIO) {",
      "    const int qhead_idx = lane16id;",
      "    const int offset = seq_idx * total_num_heads * max_num_partitions +",
      "                       (wg_start_head_idx + qhead_idx) * max_num_partitions +",
      "                       partition_idx;",
      "    max_logits[offset] = partition_qk_max;",
      "    exp_sums[offset] = partition_exp_sum;",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  _B16x8 outelems[VHELOOP];",
      "  // Softmax V wmma",
      "  // v layout: 16he across lanes x (16x2) tokens per lane",
      "  for (int vhe_depth = 0; vhe_depth < VHELOOP; vhe_depth++) {",
      "    floatx8 tmp_out = {0};",
      "    for (int vtoken_depth = 0; vtoken_depth < VTLOOP; vtoken_depth++) {",
      "      for (int vfetch_depth = 0; vfetch_depth < VTLANELOOP / 2;",
      "           vfetch_depth++) {",
      "        const int offset = vfetch_depth;",
      "        // if output format is 16 qheads across 16 lanes, 16 head elems spread",
      "        // across rows",
      "        tmp_out = gcn_wmma16x16x16_instr<scalar_t, 0, 0, 0>(",
      "            Vlocal[vtoken_depth][vhe_depth][vfetch_depth].u16x16,",
      "            shared_logits[vtoken_depth][offset][lane16id][0].u16x16, tmp_out);",
      "      }",
      "    }",
      "    outelems[vhe_depth] = from_floatx8<scalar_t>(tmp_out);",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  #pragma unroll",
      "  for (int vhe_depth = 0; vhe_depth < VHELOOP; vhe_depth++) {",
      "    shared_logits[warpid][vhe_depth][lane16id][0].xy[rowid] =",
      "        outelems[vhe_depth];  // lane16 id head dimension; rowid head element",
      "                              // dimension",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  #pragma unroll",
      "  for (int vhe_depth = 0; vhe_depth < VHELOOP; vhe_depth++) {",
      "    swp_buf[vhe_depth][0] = shared_logits[warpid][vhe_depth][lane16id][0].xy[0];",
      "    swp_buf[vhe_depth][1] = shared_logits[warpid][vhe_depth][lane16id][0].xy[1];",
      "  }",
      "",
      "  #pragma unroll",
      "  for (int vhe_depth = 0; vhe_depth < VHELOOP; vhe_depth++) {",
      "  #pragma unroll",
      "    for (int i = 0; i < 8; i++) {",
      "      shared_logits[warpid][vhe_depth][lane16id][0].xy[rowid].u16x8[i] =",
      "          swp_buf[vhe_depth][i % 2].u16x8[4 * rowid + (i / 2)];",
      "    }",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  // write to tmp_out with coalesced writes after reading from shared mem",
      "  if (warpid == 0) {",
      "    _B16x8 vout[GQA_RATIO2];",
      "    // each lane writes out 16Bytes of tmp_out along head elem dimension",
      "    const int head_elem_idx = lane16id * 8;",
      "    if (head_elem_idx < HEAD_SIZE) {",
      "      for (int h = 0; h < GQA_RATIO2; h++) {",
      "        const int local_head_idx = 2 * h + rowid;",
      "        const int offset1 = (head_elem_idx / 16) % NWARPS;",
      "        const int offset2 = head_elem_idx / 16 / NWARPS;",
      "        const int offset3 = (head_elem_idx / 8) % 2;  // num_he % num_row",
      "        vout[h] =",
      "            shared_logits[offset1][offset2][local_head_idx][0].xy[offset3];",
      "      }",
      "",
      "      const int hsz_maxp_mult = HEAD_SIZE * max_num_partitions;",
      "      scalar_t* out_ptr = out + seq_idx * total_num_heads * hsz_maxp_mult +",
      "                          partition_idx * HEAD_SIZE;",
      "      for (int h = 0; h < GQA_RATIO2; h++) {",
      "        const int local_head_idx = 2 * h + rowid;",
      "        if (local_head_idx < GQA_RATIO) {",
      "          const int out_head_idx = wg_start_head_idx + local_head_idx;",
      "          scalar_t* out_ptr2 = out_ptr + out_head_idx * hsz_maxp_mult;",
      "          scalar_t* out_ptr3 = out_ptr2 + head_elem_idx;",
      "          _B16x8* out_ptr_B16x8 = reinterpret_cast<_B16x8*>(out_ptr3);",
      "          *out_ptr_B16x8 = vout[h];",
      "        }",
      "      }",
      "    }",
      "  }",
      "}",
      "",
      "template <typename scalar_t, typename cache_t,",
      "          vllm::Fp8KVCacheDataType KV_DTYPE, typename OUTT, int BLOCK_SIZE,",
      "          int HEAD_SIZE, int NUM_THREADS, bool ALIBI_ENABLED,",
      "          int GQA_RATIO>",
      "__global__",
      "__launch_bounds__(NUM_THREADS) void paged_attention_ll4mi_QKV_mfma4_kernel(",
      "    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]",
      "    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,",
      "                                          // head_size/x, block_size, x]",
      "    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,",
      "                                          // head_size, block_size]",
      "    const int num_kv_heads, const float scale,",
      "    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]",
      "    const int* __restrict__ seq_lens,      // [num_seqs]",
      "    const int* __restrict__ query_start_loc_ptr,  // [num_seqs]",
      "    const int max_num_blocks_per_seq,",
      "    const float* __restrict__ alibi_slopes,  // [num_heads]",
      "    const int q_stride, const int kv_block_stride, const int kv_head_stride,",
      "    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]",
      "    float* __restrict__ max_logits,  // [num_seqs, num_heads,",
      "                                     // max_num_partitions]",
      "    scalar_t* __restrict__ out,    // [num_seqs, num_heads, max_num_partitions,",
      "                                   // head_size]",
      "    OUTT* __restrict__ final_out,  // [num_seqs, num_heads, head_size]",
      "    int max_ctx_blocks, const float* k_scale, const float* v_scale) {",
      "  UNREACHABLE_CODE",
      "}",
      "",
      "// Grid: (num_heads, num_seqs).",
      "template <typename scalar_t, typename OUTT, int HEAD_SIZE, int NUM_THREADS,",
      "          int PARTITION_SIZE, int NPAR_LOOPS>",
      "__global__",
      "__launch_bounds__(NUM_THREADS) void paged_attention_ll4mi_reduce_kernel(",
      "    OUTT* __restrict__ out,                // [num_seqs, num_heads, head_size]",
      "    const float* __restrict__ exp_sums,    // [num_seqs, num_heads,",
      "                                           // max_num_partitions]",
      "    const float* __restrict__ max_logits,  // [num_seqs, num_heads,",
      "                                           // max_num_partitions]",
      "    const scalar_t* __restrict__ tmp_out,  // [num_seqs, num_heads,",
      "                                           // max_num_partitions, head_size]",
      "    const int* __restrict__ seq_lens,      // [num_seqs]",
      "    const int* __restrict__ query_start_loc_ptr,  // [num_seqs]",
      "    const int max_num_partitions, const float* __restrict__ fp8_out_scale_ptr) {",
      "  const auto num_heads = gridDim.x;",
      "  const auto head_idx = blockIdx.x;",
      "  const auto seq_idx = blockIdx.y;",
      "",
      "  // NOTE queries with sequence len > 1 are prefills and taken care by another",
      "  // kernel.",
      "  if (query_start_loc_ptr != nullptr &&",
      "      (query_start_loc_ptr[seq_idx + 1] - query_start_loc_ptr[seq_idx] != 1)) {",
      "    return;",
      "  }",
      "",
      "  const int seq_len = seq_lens[seq_idx];",
      "  const int num_partitions = DIVIDE_ROUND_UP(seq_len, PARTITION_SIZE);",
      "  const int warpid = threadIdx.x / WARP_SIZE;",
      "",
      "  __shared__ float shared_global_exp_sum;",
      "  // max num partitions supported is warp_size * NPAR_LOOPS",
      "  __shared__ float shared_exp_sums[NPAR_LOOPS * WARP_SIZE];",
      "",
      "  if (warpid == 0) {",
      "    const float* max_logits_ptr = max_logits +",
      "                                  seq_idx * num_heads * max_num_partitions +",
      "                                  head_idx * max_num_partitions;",
      "",
      "    // valid partition is the last valid partition in case threadid > num",
      "    // partitions",
      "    int valid_partition[NPAR_LOOPS];",
      "    float reg_max_logit[NPAR_LOOPS];",
      "    const int last_valid_partition = num_partitions - 1;",
      "",
      "  #pragma unroll",
      "    for (int i = 0; i < NPAR_LOOPS; i++) {",
      "      const int partition_no = i * WARP_SIZE + threadIdx.x;",
      "      valid_partition[i] =",
      "          (partition_no < num_partitions) ? partition_no : last_valid_partition;",
      "    }",
      "  #pragma unroll",
      "    for (int i = 0; i < NPAR_LOOPS; i++) {",
      "      reg_max_logit[i] = max_logits_ptr[valid_partition[i]];",
      "    }",
      "    float max_logit = reg_max_logit[0];",
      "  #pragma unroll",
      "    for (int i = 1; i < NPAR_LOOPS; i++) {",
      "      max_logit = fmaxf(max_logit, reg_max_logit[i]);",
      "    }",
      "",
      "  #pragma unroll",
      "    for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {",
      "      max_logit = fmaxf(max_logit, __shfl_xor(max_logit, mask));",
      "    }",
      "",
      "    const float* exp_sums_ptr = exp_sums +",
      "                                seq_idx * num_heads * max_num_partitions +",
      "                                head_idx * max_num_partitions;",
      "",
      "    float rescaled_exp_sum[NPAR_LOOPS];",
      "  #pragma unroll",
      "    for (int i = 0; i < NPAR_LOOPS; i++) {",
      "      rescaled_exp_sum[i] = exp_sums_ptr[valid_partition[i]];",
      "    }",
      "  #pragma unroll",
      "    for (int i = 0; i < NPAR_LOOPS; i++) {",
      "      const int partition_no = i * WARP_SIZE + threadIdx.x;",
      "      rescaled_exp_sum[i] *= (partition_no < num_partitions)",
      "                                 ? expf(reg_max_logit[i] - max_logit)",
      "                                 : 0.0f;",
      "    }",
      "    float global_exp_sum = rescaled_exp_sum[0];",
      "  #pragma unroll",
      "    for (int i = 1; i < NPAR_LOOPS; i++) {",
      "      global_exp_sum += rescaled_exp_sum[i];",
      "    }",
      "  #pragma unroll",
      "    for (int i = 0; i < NPAR_LOOPS; i++) {",
      "      const int partition_no = i * WARP_SIZE + threadIdx.x;",
      "      shared_exp_sums[partition_no] = rescaled_exp_sum[i];",
      "    }",
      "",
      "  #pragma unroll",
      "    for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {",
      "      global_exp_sum += __shfl_xor(global_exp_sum, mask);",
      "    }",
      "    if (threadIdx.x == 0) {",
      "      shared_global_exp_sum = global_exp_sum;",
      "    }",
      "  }  // warpid == 0",
      "  const scalar_t* tmp_out_ptr =",
      "      tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +",
      "      head_idx * max_num_partitions * HEAD_SIZE + threadIdx.x;",
      "  constexpr int MAX_NPAR = 32;",
      "  scalar_t tmps[MAX_NPAR];",
      "  const float dzero = 0.0f;",
      "  #pragma unroll",
      "  for (int j = 0; j < MAX_NPAR; j++) {",
      "    tmps[j] = from_float<scalar_t>(dzero);",
      "  }",
      "  const int last_partition_offset = (num_partitions - 1) * HEAD_SIZE;",
      "  const int num_partition_offset = (num_partitions)*HEAD_SIZE;",
      "  int idx = 0;",
      "",
      "  constexpr int JCHUNK = 16;",
      "",
      "  #pragma unroll",
      "  for (int j = 0; j < JCHUNK * HEAD_SIZE; j += HEAD_SIZE) {",
      "    // lastj is last valid partition",
      "    const int lastj_offset =",
      "        (j < num_partition_offset) ? j : last_partition_offset;",
      "    tmps[idx] = tmp_out_ptr[lastj_offset];",
      "    idx++;",
      "  }",
      "  __syncthreads();",
      "",
      "  if (num_partitions > JCHUNK) {",
      "  #pragma unroll",
      "    for (int j = JCHUNK * HEAD_SIZE; j < 2 * JCHUNK * HEAD_SIZE;",
      "         j += HEAD_SIZE) {",
      "      const int lastj_offset =",
      "          (j < num_partition_offset) ? j : last_partition_offset;",
      "      tmps[idx] = tmp_out_ptr[lastj_offset];",
      "      idx++;",
      "    }",
      "",
      "    if (num_partitions > 2 * JCHUNK) {",
      "  #pragma unroll",
      "      for (int j = 2 * JCHUNK * HEAD_SIZE; j < MAX_NPAR * HEAD_SIZE;",
      "           j += HEAD_SIZE) {",
      "        const int lastj_offset =",
      "            (j < num_partition_offset) ? j : last_partition_offset;",
      "        tmps[idx] = tmp_out_ptr[lastj_offset];",
      "        idx++;",
      "      }",
      "    }",
      "  }  // num_partitions > JCHUNK",
      "",
      "  // Aggregate tmp_out to out.",
      "  float acc = 0.0f;",
      "  #pragma unroll",
      "  for (int j = 0; j < JCHUNK; j++) {",
      "    acc += to_float<scalar_t>(tmps[j]) * shared_exp_sums[j];",
      "  }",
      "  if (num_partitions > JCHUNK) {",
      "  #pragma unroll",
      "    for (int j = JCHUNK; j < 2 * JCHUNK; j++) {",
      "      acc += to_float<scalar_t>(tmps[j]) * shared_exp_sums[j];",
      "    }",
      "    if (num_partitions > 2 * JCHUNK) {",
      "  #pragma unroll",
      "      for (int j = 2 * JCHUNK; j < MAX_NPAR; j++) {",
      "        acc += to_float<scalar_t>(tmps[j]) * shared_exp_sums[j];",
      "      }",
      "    }",
      "  }",
      "",
      "  for (int p = 1; p < NPAR_LOOPS; p++) {",
      "    if (num_partitions > p * MAX_NPAR) {",
      "      idx = 0;",
      "  #pragma unroll",
      "      for (int j = p * MAX_NPAR * HEAD_SIZE; j < (p + 1) * MAX_NPAR * HEAD_SIZE;",
      "           j += HEAD_SIZE) {",
      "        // lastj is last valid partition",
      "        const int lastj_offset =",
      "            (j < num_partition_offset) ? j : last_partition_offset;",
      "        tmps[idx] = tmp_out_ptr[lastj_offset];",
      "        idx++;",
      "      }",
      "",
      "  #pragma unroll",
      "      for (int j = 0; j < MAX_NPAR; j++) {",
      "        acc += to_float<scalar_t>(tmps[j]) * shared_exp_sums[j + p * MAX_NPAR];",
      "      }",
      "    }",
      "  }",
      "",
      "  const float inv_global_exp_sum =",
      "      __fdividef(1.0f, shared_global_exp_sum + 1e-6f);",
      "  acc *= inv_global_exp_sum;",
      "",
      "  const int64_t query_start_off = static_cast<int64_t>(",
      "      query_start_loc_ptr ? query_start_loc_ptr[seq_idx] : seq_idx);",
      "  OUTT* out_ptr = out + query_start_off * num_heads * HEAD_SIZE +",
      "                  static_cast<int64_t>(head_idx) * HEAD_SIZE;",
      "  out_ptr[threadIdx.x] = from_float<scalar_t>(acc);",
      "}",
      "",
      "#elif defined(__HIP__GFX12__)",
      "",
      "using floatx8 = __attribute__((__vector_size__(8 * sizeof(float)))) float;",
      "",
      "using bit16_t = uint16_t;",
      "using bit16x4 = __attribute__((__vector_size__(4 * sizeof(uint16_t)))) uint16_t;",
      "typedef bit16x4 _B16x4;",
      "",
      "using bit16x8 = __attribute__((__vector_size__(8 * sizeof(uint16_t)))) uint16_t;",
      "union b16x8_u {",
      "  bit16x8 u16x8;",
      "  _B16x4 xy[2];",
      "};",
      "typedef b16x8_u _B16x8;",
      "",
      "using _B8x8 = uint2;",
      "using bit8_t = uint8_t;",
      "",
      "typedef struct _B8x16 {",
      "  _B8x8 xy[2];",
      "} _B8x16;",
      "",
      "template <typename T, int absz, int cbid, int blgp>",
      "__device__ __forceinline__ floatx8 gcn_wmma16x16x16_instr(const bit16x8& inpA,",
      "                                                          const bit16x8& inpB,",
      "                                                          const floatx8& inpC) {",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    return __builtin_amdgcn_wmma_f32_16x16x16_f16_w32_gfx12(inpA, inpB, inpC);",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    return __builtin_amdgcn_wmma_f32_16x16x16_bf16_w32_gfx12(inpA, inpB, inpC);",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ float to_float(const T& inp) {",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    return (float)inp;",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    return __bfloat162float(inp);",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ float to_float_b16(const bit16_t& inp) {",
      "  union tmpcvt {",
      "    bit16_t u;",
      "    _Float16 f;",
      "    __hip_bfloat16 b;",
      "  } t16;",
      "  t16.u = inp;",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    return (float)t16.f;",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    return __bfloat162float(t16.b);",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ T from_float(const float& inp) {",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    return (_Float16)inp;",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    return __float2bfloat16(inp);",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ _B16x8 from_floatx8(const floatx8& inp) {",
      "  if constexpr (std::is_same<T, _Float16>::value) {",
      "    union h2cvt {",
      "      __half2 h2[4];",
      "      _B16x8 b16x8;",
      "    } u;",
      "    u.h2[0] = __float22half2_rn(make_float2(inp[0], inp[1]));",
      "    u.h2[1] = __float22half2_rn(make_float2(inp[2], inp[3]));",
      "    u.h2[2] = __float22half2_rn(make_float2(inp[4], inp[5]));",
      "    u.h2[3] = __float22half2_rn(make_float2(inp[6], inp[7]));",
      "    return u.b16x8;",
      "  } else if constexpr (std::is_same<T, __hip_bfloat16>::value) {",
      "    union b2cvt {",
      "      __hip_bfloat162 b2[4];",
      "      _B16x8 b16x8;",
      "    } u;",
      "",
      "    u.b2[0] = __float22bfloat162_rn(make_float2(inp[0], inp[1]));",
      "    u.b2[1] = __float22bfloat162_rn(make_float2(inp[2], inp[3]));",
      "    u.b2[2] = __float22bfloat162_rn(make_float2(inp[4], inp[5]));",
      "    u.b2[3] = __float22bfloat162_rn(make_float2(inp[6], inp[7]));",
      "",
      "    return u.b16x8;",
      "  } else {",
      "    static_assert(false, \"unsupported 16b dtype\");",
      "  }",
      "}",
      "",
      "// clang-format off",
      "template <typename scalar_t, typename cache_t,",
      "          vllm::Fp8KVCacheDataType KV_DTYPE, typename OUTT, int BLOCK_SIZE,",
      "          int HEAD_SIZE, int NUM_THREADS, bool ALIBI_ENABLED, int GQA_RATIO>",
      "__global__",
      "__launch_bounds__(NUM_THREADS, 3) void paged_attention_ll4mi_QKV_mfma16_kernel(",
      "    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]",
      "    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,",
      "                                          // head_size/x, block_size, x]",
      "    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,",
      "                                          // head_size, block_size]",
      "    const int num_kv_heads, const float scale,",
      "    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]",
      "    const int* __restrict__ seq_lens,  // [num_seqs]",
      "    const int* __restrict__ query_start_loc_ptr,   // [num_seqs]",
      "    const int max_num_blocks_per_seq,",
      "    const float* __restrict__ alibi_slopes,  // [num_heads]",
      "    const int q_stride, const int kv_block_stride, const int kv_head_stride,",
      "    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]",
      "    float* __restrict__ max_logits,  // [num_seqs, num_heads,",
      "                                     // max_num_partitions]",
      "    scalar_t* __restrict__ out,    // [num_seqs, num_heads, max_num_partitions,",
      "                                   // head_size]",
      "    OUTT* __restrict__ final_out,  // [num_seqs, num_heads, head_size]",
      "    int max_ctx_blocks, const float* k_scale, const float* v_scale) {",
      "  // clang-format on",
      "  constexpr int NWARPS = NUM_THREADS / WARP_SIZE;  // 8 warps on gfx11",
      "  const int warpid = threadIdx.x / WARP_SIZE;",
      "  const int laneid = threadIdx.x % WARP_SIZE;",
      "  const int lane2id = laneid % 2;",
      "  const int lane16id = laneid % 16;",
      "  const int rowid = laneid / 16;",
      "",
      "  const int seq_idx = blockIdx.x;",
      "  // NOTE queries with sequence len > 1 are prefills and taken care by another",
      "  // kernel.",
      "  if (query_start_loc_ptr != nullptr &&",
      "      (query_start_loc_ptr[seq_idx + 1] - query_start_loc_ptr[seq_idx] != 1)) {",
      "    return;",
      "  }",
      "  const int partition_idx = blockIdx.y;",
      "",
      "  constexpr int T_PAR_SIZE = 256;  // token partition size set to 256",
      "",
      "  const int max_num_partitions = gridDim.y;",
      "",
      "  const int seq_len = seq_lens[seq_idx];  // length of a seq",
      "",
      "  const int partition_start_token_idx = partition_idx * T_PAR_SIZE;",
      "  // exit if partition is out of context for seq",
      "  if (partition_start_token_idx >= seq_len) {",
      "    return;",
      "  }",
      "",
      "  constexpr int GQA_RATIO2 = DIVIDE_ROUND_UP(GQA_RATIO, 2);",
      "",
      "  __shared__ float shared_qk_max[NWARPS][16 + 1];",
      "  __shared__ float shared_exp_sum[NWARPS][16 + 1];",
      "  // shared_logits is used for multiple purposes",
      "  __shared__ _B16x8 shared_logits[NWARPS][2][16][2];",
      "",
      "  // for QK wmma16x16_gfx12, layout is QHead/Tokenx16 across every 16 lanes,",
      "  // 16 Bytes HeadElements in each lane, 2x16B HeadElements across 2 rows of",
      "  // warp",
      "  constexpr int ROWS_PER_WARP =",
      "      WARP_SIZE / 16;  // rows refers to 16 lanes; refer dpp terminology",
      "  constexpr int CONTIGUOUS_KV_ELEMS_16B_LOAD =",
      "      16 / sizeof(cache_t);  // 8 for 16 bit cache type, 16 for 8 bit types",
      "  constexpr int QKHE_PER_FETCH =",
      "      CONTIGUOUS_KV_ELEMS_16B_LOAD *",
      "      ROWS_PER_WARP;  // each fetch across a warp fetches these many elements",
      "  constexpr int QKHELOOP = HEAD_SIZE / QKHE_PER_FETCH;  // 2xQKHE_16B across",
      "                                                        // warp",
      "",
      "  _B16x8 Qlocal[QKHELOOP];  // note that 16 contiguous elements of Q should",
      "                            // be fetched per lane for 16 bit cache types",
      "",
      "  constexpr int CONTIGUOUS_SCALAR_ELEMS_16B = 16 / sizeof(scalar_t);",
      "",
      "  constexpr int TOKENS_PER_WARP =",
      "      T_PAR_SIZE /",
      "      NWARPS;  // sub partition of tokens per warp for qk calculation",
      "  constexpr int TLOOP =",
      "      TOKENS_PER_WARP /",
      "      16;  // each wmma16x16x16 instruction processes 16 tokens",
      "",
      "  _B16x8 Klocal[TLOOP]",
      "               [QKHELOOP];  // can be interpreted as B8x16 for 8 bit types",
      "",
      "  const int wg_start_head_idx = blockIdx.z * GQA_RATIO;",
      "  const int wg_start_kv_head_idx = blockIdx.z;",
      "  const int total_num_heads = gridDim.z * GQA_RATIO;",
      "",
      "  // for QK wmma, tokens in multiples of TOKENS_PER_WARP are spread across warps",
      "  // each wmma takes QH16xT16x16HE across warp",
      "  // repeat wmma across QKHELOOP dimension",
      "  // output layout from QKwmma : QH16xT8x2 16 qheads across 16 lanes, 16 tokens",
      "  // across 2 rows x 8 tokens per lane",
      "",
      "  const int64_t query_start_off = static_cast<int64_t>(",
      "      query_start_loc_ptr ? query_start_loc_ptr[seq_idx] : seq_idx);",
      "",
      "  if (GQA_RATIO == 1) {",
      "    const int local_qhead_idx = lane16id % GQA_RATIO;",
      "    const int global_qhead_idx = wg_start_head_idx + local_qhead_idx;",
      "    const scalar_t* q_ptr = q + query_start_off * q_stride +",
      "                            global_qhead_idx * HEAD_SIZE +",
      "                            rowid * CONTIGUOUS_KV_ELEMS_16B_LOAD;",
      "    if (lane16id < GQA_RATIO) {",
      "  #pragma unroll",
      "      for (int qkhe_depth = 0; qkhe_depth < QKHELOOP; qkhe_depth++) {",
      "        const scalar_t* q_fetch_ptr = q_ptr + qkhe_depth * QKHE_PER_FETCH;",
      "        const _B16x8* q_fetch_ptr_16B =",
      "            reinterpret_cast<const _B16x8*>(q_fetch_ptr);",
      "        Qlocal[qkhe_depth] = *q_fetch_ptr_16B;",
      "      }",
      "    }",
      "  } else {",
      "    // fetch Q in shared across warps and then write to registers",
      "    const int local_qhead_idx = 2 * warpid + rowid;",
      "    const int global_qhead_idx = wg_start_head_idx + local_qhead_idx;",
      "    const scalar_t* q_ptr =",
      "        q + query_start_off * q_stride + global_qhead_idx * HEAD_SIZE;",
      "",
      "    const int qhead_element = lane16id * CONTIGUOUS_SCALAR_ELEMS_16B;",
      "    if ((local_qhead_idx < GQA_RATIO) && (qhead_element < HEAD_SIZE)) {",
      "      const scalar_t* q_fetch_ptr = q_ptr + qhead_element;",
      "      const _B16x8* q_fetch_ptr_16B =",
      "          reinterpret_cast<const _B16x8*>(q_fetch_ptr);",
      "      _B16x8 tmp = *q_fetch_ptr_16B;",
      "",
      "      const int offset1 =",
      "          lane16id /",
      "          2;  // 16 contiguous chunks of head elems are spread across 8x2lanes",
      "      shared_logits[offset1][lane2id][local_qhead_idx][0] = tmp;",
      "    }",
      "",
      "    __syncthreads();",
      "",
      "  #pragma unroll",
      "    for (int qkhe_depth = 0; qkhe_depth < QKHELOOP; qkhe_depth++) {",
      "      Qlocal[qkhe_depth] =",
      "          shared_logits[qkhe_depth][rowid][lane16id % GQA_RATIO][0];",
      "    }",
      "  }",
      "",
      "  const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);",
      "  const int last_seq_block = num_seq_blocks - 1;",
      "",
      "  const int* block_table_seq = block_tables + seq_idx * max_num_blocks_per_seq;",
      "",
      "  int kphysical_block_number[TLOOP];",
      "",
      "  // fetch k physical block numbers",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    const int klocal_token_idx =",
      "        TOKENS_PER_WARP * warpid + token_depth * 16 + lane16id;",
      "    const int kglobal_token_idx = partition_start_token_idx + klocal_token_idx;",
      "    const int kblock_idx = (kglobal_token_idx < seq_len)",
      "                               ? kglobal_token_idx / BLOCK_SIZE",
      "                               : last_seq_block;",
      "    kphysical_block_number[token_depth] = block_table_seq[kblock_idx];",
      "  }",
      "",
      "  constexpr int KX = 16 / sizeof(cache_t);",
      "  const cache_t* k_ptr = k_cache + wg_start_kv_head_idx * kv_head_stride;",
      "",
      "  const int row_head_elem = rowid * CONTIGUOUS_KV_ELEMS_16B_LOAD;",
      "",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    const int64_t kblock_number =",
      "        static_cast<int64_t>(kphysical_block_number[token_depth]);",
      "    const cache_t* k_ptr2 = k_ptr + kblock_number * kv_block_stride;",
      "    const int klocal_token_idx =",
      "        TOKENS_PER_WARP * warpid + token_depth * 16 + lane16id;",
      "    const int kphysical_block_offset = klocal_token_idx % BLOCK_SIZE;",
      "    const cache_t* k_ptr3 = k_ptr2 + kphysical_block_offset * KX;",
      "",
      "    for (int qkhe_depth = 0; qkhe_depth < QKHELOOP; qkhe_depth++) {",
      "      const int head_elem = row_head_elem + qkhe_depth * QKHE_PER_FETCH;",
      "      const int offset1 = head_elem / KX;",
      "      const int offset2 = head_elem % KX;",
      "      const cache_t* k_fetch_ptr = k_ptr3 + offset1 * BLOCK_SIZE * KX + offset2;",
      "      const _B16x8* k_fetch_ptr_16B =",
      "          reinterpret_cast<const _B16x8*>(k_fetch_ptr);",
      "      Klocal[token_depth][qkhe_depth] = *k_fetch_ptr_16B;",
      "    }",
      "  }",
      "",
      "  constexpr int VTOKENS_PER_LANE =",
      "      TOKENS_PER_WARP / ROWS_PER_WARP;  // 32/2 = 16 vtokens per lane",
      "  constexpr int VBLOCKS_PER_LANE = 1;   // assumes block size >=16",
      "  constexpr int VTLOOP = NWARPS;        // corresponds to tokens across warps",
      "  constexpr int VTLANELOOP = DIVIDE_ROUND_UP(",
      "      VTOKENS_PER_LANE,",
      "      CONTIGUOUS_KV_ELEMS_16B_LOAD);  // optimized for 16B fetches; assumes",
      "                                      // minimum block size is 16",
      "  constexpr int VHELOOP = DIVIDE_ROUND_UP(",
      "      (HEAD_SIZE / 16), NWARPS);  // head_size distributed across warps; each",
      "                                  // wmma instr works on 16 head elements",
      "",
      "  int vphysical_block_number[VTLOOP][VBLOCKS_PER_LANE];",
      "",
      "  // fetch v physical block numbers",
      "  for (int vtoken_depth = 0; vtoken_depth < VTLOOP; vtoken_depth++) {",
      "    for (int vblock_depth = 0; vblock_depth < VBLOCKS_PER_LANE;",
      "         vblock_depth++) {",
      "      const int vlocal_token_idx =",
      "          vtoken_depth * VTOKENS_PER_LANE * ROWS_PER_WARP +",
      "          rowid * VTOKENS_PER_LANE + vblock_depth * BLOCK_SIZE;",
      "      const int vglobal_token_idx =",
      "          partition_start_token_idx + vlocal_token_idx;",
      "      const int vblock_idx = (vglobal_token_idx < seq_len)",
      "                                 ? vglobal_token_idx / BLOCK_SIZE",
      "                                 : last_seq_block;",
      "      vphysical_block_number[vtoken_depth][vblock_depth] =",
      "          block_table_seq[vblock_idx];",
      "    }",
      "  }",
      "",
      "  _B16x8 Vlocal[VTLOOP][VHELOOP]",
      "               [VTLANELOOP];  // this can be interpreted as B8x16 too",
      "",
      "  const cache_t* v_ptr = v_cache + wg_start_kv_head_idx * kv_head_stride +",
      "                         ((rowid * VTOKENS_PER_LANE) % BLOCK_SIZE);",
      "",
      "  // v fetches are 16head elems across lanes x 16 tokens per lane",
      "  for (int vhe_depth = 0; vhe_depth < VHELOOP; vhe_depth++) {",
      "    const int vhead_elem = vhe_depth * NWARPS * 16 + warpid * 16 + lane16id;",
      "    const cache_t* v_ptr2 = v_ptr + vhead_elem * BLOCK_SIZE;",
      "",
      "    for (int vtoken_depth = 0; vtoken_depth < VTLOOP; vtoken_depth++) {",
      "      for (int vfetch_depth = 0; vfetch_depth < VTLANELOOP; vfetch_depth++) {",
      "        const int vblock_depth = 0;",
      "        const int64_t vblock_number = static_cast<int64_t>(",
      "            vphysical_block_number[vtoken_depth][vblock_depth]);",
      "        const cache_t* v_ptr3 = v_ptr2 + (vblock_number * kv_block_stride);",
      "",
      "        const cache_t* v_fetch_ptr =",
      "            v_ptr3 + vfetch_depth * CONTIGUOUS_KV_ELEMS_16B_LOAD;",
      "        const _B16x8* v_fetch_ptr_16B =",
      "            reinterpret_cast<const _B16x8*>(v_fetch_ptr);",
      "        Vlocal[vtoken_depth][vhe_depth][vfetch_depth] = *v_fetch_ptr_16B;",
      "      }",
      "    }",
      "  }",
      "",
      "  floatx8 dout[TLOOP];",
      "  // qk wmma",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    dout[token_depth] = {0};",
      "    for (int qkhe_depth = 0; qkhe_depth < QKHELOOP; qkhe_depth++) {",
      "      dout[token_depth] = gcn_wmma16x16x16_instr<scalar_t, 0, 0, 0>(",
      "          Klocal[token_depth][qkhe_depth].u16x8, Qlocal[qkhe_depth].u16x8,",
      "          dout[token_depth]);",
      "    }",
      "    dout[token_depth] *= scale;",
      "  }",
      "",
      "  // calculate qk_max and exp_sum per warp and write to shared memory",
      "  float qk_max = -FLT_MAX;",
      "  float exp_sum = 0.0f;",
      "  const int qkout_token_idx =",
      "      partition_start_token_idx + TOKENS_PER_WARP * warpid + rowid * 8;",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    const int local_token_idx = qkout_token_idx + token_depth * 16;",
      "    for (int i = 0; i < 8; i++) {",
      "      const float tmp =",
      "          (local_token_idx + i < seq_len) ? dout[token_depth][i] : -FLT_MAX;",
      "      qk_max = fmaxf(qk_max, tmp);",
      "    }",
      "  }",
      "",
      "  qk_max = fmaxf(qk_max, __shfl_xor(qk_max, 16));",
      "",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    const int local_token_idx = qkout_token_idx + token_depth * 16;",
      "    for (int i = 0; i < 8; i++) {",
      "      const float tmp = (local_token_idx + i < seq_len)",
      "                            ? __expf(dout[token_depth][i] - qk_max)",
      "                            : 0.0f;",
      "      dout[token_depth][i] = tmp;",
      "      exp_sum += tmp;",
      "    }",
      "  }",
      "",
      "  exp_sum += __shfl_xor(exp_sum, 16);",
      "",
      "  __syncthreads();",
      "",
      "  if (laneid < 16) {",
      "    shared_qk_max[warpid][lane16id] = qk_max;",
      "    shared_exp_sum[warpid][lane16id] = exp_sum;",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  // calculate partition qk_max and exp_sum",
      "  float partition_qk_max = -FLT_MAX;",
      "  float warp_qk_max_exp[NWARPS];",
      "  float partition_exp_sum = 0.0f;",
      "",
      "  #pragma unroll",
      "  for (int w = 0; w < NWARPS; w++) {",
      "    warp_qk_max_exp[w] = shared_qk_max[w][lane16id];",
      "    partition_qk_max = fmaxf(partition_qk_max, warp_qk_max_exp[w]);",
      "  }",
      "",
      "  for (int w = 0; w < NWARPS; w++) {",
      "    warp_qk_max_exp[w] = __expf(warp_qk_max_exp[w] - partition_qk_max);",
      "    partition_exp_sum += shared_exp_sum[w][lane16id] * warp_qk_max_exp[w];",
      "  }",
      "",
      "  const float inv_sum_scale =",
      "      __fdividef(1.f, partition_exp_sum + 1e-6f) * warp_qk_max_exp[warpid];",
      "",
      "  __syncthreads();",
      "",
      "  // write logits to shared mem",
      "  #pragma unroll",
      "  for (int token_depth = 0; token_depth < TLOOP; token_depth++) {",
      "    dout[token_depth] *= inv_sum_scale;",
      "    shared_logits[warpid][token_depth][lane16id][rowid] =",
      "        from_floatx8<scalar_t>(dout[token_depth]);",
      "  }",
      "",
      "  // write out partition max_logits and exp_sum",
      "  if (threadIdx.x < GQA_RATIO) {",
      "    const int qhead_idx = lane16id;",
      "    const int offset = seq_idx * total_num_heads * max_num_partitions +",
      "                       (wg_start_head_idx + qhead_idx) * max_num_partitions +",
      "                       partition_idx;",
      "    max_logits[offset] = partition_qk_max;",
      "    exp_sums[offset] = partition_exp_sum;",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  _B16x8 outelems[VHELOOP];",
      "  // Softmax V wmma",
      "  // v layout: 16he across lanes x 16 tokens per lane",
      "  for (int vhe_depth = 0; vhe_depth < VHELOOP; vhe_depth++) {",
      "    floatx8 tmp_out = {0};",
      "",
      "    for (int vtoken_depth = 0; vtoken_depth < VTLOOP; vtoken_depth++) {",
      "      for (int vfetch_depth = 0; vfetch_depth < VTLANELOOP; vfetch_depth++) {",
      "        const int offset = rowid * VTLANELOOP + vfetch_depth;",
      "        const int offset1 = offset % ROWS_PER_WARP;",
      "        const int offset2 = offset / ROWS_PER_WARP;",
      "        // if output format is 16 qheads across 16 lanes, 16 head elems spread",
      "        // across rows",
      "        tmp_out = gcn_wmma16x16x16_instr<scalar_t, 0, 0, 0>(",
      "            Vlocal[vtoken_depth][vhe_depth][vfetch_depth].u16x8,",
      "            shared_logits[vtoken_depth][offset2][lane16id][offset1].u16x8,",
      "            tmp_out);",
      "      }",
      "    }",
      "    outelems[vhe_depth] = from_floatx8<scalar_t>(tmp_out);",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  #pragma unroll",
      "  for (int vhe_depth = 0; vhe_depth < VHELOOP; vhe_depth++) {",
      "    shared_logits[warpid][vhe_depth][lane16id][rowid] =",
      "        outelems[vhe_depth];  // lane16 id head dimension; rowid head element",
      "                              // dimension",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  // write to tmp_out with coalesced writes after reading from shared mem",
      "  if (warpid == 0) {",
      "    _B16x8 vout[GQA_RATIO2];",
      "    // each lane writes out 16Bytes of tmp_out along head elem dimension",
      "    const int head_elem_idx = lane16id * 8;",
      "    if (head_elem_idx < HEAD_SIZE) {",
      "      for (int h = 0; h < GQA_RATIO2; h++) {",
      "        const int local_head_idx = 2 * h + rowid;",
      "        const int offset1 = (head_elem_idx / 16) % NWARPS;",
      "        const int offset2 = head_elem_idx / 16 / NWARPS;",
      "        const int offset3 = (head_elem_idx / 8) % 2;  // num_he % num_row",
      "        vout[h] = shared_logits[offset1][offset2][local_head_idx][offset3];",
      "      }",
      "",
      "      const int hsz_maxp_mult = HEAD_SIZE * max_num_partitions;",
      "      scalar_t* out_ptr = out + seq_idx * total_num_heads * hsz_maxp_mult +",
      "                          partition_idx * HEAD_SIZE;",
      "      for (int h = 0; h < GQA_RATIO2; h++) {",
      "        const int local_head_idx = 2 * h + rowid;",
      "        if (local_head_idx < GQA_RATIO) {",
      "          const int out_head_idx = wg_start_head_idx + local_head_idx;",
      "          scalar_t* out_ptr2 = out_ptr + out_head_idx * hsz_maxp_mult;",
      "          scalar_t* out_ptr3 = out_ptr2 + head_elem_idx;",
      "          _B16x8* out_ptr_B16x8 = reinterpret_cast<_B16x8*>(out_ptr3);",
      "          *out_ptr_B16x8 = vout[h];",
      "        }",
      "      }",
      "    }",
      "  }",
      "}",
      "",
      "template <typename scalar_t, typename cache_t,",
      "          vllm::Fp8KVCacheDataType KV_DTYPE, typename OUTT, int BLOCK_SIZE,",
      "          int HEAD_SIZE, int NUM_THREADS, bool ALIBI_ENABLED,",
      "          int GQA_RATIO>",
      "__global__",
      "__launch_bounds__(NUM_THREADS) void paged_attention_ll4mi_QKV_mfma4_kernel(",
      "    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]",
      "    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,",
      "                                          // head_size/x, block_size, x]",
      "    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,",
      "                                          // head_size, block_size]",
      "    const int num_kv_heads, const float scale,",
      "    const int* __restrict__ block_tables,  // [num_seqs, max_num_blocks_per_seq]",
      "    const int* __restrict__ seq_lens,      // [num_seqs]",
      "    const int* __restrict__ query_start_loc_ptr,  // [num_seqs]",
      "    const int max_num_blocks_per_seq,",
      "    const float* __restrict__ alibi_slopes,  // [num_heads]",
      "    const int q_stride, const int kv_block_stride, const int kv_head_stride,",
      "    float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]",
      "    float* __restrict__ max_logits,  // [num_seqs, num_heads,",
      "                                     // max_num_partitions]",
      "    scalar_t* __restrict__ out,    // [num_seqs, num_heads, max_num_partitions,",
      "                                   // head_size]",
      "    OUTT* __restrict__ final_out,  // [num_seqs, num_heads, head_size]",
      "    int max_ctx_blocks, const float* k_scale, const float* v_scale) {",
      "  UNREACHABLE_CODE",
      "}",
      "",
      "// Grid: (num_heads, num_seqs).",
      "template <typename scalar_t, typename OUTT, int HEAD_SIZE, int NUM_THREADS,",
      "          int PARTITION_SIZE, int NPAR_LOOPS>",
      "__global__",
      "__launch_bounds__(NUM_THREADS) void paged_attention_ll4mi_reduce_kernel(",
      "    OUTT* __restrict__ out,                // [num_seqs, num_heads, head_size]",
      "    const float* __restrict__ exp_sums,    // [num_seqs, num_heads,",
      "                                           // max_num_partitions]",
      "    const float* __restrict__ max_logits,  // [num_seqs, num_heads,",
      "                                           // max_num_partitions]",
      "    const scalar_t* __restrict__ tmp_out,  // [num_seqs, num_heads,",
      "                                           // max_num_partitions, head_size]",
      "    const int* __restrict__ seq_lens,      // [num_seqs]",
      "    const int* __restrict__ query_start_loc_ptr,  // [num_seqs]",
      "    const int max_num_partitions, const float* __restrict__ fp8_out_scale_ptr) {",
      "  const auto num_heads = gridDim.x;",
      "  const auto head_idx = blockIdx.x;",
      "  const auto seq_idx = blockIdx.y;",
      "",
      "  // NOTE queries with sequence len > 1 are prefills and taken care by another",
      "  // kernel.",
      "  if (query_start_loc_ptr != nullptr &&",
      "      (query_start_loc_ptr[seq_idx + 1] - query_start_loc_ptr[seq_idx] != 1)) {",
      "    return;",
      "  }",
      "",
      "  const int seq_len = seq_lens[seq_idx];",
      "  const int num_partitions = DIVIDE_ROUND_UP(seq_len, PARTITION_SIZE);",
      "  const int warpid = threadIdx.x / WARP_SIZE;",
      "",
      "  __shared__ float shared_global_exp_sum;",
      "  // max num partitions supported is warp_size * NPAR_LOOPS",
      "  __shared__ float shared_exp_sums[NPAR_LOOPS * WARP_SIZE];",
      "",
      "  if (warpid == 0) {",
      "    const float* max_logits_ptr = max_logits +",
      "                                  seq_idx * num_heads * max_num_partitions +",
      "                                  head_idx * max_num_partitions;",
      "",
      "    // valid partition is the last valid partition in case threadid > num",
      "    // partitions",
      "    int valid_partition[NPAR_LOOPS];",
      "    float reg_max_logit[NPAR_LOOPS];",
      "    const int last_valid_partition = num_partitions - 1;",
      "",
      "  #pragma unroll",
      "    for (int i = 0; i < NPAR_LOOPS; i++) {",
      "      const int partition_no = i * WARP_SIZE + threadIdx.x;",
      "      valid_partition[i] =",
      "          (partition_no < num_partitions) ? partition_no : last_valid_partition;",
      "    }",
      "  #pragma unroll",
      "    for (int i = 0; i < NPAR_LOOPS; i++) {",
      "      reg_max_logit[i] = max_logits_ptr[valid_partition[i]];",
      "    }",
      "    float max_logit = reg_max_logit[0];",
      "  #pragma unroll",
      "    for (int i = 1; i < NPAR_LOOPS; i++) {",
      "      max_logit = fmaxf(max_logit, reg_max_logit[i]);",
      "    }",
      "",
      "  #pragma unroll",
      "    for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {",
      "      max_logit = fmaxf(max_logit, __shfl_xor(max_logit, mask));",
      "    }",
      "",
      "    const float* exp_sums_ptr = exp_sums +",
      "                                seq_idx * num_heads * max_num_partitions +",
      "                                head_idx * max_num_partitions;",
      "",
      "    float rescaled_exp_sum[NPAR_LOOPS];",
      "  #pragma unroll",
      "    for (int i = 0; i < NPAR_LOOPS; i++) {",
      "      rescaled_exp_sum[i] = exp_sums_ptr[valid_partition[i]];",
      "    }",
      "  #pragma unroll",
      "    for (int i = 0; i < NPAR_LOOPS; i++) {",
      "      const int partition_no = i * WARP_SIZE + threadIdx.x;",
      "      rescaled_exp_sum[i] *= (partition_no < num_partitions)",
      "                                 ? expf(reg_max_logit[i] - max_logit)",
      "                                 : 0.0f;",
      "    }",
      "    float global_exp_sum = rescaled_exp_sum[0];",
      "  #pragma unroll",
      "    for (int i = 1; i < NPAR_LOOPS; i++) {",
      "      global_exp_sum += rescaled_exp_sum[i];",
      "    }",
      "  #pragma unroll",
      "    for (int i = 0; i < NPAR_LOOPS; i++) {",
      "      const int partition_no = i * WARP_SIZE + threadIdx.x;",
      "      shared_exp_sums[partition_no] = rescaled_exp_sum[i];",
      "    }",
      "",
      "  #pragma unroll",
      "    for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {",
      "      global_exp_sum += __shfl_xor(global_exp_sum, mask);",
      "    }",
      "    if (threadIdx.x == 0) {",
      "      shared_global_exp_sum = global_exp_sum;",
      "    }",
      "  }  // warpid == 0",
      "  const scalar_t* tmp_out_ptr =",
      "      tmp_out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +",
      "      head_idx * max_num_partitions * HEAD_SIZE + threadIdx.x;",
      "  constexpr int MAX_NPAR = 32;",
      "  scalar_t tmps[MAX_NPAR];",
      "  const float dzero = 0.0f;",
      "  #pragma unroll",
      "  for (int j = 0; j < MAX_NPAR; j++) {",
      "    tmps[j] = from_float<scalar_t>(dzero);",
      "  }",
      "  const int last_partition_offset = (num_partitions - 1) * HEAD_SIZE;",
      "  const int num_partition_offset = (num_partitions)*HEAD_SIZE;",
      "  int idx = 0;",
      "",
      "  constexpr int JCHUNK = 16;",
      "",
      "  #pragma unroll",
      "  for (int j = 0; j < JCHUNK * HEAD_SIZE; j += HEAD_SIZE) {",
      "    // lastj is last valid partition",
      "    const int lastj_offset =",
      "        (j < num_partition_offset) ? j : last_partition_offset;",
      "    tmps[idx] = tmp_out_ptr[lastj_offset];",
      "    idx++;",
      "  }",
      "  __syncthreads();",
      "",
      "  if (num_partitions > JCHUNK) {",
      "  #pragma unroll",
      "    for (int j = JCHUNK * HEAD_SIZE; j < 2 * JCHUNK * HEAD_SIZE;",
      "         j += HEAD_SIZE) {",
      "      const int lastj_offset =",
      "          (j < num_partition_offset) ? j : last_partition_offset;",
      "      tmps[idx] = tmp_out_ptr[lastj_offset];",
      "      idx++;",
      "    }",
      "",
      "    if (num_partitions > 2 * JCHUNK) {",
      "  #pragma unroll",
      "      for (int j = 2 * JCHUNK * HEAD_SIZE; j < MAX_NPAR * HEAD_SIZE;",
      "           j += HEAD_SIZE) {",
      "        const int lastj_offset =",
      "            (j < num_partition_offset) ? j : last_partition_offset;",
      "        tmps[idx] = tmp_out_ptr[lastj_offset];",
      "        idx++;",
      "      }",
      "    }",
      "  }  // num_partitions > JCHUNK",
      "",
      "  // Aggregate tmp_out to out.",
      "  float acc = 0.0f;",
      "  #pragma unroll",
      "  for (int j = 0; j < JCHUNK; j++) {",
      "    acc += to_float<scalar_t>(tmps[j]) * shared_exp_sums[j];",
      "  }",
      "  if (num_partitions > JCHUNK) {",
      "  #pragma unroll",
      "    for (int j = JCHUNK; j < 2 * JCHUNK; j++) {",
      "      acc += to_float<scalar_t>(tmps[j]) * shared_exp_sums[j];",
      "    }",
      "    if (num_partitions > 2 * JCHUNK) {",
      "  #pragma unroll",
      "      for (int j = 2 * JCHUNK; j < MAX_NPAR; j++) {",
      "        acc += to_float<scalar_t>(tmps[j]) * shared_exp_sums[j];",
      "      }",
      "    }",
      "  }",
      "",
      "  for (int p = 1; p < NPAR_LOOPS; p++) {",
      "    if (num_partitions > p * MAX_NPAR) {",
      "      idx = 0;",
      "  #pragma unroll",
      "      for (int j = p * MAX_NPAR * HEAD_SIZE; j < (p + 1) * MAX_NPAR * HEAD_SIZE;",
      "           j += HEAD_SIZE) {",
      "        // lastj is last valid partition",
      "        const int lastj_offset =",
      "            (j < num_partition_offset) ? j : last_partition_offset;",
      "        tmps[idx] = tmp_out_ptr[lastj_offset];",
      "        idx++;",
      "      }",
      "",
      "  #pragma unroll",
      "      for (int j = 0; j < MAX_NPAR; j++) {",
      "        acc += to_float<scalar_t>(tmps[j]) * shared_exp_sums[j + p * MAX_NPAR];",
      "      }",
      "    }",
      "  }",
      "",
      "  const float inv_global_exp_sum =",
      "      __fdividef(1.0f, shared_global_exp_sum + 1e-6f);",
      "  acc *= inv_global_exp_sum;",
      "",
      "  const int64_t query_start_off = static_cast<int64_t>(",
      "      query_start_loc_ptr ? query_start_loc_ptr[seq_idx] : seq_idx);",
      "  OUTT* out_ptr = out + query_start_off * num_heads * HEAD_SIZE +",
      "                  static_cast<int64_t>(head_idx) * HEAD_SIZE;",
      "  out_ptr[threadIdx.x] = from_float<scalar_t>(acc);",
      "}",
      "",
      "#else",
      "",
      "// clang-format off",
      "template <typename scalar_t, typename cache_t,",
      "          vllm::Fp8KVCacheDataType KV_DTYPE, typename OUTT, int BLOCK_SIZE,",
      "          int HEAD_SIZE, int NUM_THREADS, bool ALIBI_ENABLED,",
      "          int GQA_RATIO>",
      "__global__",
      "__launch_bounds__(NUM_THREADS) void paged_attention_ll4mi_QKV_mfma16_kernel(",
      "    const scalar_t* __restrict__ q,         // [num_seqs, num_heads, head_size]",
      "    const cache_t* __restrict__ k_cache,    // [num_blocks, num_kv_heads, head_size/x, block_size, x]",
      "    const cache_t* __restrict__ v_cache,    // [num_blocks, num_kv_heads, head_size, block_size]",
      "    const int num_kv_heads,",
      "    const float scale,",
      "    const int* __restrict__ block_tables,    // [num_seqs, max_num_blocks_per_seq]",
      "    const int* __restrict__ seq_lens,    // [num_seqs]",
      "    const int* __restrict__ query_start_loc_ptr,  // [num_seqs]",
      "    const int max_num_blocks_per_seq,",
      "    const float* __restrict__ alibi_slopes,  // [num_heads]",
      "    const int q_stride,",
      "    const int kv_block_stride,",
      "    const int kv_head_stride,",
      "    float* __restrict__ exp_sums,             // [num_seqs, num_heads, max_num_partitions]",
      "    float* __restrict__ max_logits,           // [num_seqs, num_heads, max_num_partitions]",
      "    scalar_t* __restrict__ out,               // [num_seqs, num_heads, max_num_partitions, head_size]",
      "    OUTT* __restrict__ final_out,             // [num_seqs, num_heads, head_size]",
      "    int max_ctx_blocks, const float* k_scale, const float* v_scale) {",
      "  UNREACHABLE_CODE",
      "}",
      "",
      "template <typename scalar_t, typename cache_t,",
      "          vllm::Fp8KVCacheDataType KV_DTYPE, typename OUTT, int BLOCK_SIZE,",
      "          int HEAD_SIZE, int NUM_THREADS, bool ALIBI_ENABLED,",
      "          int GQA_RATIO>",
      "__global__",
      "__launch_bounds__(NUM_THREADS) void paged_attention_ll4mi_QKV_mfma4_kernel(",
      "    const scalar_t* __restrict__ q,          // [num_seqs, num_heads, head_size]",
      "    const cache_t* __restrict__ k_cache,     // [num_blocks, num_kv_heads, head_size/x, block_size, x]",
      "    const cache_t* __restrict__ v_cache,     // [num_blocks, num_kv_heads, head_size, block_size]",
      "    const int num_kv_heads,",
      "    const float scale,",
      "    const int* __restrict__ block_tables,    // [num_seqs, max_num_blocks_per_seq]",
      "    const int* __restrict__ seq_lens,    // [num_seqs]",
      "    const int* __restrict__ query_start_loc_ptr,  // [num_seqs]",
      "    const int max_num_blocks_per_seq,",
      "    const float* __restrict__ alibi_slopes,  // [num_heads]",
      "    const int q_stride,",
      "    const int kv_block_stride,",
      "    const int kv_head_stride,",
      "    float* __restrict__ exp_sums,            // [num_seqs, num_heads, max_num_partitions]",
      "    float* __restrict__ max_logits,          // [num_seqs, num_heads, max_num_partitions]",
      "    scalar_t* __restrict__ out,              // [num_seqs, num_heads, max_num_partitions, head_size]",
      "    OUTT* __restrict__ final_out,            // [num_seqs, num_heads, head_size]",
      "    int max_ctx_blocks, const float* k_scale, const float* v_scale) {",
      "  UNREACHABLE_CODE",
      "}",
      "",
      "// Grid: (num_heads, num_seqs).",
      "template <typename scalar_t, typename OUTT, int HEAD_SIZE, int NUM_THREADS,",
      "          int PARTITION_SIZE, int NPAR_LOOPS>",
      "__global__",
      "__launch_bounds__(NUM_THREADS) void paged_attention_ll4mi_reduce_kernel(",
      "    OUTT* __restrict__ out,                // [num_seqs, num_heads, head_size]",
      "    const float* __restrict__ exp_sums,    // [num_seqs, num_heads, max_num_partitions]",
      "    const float* __restrict__ max_logits,  // [num_seqs, num_heads, max_num_partitions]",
      "    const scalar_t* __restrict__ tmp_out,  // [num_seqs, num_heads, max_num_partitions, head_size]",
      "    const int* __restrict__ seq_lens,  // [num_seqs]",
      "    const int* __restrict__ query_start_loc_ptr,  // [num_seqs]",
      "    const int max_num_partitions, const float* __restrict__ fp8_out_scale_ptr) {",
      "  UNREACHABLE_CODE",
      "}",
      "// clang-format on",
      "",
      "#endif",
      "",
      "#define LAUNCH_CUSTOM_ATTENTION_MFMA16(GQA_RATIO)                              \\",
      "  paged_attention_ll4mi_QKV_mfma16_kernel<T, KVT, KV_DTYPE, OUTT, BLOCK_SIZE,  \\",
      "                                          HEAD_SIZE, NTHR, ALIBI_ENABLED,      \\",
      "                                          GQA_RATIO>                           \\",
      "      <<<grid, block, 0, stream>>>(                                            \\",
      "          query_ptr, key_cache_ptr, value_cache_ptr, num_kv_heads, scale,      \\",
      "          block_tables_ptr, seq_lens_ptr, query_start_loc_ptr,                 \\",
      "          max_num_blocks_per_seq, alibi_slopes_ptr, q_stride, kv_block_stride, \\",
      "          kv_head_stride, exp_sums_ptr, max_logits_ptr, tmp_out_ptr, out_ptr,  \\",
      "          max_ctx_blocks, k_scale_ptr, v_scale_ptr);",
      "",
      "#define LAUNCH_CUSTOM_ATTENTION_MFMA4(GQA_RATIO)                               \\",
      "  paged_attention_ll4mi_QKV_mfma4_kernel<T, KVT, KV_DTYPE, OUTT, BLOCK_SIZE,   \\",
      "                                         HEAD_SIZE, NTHR, ALIBI_ENABLED,       \\",
      "                                         GQA_RATIO>                            \\",
      "      <<<grid, block, 0, stream>>>(                                            \\",
      "          query_ptr, key_cache_ptr, value_cache_ptr, num_kv_heads, scale,      \\",
      "          block_tables_ptr, seq_lens_ptr, query_start_loc_ptr,                 \\",
      "          max_num_blocks_per_seq, alibi_slopes_ptr, q_stride, kv_block_stride, \\",
      "          kv_head_stride, exp_sums_ptr, max_logits_ptr, tmp_out_ptr, out_ptr,  \\",
      "          max_ctx_blocks, k_scale_ptr, v_scale_ptr);",
      "",
      "#define LAUNCH_CUSTOM_REDUCTION(NPAR_LOOPS)                                 \\",
      "  paged_attention_ll4mi_reduce_kernel<T, OUTT, HEAD_SIZE, HEAD_SIZE,        \\",
      "                                      PARTITION_SIZE, NPAR_LOOPS>           \\",
      "      <<<reduce_grid, reduce_block, 0, stream>>>(                           \\",
      "          out_ptr, exp_sums_ptr, max_logits_ptr, tmp_out_ptr, seq_lens_ptr, \\",
      "          query_start_loc_ptr, max_num_partitions, fp8_out_scale_ptr);",
      "",
      "template <typename T, typename KVT, vllm::Fp8KVCacheDataType KV_DTYPE,",
      "          int BLOCK_SIZE, int HEAD_SIZE, typename OUTT, int PARTITION_SIZE_OLD,",
      "          bool ALIBI_ENABLED>",
      "void paged_attention_custom_launcher(",
      "    torch::Tensor& out, torch::Tensor& exp_sums, torch::Tensor& max_logits,",
      "    torch::Tensor& tmp_out, torch::Tensor& query, torch::Tensor& key_cache,",
      "    torch::Tensor& value_cache, const int num_kv_heads, float scale,",
      "    torch::Tensor& block_tables, torch::Tensor& seq_lens,",
      "    const std::optional<torch::Tensor>& query_start_loc, int max_seq_len,",
      "    const std::optional<torch::Tensor>& alibi_slopes, torch::Tensor& k_scale,",
      "    torch::Tensor& v_scale, const std::optional<torch::Tensor>& fp8_out_scale) {",
      "  int num_seqs = block_tables.size(0);",
      "  int num_heads = query.size(1);",
      "  int head_size = query.size(2);",
      "  int max_num_blocks_per_seq = block_tables.size(1);",
      "  int q_stride = query.stride(0);",
      "  int kv_block_stride = key_cache.stride(0);",
      "  int kv_head_stride = key_cache.stride(1);",
      "",
      "  // NOTE: query start location is optional for V0 decode should not be used.",
      "  // If batch contains mix of prefills and decode, prefills should be skipped.",
      "  const int* query_start_loc_ptr =",
      "      query_start_loc",
      "          ? reinterpret_cast<const int*>(query_start_loc.value().data_ptr())",
      "          : nullptr;",
      "",
      "  // NOTE: alibi_slopes is optional.",
      "  const float* alibi_slopes_ptr =",
      "      alibi_slopes",
      "          ? reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())",
      "          : nullptr;",
      "",
      "  float* exp_sums_ptr = reinterpret_cast<float*>(exp_sums.data_ptr());",
      "  float* max_logits_ptr = reinterpret_cast<float*>(max_logits.data_ptr());",
      "  T* tmp_out_ptr = reinterpret_cast<T*>(tmp_out.data_ptr());",
      "  T* query_ptr = reinterpret_cast<T*>(query.data_ptr());",
      "  KVT* key_cache_ptr = reinterpret_cast<KVT*>(key_cache.data_ptr());",
      "  KVT* value_cache_ptr = reinterpret_cast<KVT*>(value_cache.data_ptr());",
      "  int* block_tables_ptr = block_tables.data_ptr<int>();",
      "  int* seq_lens_ptr = seq_lens.data_ptr<int>();",
      "  const float* k_scale_ptr = reinterpret_cast<const float*>(k_scale.data_ptr());",
      "  const float* v_scale_ptr = reinterpret_cast<const float*>(v_scale.data_ptr());",
      "  // NOTE: fp8_out_scale is optional.",
      "  const auto fp8_out_scale_ptr =",
      "      fp8_out_scale",
      "          ? static_cast<const float*>(fp8_out_scale.value().data_ptr())",
      "          : nullptr;",
      "  OUTT* out_ptr = reinterpret_cast<OUTT*>(out.data_ptr());",
      "",
      "  const int max_ctx_blocks = DIVIDE_ROUND_UP(max_seq_len, BLOCK_SIZE);",
      "",
      "  // partition size is fixed at 256 since both mfma4 and mfma16 kernels support",
      "  // it mfma4 kernel also supports partition size 512",
      "  constexpr int PARTITION_SIZE = 256;",
      "  const int max_num_partitions = DIVIDE_ROUND_UP(max_seq_len, PARTITION_SIZE);",
      "  const int gqa_ratio = num_heads / num_kv_heads;",
      "  assert(num_heads % num_kv_heads == 0);",
      "  assert(head_size == HEAD_SIZE);",
      "",
      "  constexpr int NTHR = 256;",
      "  dim3 grid(num_seqs, max_num_partitions, num_kv_heads);",
      "  dim3 block(NTHR);",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(query));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "",
      "  // mfma4 kernel is faster than mfma16 for gqa_ratio <= 4",
      "  switch (gqa_ratio) {",
      "    case 1:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA4(1);",
      "      break;",
      "    case 2:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA4(2);",
      "      break;",
      "    case 3:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA4(3);",
      "      break;",
      "    case 4:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA4(4);",
      "      break;",
      "    case 5:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(5);",
      "      break;",
      "    case 6:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(6);",
      "      break;",
      "    case 7:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(7);",
      "      break;",
      "    case 8:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(8);",
      "      break;",
      "    case 9:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(9);",
      "      break;",
      "    case 10:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(10);",
      "      break;",
      "    case 11:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(11);",
      "      break;",
      "    case 12:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(12);",
      "      break;",
      "    case 13:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(13);",
      "      break;",
      "    case 14:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(14);",
      "      break;",
      "    case 15:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(15);",
      "      break;",
      "    case 16:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(16);",
      "      break;",
      "    default:",
      "      TORCH_CHECK(false, \"Unsupported gqa ratio: \", gqa_ratio);",
      "      break;",
      "  }",
      "",
      "  dim3 reduce_grid(num_heads, num_seqs);",
      "  dim3 reduce_block(head_size);",
      "  const int npar_loops = DIVIDE_ROUND_UP(max_num_partitions, WARP_SIZE);",
      "  // reduction kernel supports upto 8 NPAR_loops * 64 (warp_size) * 256",
      "  // (partition size) = 128K context length",
      "  switch (npar_loops) {",
      "    case 1:",
      "      LAUNCH_CUSTOM_REDUCTION(1);",
      "      break;",
      "    case 2:",
      "      LAUNCH_CUSTOM_REDUCTION(2);",
      "      break;",
      "    case 3:",
      "      LAUNCH_CUSTOM_REDUCTION(3);",
      "      break;",
      "    case 4:",
      "      LAUNCH_CUSTOM_REDUCTION(4);",
      "      break;",
      "    case 5:",
      "      LAUNCH_CUSTOM_REDUCTION(5);",
      "      break;",
      "    case 6:",
      "      LAUNCH_CUSTOM_REDUCTION(6);",
      "      break;",
      "    case 7:",
      "      LAUNCH_CUSTOM_REDUCTION(7);",
      "      break;",
      "    case 8:",
      "      LAUNCH_CUSTOM_REDUCTION(8);",
      "      break;",
      "    default:",
      "      TORCH_CHECK(false, \"Unsupported npar_loops: \", npar_loops);",
      "      break;",
      "  }",
      "}",
      "",
      "template <typename T, typename KVT, vllm::Fp8KVCacheDataType KV_DTYPE,",
      "          int BLOCK_SIZE, int HEAD_SIZE, typename OUTT, int PARTITION_SIZE_OLD,",
      "          bool ALIBI_ENABLED>",
      "void paged_attention_custom_launcher_navi(",
      "    torch::Tensor& out, torch::Tensor& exp_sums, torch::Tensor& max_logits,",
      "    torch::Tensor& tmp_out, torch::Tensor& query, torch::Tensor& key_cache,",
      "    torch::Tensor& value_cache, const int num_kv_heads, float scale,",
      "    torch::Tensor& block_tables, torch::Tensor& seq_lens,",
      "    const std::optional<torch::Tensor>& query_start_loc, int max_seq_len,",
      "    const std::optional<torch::Tensor>& alibi_slopes, torch::Tensor& k_scale,",
      "    torch::Tensor& v_scale) {",
      "  int num_seqs = block_tables.size(0);",
      "  int num_heads = query.size(1);",
      "  int head_size = query.size(2);",
      "  int max_num_blocks_per_seq = block_tables.size(1);",
      "  int q_stride = query.stride(0);",
      "  int kv_block_stride = key_cache.stride(0);",
      "  int kv_head_stride = key_cache.stride(1);",
      "",
      "  // NOTE: query start location is optional for V0 decode should not be used.",
      "  // If batch contains mix of prefills and decode, prefills should be skipped.",
      "  const int* query_start_loc_ptr =",
      "      query_start_loc",
      "          ? reinterpret_cast<const int*>(query_start_loc.value().data_ptr())",
      "          : nullptr;",
      "",
      "  // NOTE: Navi does not support alibi_slopes.",
      "  const float* alibi_slopes_ptr = nullptr;",
      "",
      "  float* exp_sums_ptr = reinterpret_cast<float*>(exp_sums.data_ptr());",
      "  float* max_logits_ptr = reinterpret_cast<float*>(max_logits.data_ptr());",
      "  T* tmp_out_ptr = reinterpret_cast<T*>(tmp_out.data_ptr());",
      "  T* query_ptr = reinterpret_cast<T*>(query.data_ptr());",
      "  KVT* key_cache_ptr = reinterpret_cast<KVT*>(key_cache.data_ptr());",
      "  KVT* value_cache_ptr = reinterpret_cast<KVT*>(value_cache.data_ptr());",
      "  int* block_tables_ptr = block_tables.data_ptr<int>();",
      "  int* seq_lens_ptr = seq_lens.data_ptr<int>();",
      "",
      "  const float* k_scale_ptr = reinterpret_cast<const float*>(k_scale.data_ptr());",
      "  const float* v_scale_ptr = reinterpret_cast<const float*>(v_scale.data_ptr());",
      "  // NOTE: Navi does not support fp8.",
      "  const auto fp8_out_scale_ptr = nullptr;",
      "  OUTT* out_ptr = reinterpret_cast<OUTT*>(out.data_ptr());",
      "",
      "  const int max_ctx_blocks = DIVIDE_ROUND_UP(max_seq_len, BLOCK_SIZE);",
      "",
      "  constexpr int PARTITION_SIZE = 256;",
      "  const int max_num_partitions = DIVIDE_ROUND_UP(max_seq_len, PARTITION_SIZE);",
      "  const int gqa_ratio = num_heads / num_kv_heads;",
      "  assert(num_heads % num_kv_heads == 0);",
      "  assert(head_size == HEAD_SIZE);",
      "",
      "  constexpr int NTHR = 256;",
      "  dim3 grid(num_seqs, max_num_partitions, num_kv_heads);",
      "  dim3 block(NTHR);",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(query));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "",
      "  switch (gqa_ratio) {",
      "    case 1:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(1);",
      "      break;",
      "    case 2:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(2);",
      "      break;",
      "    case 3:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(3);",
      "      break;",
      "    case 4:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(4);",
      "      break;",
      "    case 5:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(5);",
      "      break;",
      "    case 6:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(6);",
      "      break;",
      "    case 7:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(7);",
      "      break;",
      "    case 8:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(8);",
      "      break;",
      "    case 9:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(9);",
      "      break;",
      "    case 10:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(10);",
      "      break;",
      "    case 11:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(11);",
      "      break;",
      "    case 12:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(12);",
      "      break;",
      "    case 13:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(13);",
      "      break;",
      "    case 14:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(14);",
      "      break;",
      "    case 15:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(15);",
      "      break;",
      "    case 16:",
      "      LAUNCH_CUSTOM_ATTENTION_MFMA16(16);",
      "      break;",
      "    default:",
      "      TORCH_CHECK(false, \"Unsupported gqa ratio: \", gqa_ratio);",
      "      break;",
      "  }",
      "",
      "  dim3 reduce_grid(num_heads, num_seqs);",
      "  dim3 reduce_block(head_size);",
      "  const int warp_size = 32;",
      "  const int npar_loops = DIVIDE_ROUND_UP(max_num_partitions, warp_size);",
      "  // reduction kernel supports upto 16 NPAR_loops * 32 (warp_size) * 256",
      "  // (partition size) = 128K context length",
      "  switch (npar_loops) {",
      "    case 1:",
      "      LAUNCH_CUSTOM_REDUCTION(1);",
      "      break;",
      "    case 2:",
      "      LAUNCH_CUSTOM_REDUCTION(2);",
      "      break;",
      "    case 3:",
      "      LAUNCH_CUSTOM_REDUCTION(3);",
      "      break;",
      "    case 4:",
      "      LAUNCH_CUSTOM_REDUCTION(4);",
      "      break;",
      "    case 5:",
      "      LAUNCH_CUSTOM_REDUCTION(5);",
      "      break;",
      "    case 6:",
      "      LAUNCH_CUSTOM_REDUCTION(6);",
      "      break;",
      "    case 7:",
      "      LAUNCH_CUSTOM_REDUCTION(7);",
      "      break;",
      "    case 8:",
      "      LAUNCH_CUSTOM_REDUCTION(8);",
      "      break;",
      "    case 9:",
      "      LAUNCH_CUSTOM_REDUCTION(9);",
      "      break;",
      "    case 10:",
      "      LAUNCH_CUSTOM_REDUCTION(10);",
      "      break;",
      "    case 11:",
      "      LAUNCH_CUSTOM_REDUCTION(11);",
      "      break;",
      "    case 12:",
      "      LAUNCH_CUSTOM_REDUCTION(12);",
      "      break;",
      "    case 13:",
      "      LAUNCH_CUSTOM_REDUCTION(13);",
      "      break;",
      "    case 14:",
      "      LAUNCH_CUSTOM_REDUCTION(14);",
      "      break;",
      "    case 15:",
      "      LAUNCH_CUSTOM_REDUCTION(15);",
      "      break;",
      "    case 16:",
      "      LAUNCH_CUSTOM_REDUCTION(16);",
      "      break;",
      "    default:",
      "      TORCH_CHECK(false, \"Unsupported npar_loops: \", npar_loops);",
      "      break;",
      "  }",
      "}",
      "",
      "#define CALL_CUSTOM_LAUNCHER(T, KVT, KV_DTYPE, BLK_SIZE, HEAD_SIZE, OUTT,   \\",
      "                             PSIZE, ALIBI_ENABLED)                          \\",
      "  if (!is_navi) {                                                           \\",
      "    paged_attention_custom_launcher<T, KVT, KV_DTYPE, BLK_SIZE, HEAD_SIZE,  \\",
      "                                    OUTT, PSIZE, ALIBI_ENABLED>(            \\",
      "        out, exp_sums, max_logits, tmp_out, query, key_cache, value_cache,  \\",
      "        num_kv_heads, scale, block_tables, seq_lens, query_start_loc,       \\",
      "        max_seq_len, alibi_slopes, k_scale, v_scale, fp8_out_scale);        \\",
      "  } else {                                                                  \\",
      "    paged_attention_custom_launcher_navi<                                   \\",
      "        T, KVT, KV_DTYPE, BLK_SIZE, HEAD_SIZE, OUTT, PSIZE, ALIBI_ENABLED>( \\",
      "        out, exp_sums, max_logits, tmp_out, query, key_cache, value_cache,  \\",
      "        num_kv_heads, scale, block_tables, seq_lens, query_start_loc,       \\",
      "        max_seq_len, alibi_slopes, k_scale, v_scale);                       \\",
      "  }",
      "",
      "#define CALL_CUSTOM_LAUNCHER_ALIBI(T, KVT, KV_DTYPE, BLK_SIZE, HEAD_SIZE,    \\",
      "                                   OUTT, PSIZE)                              \\",
      "  if (alibi_slopes) {                                                        \\",
      "    CALL_CUSTOM_LAUNCHER(T, KVT, KV_DTYPE, BLK_SIZE, HEAD_SIZE, OUTT, PSIZE, \\",
      "                         true);                                              \\",
      "  } else {                                                                   \\",
      "    CALL_CUSTOM_LAUNCHER(T, KVT, KV_DTYPE, BLK_SIZE, HEAD_SIZE, OUTT, PSIZE, \\",
      "                         false);                                             \\",
      "  }",
      "",
      "#if defined(__HIPCC__) && defined(__gfx90a__)",
      "  #define CALL_CUSTOM_LAUNCHER_OUT(T, KVT, KV_DTYPE, BLK_SIZE, HEAD_SIZE)  \\",
      "    if (fp8_out_scale) {                                                   \\",
      "      TORCH_CHECK(false, \"fp8 out scale unsupported for gfx90a\");          \\",
      "    } else {                                                               \\",
      "      CALL_CUSTOM_LAUNCHER_ALIBI(T, KVT, KV_DTYPE, BLK_SIZE, HEAD_SIZE, T, \\",
      "                                 256);                                     \\",
      "    }",
      "#else",
      "  #define CALL_CUSTOM_LAUNCHER_OUT(T, KVT, KV_DTYPE, BLK_SIZE, HEAD_SIZE)  \\",
      "    if (fp8_out_scale) {                                                   \\",
      "      CALL_CUSTOM_LAUNCHER_ALIBI(T, KVT, KV_DTYPE, BLK_SIZE, HEAD_SIZE,    \\",
      "                                 uint8_t, 256);                            \\",
      "    } else {                                                               \\",
      "      CALL_CUSTOM_LAUNCHER_ALIBI(T, KVT, KV_DTYPE, BLK_SIZE, HEAD_SIZE, T, \\",
      "                                 256);                                     \\",
      "    }",
      "#endif",
      "",
      "#define CALL_CUSTOM_LAUNCHER_BLK(T, KVT, KV_DTYPE, HEAD_SIZE)     \\",
      "  switch (block_size) {                                           \\",
      "    case 16:                                                      \\",
      "      CALL_CUSTOM_LAUNCHER_OUT(T, KVT, KV_DTYPE, 16, HEAD_SIZE);  \\",
      "      break;                                                      \\",
      "    case 32:                                                      \\",
      "      CALL_CUSTOM_LAUNCHER_OUT(T, KVT, KV_DTYPE, 32, HEAD_SIZE);  \\",
      "      break;                                                      \\",
      "    default:                                                      \\",
      "      TORCH_CHECK(false, \"Unsupported block size: \", block_size); \\",
      "      break;                                                      \\",
      "  }",
      "",
      "#define CALL_CUSTOM_LAUNCHER_BLK_HEAD(T, KVT, KV_DTYPE)         \\",
      "  switch (head_size) {                                          \\",
      "    case 64:                                                    \\",
      "      CALL_CUSTOM_LAUNCHER_BLK(T, KVT, KV_DTYPE, 64);           \\",
      "      break;                                                    \\",
      "    case 128:                                                   \\",
      "      CALL_CUSTOM_LAUNCHER_BLK(T, KVT, KV_DTYPE, 128);          \\",
      "      break;                                                    \\",
      "    default:                                                    \\",
      "      TORCH_CHECK(false, \"Unsupported head size: \", head_size); \\",
      "      break;                                                    \\",
      "  }",
      "",
      "bool is_navi_gpu() {",
      "  static bool is_cached = false;",
      "  static bool result;",
      "",
      "  if (!is_cached) {",
      "    int device_id;",
      "    hipDeviceProp_t deviceProp;",
      "    hipGetDevice(&device_id);",
      "    hipGetDeviceProperties(&deviceProp, device_id);",
      "",
      "    std::string arch = deviceProp.gcnArchName;",
      "    result = arch.find(\"gfx11\") == 0 || arch.find(\"gfx12\") == 0;",
      "    is_cached = true;",
      "  }",
      "",
      "  return result;",
      "}",
      "",
      "// clang-format off",
      "void paged_attention(",
      "    torch::Tensor& out,         // [num_seqs, num_heads, head_size]",
      "    torch::Tensor& exp_sums,    // [num_seqs, num_heads, max_num_partitions]",
      "    torch::Tensor& max_logits,  // [num_seqs, num_heads, max_num_partitions]",
      "    torch::Tensor& tmp_out,     // [num_seqs, num_heads, max_num_partitions, head_size]",
      "    torch::Tensor& query,       // [num_seqs, num_heads, head_size]",
      "    torch::Tensor& key_cache,   // [num_blocks, num_heads, head_size/x, block_size, x]",
      "    torch::Tensor& value_cache, // [num_blocks, num_heads, head_size, block_size]",
      "    int64_t num_kv_heads, ",
      "    double scale,",
      "    torch::Tensor& block_tables, // [num_seqs, max_num_blocks_per_seq]",
      "    torch::Tensor& seq_lens, // [num_seqs]",
      "    const std::optional<torch::Tensor>& query_start_loc, // [num_seqs]",
      "    int64_t block_size, int64_t max_seq_len,",
      "    const std::optional<torch::Tensor>& alibi_slopes,",
      "    const std::string& kv_cache_dtype, torch::Tensor& k_scale,",
      "    torch::Tensor& v_scale,",
      "    const std::optional<torch::Tensor>& fp8_out_scale) {",
      "  // clang-format on",
      "  bool is_navi = is_navi_gpu();",
      "",
      "  const int head_size = query.size(2);",
      "  if (kv_cache_dtype == \"auto\") {",
      "    if (query.dtype() == at::ScalarType::Half) {",
      "      CALL_CUSTOM_LAUNCHER_BLK_HEAD(_Float16, _Float16,",
      "                                    vllm::Fp8KVCacheDataType::kAuto);",
      "    } else if (query.dtype() == at::ScalarType::BFloat16) {",
      "      CALL_CUSTOM_LAUNCHER_BLK_HEAD(__hip_bfloat16, __hip_bfloat16,",
      "                                    vllm::Fp8KVCacheDataType::kAuto);",
      "    } else {",
      "      TORCH_CHECK(false, \"Unsupported data type: \", query.dtype());",
      "    }",
      "  } else if (kv_cache_dtype == \"fp8\" || kv_cache_dtype == \"fp8_e4m3\") {",
      "    if (query.dtype() == at::ScalarType::Half) {",
      "      CALL_CUSTOM_LAUNCHER_BLK_HEAD(_Float16, uint8_t,",
      "                                    vllm::Fp8KVCacheDataType::kFp8E4M3);",
      "    } else if (query.dtype() == at::ScalarType::BFloat16) {",
      "      CALL_CUSTOM_LAUNCHER_BLK_HEAD(__hip_bfloat16, uint8_t,",
      "                                    vllm::Fp8KVCacheDataType::kFp8E4M3);",
      "    } else {",
      "      TORCH_CHECK(false, \"Unsupported data type: \", query.dtype());",
      "    }",
      "  } else {",
      "    TORCH_CHECK(false, \"Unsupported KV cache dtype: \", kv_cache_dtype);",
      "  }",
      "}",
      "",
      "#undef WARP_SIZE",
      "#undef MAX",
      "#undef MIN",
      "#undef DIVIDE_ROUND_UP"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/rocm/skinny_gemms.cu",
    "source": [
      "#include <torch/all.h>",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#include <cuda_runtime.h>",
      "#include <cuda_fp16.h>",
      "#include <cuda_bf16.h>",
      "",
      "#include <stdexcept>",
      "#include <algorithm>",
      "",
      "#include \"../cuda_compat.h\"",
      "#include \"dispatch_utils.h\"",
      "#include \"quantization/fp8/common.cuh\"",
      "",
      "#if defined(__HIPCC__) && \\",
      "    (defined(__gfx90a__) || defined(__gfx942__) || defined(__gfx950__))",
      "  #define __HIP__GFX9__",
      "#endif",
      "",
      "#if defined(__HIPCC__) && (defined(__gfx942__) || defined(__gfx950__))",
      "  #define __HIP__MI3XX__",
      "#endif",
      "",
      "#if defined(__gfx950__)",
      "  #define LDS_SIZE 160 * 1024",
      "#else",
      "  #define LDS_SIZE 64 * 1024",
      "#endif",
      "",
      "int get_lds_size() {",
      "  static bool is_cached = false;",
      "  static int result;",
      "  if (is_cached == false) {",
      "    auto dprops = at::cuda::getCurrentDeviceProperties();",
      "    std::string device_arch = dprops->gcnArchName;",
      "    size_t substring = device_arch.find(\"gfx95\");",
      "    result = (substring == std::string::npos ? 64 * 1024 : 160 * 1024);",
      "    is_cached = true;",
      "  }",
      "  return result;",
      "}",
      "",
      "#if defined(NDEBUG)",
      "  #undef NDEBUG",
      "  #include <assert.h>",
      "  #define UNREACHABLE_CODE assert(false);",
      "  #define NDEBUG",
      "#else",
      "  #define UNREACHABLE_CODE assert(false);",
      "#endif",
      "",
      "template <typename T>",
      "struct scalar {};",
      "",
      "template <typename T>",
      "struct scalar2 {};",
      "",
      "template <typename T>",
      "__device__ __forceinline__ float2 __s22float2(T v);",
      "",
      "template <typename T>",
      "__device__ __forceinline__ T __float2s(float v);",
      "",
      "template <typename T>",
      "__device__ __forceinline__ T __float22s2_rn(float2 v);",
      "",
      "// Definitions and cvt functions for fp16",
      "template <>",
      "struct scalar<c10::Half> {",
      "  using type = half;",
      "};",
      "",
      "template <>",
      "struct scalar2<c10::Half> {",
      "  using type = __half2;",
      "};",
      "",
      "template <>",
      "__device__ __forceinline__ half __float2s(float v) {",
      "  return __float2half(v);",
      "}",
      "",
      "template <>",
      "__device__ __forceinline__ float2 __s22float2(__half2 v) {",
      "  return __half22float2(v);",
      "}",
      "",
      "template <>",
      "__device__ __forceinline__ __half2 __float22s2_rn(float2 v) {",
      "  return __float22half2_rn(v);",
      "}",
      "",
      "// Definitions and cvt functions for bf16",
      "template <>",
      "struct scalar<c10::BFloat16> {",
      "  using type = __hip_bfloat16;",
      "};",
      "",
      "template <>",
      "struct scalar2<c10::BFloat16> {",
      "  using type = __hip_bfloat162;",
      "};",
      "",
      "template <>",
      "__device__ __forceinline__ __hip_bfloat16 __float2s(float v) {",
      "  return __float2bfloat16(v);",
      "}",
      "",
      "template <>",
      "__device__ __forceinline__ float2 __s22float2(__hip_bfloat162 v) {",
      "  return __bfloat1622float2(v);",
      "}",
      "",
      "template <>",
      "__device__ __forceinline__ __hip_bfloat162 __float22s2_rn(float2 v) {",
      "  return __float22bfloat162_rn(v);",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ T loadnt(T* addr) {",
      "  return __builtin_nontemporal_load(addr);",
      "}",
      "",
      "__device__ __forceinline__ float4 load_ntmprl(const float4* addr) {",
      "  auto addr_alias = reinterpret_cast<const float*>(addr);",
      "  auto dat0 = loadnt(addr_alias);",
      "  auto dat1 = loadnt(addr_alias + 1);",
      "  auto dat2 = loadnt(addr_alias + 2);",
      "  auto dat3 = loadnt(addr_alias + 3);",
      "  return make_float4(dat0, dat1, dat2, dat3);",
      "}",
      "",
      "// TBlock fetches entire rows of A, and entire col of B (K dimension); assume",
      "// N=1 for time being grid is M/A_NUM_ROWS blocks",
      "template <typename scalar_t, int NUM_A_ROWS_PER_BLOCK>",
      "__global__ void LLGemm1_kernel(const scalar_t* in_a, const scalar_t* in_b,",
      "                               scalar_t* out_c, const int K) {",
      "  using scalar2_t = typename scalar2<scalar_t>::type;",
      "  auto af4 = reinterpret_cast<const float4*>(in_a);",
      "  auto bf4 = reinterpret_cast<const scalar2_t*>(in_b);",
      "  auto c = reinterpret_cast<scalar2_t*>(out_c);",
      "  __shared__ float red_smem[NUM_A_ROWS_PER_BLOCK][WARP_SIZE];",
      "  const int row_addr = blockIdx.x * NUM_A_ROWS_PER_BLOCK * K / 8;",
      "  const int threadid = threadIdx.x;",
      "  const int warp = threadIdx.x / WARP_SIZE;",
      "  const int lane = threadIdx.x % WARP_SIZE;",
      "  const int num_warps = blockDim.x / WARP_SIZE;",
      "  const int qwarpid = threadid / 16;",
      "  const int qthreadid = threadid % 16;",
      "  float4 rowA_elem4[NUM_A_ROWS_PER_BLOCK];",
      "  scalar2_t colB_elem4x, colB_elem4y, colB_elem4z, colB_elem4w;",
      "  float acc[NUM_A_ROWS_PER_BLOCK];",
      "  scalar2_t acch2;",
      "  scalar2_t oval;",
      "",
      "  // As we later use warp shuffle operations, we may have more threads in the",
      "  // block than the actual available data, hence the if guard here.",
      "  if (threadid * 8 < K) {",
      "#pragma unroll",
      "    for (int i = 0; i < NUM_A_ROWS_PER_BLOCK; i++) {",
      "      // rowA_elem4[i] holds 8 * half numbers seen as a single float4.",
      "      rowA_elem4[i] = load_ntmprl(&af4[row_addr + threadid + K / 8 * i]);",
      "    }",
      "    colB_elem4x = bf4[threadid * 4 + 0];",
      "    colB_elem4y = bf4[threadid * 4 + 1];",
      "    colB_elem4z = bf4[threadid * 4 + 2];",
      "    colB_elem4w = bf4[threadid * 4 + 3];",
      "  }",
      "",
      "  scalar2_t Af2;",
      "  float2 S;",
      "",
      "  auto Ah2ptr = reinterpret_cast<scalar2_t*>(&rowA_elem4);",
      "  scalar2_t* ah2lptr;",
      "",
      "#pragma unroll",
      "  for (int i = 0; i < NUM_A_ROWS_PER_BLOCK; i++) {",
      "    // Multiply-add on 8 scalar_t.",
      "    ah2lptr = Ah2ptr + i * 4;",
      "    Af2 = *(ah2lptr);",
      "    acch2 = __hmul2(Af2, colB_elem4x);",
      "    Af2 = *(ah2lptr + 1);",
      "    acch2 = __hfma2(Af2, colB_elem4y, acch2);",
      "    Af2 = *(ah2lptr + 2);",
      "    acch2 = __hfma2(Af2, colB_elem4z, acch2);",
      "    Af2 = *(ah2lptr + 3);",
      "    acch2 = __hfma2(Af2, colB_elem4w, acch2);",
      "    S = __s22float2(acch2);",
      "",
      "    // See comment above concerning the if guard.",
      "    acc[i] = (threadid * 8 < K ? S.x + S.y : 0.f);",
      "  }",
      "",
      "// all reduce across warp.",
      "#pragma unroll",
      "  for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {",
      "#pragma unroll",
      "    for (int i = 0; i < NUM_A_ROWS_PER_BLOCK; i++) {",
      "      acc[i] += __shfl_xor(acc[i], mask);",
      "    }",
      "  }",
      "",
      "  // Warp leaders store the data to shared memory.",
      "  if (lane < NUM_A_ROWS_PER_BLOCK) {",
      "    red_smem[lane][warp] = acc[lane];",
      "  }",
      "",
      "  // Make sure the data is in shared memory.",
      "  __syncthreads();",
      "",
      "  if (qwarpid < NUM_A_ROWS_PER_BLOCK) {",
      "    acc[qwarpid] = qthreadid < num_warps ? red_smem[qwarpid][qthreadid] : 0.f;",
      "#pragma unroll",
      "    for (int mask = 16 / 2; mask >= 1; mask /= 2) {",
      "      acc[qwarpid] += __shfl_xor(acc[qwarpid], mask);",
      "    }",
      "    float oval2 = __shfl_xor(acc[qwarpid], 16);",
      "",
      "    if (lane % 32 == 0) {",
      "      oval = __float22s2_rn<scalar2_t>(make_float2(acc[qwarpid], oval2));",
      "      c[blockIdx.x * NUM_A_ROWS_PER_BLOCK / 2 + qwarpid / 2] = oval;",
      "    }",
      "  }",
      "}",
      "",
      "torch::Tensor LLMM1(at::Tensor& in_a, at::Tensor& in_b,",
      "                    const int64_t rows_per_block) {",
      "  auto M = in_a.size(0);",
      "  auto K = in_a.size(1);",
      "  auto N = in_b.size(0);",
      "",
      "  TORCH_CHECK(N == 1, \"Row number of activation tensor must be 1.\");",
      "  TORCH_CHECK(in_a.dtype() == in_b.dtype());",
      "  TORCH_CHECK(in_b.dtype() == torch::kFloat16 ||",
      "              in_b.dtype() == torch::kBFloat16);",
      "",
      "  auto out_c = torch::empty(",
      "      {N, M}, torch::TensorOptions().dtype(in_b.dtype()).device(in_b.device()));",
      "",
      "  // NUM_TREADS need to be a multiple of WARP_SIZE, as we are using warp shuffle",
      "  // operations.",
      "  const int NUM_THREADS =",
      "      max(rows_per_block * 16,",
      "          K * 2 / 16 % WARP_SIZE == 0",
      "              ? K * 2 / 16",
      "              : K * 2 / 16 + (WARP_SIZE - K * 2 / 16 % WARP_SIZE));",
      "",
      "  int NUM_BLOCKS = M / rows_per_block;",
      "",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(in_b));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "",
      "  // call the kernel function...",
      "  AT_DISPATCH_REDUCED_FLOATING_TYPES(in_b.scalar_type(), \"LLGemm1\", [&] {",
      "    auto a_ptr = in_a.data_ptr<scalar_t>();",
      "    auto b_ptr = in_b.data_ptr<scalar_t>();",
      "    auto c_ptr = out_c.data_ptr<scalar_t>();",
      "    if (rows_per_block == 2) {",
      "      LLGemm1_kernel<scalar_t, 2>",
      "          <<<NUM_BLOCKS, NUM_THREADS, 0, stream>>>(a_ptr, b_ptr, c_ptr, K);",
      "    } else if (rows_per_block == 4) {",
      "      LLGemm1_kernel<scalar_t, 4>",
      "          <<<NUM_BLOCKS, NUM_THREADS, 0, stream>>>(a_ptr, b_ptr, c_ptr, K);",
      "    } else if (rows_per_block == 8) {",
      "      LLGemm1_kernel<scalar_t, 8>",
      "          <<<NUM_BLOCKS, NUM_THREADS, 0, stream>>>(a_ptr, b_ptr, c_ptr, K);",
      "    } else if (rows_per_block == 16) {",
      "      LLGemm1_kernel<scalar_t, 16>",
      "          <<<NUM_BLOCKS, NUM_THREADS, 0, stream>>>(a_ptr, b_ptr, c_ptr, K);",
      "    } else {",
      "      NUM_BLOCKS = M / 4;",
      "      LLGemm1_kernel<scalar_t, 4>",
      "          <<<NUM_BLOCKS, NUM_THREADS, 0, stream>>>(a_ptr, b_ptr, c_ptr, K);",
      "    }",
      "  });",
      "",
      "  return out_c;",
      "}",
      "",
      "#define DOT2C(V0, V2, V3)                                                     \\",
      "  if constexpr (std::is_same_v<scalar_t, half>) {                             \\",
      "    asm(\"v_dot2c_f32_f16 %0, %2, %3\" : \"=v\"(V0) : \"0\"(V0), \"v\"(V2), \"v\"(V3)); \\",
      "  } else if constexpr (std::is_same_v<scalar_t, __hip_bfloat16>) {            \\",
      "    float2 s = __bfloat1622float2(*((__hip_bfloat162*)(&(V2)))) *             \\",
      "               __bfloat1622float2(*((__hip_bfloat162*)(&(V3))));              \\",
      "    V0 += (s.x + s.y);                                                        \\",
      "  }",
      "",
      "#if defined(__HIP__GFX9__)  // TODO: Add NAVI support",
      "// This version targets cases where A[] fits LDS capacity",
      "template <typename scalar_t, int THRDS, int YTILE, int WvPrGrp, int A_CHUNK,",
      "          int UNRL, int N>",
      "__global__ void __launch_bounds__(WvPrGrp* THRDS)",
      "    wvSplitK_hf_sml_(const int K, const int M, const scalar_t* B,",
      "                     const scalar_t* __restrict__ A, scalar_t* C,",
      "                     const int _WvPrGrp, const int CuCount) {",
      "  constexpr int max_lds_len = LDS_SIZE / 2;",
      "  #if defined(__HIP__MI3XX__)",
      "  constexpr bool use_mfma = (std::is_same_v<scalar_t, __hip_bfloat16>);",
      "  #else",
      "  constexpr bool use_mfma = false;",
      "  #endif",
      "",
      "  using scalar8 =",
      "      __attribute__((__vector_size__((A_CHUNK / 2) * sizeof(float)))) float;",
      "  using half4 =",
      "      __attribute__((__vector_size__((A_CHUNK / 2) * sizeof(__bf16)))) __bf16;",
      "  union bigType {",
      "    scalar_t h[A_CHUNK];",
      "    float f[A_CHUNK / 2];",
      "    float2 f2[A_CHUNK / 4];",
      "    double d[A_CHUNK / 4];",
      "    half4 h4[A_CHUNK / 4];",
      "    scalar8 h8;",
      "  };",
      "",
      "  //----------------------------------------------------",
      "  // Reserving 64/160 KB of LDS to have 1 WG / CU",
      "  // Goal is to bring the activation matrix A to the LDS",
      "  // and use it across the lifetime of the work group",
      "  // TODO: When activation matrix is larger than 64 KB",
      "  //\t     then this is not going to work!",
      "  //----------------------------------------------------",
      "  __shared__ scalar_t s[max_lds_len];",
      "",
      "  //----------------------------------------------------",
      "  // Fetch the activation matrix to LDS",
      "  // Loop iteration:",
      "  // - Each thread (lane) is fetching 8 elements (A_Chunk)",
      "  // - Each wave will fetch 64*8=> 512 elements",
      "  // - Each WG will fetch 512 * 16 => 8K elements",
      "  // - Then the WG will move to another 8 K elements",
      "  // TODO: Logic below will only work when K is multiple of 8",
      "  //----------------------------------------------------",
      "  for (uint32_t k = 0; k < min(K * N, max_lds_len);",
      "       k += THRDS * WvPrGrp * A_CHUNK) {",
      "    uint32_t k_in = k + ((threadIdx.y * THRDS + threadIdx.x) * A_CHUNK);",
      "",
      "    if (k_in >= min(K * N, max_lds_len)) break;",
      "",
      "    *((bigType*)(&s[k_in])) = *((bigType*)(&A[k_in]));",
      "  }",
      "  __syncthreads();",
      "",
      "  if (threadIdx.y >= _WvPrGrp) return;",
      "",
      "  uint32_t m = (blockIdx.x * _WvPrGrp + (threadIdx.y % _WvPrGrp)) * YTILE;",
      "",
      "  float sum[N][YTILE];",
      "  scalar8 sum4[N][YTILE];",
      "",
      "  //----------------------------------------------------",
      "  // Each wave works on a single column of weight matrix.",
      "  // There are 16 waves per WG, and hence, each WG is",
      "  // working on 16 columns of weight matrix. Moreover,",
      "  // we tile in column direction by YTILE, so when YTILE=1",
      "  // the above math is right, however, when YTILE=2 then",
      "  // each wave  will be working on 2 columns and WG will",
      "  // be working on 32 columns.",
      "  //",
      "  // Top level loop that makes WGs persistent!",
      "  // - WGs iterates across columns of weight matrix",
      "  // - Each wave within WG works on a given column(s)",
      "  // - After completing first set of columns, WGs start",
      "  //   working on the next set of available columns",
      "  //----------------------------------------------------",
      "  while (m < M) {",
      "    //----------------------------------------------------",
      "    // 'sum' accumulates the matrix A x B computation",
      "    // split across 64 lanes.",
      "    //",
      "    // YTILE represents how many column of weight matrix",
      "    // are being worked on by each wave.",
      "    //----------------------------------------------------",
      "    for (int i = 0; i < YTILE; i++)",
      "      for (int n = 0; n < N; n++)",
      "        if constexpr (!use_mfma)",
      "          sum[n][i] = 0;",
      "        else",
      "          sum4[n][i] = {0, 0, 0, 0};",
      "",
      "    bigType bigA[N][UNRL];",
      "    bigType bigB[YTILE][UNRL];",
      "    //----------------------------------------------------",
      "    // Fetch weight matrix B in interleaved K-split!",
      "    // - Each thread (lane) is fetching 8 elements (A_Chunk)",
      "    // - Each wave will fetch 64*8=> 512 elements (1024B)",
      "    // - YTILE represents the number of column being serviced",
      "    //   by wave",
      "    // - Loop for fetching weight matrix (B) are unrolled",
      "    //",
      "    // Fetch activation matrix A from LDS",
      "    // - Loop for fetching activation matrix (A) are unrolled",
      "    //",
      "    // Finally, do the matrix multiplication in an unrolled",
      "    // fashion. This provides lot of food for compiler",
      "    // scheduling.",
      "    //",
      "    // TODO: Logic below will only work when K is multiple of 8",
      "    //----------------------------------------------------",
      "    // for (uint32_t k1 = 0; k1 < K; k1 += THRDS * A_CHUNK * UNRL) {",
      "    for (uint32_t k1 = 0; k1 < K; k1 += THRDS * A_CHUNK * UNRL) {",
      "      // Fetch the weight matrix from memory!",
      "  #pragma unroll",
      "      for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "        uint32_t k = k1 + k2 * THRDS * A_CHUNK;",
      "        uint32_t k_ = k + threadIdx.x * A_CHUNK;",
      "        if (k_ >= K) break;",
      "",
      "        const scalar_t* B_ = &B[(m + 0) * K + k_];",
      "        for (int y = 0; y < YTILE; y++)",
      "          bigB[y][k2].h8 = (loadnt((scalar8*)(&B_[y * K])));",
      "      }",
      "",
      "      // Fetch activation matrix from either just LDS or from both LDS / memory",
      "  #pragma unroll",
      "      for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "        uint32_t k = k1 + k2 * THRDS * A_CHUNK;",
      "        uint32_t k_ = k + threadIdx.x * A_CHUNK;",
      "        if (k_ >= K) break;",
      "",
      "        // Fetch A activation matrix in interleaved fashion from LDS or memory",
      "",
      "        for (int n = 0; n < N; n++) {",
      "          bigA[n][k2] = *((const bigType*)(&(s[k_ + K * n])));",
      "        }",
      "      }",
      "",
      "      // Do the matrix multiplication in interleaved manner",
      "  #pragma unroll",
      "      for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "        uint32_t k = k1 + k2 * THRDS * A_CHUNK;",
      "        uint32_t k_ = k + threadIdx.x * A_CHUNK;",
      "        if (k_ >= K) break;",
      "        // Do the matrix multiplication of activation and weight matrix",
      "        // - Remember the accumulation is happening for K-split of 64!",
      "  #pragma unroll",
      "        for (uint32_t n = 0; n < N; n++) {",
      "  #pragma unroll",
      "          for (int y = 0; y < YTILE; y++) {",
      "            if constexpr (!use_mfma)",
      "  #pragma unroll",
      "              for (uint32_t b = 0; b < A_CHUNK / 2; b++) {",
      "                DOT2C(sum[n][y], bigA[n][k2].f[b], bigB[y][k2].f[b])",
      "              }",
      "            else",
      "  #pragma unroll",
      "              for (uint32_t b = 0; b < A_CHUNK / 4; b++)",
      "                sum4[n][y] = __builtin_amdgcn_mfma_f32_4x4x4bf16_1k(",
      "                    bigA[n][k2].h4[b], bigB[y][k2].h4[b], sum4[n][y], 0, 0, 0);",
      "          }",
      "        }",
      "      }",
      "    }",
      "",
      "    //----------------------------------------------------",
      "    // Final reduction step using shuffle",
      "    //----------------------------------------------------",
      "    if constexpr (!use_mfma) {",
      "      for (int n = 0; n < N; n++) {",
      "        for (int y = 0; y < YTILE; y++) {",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shr:8 bound_ctrl:0 \"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shr:4 bound_ctrl:0 \"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shr:2 bound_ctrl:0 \"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 wave_shr:1 bound_ctrl:0\"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_bcast:15 bound_ctrl:0\"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_bcast:31 bound_ctrl:0\"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "        }",
      "      }",
      "",
      "      if (threadIdx.x == 63) {",
      "        for (int n = 0; n < N; n++) {",
      "          for (int i = 0; i < YTILE; i++) {",
      "            // if (commitColumn[i]) C[m + i + n * M] = __float2half(sum[n][i]);",
      "            C[m + i + n * M] = __float2s<scalar_t>(sum[n][i]);",
      "          }",
      "        }",
      "      }",
      "    } else {",
      "  #pragma unroll",
      "      for (int n = 0; n < N; n++) {",
      "  #pragma unroll",
      "        for (int y = 0; y < YTILE; y++) {",
      "          // float accm1 = 0;",
      "          // for (int i=0; i<64; i++)",
      "          //    accm1 += __shfl(sum4[n][y][i%4], i);",
      "          float accm = sum4[n][y][0];",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shl:1 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(sum4[n][y][1]), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shl:2 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(sum4[n][y][2]), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shl:3 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(sum4[n][y][3]), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shl:4 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(accm), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shl:8 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(accm), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_mov_b32 %0, %2 row_shr:15 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_bcast:15 bound_ctrl:0\"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(accm), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_bcast:31 bound_ctrl:0\"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(accm), \"v\"(accm));",
      "",
      "          sum4[n][y][0] = accm;",
      "        }",
      "      }",
      "      if (threadIdx.x == 63) {",
      "        for (int n = 0; n < N; n++) {",
      "          for (int i = 0; i < YTILE; i++) {",
      "            // if (commitColumn[i]) C[n + i + m * N] = __float2half(sum[n][i]);",
      "            C[m + i + n * M] = __float2bfloat16(sum4[n][i][0]);",
      "          }",
      "        }",
      "      }",
      "    }",
      "    m += CuCount * _WvPrGrp * YTILE;",
      "  }",
      "}",
      "#else   // !defined(__HIP__GFX9__) TODO: Add NAVI support",
      "template <typename scalar_t, int THRDS, int YTILE, int WvPrGrp, int A_CHUNK,",
      "          int UNRL, int N>",
      "__global__ void wvSplitK_hf_sml_(const int K, const int M, const scalar_t* B,",
      "                                 const scalar_t* __restrict__ A, scalar_t* C,",
      "                                 const int _WvPrGrp, const int CuCount) {",
      "  UNREACHABLE_CODE",
      "}",
      "#endif  // defined(__HIP__GFX9__) TODO: Add NAVI support",
      "",
      "#if defined(__HIP__GFX9__)  // TODO: Add NAVI support",
      "// This version targets cases where A[] marginally exceeds LDS capacity",
      "template <typename scalar_t, int THRDS, int YTILE, int WvPrGrp, int A_CHUNK,",
      "          int UNRL, int N>",
      "__global__ void __launch_bounds__(WvPrGrp* THRDS)",
      "    wvSplitK_hf_(const int K, const int M, const scalar_t* B,",
      "                 const scalar_t* __restrict__ A, scalar_t* C,",
      "                 const int _WvPrGrp, const int CuCount) {",
      "  constexpr int max_lds_len = LDS_SIZE / 2;",
      "  #if defined(__HIP__MI3XX__)",
      "  constexpr bool use_mfma = (std::is_same_v<scalar_t, __hip_bfloat16>);",
      "  #else",
      "  constexpr bool use_mfma = false;",
      "  #endif",
      "",
      "  using scalar8 =",
      "      __attribute__((__vector_size__((A_CHUNK / 2) * sizeof(float)))) float;",
      "  using half4 =",
      "      __attribute__((__vector_size__((A_CHUNK / 2) * sizeof(__bf16)))) __bf16;",
      "  union bigType {",
      "    scalar_t h[A_CHUNK];",
      "    float f[A_CHUNK / 2];",
      "    float2 f2[A_CHUNK / 4];",
      "    double d[A_CHUNK / 4];",
      "    half4 h4[A_CHUNK / 4];",
      "    scalar8 h8;",
      "  };",
      "",
      "  //----------------------------------------------------",
      "  // Reserving 64 KB of LDS to have 1 WG / CU",
      "  // Goal is to bring the activation matrix A to the LDS",
      "  // and use it across the lifetime of the work group",
      "  // TODO: When activation matrix is larger than 64 KB",
      "  //\t     then this is not going to work!",
      "  //----------------------------------------------------",
      "  __shared__ scalar_t s[max_lds_len];",
      "",
      "  //----------------------------------------------------",
      "  // Computation of columns that need to be committed to memory!",
      "  //----------------------------------------------------",
      "  uint32_t commitColumn[YTILE];",
      "  for (uint32_t i = 0; i < YTILE; i++) {",
      "    commitColumn[i] = 1;",
      "  }",
      "",
      "  //----------------------------------------------------",
      "  // Indexing function into the column of weight matrix B",
      "  // Algorithm does 64 lane k-splitting / wave and uses",
      "  // WG ID and Thread ID to find the index.",
      "  //----------------------------------------------------",
      "  // int _WvPrGrp = mindiv(N, CuCount * YTILE, WvPrGrp);",
      "  uint32_t m = (blockIdx.x * _WvPrGrp + threadIdx.y) * YTILE;",
      "",
      "  // Check whether there will be fragmentation!",
      "  // This will happen only for the last wave!",
      "  if (m < M && (m + YTILE) >= M) {",
      "    uint32_t startColumn = M - YTILE;",
      "    for (uint32_t i = 0; i < (m - startColumn); i++) {",
      "      commitColumn[i] = 0;",
      "    }",
      "    m = startColumn;",
      "  }",
      "",
      "  //----------------------------------------------------",
      "  // Fetch the activation matrix to LDS",
      "  // Loop iteration:",
      "  // - Each thread (lane) is fetching 8 elements (A_Chunk)",
      "  // - Each wave will fetch 64*8=> 512 elements",
      "  // - Each WG will fetch 512 * 16 => 8K elements",
      "  // - Then the WG will move to another 8 K elements",
      "  // TODO: Logic below will only work when K is multiple of 8",
      "  //----------------------------------------------------",
      "  for (uint32_t k = 0; k < min(K * N, max_lds_len);",
      "       k += THRDS * WvPrGrp * A_CHUNK) {",
      "    uint32_t k_in = k + ((threadIdx.y * THRDS + threadIdx.x) * A_CHUNK);",
      "",
      "    if (k_in >= min(K * N, max_lds_len)) break;",
      "",
      "    *((bigType*)(&s[k_in])) = *((bigType*)(&A[k_in]));",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  if (threadIdx.y >= _WvPrGrp) return;",
      "",
      "  float sum[N][YTILE];",
      "  scalar8 sum4[N][YTILE];",
      "",
      "  //----------------------------------------------------",
      "  // Each wave works on a single column of weight matrix.",
      "  // There are 16 waves per WG, and hence, each WG is",
      "  // working on 16 columns of weight matrix. Moreover,",
      "  // we tile in column direction by YTILE, so when YTILE=1",
      "  // the above math is right, however, when YTILE=2 then",
      "  // each wave  will be working on 2 columns and WG will",
      "  // be working on 32 columns.",
      "  //",
      "  // Top level loop that makes WGs persistent!",
      "  // - WGs iterates across columns of weight matrix",
      "  // - Each wave within WG works on a given column(s)",
      "  // - After completing first set of columns, WGs start",
      "  //   working on the next set of available columns",
      "  //----------------------------------------------------",
      "  while (m < M) {",
      "    //----------------------------------------------------",
      "    // 'sum' accumulates the matrix A x B computation",
      "    // split across 64 lanes.",
      "    //",
      "    // YTILE represents how many column of weight matrix",
      "    // are being worked on by each wave.",
      "    //----------------------------------------------------",
      "    for (int i = 0; i < YTILE; i++)",
      "      for (int n = 0; n < N; n++)",
      "        if constexpr (!use_mfma)",
      "          sum[n][i] = 0;",
      "        else",
      "          sum4[n][i] = {0, 0, 0, 0};",
      "",
      "    bigType bigA[N][UNRL];",
      "    bigType bigB[YTILE][UNRL];",
      "    //----------------------------------------------------",
      "    // Fetch weight matrix B in interleaved K-split!",
      "    // - Each thread (lane) is fetching 8 elements (A_Chunk)",
      "    // - Each wave will fetch 64*8=> 512 elements (1024B)",
      "    // - YTILE represents the number of column being serviced",
      "    //   by wave",
      "    // - Loop for fetching weight matrix (B) are unrolled",
      "    //",
      "    // Fetch activation matrix A from LDS",
      "    // - Loop for fetching activation matrix (A) are unrolled",
      "    //",
      "    // Finally, do the matrix multiplication in an unrolled",
      "    // fashion. This provides lot of food for compiler",
      "    // scheduling.",
      "    //",
      "    // TODO: Logic below will only work when K is multiple of 8",
      "    //----------------------------------------------------",
      "    for (uint32_t k1 = 0; k1 < K; k1 += THRDS * A_CHUNK * UNRL) {",
      "      // Fetch the weight matrix from memory!",
      "  #pragma unroll",
      "      for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "        uint32_t k = k1 + k2 * THRDS * A_CHUNK;",
      "        uint32_t k_ = k + threadIdx.x * A_CHUNK;",
      "        if (k_ >= K) break;",
      "",
      "        const scalar_t* B_ = &B[(m + 0) * K + k_];",
      "        for (int b = 0; b < YTILE; b++)",
      "          bigB[b][k2].h8 = (loadnt((scalar8*)(&B_[b * K])));",
      "      }",
      "",
      "      // Fetch activation matrix from either just LDS or from both LDS / memory",
      "  #pragma unroll",
      "      for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "        uint32_t k = k1 + k2 * THRDS * A_CHUNK;",
      "        uint32_t k_ = k + threadIdx.x * A_CHUNK;",
      "        if (k_ >= K) break;",
      "",
      "        // Fetch A activation matrix in interleaved fashion from LDS or memory",
      "",
      "        for (int n = 0; n < N; n++) {",
      "          if (k_ + K * n < max_lds_len)",
      "            bigA[n][k2] = *((const bigType*)(&(s[k_ + K * n])));",
      "          else",
      "            bigA[n][k2] = *((const bigType*)(&(A[k_ + K * n])));",
      "        }",
      "      }",
      "",
      "      // Do the matrix multiplication in interleaved manner",
      "  #pragma unroll",
      "      for (uint32_t n = 0; n < N; n++) {",
      "  #pragma unroll",
      "        for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "          uint32_t k = k1 + k2 * THRDS * A_CHUNK;",
      "          uint32_t k_ = k + threadIdx.x * A_CHUNK;",
      "          if (k_ >= K) break;",
      "          // Do the matrix multiplication of activation and weight matrix",
      "          // - Remember the accumulation is happening for K-split of 64!",
      "  #pragma unroll",
      "          for (int y = 0; y < YTILE; y++) {",
      "            if constexpr (!use_mfma)",
      "  #pragma unroll",
      "              for (uint32_t b = 0; b < A_CHUNK / 2; b++) {",
      "                DOT2C(sum[n][y], bigA[n][k2].f[b], bigB[y][k2].f[b])",
      "              }",
      "            else",
      "  #pragma unroll",
      "              for (uint32_t b = 0; b < A_CHUNK / 4; b++)",
      "                sum4[n][y] = __builtin_amdgcn_mfma_f32_4x4x4bf16_1k(",
      "                    bigA[n][k2].h4[b], bigB[y][k2].h4[b], sum4[n][y], 0, 0, 0);",
      "          }",
      "        }",
      "      }",
      "    }",
      "",
      "    //----------------------------------------------------",
      "    // Final reduction step using shuffle",
      "    //----------------------------------------------------",
      "    if constexpr (!use_mfma) {",
      "      for (int n = 0; n < N; n++) {",
      "        for (int y = 0; y < YTILE; y++) {",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shr:8 bound_ctrl:0 \"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shr:4 bound_ctrl:0 \"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shr:2 bound_ctrl:0 \"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 wave_shr:1 bound_ctrl:0\"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_bcast:15 bound_ctrl:0\"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_bcast:31 bound_ctrl:0\"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "        }",
      "      }",
      "",
      "      if (threadIdx.x == 63) {",
      "        for (int n = 0; n < N; n++) {",
      "          for (int i = 0; i < YTILE; i++) {",
      "            if (commitColumn[i])",
      "              C[m + i + n * M] = __float2s<scalar_t>(sum[n][i]);",
      "          }",
      "        }",
      "      }",
      "    } else {",
      "  #pragma unroll",
      "      for (int n = 0; n < N; n++) {",
      "  #pragma unroll",
      "        for (int y = 0; y < YTILE; y++) {",
      "          // float accm1 = 0;",
      "          // for (int i=0; i<64; i++)",
      "          //    accm1 += __shfl(sum4[n][y][i%4], i);",
      "",
      "          float accm = sum4[n][y][0];",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shl:1 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(sum4[n][y][1]), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shl:2 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(sum4[n][y][2]), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shl:3 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(sum4[n][y][3]), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shl:4 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(accm), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shl:8 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(accm), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_mov_b32 %0, %2 row_shr:15 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_bcast:15 bound_ctrl:0\"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(accm), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_bcast:31 bound_ctrl:0\"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(accm), \"v\"(accm));",
      "",
      "          sum4[n][y][0] = accm;",
      "        }",
      "      }",
      "      if (threadIdx.x == 63) {",
      "        for (int n = 0; n < N; n++) {",
      "          for (int i = 0; i < YTILE; i++) {",
      "            // if (commitColumn[i]) C[n + i + m * N] = __float2half(sum[n][i]);",
      "            C[m + i + n * M] = __float2bfloat16(sum4[n][i][0]);",
      "          }",
      "        }",
      "      }",
      "    }",
      "",
      "    m += CuCount * _WvPrGrp * YTILE;",
      "",
      "    // Check whether there will be fragmentation!",
      "    // This will happen only for the last wave!",
      "    if (m < M && (m + YTILE) >= M) {",
      "      uint32_t startColumn = M - YTILE;",
      "      for (uint32_t i = 0; i < (m - startColumn); i++) {",
      "        commitColumn[i] = 0;",
      "      }",
      "      m = startColumn;",
      "    }",
      "  }",
      "}",
      "",
      "#else   // !defined(__HIP__GFX9__) TODO: Add NAVI support",
      "template <typename scalar_t, int THRDS, int YTILE, int WvPrGrp, int A_CHUNK,",
      "          int UNRL, int N>",
      "__global__ void wvSplitK_hf_(const int K, const int M, const scalar_t* B,",
      "                             const scalar_t* __restrict__ A, scalar_t* C,",
      "                             const int _WvPrGrp, const int CuCount) {",
      "  UNREACHABLE_CODE",
      "}",
      "#endif  // defined(__HIP__GFX9__) TODO: Add NAVI support",
      "",
      "#if defined(__HIP__GFX9__)  // TODO: Add NAVI support",
      "// This version targets big A[] cases, where it is much larger than LDS capacity",
      "template <typename scalar_t, int THRDS, int YTILE, int WvPrGrp, int A_CHUNK,",
      "          int UNRL, int N>",
      "__global__ void __launch_bounds__(WvPrGrp* THRDS)",
      "    wvSplitK_hf_big_(const int K, const int M, const scalar_t* B,",
      "                     const scalar_t* __restrict__ A, scalar_t* C,",
      "                     const int _WvPrGrp, const int CuCount) {",
      "  constexpr int max_lds_len = LDS_SIZE / 2;",
      "  #if defined(__HIP__MI3XX__)",
      "  constexpr bool use_mfma = (std::is_same_v<scalar_t, __hip_bfloat16>);",
      "  #else",
      "  constexpr bool use_mfma = false;",
      "  #endif",
      "",
      "  using scalar8 =",
      "      __attribute__((__vector_size__((A_CHUNK / 2) * sizeof(float)))) float;",
      "  using half4 =",
      "      __attribute__((__vector_size__((A_CHUNK / 2) * sizeof(__bf16)))) __bf16;",
      "  union bigType {",
      "    scalar_t h[A_CHUNK];",
      "    float f[A_CHUNK / 2];",
      "    float2 f2[A_CHUNK / 4];",
      "    double d[A_CHUNK / 4];",
      "    half4 h4[A_CHUNK / 4];",
      "    scalar8 h8;",
      "  };",
      "",
      "  //----------------------------------------------------",
      "  // Reserving 64/160 KB of LDS to have 1 WG / CU",
      "  // Goal is to bring the activation matrix A to the LDS",
      "  // and use it across the lifetime of the work group",
      "  // TODO: When activation matrix is larger than 64 KB",
      "  //\t     then this is not going to work!",
      "  //----------------------------------------------------",
      "  __shared__ scalar_t s[max_lds_len];",
      "",
      "  //----------------------------------------------------",
      "  // Computation of columns that need to be committed to memory!",
      "  //----------------------------------------------------",
      "  uint32_t commitColumn[YTILE];",
      "  for (uint32_t i = 0; i < YTILE; i++) {",
      "    commitColumn[i] = 1;",
      "  }",
      "",
      "  // int _WvPrGrp = mindiv(N, CuCount * YTILE, WvPrGrp);",
      "  if (threadIdx.y >= _WvPrGrp) return;",
      "",
      "  //----------------------------------------------------",
      "  // Indexing function into the column of weight matrix B",
      "  // Algorithm does 64 lane k-splitting / wave and uses",
      "  // WG ID and Thread ID to find the index.",
      "  //----------------------------------------------------",
      "  uint32_t m = (blockIdx.x * _WvPrGrp + threadIdx.y) * YTILE;",
      "",
      "  // Check whether there will be fragmentation!",
      "  // This will happen only for the last wave!",
      "  if (m < M && (m + YTILE) >= M) {",
      "    uint32_t startColumn = M - YTILE;",
      "    for (uint32_t i = 0; i < (m - startColumn); i++) {",
      "      commitColumn[i] = 0;",
      "    }",
      "    m = startColumn;",
      "  }",
      "",
      "  //----------------------------------------------------",
      "  // Fetch the activation matrix to LDS",
      "  // Loop iteration:",
      "  // - Each thread (lane) is fetching 8 elements (A_Chunk)",
      "  // - Each wave will fetch 64*8=> 512 elements",
      "  // - Each WG will fetch 512 * 16 => 8K elements",
      "  // - Then the WG will move to another 8 K elements",
      "  // TODO: Logic below will only work when K is multiple of 8",
      "  //----------------------------------------------------",
      "  #define PCML",
      "  #ifndef PCML",
      "  for (uint32_t k = 0; k < min(K * N, max_lds_len);",
      "       k += THRDS * WvPrGrp * A_CHUNK) {",
      "    uint32_t k_in = k + ((threadIdx.y * THRDS + threadIdx.x) * A_CHUNK);",
      "",
      "    if (k_in >= min(K * N, max_lds_len)) break;",
      "",
      "    *((bigType*)(&s[k_in])) = *((bigType*)(&A[k_in]));",
      "  }",
      "  __syncthreads();",
      "  #endif",
      "",
      "  #define TUC (THRDS * UNRL * A_CHUNK)",
      "  uint32_t kBase = 0;",
      "  // find biggest k size that fits in LDS",
      "  uint32_t kFit = (max_lds_len) / N;",
      "  // kFit = (kFit%TWC==0) ? kFit : (kFit-kFit%TWC+TWC); //round up to multiple",
      "  // of TUC",
      "  kFit = (kFit % TUC == 0)",
      "             ? kFit",
      "             : (kFit - kFit % TUC);  // round up to multiple of TUC",
      "  // if (kFit == 0) kFit = TUC;",
      "  kFit = min(kFit, K);",
      "",
      "  float sum[N][YTILE];",
      "  scalar8 sum4[N][YTILE];",
      "",
      "  //----------------------------------------------------",
      "  // Each wave works on a single column of weight matrix.",
      "  // There are 16 waves per WG, and hence, each WG is",
      "  // working on 16 columns of weight matrix. Moreover,",
      "  // we tile in column direction by YTILE, so when YTILE=1",
      "  // the above math is right, however, when YTILE=2 then",
      "  // each wave  will be working on 2 columns and WG will",
      "  // be working on 32 columns.",
      "  //",
      "  // Top level loop that makes WGs persistent!",
      "  // - WGs iterates across columns of weight matrix",
      "  // - Each wave within WG works on a given column(s)",
      "  // - After completing first set of columns, WGs start",
      "  //   working on the next set of available columns",
      "  //----------------------------------------------------",
      "  #ifdef PCML",
      "  int YW = (YTILE * _WvPrGrp);",
      "  uint32_t Mrndp = (M % YW == 0) ? M : (M - M % YW + YW);",
      "  while (m < Mrndp) {",
      "  #else",
      "  while (m < M) {",
      "  #endif",
      "    //----------------------------------------------------",
      "    // 'sum' accumulates the matrix A x B computation",
      "    // split across 64 lanes.",
      "    //",
      "    // YTILE represents how many column of weight matrix",
      "    // are being worked on by each wave.",
      "    //----------------------------------------------------",
      "    for (int i = 0; i < YTILE; i++)",
      "      for (int n = 0; n < N; n++)",
      "        if constexpr (!use_mfma)",
      "          sum[n][i] = 0;",
      "        else",
      "          sum4[n][i] = {0, 0, 0, 0};",
      "",
      "    bigType bigA[N][UNRL];",
      "    bigType bigB[YTILE][UNRL];",
      "    //----------------------------------------------------",
      "    // Fetch weight matrix B in interleaved K-split!",
      "    // - Each thread (lane) is fetching 8 elements (A_Chunk)",
      "    // - Each wave will fetch 64*8=> 512 elements (1024B)",
      "    // - YTILE represents the number of column being serviced",
      "    //   by wave",
      "    // - Loop for fetching weight matrix (B) are unrolled",
      "    //",
      "    // Fetch activation matrix A from LDS",
      "    // - Loop for fetching activation matrix (A) are unrolled",
      "    //",
      "    // Finally, do the matrix multiplication in an unrolled",
      "    // fashion. This provides lot of food for compiler",
      "    // scheduling.",
      "    //",
      "    // TODO: Logic below will only work when K is multiple of 8",
      "    //----------------------------------------------------",
      "    for (uint32_t k1 = 0; k1 < K; k1 += THRDS * A_CHUNK * UNRL) {",
      "  #ifdef PCML",
      "      if ((k1 == 0) || (k1 == kBase + kFit)) {  // load next chunk of A[] to LDS",
      "        if (k1 != 0) kBase += kFit;",
      "        __syncthreads();",
      "        for (uint32_t k = 0; k < kFit; k += THRDS * _WvPrGrp * A_CHUNK) {",
      "          uint32_t kOff = k + ((threadIdx.y * THRDS + threadIdx.x) * A_CHUNK);",
      "          if (kBase + kOff >= K) break;",
      "          if (kOff >= kFit) break;",
      "          for (uint32_t n = 0; n < N; n++) {",
      "            uint32_t k_in = kBase + n * K + kOff;",
      "            uint32_t k_ot = n * kFit + kOff;",
      "            *((bigType*)(&s[k_ot])) = *((bigType*)(&A[k_in]));",
      "          }",
      "        }",
      "        __syncthreads();",
      "      }",
      "      if (m >= M) continue;",
      "  #endif",
      "",
      "      // Fetch the weight matrix from memory!",
      "  #pragma unroll",
      "      for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "        uint32_t k = k1 + k2 * THRDS * A_CHUNK;",
      "        uint32_t k_ = k + threadIdx.x * A_CHUNK;",
      "        if (k_ >= K) break;",
      "",
      "        const scalar_t* B_ = &B[(m + 0) * K + k_];",
      "        for (int b = 0; b < YTILE; b++)",
      "          bigB[b][k2].h8 = (loadnt((scalar8*)(&B_[b * K])));",
      "      }",
      "",
      "      // Fetch activation matrix from either just LDS or from both LDS / memory",
      "  #pragma unroll",
      "      for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "        uint32_t k = k1 + k2 * THRDS * A_CHUNK;",
      "        uint32_t k_ = k + threadIdx.x * A_CHUNK;",
      "        if (k_ >= K) break;",
      "",
      "        // Fetch A activation matrix in interleaved fashion from LDS or memory",
      "",
      "        for (int n = 0; n < N; n++) {",
      "  #ifdef PCML",
      "          bigA[n][k2] = *((const bigType*)(&(s[k_ - kBase + kFit * n])));",
      "  #else",
      "          if (k_ + K * n < 32 * 1024)",
      "            bigA[n][k2] = *((const bigType*)(&(s[k_ + K * n])));",
      "          else",
      "            bigA[n][k2] = *((const bigType*)(&(A[k_ + K * n])));",
      "  #endif",
      "        }",
      "      }",
      "",
      "      // Do the matrix multiplication in interleaved manner",
      "  #pragma unroll",
      "      for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "        uint32_t k = k1 + k2 * THRDS * A_CHUNK;",
      "        uint32_t k_ = k + threadIdx.x * A_CHUNK;",
      "        if (k_ >= K) break;",
      "  #pragma unroll",
      "        for (uint32_t n = 0; n < N; n++) {",
      "          // Do the matrix multiplication of activation and weight matrix",
      "          // - Remember the accumulation is happening for K-split of 64!",
      "  #pragma unroll",
      "          for (int y = 0; y < YTILE; y++) {",
      "            if constexpr (!use_mfma)",
      "  #pragma unroll",
      "              for (uint32_t b = 0; b < A_CHUNK / 2; b++) {",
      "                DOT2C(sum[n][y], bigA[n][k2].f[b], bigB[y][k2].f[b])",
      "              }",
      "            else",
      "  #pragma unroll",
      "              for (uint32_t b = 0; b < A_CHUNK / 4; b++)",
      "                sum4[n][y] = __builtin_amdgcn_mfma_f32_4x4x4bf16_1k(",
      "                    bigA[n][k2].h4[b], bigB[y][k2].h4[b], sum4[n][y], 0, 0, 0);",
      "          }",
      "        }",
      "      }",
      "    }",
      "",
      "  #ifdef PCML",
      "    if (m >= M) {",
      "      m += CuCount * _WvPrGrp * YTILE;",
      "      kBase = 0;",
      "      continue;",
      "    }",
      "  #endif",
      "",
      "    //----------------------------------------------------",
      "    // Final reduction step using shuffle",
      "    //----------------------------------------------------",
      "    if constexpr (!use_mfma) {",
      "      for (int n = 0; n < N; n++) {",
      "        for (int y = 0; y < YTILE; y++) {",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shr:8 bound_ctrl:0 \"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shr:4 bound_ctrl:0 \"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shr:2 bound_ctrl:0 \"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 wave_shr:1 bound_ctrl:0\"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_bcast:15 bound_ctrl:0\"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_bcast:31 bound_ctrl:0\"",
      "              : \"=v\"(sum[n][y])",
      "              : \"0\"(sum[n][y]), \"v\"(sum[n][y]), \"v\"(sum[n][y]));",
      "        }",
      "      }",
      "",
      "      if (threadIdx.x == 63) {",
      "        for (int n = 0; n < N; n++) {",
      "          for (int i = 0; i < YTILE; i++) {",
      "            if (commitColumn[i])",
      "              C[m + i + n * M] = __float2s<scalar_t>(sum[n][i]);",
      "          }",
      "        }",
      "      }",
      "    } else {",
      "  #pragma unroll",
      "      for (int n = 0; n < N; n++) {",
      "  #pragma unroll",
      "        for (int y = 0; y < YTILE; y++) {",
      "          float accm = sum4[n][y][0];",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shl:1 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(sum4[n][y][1]), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shl:2 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(sum4[n][y][2]), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shl:3 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(sum4[n][y][3]), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shl:4 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(accm), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_shl:8 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(accm), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_mov_b32 %0, %2 row_shr:15 bound_ctrl:0 \"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_bcast:15 bound_ctrl:0\"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(accm), \"v\"(accm));",
      "          asm(\"s_nop 0\\n\\tv_add_f32 %0, %2, %3 row_bcast:31 bound_ctrl:0\"",
      "              : \"=v\"(accm)",
      "              : \"0\"(accm), \"v\"(accm), \"v\"(accm));",
      "",
      "          sum4[n][y][0] = accm;",
      "        }",
      "      }",
      "      if (threadIdx.x == 63) {",
      "        for (int n = 0; n < N; n++) {",
      "          for (int i = 0; i < YTILE; i++) {",
      "            // if (commitColumn[i]) C[n + i + m * N] = __float2half(sum[n][i]);",
      "            C[m + i + n * M] = __float2bfloat16(sum4[n][i][0]);",
      "          }",
      "        }",
      "      }",
      "    }",
      "",
      "    m += CuCount * _WvPrGrp * YTILE;",
      "    kBase = 0;",
      "",
      "    // Check whether there will be fragmentation!",
      "    // This will happen only for the last wave!",
      "    if (m < M && (m + YTILE) >= M) {",
      "      uint32_t startColumn = M - YTILE;",
      "      for (uint32_t i = 0; i < (m - startColumn); i++) {",
      "        commitColumn[i] = 0;",
      "      }",
      "      m = startColumn;",
      "    }",
      "  }",
      "}",
      "#else   // !defined(__HIP__GFX9__) TODO: Add NAVI support",
      "template <typename scalar_t, int THRDS, int YTILE, int WvPrGrp, int A_CHUNK,",
      "          int UNRL, int N>",
      "__global__ void wvSplitK_hf_big_(const int K, const int M, const scalar_t* B,",
      "                                 const scalar_t* __restrict__ A, scalar_t* C,",
      "                                 const int _WvPrGrp, const int CuCount) {",
      "  UNREACHABLE_CODE",
      "}",
      "#endif  // defined(__HIP__GFX9__) TODO: Add NAVI support",
      "",
      "int mindiv(int N, int div1, int div2) {",
      "  int nPrRnd = div1 * div2;",
      "  int rnds0 = N / nPrRnd;",
      "  nPrRnd -= div1 * 3;",
      "  int rnds3 = N / nPrRnd;",
      "  nPrRnd -= div1;",
      "  int rnds4 = N / nPrRnd;",
      "  nPrRnd -= div1;",
      "  int rnds5 = N / nPrRnd;",
      "  nPrRnd -= div1;",
      "  int rnds6 = N / nPrRnd;",
      "  nPrRnd -= div1;",
      "  int rnds7 = N / nPrRnd;",
      "  nPrRnd -= div1;",
      "  int rnds8 = N / nPrRnd;",
      "  nPrRnd -= div1;",
      "  int rnds9 = N / nPrRnd;",
      "  nPrRnd -= div1;",
      "  int rtn = div2;",
      "  if (rnds0 == rnds3) rtn = div2 - 3;",
      "  if (rnds0 == rnds4) rtn = div2 - 4;",
      "  if (rnds0 == rnds5) rtn = div2 - 5;",
      "  if (rnds0 == rnds6) rtn = div2 - 6;",
      "  if (rnds0 == rnds7) rtn = div2 - 7;",
      "  if (rnds0 == rnds8) rtn = div2 - 8;",
      "  if (rnds0 == rnds9) rtn = div2 - 9;",
      "  return rtn;",
      "}",
      "",
      "torch::Tensor wvSplitK(at::Tensor& in_a, at::Tensor& in_b,",
      "                       const int64_t CuCount) {",
      "  auto M_in = in_a.size(0);",
      "  auto K_in = in_a.size(1);",
      "  auto N_in = in_b.size(0);",
      "",
      "  TORCH_CHECK(in_a.dtype() == in_b.dtype());",
      "  TORCH_CHECK(K_in % 8 == 0, \"k % 8 == 0\");",
      "  TORCH_CHECK(in_a.dtype() == torch::kFloat16 ||",
      "              in_a.dtype() == torch::kBFloat16);",
      "",
      "  auto out_c = torch::empty(",
      "      {N_in, M_in},",
      "      torch::TensorOptions().dtype(in_b.dtype()).device(in_b.device()));",
      "",
      "  dim3 grid(CuCount);",
      "",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(in_a));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  const int max_lds_len = get_lds_size() / 2;",
      "",
      "#define WVSPLITK(_WvPrGrp, _YTILEs, _YTILEm, _YTILEb, _UNRLs, _UNRLm, _UNRLb, \\",
      "                 _N)                                                          \\",
      "  {                                                                           \\",
      "    dim3 block(64, _WvPrGrp);                                                 \\",
      "    if ((K_in * N_in <= max_lds_len) && (M_in % _YTILEs == 0)) {              \\",
      "      int __wvPrGrp = mindiv(M_in, CuCount * _YTILEs, _WvPrGrp);              \\",
      "      wvSplitK_hf_sml_<fptype, 64, _YTILEs, _WvPrGrp, 8, _UNRLs, _N>          \\",
      "          <<<grid, block, 0, stream>>>(K_in, M_in, af4, bf4, c, __wvPrGrp,    \\",
      "                                       CuCount);                              \\",
      "    } else if (K_in * N_in <= max_lds_len * 1.2) {                            \\",
      "      int __wvPrGrp = mindiv(M_in, CuCount * _YTILEm, _WvPrGrp);              \\",
      "      wvSplitK_hf_<fptype, 64, _YTILEm, _WvPrGrp, 8, _UNRLm, _N>              \\",
      "          <<<grid, block, 0, stream>>>(K_in, M_in, af4, bf4, c, __wvPrGrp,    \\",
      "                                       CuCount);                              \\",
      "    } else {                                                                  \\",
      "      int __wvPrGrp = mindiv(M_in, CuCount * _YTILEb, _WvPrGrp);              \\",
      "      wvSplitK_hf_big_<fptype, 64, _YTILEb, _WvPrGrp, 8, _UNRLb, _N>          \\",
      "          <<<grid, block, 0, stream>>>(K_in, M_in, af4, bf4, c, __wvPrGrp,    \\",
      "                                       CuCount);                              \\",
      "    }                                                                         \\",
      "  }",
      "",
      "  AT_DISPATCH_REDUCED_FLOATING_TYPES(in_b.scalar_type(), \"wvSplitK\", [&] {",
      "    using fptype = typename scalar<scalar_t>::type;",
      "    fptype* af4 = reinterpret_cast<fptype*>(in_a.data_ptr());",
      "    const fptype* bf4 = reinterpret_cast<const fptype*>(in_b.data_ptr());",
      "    fptype* c = reinterpret_cast<fptype*>(out_c.data_ptr());",
      "    switch (N_in) {",
      "      case 1:",
      "        WVSPLITK(16, 2, 2, 2, 2, 2, 2, 1)",
      "        break;",
      "      case 2:",
      "        WVSPLITK(16, 2, 2, 2, 2, 2, 2, 2)",
      "        break;",
      "      case 3:",
      "        WVSPLITK(16, 4, 7, 7, 1, 1, 1, 3)",
      "        break;",
      "      case 4:",
      "        WVSPLITK(16, 4, 7, 7, 1, 1, 1, 4)",
      "        break;",
      "      default:",
      "        throw std::runtime_error(",
      "            \"Unsupported N value: \" + std::to_string(M_in) + \",\" +",
      "            std::to_string(K_in) + \",\" + std::to_string(N_in));",
      "    }",
      "  });",
      "  return out_c;",
      "}",
      "",
      "#if defined(__HIP__MI3XX__)  // TODO: Add NAVI support",
      "template <typename scalar_t, typename fp8_t, int THRDS, int YTILE, int WvPrGrp,",
      "          int A_CHUNK, int UNRL, int N>",
      "__global__ void __launch_bounds__(WvPrGrp* THRDS)",
      "    wvSplitKQ_hf_sml_(const int K, const int Kp, const int M, const fp8_t* B,",
      "                      const fp8_t* __restrict__ A, scalar_t* C,",
      "                      const float* __restrict__ s_A,",
      "                      const float* __restrict__ s_B, const int _WvPrGrp,",
      "                      const int CuCount) {",
      "  constexpr int max_lds_len = LDS_SIZE;",
      "  using scalar8 =",
      "      __attribute__((__vector_size__((A_CHUNK / 4) * sizeof(float)))) float;",
      "  using intx2 = __attribute__((__vector_size__(2 * sizeof(int)))) int;",
      "  using intx4 = __attribute__((__vector_size__(4 * sizeof(int)))) int;",
      "  union bigType {",
      "    char f8[A_CHUNK];",
      "    char2 c2[A_CHUNK / 2];",
      "    scalar_t h[A_CHUNK / 2];",
      "    float f[A_CHUNK / 4];",
      "    int i[A_CHUNK / 4];",
      "    long l[A_CHUNK / 8];",
      "    intx4 l2[A_CHUNK / 16];",
      "    scalar8 h8;",
      "  };",
      "",
      "  __shared__ fp8_t s[max_lds_len];",
      "",
      "  for (uint32_t k = (threadIdx.y * THRDS + threadIdx.x) * A_CHUNK;",
      "       k < min(K * N, max_lds_len); k += THRDS * WvPrGrp * A_CHUNK) {",
      "    *((bigType*)(&s[k])) = *((bigType*)(&A[k]));",
      "  }",
      "  __syncthreads();",
      "",
      "  if (threadIdx.y >= _WvPrGrp) return;",
      "",
      "  uint32_t m = (blockIdx.x * _WvPrGrp + (threadIdx.y % _WvPrGrp)) * YTILE;",
      "",
      "  using floatx16 = __attribute__((__vector_size__(16 * sizeof(float)))) float;",
      "  floatx16 sum[N][YTILE];",
      "  float sA = *s_A;",
      "  float sB = *s_B;",
      "",
      "  while (m < M) {",
      "    for (int i = 0; i < YTILE; i++)",
      "      for (int n = 0; n < N; n++) sum[n][i] = {0.f};",
      "",
      "    bigType bigA[N][UNRL];",
      "    bigType bigB[YTILE][UNRL];",
      "",
      "    for (uint32_t k1 = 0; k1 < K; k1 += THRDS * A_CHUNK * UNRL) {",
      "  #pragma unroll",
      "      for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "  #pragma unroll",
      "        for (uint32_t n = 0; n < N; ++n) bigA[n][k2].h8 = {0.f};",
      "  #pragma unroll",
      "        for (uint32_t y = 0; y < YTILE; ++y) bigB[y][k2].h8 = {0.f};",
      "      }",
      "",
      "      // Fetch the weight matrix from memory!",
      "  #pragma unroll",
      "      for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "        uint32_t k = k1 + k2 * THRDS * A_CHUNK;",
      "        uint32_t k_ = k + threadIdx.x * A_CHUNK;",
      "        if (k_ >= K) break;",
      "",
      "        const fp8_t* B_ = &B[(m + 0) * Kp + k_];",
      "  #pragma unroll",
      "        for (uint32_t y = 0; y < YTILE; ++y) {",
      "          bigB[y][k2].h8 = (loadnt((scalar8*)(&B_[y * Kp])));",
      "        }",
      "      }",
      "",
      "  // Fetch activation matrix from either just LDS or from both LDS / memory",
      "  #pragma unroll",
      "      for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "        uint32_t k = k1 + k2 * THRDS * A_CHUNK;",
      "        uint32_t k_ = k + threadIdx.x * A_CHUNK;",
      "        if (k_ >= K) break;",
      "        for (int n = 0; n < N; n++) {",
      "          bigA[n][k2] = *((const bigType*)(&(s[k_ + K * n])));",
      "        }",
      "      }",
      "",
      "  // Do the matrix multiplication in interleaved manner",
      "  #pragma unroll",
      "      for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "        uint32_t k = k1 + k2 * THRDS * A_CHUNK;",
      "        if (k >= K) break;",
      "",
      "        for (uint32_t n = 0; n < N; n++) {",
      "          for (int i = 0; i < A_CHUNK; i += 8) {",
      "            for (int y = 0; y < YTILE; ++y) {",
      "              sum[n][y] = __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8(",
      "                  bigA[n][k2].l[i / 8], bigB[y][k2].l[i / 8], sum[n][y], 0, 0,",
      "                  0);",
      "            }",
      "          }",
      "        }",
      "      }",
      "    }",
      "",
      "    // Final reduction",
      "    for (int n = 0; n < N; n++) {",
      "      for (int y = 0; y < YTILE; y++) {",
      "        float accm0 = sum[n][y][0];",
      "        float accm16 = sum[n][y][8];",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:1 bound_ctrl:0 \"",
      "            : \"=v\"(accm0)",
      "            : \"0\"(accm0), \"v\"(sum[n][y][1]), \"v\"(accm0));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:1 bound_ctrl:0 \"",
      "            : \"=v\"(accm16)",
      "            : \"0\"(accm16), \"v\"(sum[n][y][9]), \"v\"(accm16));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:2 bound_ctrl:0 \"",
      "            : \"=v\"(accm0)",
      "            : \"0\"(accm0), \"v\"(sum[n][y][2]), \"v\"(accm0));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:2 bound_ctrl:0 \"",
      "            : \"=v\"(accm16)",
      "            : \"0\"(accm16), \"v\"(sum[n][y][10]), \"v\"(accm16));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:3 bound_ctrl:0 \"",
      "            : \"=v\"(accm0)",
      "            : \"0\"(accm0), \"v\"(sum[n][y][3]), \"v\"(accm0));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:3 bound_ctrl:0 \"",
      "            : \"=v\"(accm16)",
      "            : \"0\"(accm16), \"v\"(sum[n][y][11]), \"v\"(accm16));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:8 bound_ctrl:0 \"",
      "            : \"=v\"(accm0)",
      "            : \"0\"(accm0), \"v\"(sum[n][y][4]), \"v\"(accm0));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:8 bound_ctrl:0 \"",
      "            : \"=v\"(accm16)",
      "            : \"0\"(accm16), \"v\"(sum[n][y][12]), \"v\"(accm16));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:9 bound_ctrl:0 \"",
      "            : \"=v\"(accm0)",
      "            : \"0\"(accm0), \"v\"(sum[n][y][5]), \"v\"(accm0));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:9 bound_ctrl:0 \"",
      "            : \"=v\"(accm16)",
      "            : \"0\"(accm16), \"v\"(sum[n][y][13]), \"v\"(accm16));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:10 bound_ctrl:0 \"",
      "            : \"=v\"(accm0)",
      "            : \"0\"(accm0), \"v\"(sum[n][y][6]), \"v\"(accm0));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:10 bound_ctrl:0 \"",
      "            : \"=v\"(accm16)",
      "            : \"0\"(accm16), \"v\"(sum[n][y][14]), \"v\"(accm16));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:11 bound_ctrl:0 \"",
      "            : \"=v\"(accm0)",
      "            : \"0\"(accm0), \"v\"(sum[n][y][7]), \"v\"(accm0));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:11 bound_ctrl:0 \"",
      "            : \"=v\"(accm16)",
      "            : \"0\"(accm16), \"v\"(sum[n][y][15]), \"v\"(accm16));",
      "        accm0 += __shfl(accm0, 36);",
      "        accm16 += __shfl(accm16, 52);",
      "        sum[n][y][0] = accm0 + __shfl(accm16, 16);",
      "      }",
      "    }",
      "",
      "    if (threadIdx.x == 0) {",
      "      for (int n = 0; n < N; n++) {",
      "        for (int y = 0; y < YTILE; y++) {",
      "          C[m + y + n * M] = __float2s<scalar_t>(sum[n][y][0] * sA * sB);",
      "        }",
      "      }",
      "    }",
      "",
      "    m += CuCount * _WvPrGrp * YTILE;",
      "  }",
      "}",
      "#else   // !defined(__HIP__MI3XX__) TODO: Add NAVI support",
      "template <typename scalar_t, typename fp8_t, int THRDS, int YTILE, int WvPrGrp,",
      "          int A_CHUNK, int UNRL, int N>",
      "__global__ void wvSplitKQ_hf_sml_(const int K, const int Kp, const int M,",
      "                                  const fp8_t* B, const fp8_t* __restrict__ A,",
      "                                  scalar_t* C, const float* __restrict__ s_A,",
      "                                  const float* __restrict__ s_B,",
      "                                  const int _WvPrGrp, const int CuCount) {",
      "  UNREACHABLE_CODE",
      "}",
      "#endif  // defined(__HIP__MI3XX__) TODO: Add NAVI support",
      "",
      "#if defined(__HIP__MI3XX__)  // TODO: Add NAVI support",
      "template <typename scalar_t, typename fp8_t, int THRDS, int YTILE, int WvPrGrp,",
      "          int A_CHUNK, int UNRL, int N>",
      "__global__ void __launch_bounds__(WvPrGrp* THRDS)",
      "    wvSplitKQ_hf_(const int K, const int Kp, const int M, const fp8_t* B,",
      "                  const fp8_t* __restrict__ A, scalar_t* C,",
      "                  const float* __restrict__ s_A, const float* __restrict__ s_B,",
      "                  const int _WvPrGrp, const int CuCount) {",
      "  constexpr int max_lds_len = LDS_SIZE;",
      "  using scalar8 =",
      "      __attribute__((__vector_size__((A_CHUNK / 4) * sizeof(float)))) float;",
      "  using intx2 = __attribute__((__vector_size__(2 * sizeof(int)))) int;",
      "  using intx4 = __attribute__((__vector_size__(4 * sizeof(int)))) int;",
      "  union bigType {",
      "    char f8[A_CHUNK];",
      "    char2 c2[A_CHUNK / 2];",
      "    scalar_t h[A_CHUNK / 2];",
      "    float f[A_CHUNK / 4];",
      "    int i[A_CHUNK / 4];",
      "    long l[A_CHUNK / 8];",
      "    intx4 l2[A_CHUNK / 16];",
      "    scalar8 h8;",
      "  };",
      "",
      "  __shared__ fp8_t s[max_lds_len];",
      "",
      "  for (uint32_t k = (threadIdx.y * THRDS + threadIdx.x) * A_CHUNK;",
      "       k < min(K * N, max_lds_len); k += THRDS * WvPrGrp * A_CHUNK) {",
      "    *((bigType*)(&s[k])) = *((bigType*)(&A[k]));",
      "  }",
      "  __syncthreads();",
      "",
      "  if (threadIdx.y >= _WvPrGrp) return;",
      "",
      "  uint32_t m = (blockIdx.x * _WvPrGrp + (threadIdx.y % _WvPrGrp)) * YTILE;",
      "",
      "  using floatx16 = __attribute__((__vector_size__(16 * sizeof(float)))) float;",
      "  floatx16 sum[N][YTILE];",
      "  float sA = *s_A;",
      "  float sB = *s_B;",
      "",
      "  while (m < M) {",
      "    for (int i = 0; i < YTILE; i++)",
      "      for (int n = 0; n < N; n++) sum[n][i] = {0};",
      "",
      "    bigType bigA[N][UNRL];",
      "    bigType bigB[YTILE][UNRL];",
      "",
      "    for (uint32_t k1 = 0; k1 < K; k1 += THRDS * A_CHUNK * UNRL) {",
      "      // Fetch the weight matrix from memory!",
      "  #pragma unroll",
      "      for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "        uint32_t k = k1 + k2 * THRDS * A_CHUNK;",
      "        uint32_t k_ = k + threadIdx.x * A_CHUNK;",
      "        if (k_ >= K) break;",
      "",
      "        const fp8_t* B_ = &B[(m + 0) * Kp + k_];",
      "        for (int y = 0; y < YTILE; ++y) {",
      "          if (y + m >= M) break;  // To avoid mem access fault.",
      "          bigB[y][k2].h8 = (loadnt((scalar8*)(&B_[y * Kp])));",
      "        }",
      "      }",
      "",
      "  // Fetch activation matrix from either just LDS or from both LDS / memory",
      "  #pragma unroll",
      "      for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "        uint32_t k = k1 + k2 * THRDS * A_CHUNK;",
      "        uint32_t k_ = k + threadIdx.x * A_CHUNK;",
      "        if (k_ >= K) break;",
      "        for (int n = 0; n < N; n++) {",
      "          if (k_ + K * n < max_lds_len)",
      "            bigA[n][k2] = *((const bigType*)(&(s[k_ + K * n])));",
      "          else",
      "            bigA[n][k2] = *((const bigType*)(&(A[k_ + K * n])));",
      "        }",
      "      }",
      "",
      "  // Do the matrix multiplication in interleaved manner",
      "  #pragma unroll",
      "      for (uint32_t k2 = 0; k2 < UNRL; k2++) {",
      "        uint32_t k = k1 + k2 * THRDS * A_CHUNK;",
      "        uint32_t k_ = k + threadIdx.x * A_CHUNK;",
      "        if (k_ >= K) break;",
      "",
      "        for (uint32_t n = 0; n < N; n++) {",
      "          for (int i = 0; i < A_CHUNK; i += 8) {",
      "            for (int y = 0; y < YTILE; ++y) {",
      "              sum[n][y] = __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8(",
      "                  bigA[n][k2].l[i / 8], bigB[y][k2].l[i / 8], sum[n][y], 0, 0,",
      "                  0);",
      "            }",
      "          }",
      "        }",
      "      }",
      "    }",
      "",
      "    // Final reduction",
      "    for (int n = 0; n < N; n++) {",
      "      for (int y = 0; y < YTILE; y++) {",
      "        float accm0 = sum[n][y][0];",
      "        float accm16 = sum[n][y][8];",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:1 bound_ctrl:0 \"",
      "            : \"=v\"(accm0)",
      "            : \"0\"(accm0), \"v\"(sum[n][y][1]), \"v\"(accm0));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:1 bound_ctrl:0 \"",
      "            : \"=v\"(accm16)",
      "            : \"0\"(accm16), \"v\"(sum[n][y][9]), \"v\"(accm16));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:2 bound_ctrl:0 \"",
      "            : \"=v\"(accm0)",
      "            : \"0\"(accm0), \"v\"(sum[n][y][2]), \"v\"(accm0));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:2 bound_ctrl:0 \"",
      "            : \"=v\"(accm16)",
      "            : \"0\"(accm16), \"v\"(sum[n][y][10]), \"v\"(accm16));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:3 bound_ctrl:0 \"",
      "            : \"=v\"(accm0)",
      "            : \"0\"(accm0), \"v\"(sum[n][y][3]), \"v\"(accm0));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:3 bound_ctrl:0 \"",
      "            : \"=v\"(accm16)",
      "            : \"0\"(accm16), \"v\"(sum[n][y][11]), \"v\"(accm16));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:8 bound_ctrl:0 \"",
      "            : \"=v\"(accm0)",
      "            : \"0\"(accm0), \"v\"(sum[n][y][4]), \"v\"(accm0));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:8 bound_ctrl:0 \"",
      "            : \"=v\"(accm16)",
      "            : \"0\"(accm16), \"v\"(sum[n][y][12]), \"v\"(accm16));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:9 bound_ctrl:0 \"",
      "            : \"=v\"(accm0)",
      "            : \"0\"(accm0), \"v\"(sum[n][y][5]), \"v\"(accm0));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:9 bound_ctrl:0 \"",
      "            : \"=v\"(accm16)",
      "            : \"0\"(accm16), \"v\"(sum[n][y][13]), \"v\"(accm16));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:10 bound_ctrl:0 \"",
      "            : \"=v\"(accm0)",
      "            : \"0\"(accm0), \"v\"(sum[n][y][6]), \"v\"(accm0));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:10 bound_ctrl:0 \"",
      "            : \"=v\"(accm16)",
      "            : \"0\"(accm16), \"v\"(sum[n][y][14]), \"v\"(accm16));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:11 bound_ctrl:0 \"",
      "            : \"=v\"(accm0)",
      "            : \"0\"(accm0), \"v\"(sum[n][y][7]), \"v\"(accm0));",
      "        asm(\"v_add_f32 %0, %2, %3 row_shl:11 bound_ctrl:0 \"",
      "            : \"=v\"(accm16)",
      "            : \"0\"(accm16), \"v\"(sum[n][y][15]), \"v\"(accm16));",
      "        accm0 += __shfl(accm0, 36);",
      "        accm16 += __shfl(accm16, 52);",
      "        sum[n][y][0] = accm0 + __shfl(accm16, 16);",
      "      }",
      "    }",
      "",
      "    if (threadIdx.x == 0) {",
      "      for (int n = 0; n < N; n++) {",
      "        for (int y = 0; y < YTILE; y++) {",
      "          if (y + m >= M) break;  // To avoid mem access fault.",
      "          C[m + y + n * M] = __float2s<scalar_t>(sum[n][y][0] * sA * sB);",
      "        }",
      "      }",
      "    }",
      "",
      "    m += CuCount * _WvPrGrp * YTILE;",
      "  }",
      "}",
      "#else   // !defined(__HIP__MI3XX__) TODO: Add NAVI support",
      "template <typename scalar_t, typename fp8_t, int THRDS, int YTILE, int WvPrGrp,",
      "          int A_CHUNK, int UNRL, int N>",
      "__global__ void wvSplitKQ_hf_(const int K, const int Kp, const int M,",
      "                              const fp8_t* B, const fp8_t* __restrict__ A,",
      "                              scalar_t* C, const float* __restrict__ s_A,",
      "                              const float* __restrict__ s_B, const int _WvPrGrp,",
      "                              const int CuCount) {",
      "  UNREACHABLE_CODE",
      "}",
      "#endif  // defined(__HIP__MI3XX__) TODO: Add NAVI support",
      "",
      "void wvSplitKQ(at::Tensor& in_a, at::Tensor& in_b, at::Tensor& out_c,",
      "               at::Tensor& scale_a, at::Tensor& scale_b,",
      "               const int64_t CuCount) {",
      "  static c10::ScalarType kFp8Type = is_fp8_ocp()",
      "                                        ? c10::ScalarType::Float8_e4m3fn",
      "                                        : c10::ScalarType::Float8_e4m3fnuz;",
      "  auto M_in = in_a.size(0);",
      "  auto K_in = in_a.size(1);",
      "  auto N_in = in_b.size(0);",
      "  auto Kp_in = in_a.stride(0);",
      "  TORCH_CHECK(K_in % 16 == 0, \"k % 16 == 0\");",
      "  TORCH_CHECK(in_a.dtype() == in_b.dtype() && in_a.dtype() == kFp8Type);",
      "  TORCH_CHECK(out_c.dtype() == torch::kFloat16 ||",
      "              out_c.dtype() == torch::kBFloat16);",
      "",
      "  dim3 grid(CuCount);",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(in_a));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  const int max_lds_len = get_lds_size();",
      "",
      "#define WVSPLITKQ(_WvPrGrp, _YTILEs, _YTILEm, _YTILEb, _UNRLs, _UNRLm, _UNRLb, \\",
      "                  _N)                                                          \\",
      "  {                                                                            \\",
      "    dim3 block(64, _WvPrGrp);                                                  \\",
      "    if ((K_in * N_in <= max_lds_len) && (M_in % _YTILEs == 0)) {               \\",
      "      int __wvPrGrp = mindiv(M_in, CuCount * _YTILEs, _WvPrGrp);               \\",
      "      wvSplitKQ_hf_sml_<fptype, fp8_t, 64, _YTILEs, _WvPrGrp, 16, _UNRLs, _N>  \\",
      "          <<<grid, block, 0, stream>>>(K_in, Kp_in, M_in, a_ptr, b_ptr, c_ptr, \\",
      "                                       s_a, s_b, __wvPrGrp, CuCount);          \\",
      "    } else {                                                                   \\",
      "      int __wvPrGrp = mindiv(M_in, CuCount * _YTILEm, _WvPrGrp);               \\",
      "      wvSplitKQ_hf_<fptype, fp8_t, 64, _YTILEm, _WvPrGrp, 16, _UNRLm, _N>      \\",
      "          <<<grid, block, 0, stream>>>(K_in, Kp_in, M_in, a_ptr, b_ptr, c_ptr, \\",
      "                                       s_a, s_b, __wvPrGrp, CuCount);          \\",
      "    }                                                                          \\",
      "  }",
      "",
      "  AT_DISPATCH_REDUCED_FLOATING_TYPES(out_c.scalar_type(), \"wvSplitKQ\", [&] {",
      "    using fptype = typename scalar<scalar_t>::type;",
      "    auto c_ptr = reinterpret_cast<fptype*>(out_c.data_ptr());",
      "    auto s_a = scale_a.data_ptr<float>();",
      "    auto s_b = scale_b.data_ptr<float>();",
      "    VLLM_DISPATCH_FP8_TYPES(in_a.scalar_type(), \"wvSplitKQ\", [&] {",
      "      auto a_ptr = in_a.data_ptr<fp8_t>();",
      "      auto b_ptr = in_b.data_ptr<fp8_t>();",
      "      switch (N_in) {",
      "        case 1:",
      "          WVSPLITKQ(16, 2, 2, 2, 2, 2, 2, 1)",
      "          break;",
      "        case 2:",
      "          WVSPLITKQ(16, 2, 2, 2, 2, 2, 2, 2)",
      "          break;",
      "        case 3:",
      "          WVSPLITKQ(16, 4, 7, 7, 1, 1, 1, 3)",
      "          break;",
      "        case 4:",
      "          WVSPLITKQ(16, 4, 7, 7, 1, 1, 1, 4)",
      "          break;",
      "        default:",
      "          throw std::runtime_error(",
      "              \"Unsupported N value: \" + std::to_string(M_in) + \",\" +",
      "              std::to_string(K_in) + \",\" + std::to_string(N_in));",
      "      }",
      "    });",
      "  });",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/utils.cuh",
    "source": [
      "#pragma once",
      "",
      "/**",
      " * Quantization utilities including:",
      " *   Adjusted maximum values for qtypes.",
      " *   Minimum scaling factors for qtypes.",
      " */",
      "",
      "#include <cmath>",
      "#include <torch/types.h>",
      "",
      "#ifndef USE_ROCM",
      "  #include <c10/util/Float8_e4m3fn.h>",
      "  #define MAYBE_HOST_DEVICE C10_HOST_DEVICE",
      "#else",
      "  #include <ATen/hip/HIPContext.h>",
      "  #include <c10/util/Float8_e4m3fn.h>",
      "  #include <c10/util/Float8_e4m3fnuz.h>",
      "  // ROCm doesn't seem to need C10_HOST_DEVICE for static constexpr",
      "  #define MAYBE_HOST_DEVICE",
      "#endif",
      "",
      "template <typename T,",
      "          typename = std::enable_if_t<std::is_same_v<T, c10::Float8_e4m3fn> ||",
      "                                      std::is_same_v<T, c10::Float8_e4m3fnuz> ||",
      "                                      std::is_same_v<T, int8_t>>>",
      "struct quant_type_max {",
      "  static constexpr T val() { return std::numeric_limits<T>::max(); }",
      "};",
      "",
      "// Using the default max value from pytorch (240.0 0x7F) will cause accuracy",
      "// issues when running dynamic quantization. Here use 224.0 0x7E for rocm.",
      "template <>",
      "struct quant_type_max<c10::Float8_e4m3fnuz> {",
      "  static constexpr c10::Float8_e4m3fnuz val() {",
      "    return c10::Float8_e4m3fnuz(0x7E, c10::Float8_e4m3fnuz::from_bits());",
      "  }",
      "};",
      "",
      "template <typename T>",
      "MAYBE_HOST_DEVICE static constexpr T quant_type_max_v =",
      "    quant_type_max<T>::val();",
      "",
      "template <typename T,",
      "          typename = std::enable_if_t<std::is_same_v<T, c10::Float8_e4m3fn> ||",
      "                                      std::is_same_v<T, c10::Float8_e4m3fnuz> ||",
      "                                      std::is_same_v<T, int8_t>>>",
      "struct min_scaling_factor {",
      "  C10_DEVICE C10_ALWAYS_INLINE static float val() {",
      "    return 1.0f / (quant_type_max_v<T> * 512.0f);",
      "  }",
      "};",
      "",
      "template <>",
      "struct min_scaling_factor<int8_t> {",
      "  C10_DEVICE C10_ALWAYS_INLINE static float val() {",
      "    return std::numeric_limits<float>::epsilon();",
      "  }",
      "};"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/vectorization_utils.cuh",
    "source": [
      "#pragma once",
      "#include \"vectorization.cuh\"",
      "",
      "namespace vllm {",
      "",
      "template <int VEC_SIZE, typename InT, typename OutT, typename ScaOp>",
      "struct DefaultVecOp {",
      "  ScaOp scalar_op;",
      "",
      "  __device__ __forceinline__ void operator()(",
      "      vec_n_t<OutT, VEC_SIZE>& dst, const vec_n_t<InT, VEC_SIZE>& src) const {",
      "#pragma unroll",
      "    for (int i = 0; i < VEC_SIZE; ++i) {",
      "      scalar_op(dst.val[i], src.val[i]);",
      "    }",
      "  }",
      "};",
      "",
      "template <int VEC_SIZE, typename InT, typename OutT, typename VecOp,",
      "          typename ScaOp>",
      "__device__ inline void vectorize_with_alignment(",
      "    const InT* in, OutT* out, int len, int tid, int stride,",
      "    VecOp&& vec_op,       // vec_n_t<InT,16> -> vec_n_t<OutT,16>",
      "    ScaOp&& scalar_op) {  // InT -> OutT",
      "  static_assert(VEC_SIZE > 0 && (VEC_SIZE & (VEC_SIZE - 1)) == 0,",
      "                \"VEC_SIZE must be a positive power-of-two\");",
      "  constexpr int WIDTH = VEC_SIZE * sizeof(InT);  // eg: 64 B",
      "  uintptr_t addr = reinterpret_cast<uintptr_t>(in);",
      "",
      "  // fast path when the whole region is already aligned",
      "  // Note: currently the output is guaranteed to be same as the input, so we",
      "  // don't check it here, comments here just for future reference.",
      "  bool can_vec = ((addr & (WIDTH - 1)) == 0) && ((len & (VEC_SIZE - 1)) == 0);",
      "  if (can_vec) {",
      "    int num_vec = len / VEC_SIZE;",
      "",
      "    using vin_t = vec_n_t<InT, VEC_SIZE>;",
      "    using vout_t = vec_n_t<OutT, VEC_SIZE>;",
      "    auto* v_in = reinterpret_cast<const vin_t*>(in);",
      "    auto* v_out = reinterpret_cast<vout_t*>(out);",
      "",
      "    for (int i = tid; i < num_vec; i += stride) {",
      "      vout_t tmp;",
      "      // Make a local copy of the entire pack",
      "      vin_t src = v_in[i];  // <- encourages a single vector ld",
      "      vec_op(tmp, src);",
      "      v_out[i] = tmp;  // <- encourages a single vector st",
      "    }",
      "    return;",
      "  }",
      "",
      "  int misalignment_offset = addr & (WIDTH - 1);       // addr % 64",
      "  int alignment_bytes = WIDTH - misalignment_offset;  // 64 - (addr % 64)",
      "  int prefix_elems = alignment_bytes & (WIDTH - 1);   // handle 64",
      "  prefix_elems /= sizeof(InT);",
      "  prefix_elems = min(prefix_elems, len);  // 0 \u2264 prefix < 16",
      "",
      "  // 1. prefill the when it is unsafe to vectorize",
      "  for (int i = tid; i < prefix_elems; i += stride) {",
      "    scalar_op(out[i], in[i]);",
      "  }",
      "",
      "  in += prefix_elems;",
      "  out += prefix_elems;",
      "  len -= prefix_elems;",
      "",
      "  int num_vec = len / VEC_SIZE;",
      "  using vin_t = vec_n_t<InT, VEC_SIZE>;",
      "  using vout_t = vec_n_t<OutT, VEC_SIZE>;",
      "  auto* v_in = reinterpret_cast<const vin_t*>(in);",
      "  auto* v_out = reinterpret_cast<vout_t*>(out);",
      "",
      "  // 2. vectorize the main part",
      "  for (int i = tid; i < num_vec; i += stride) {",
      "    vout_t tmp;",
      "    // Make a local copy of the entire pack",
      "    vin_t src = v_in[i];  // <- encourages a single vector ld",
      "    vec_op(tmp, src);",
      "    v_out[i] = tmp;  // <- encourages a single vector st",
      "  }",
      "",
      "  // 3. handle the tail",
      "  int tail_start = num_vec * VEC_SIZE;",
      "  for (int i = tid + tail_start; i < len; i += stride) {",
      "    scalar_op(out[i], in[i]);",
      "  }",
      "}",
      "",
      "template <int VEC_SIZE, typename InT, typename OutT, typename ScaOp>",
      "__device__ __forceinline__ void vectorize_with_alignment(const InT* in,",
      "                                                         OutT* out, int len,",
      "                                                         int tid, int stride,",
      "                                                         ScaOp&& scalar_op) {",
      "  using Vec = DefaultVecOp<VEC_SIZE, InT, OutT, std::decay_t<ScaOp>>;",
      "  vectorize_with_alignment<VEC_SIZE>(in, out, len, tid, stride, Vec{scalar_op},",
      "                                     std::forward<ScaOp>(scalar_op));",
      "}",
      "",
      "template <int VEC_SIZE, typename InT, typename ScaOp>",
      "struct DefaultReadVecOp {",
      "  ScaOp scalar_op;",
      "",
      "  __device__ __forceinline__ void operator()(",
      "      const vec_n_t<InT, VEC_SIZE>& src) const {",
      "#pragma unroll",
      "    for (int i = 0; i < VEC_SIZE; ++i) {",
      "      scalar_op(src.val[i]);",
      "    }",
      "  }",
      "};",
      "",
      "// read-only version: iterate over the input with alignment guarantees",
      "template <int VEC_SIZE, typename InT, typename VecOp, typename ScaOp>",
      "__device__ inline void vectorize_read_with_alignment(const InT* in, int len,",
      "                                                     int tid, int stride,",
      "                                                     VecOp&& vec_op,",
      "                                                     ScaOp&& scalar_op) {",
      "  static_assert(VEC_SIZE > 0 && (VEC_SIZE & (VEC_SIZE - 1)) == 0,",
      "                \"VEC_SIZE must be a positive power-of-two\");",
      "  constexpr int WIDTH = VEC_SIZE * sizeof(InT);",
      "  uintptr_t addr = reinterpret_cast<uintptr_t>(in);",
      "",
      "  // fast path when the whole region is already aligned",
      "  bool can_vec = ((addr & (WIDTH - 1)) == 0) && ((len & (VEC_SIZE - 1)) == 0);",
      "  if (can_vec) {",
      "    int num_vec = len / VEC_SIZE;",
      "",
      "    using vin_t = vec_n_t<InT, VEC_SIZE>;",
      "    auto* v_in = reinterpret_cast<const vin_t*>(in);",
      "",
      "    for (int i = tid; i < num_vec; i += stride) {",
      "      vin_t tmp = v_in[i];",
      "      vec_op(tmp);",
      "    }",
      "    return;",
      "  }",
      "",
      "  int misalignment_offset = addr & (WIDTH - 1);",
      "  int alignment_bytes = WIDTH - misalignment_offset;",
      "  int prefix_elems = alignment_bytes & (WIDTH - 1);",
      "  prefix_elems /= sizeof(InT);",
      "  prefix_elems = min(prefix_elems, len);",
      "",
      "  // 1. handle the possibly unaligned prefix with scalar access.",
      "  for (int i = tid; i < prefix_elems; i += stride) {",
      "    scalar_op(in[i]);",
      "  }",
      "",
      "  in += prefix_elems;",
      "  len -= prefix_elems;",
      "",
      "  int num_vec = len / VEC_SIZE;",
      "  using vin_t = vec_n_t<InT, VEC_SIZE>;",
      "  auto* v_in = reinterpret_cast<const vin_t*>(in);",
      "",
      "  // 2. vectorized traversal of the main aligned region.",
      "  for (int i = tid; i < num_vec; i += stride) {",
      "    vec_op(v_in[i]);",
      "  }",
      "",
      "  // 3. handle remaining tail elements.",
      "  int tail_start = num_vec * VEC_SIZE;",
      "  for (int i = tid + tail_start; i < len; i += stride) {",
      "    scalar_op(in[i]);",
      "  }",
      "}",
      "",
      "// overload that requires only a scalar_op",
      "template <int VEC_SIZE, typename InT, typename ScaOp>",
      "__device__ __forceinline__ void vectorize_read_with_alignment(",
      "    const InT* in, int len, int tid, int stride, ScaOp&& scalar_op) {",
      "  using Vec = DefaultReadVecOp<VEC_SIZE, InT, std::decay_t<ScaOp>>;",
      "  vectorize_read_with_alignment<VEC_SIZE>(in, len, tid, stride, Vec{scalar_op},",
      "                                          std::forward<ScaOp>(scalar_op));",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/activation_kernels.cu",
    "source": [
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <torch/all.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#include <cmath>",
      "#include \"core/math.hpp\"",
      "#include \"../cuda_compat.h\"",
      "#include \"dispatch_utils.h\"",
      "",
      "#include \"quantization/fp8/common.cuh\"",
      "",
      "namespace vllm {",
      "",
      "template <typename T>",
      "__device__ __forceinline__ T silu_kernel(const T& x) {",
      "  // x * sigmoid(x)",
      "  return (T)(((float)x) / (1.0f + expf((float)-x)));",
      "}",
      "",
      "// Activation and gating kernel template.",
      "template <typename scalar_t, scalar_t (*ACT_FN)(const scalar_t&),",
      "          typename fp8_type>",
      "__global__ void act_and_mul_quant_kernel(",
      "    fp8_type* __restrict__ out,          // [..., d]",
      "    const scalar_t* __restrict__ input,  // [..., 2, d]",
      "    const float* scale, const int d) {",
      "  const int32_t blocks_per_token = gridDim.y;",
      "",
      "  const int32_t elems_per_128bit_load = (128 / 8) / sizeof(scalar_t);",
      "",
      "  // We don't expect the hidden dimension to exceed 32 bits so int32 should",
      "  // be safe here.",
      "  const int32_t tgt_elems_per_block = div_ceil(d, blocks_per_token);",
      "  const int32_t elems_per_block =",
      "      round_to_next_multiple_of(tgt_elems_per_block, elems_per_128bit_load);",
      "  const int32_t block_start = blockIdx.y * elems_per_block;",
      "  int32_t block_end = block_start + elems_per_block;",
      "  block_end = block_end > d ? d : block_end;",
      "",
      "  // token_idx is 64 bit to prevent 32 bit overflow when the number of tokens",
      "  // is very large",
      "  const int64_t token_idx = blockIdx.x;",
      "  const scalar_t* __restrict__ x_ptr = input + token_idx * 2 * d;",
      "  const scalar_t* __restrict__ y_ptr = input + token_idx * 2 * d + d;",
      "  fp8_type* __restrict__ out_ptr = out + token_idx * d;",
      "",
      "  // 128-bit vectorized code",
      "  const int32_t vec_loop_end =",
      "      round_to_previous_multiple_of(elems_per_128bit_load, block_end);",
      "  const int32_t vec_end_idx = vec_loop_end / elems_per_128bit_load;",
      "  const int32_t vec_start_idx = block_start / elems_per_128bit_load;",
      "",
      "  const int4* __restrict__ x_128bit_ptr = reinterpret_cast<const int4*>(x_ptr);",
      "  const int4* __restrict__ y_128bit_ptr = reinterpret_cast<const int4*>(y_ptr);",
      "  int2* __restrict__ out_128bit_ptr = reinterpret_cast<int2*>(out_ptr);",
      "",
      "  float inverted_scale = 1 / *scale;",
      "#pragma unroll",
      "  for (int32_t vec_idx = vec_start_idx + threadIdx.x; vec_idx < vec_end_idx;",
      "       vec_idx += blockDim.x) {",
      "    const int4 x_128bit = VLLM_LDG(&x_128bit_ptr[vec_idx]);",
      "    const int4 y_128bit = VLLM_LDG(&y_128bit_ptr[vec_idx]);",
      "    using scalar_128bit_vec_t = std::array<scalar_t, elems_per_128bit_load>;",
      "    using scalar_64bit_vec_t = std::array<fp8_type, elems_per_128bit_load>;",
      "",
      "    scalar_64bit_vec_t out_vec;",
      "    const auto x_vec = reinterpret_cast<scalar_128bit_vec_t const&>(x_128bit);",
      "    const auto y_vec = reinterpret_cast<scalar_128bit_vec_t const&>(y_128bit);",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < elems_per_128bit_load; i++) {",
      "      out_vec[i] = scaled_fp8_conversion<true, fp8_type>(",
      "          ACT_FN(x_vec[i]) * y_vec[i], inverted_scale);",
      "    }",
      "",
      "    out_128bit_ptr[vec_idx] = reinterpret_cast<const int2&>(out_vec);",
      "  }",
      "",
      "  // Scalar cleanup code",
      "  if (block_end > vec_loop_end) {",
      "    for (int64_t idx = vec_loop_end + threadIdx.x; idx < block_end;",
      "         idx += blockDim.x) {",
      "      const scalar_t x = VLLM_LDG(&x_ptr[idx]);",
      "      const scalar_t y = VLLM_LDG(&y_ptr[idx]);",
      "      out_ptr[idx] =",
      "          scaled_fp8_conversion<true, fp8_type>(ACT_FN(x) * y, inverted_scale);",
      "    }",
      "  }",
      "}",
      "}  // namespace vllm",
      "",
      "// Launch activation, gating, and quantize kernel.",
      "#define LAUNCH_ACTIVATION_GATE_KERNEL(KERNEL)                               \\",
      "  int d = input.size(-1) / 2;                                               \\",
      "  int64_t num_tokens = input.numel() / input.size(-1);                      \\",
      "  dim3 grid(num_tokens, num_tokens > 16 ? num_tokens > 32 ? 1 : 2 : 4);     \\",
      "  dim3 block(std::min(d, 512));                                             \\",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));         \\",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();             \\",
      "  VLLM_DISPATCH_FLOATING_TYPES(                                             \\",
      "      input.scalar_type(), \"act_and_mul_kernel\", [&] {                      \\",
      "        VLLM_DISPATCH_FP8_TYPES(                                            \\",
      "            out.scalar_type(), \"fused_add_rms_norm_kernel_fp8_type\", [&] {  \\",
      "              vllm::act_and_mul_quant_kernel<scalar_t, KERNEL<scalar_t>,    \\",
      "                                             fp8_t>                         \\",
      "                  <<<grid, block, 0, stream>>>(out.data_ptr<fp8_t>(),       \\",
      "                                               input.data_ptr<scalar_t>(),  \\",
      "                                               scale.data_ptr<float>(), d); \\",
      "            });                                                             \\",
      "      });",
      "",
      "void silu_and_mul_quant(torch::Tensor& out,    // [..., d]",
      "                        torch::Tensor& input,  // [..., 2 * d]",
      "                        torch::Tensor& scale) {",
      "  TORCH_CHECK(out.dtype() == torch::kFloat8_e4m3fn ||",
      "              out.dtype() == torch::kFloat8_e4m3fnuz);",
      "  TORCH_CHECK(input.dtype() == torch::kFloat16 ||",
      "              input.dtype() == torch::kBFloat16);",
      "  TORCH_CHECK(input.size(-1) % 2 == 0);",
      "  LAUNCH_ACTIVATION_GATE_KERNEL(vllm::silu_kernel);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/vectorization.cuh",
    "source": [
      "#pragma once",
      "/**",
      " * __device__ datatypes vectorized by 4",
      " */",
      "",
      "// Include both AMD and NVIDIA fp8 types to avoid circular import",
      "#include <c10/util/Float8_e4m3fnuz.h>",
      "#include <c10/util/Float8_e4m3fn.h>",
      "",
      "namespace vllm {",
      "",
      "// Vectorization containers",
      "template <typename scalar_t, size_t vec_size>",
      "struct __align__(vec_size * sizeof(scalar_t)) vec_n_t {",
      "  scalar_t val[vec_size];",
      "};",
      "",
      "template <typename quant_type_t, size_t vec_size>",
      "struct __align__(vec_size * sizeof(quant_type_t)) q8_n_t {",
      "  static_assert(std::is_same_v<quant_type_t, int8_t> ||",
      "                std::is_same_v<quant_type_t, c10::Float8_e4m3fn> ||",
      "                std::is_same_v<quant_type_t, c10::Float8_e4m3fnuz>);",
      "  quant_type_t val[vec_size];",
      "};",
      "",
      "template <typename scalar_t>",
      "using vec4_t = vec_n_t<scalar_t, 4>;",
      "template <typename quant_type_t>",
      "using q8x4_t = q8_n_t<quant_type_t, 4>;",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/awq/gemm_kernels.cu",
    "source": [
      "/*",
      "Adapted from https://github.com/mit-han-lab/llm-awq",
      "@article{lin2023awq,",
      "  title={AWQ: Activation-aware Weight Quantization for LLM Compression and",
      "Acceleration}, author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang,",
      "Shang and Dang, Xingyu and Han, Song}, journal={arXiv}, year={2023}",
      "}",
      " */",
      "",
      "#include <torch/all.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#include \"dequantize.cuh\"",
      "",
      "#include <cuda_fp16.h>",
      "",
      "namespace vllm {",
      "namespace awq {",
      "",
      "template <int N>",
      "__global__ void __launch_bounds__(64)",
      "    gemm_forward_4bit_cuda_m16nXk32(int G, int split_k_iters,",
      "                                    half* __restrict__ A, int* __restrict__ B,",
      "                                    half* __restrict__ scaling_factors,",
      "                                    int* __restrict__ zeros, int M, int IC,",
      "                                    int OC, half* __restrict__ C) {",
      "  // Only support matrix n = 64 or 128",
      "  assert(N == 64 || N == 128);",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 750",
      "  assert(false);",
      "#else",
      "  static constexpr uint32_t ZERO = 0x0;",
      "  float C_warp[32];",
      "  __shared__ half A_shared[16 * (32 + 8)];",
      "  __shared__ half B_shared[32 * (N + 8)];",
      "",
      "  int j_factors1 = ((OC + N - 1) / N);",
      "  int blockIdx_y = blockIdx.x % ((M + 16 - 1) / 16 * j_factors1);",
      "  int blockIdx_z = blockIdx.x / ((M + 16 - 1) / 16 * j_factors1);",
      "",
      "  half A_shared_warp[8];",
      "  half B_shared_warp[N / 4];",
      "  for (int j_0_4_init = 0; j_0_4_init < N / 32; ++j_0_4_init) {",
      "    for (int i = 0; i < 8; ++i) {",
      "      C_warp[(j_0_4_init * 8) + i] = 0.0;",
      "    }",
      "  }",
      "",
      "  static constexpr int row_stride_warp = 32 * 8 / 32;",
      "  static constexpr int row_stride = 2 * 32 * 8 / N;",
      "  // TODO: Haotian: blockIdx_y / j_factors1 in A loading to support bsz > 16",
      "  bool ld_A_flag =",
      "      (blockIdx_y / j_factors1 * 16 + threadIdx.y * row_stride_warp +",
      "       threadIdx.x * 8 / 32) < M;  // threadIdx.y is warp_id",
      "  // bool wb_C_flag = (threadIdx.x / 4) < M;",
      "",
      "  half* A_ptr =",
      "      A +",
      "      (((int)blockIdx_y) / j_factors1 * 16 +",
      "       (((int)threadIdx.y) * row_stride_warp) + ((int)threadIdx.x) / (32 / 8)) *",
      "          IC +",
      "      (((int)threadIdx.x) % (32 / 8)) * 8;",
      "",
      "  int* B_ptr = B + ((int)threadIdx.y) * (OC / 8) * (256 / N) +",
      "               (((int)threadIdx.x) / (N / 8)) * (OC / 8) +",
      "               (((int)blockIdx_y) % j_factors1) * (N / 8) +",
      "               (((int)threadIdx.x) % (N / 8)) * 1;",
      "  // Why * 1 in the above line?",
      "",
      "  half* A_shared_ptr = A_shared +",
      "                       ((int)threadIdx.y) * row_stride_warp * (32 + 8) +",
      "                       (((int)threadIdx.x) / (32 / 8)) * (32 + 8) +",
      "                       (((int)threadIdx.x) % (32 / 8)) * 8;",
      "",
      "  half* B_shared_ptr = B_shared +",
      "                       ((int)threadIdx.y) * (row_stride / 2) * (N + 8) +",
      "                       (((int)threadIdx.x) / (N / 8)) * (N + 8) +",
      "                       (((int)threadIdx.x) % (N / 8)) * 8;",
      "",
      "  int* zeros_ptr = zeros + (((int)blockIdx_y) % j_factors1) * (N / 8) +",
      "                   ((int)threadIdx.x) % (N / 8);",
      "",
      "  half* scaling_factors_ptr = scaling_factors +",
      "                              (((int)blockIdx_y) % j_factors1) * N +",
      "                              (((int)threadIdx.x) % (N / 8)) * 8;",
      "",
      "  half* C_ptr =",
      "      C +",
      "      static_cast<long long>(blockIdx_z) * M * OC  // blockIdz.x -> split_k dim",
      "      + (((int)blockIdx_y) % j_factors1) * N + ((int)threadIdx.y) * (N / 2) +",
      "      (((int)threadIdx.x) % 4) * 2;",
      "",
      "  // preload s.f. and zeros",
      "  int k_bound = (IC / 32 + split_k_iters - 1) / split_k_iters;",
      "  if ((k_bound - 1) * split_k_iters * 32 + blockIdx_z * 32 >= IC) k_bound -= 1;",
      "  for (int _k_0_0 = 0; _k_0_0 < k_bound; ++_k_0_0) {",
      "    int k_0_0 = _k_0_0 * split_k_iters + blockIdx_z;",
      "    __syncthreads();",
      "    // TODO: Haotian: blockIdx_y / j_factors1 in A loading to support bsz > 16",
      "    if (ld_A_flag) {",
      "      *(uint4*)(A_shared_ptr) = *(uint4*)(A_ptr + (k_0_0 * 32));",
      "    } else {",
      "      *(uint4*)(A_shared_ptr) = make_uint4(0, 0, 0, 0);",
      "    }",
      "",
      "    // for (int ax0_ax1_fused_0 = 0; ax0_ax1_fused_0 < 2; ++ax0_ax1_fused_0) {",
      "    uint32_t zeros_loaded = *(uint32_t*)(zeros_ptr + k_0_0 * 32 / G * (OC / 8));",
      "    uint4 B_loaded_zero = dequantize_s4_to_fp16x2(zeros_loaded);",
      "    uint4 B_loaded_scale =",
      "        *(uint4*)(scaling_factors_ptr + k_0_0 * 32 / G * (OC));",
      "    /*",
      "    if (blockIdx_z == 0 && blockIdx_y == 0 && k_0_0 == 0 && threadIdx.x == 0 &&",
      "    threadIdx.y == 0){ printf(\"%x %x %x %x %x %x %x %x\\n\", B_loaded_scale.x,",
      "    B_loaded_scale.y, B_loaded_scale.z, B_loaded_scale.w, B_loaded_zero.x,",
      "    B_loaded_zero.y, B_loaded_zero.z, B_loaded_zero.w);",
      "    }",
      "    */",
      "    // uint4 B_loaded_scale = make_uint4(0, 0, 0, 0);",
      "    int* B_ptr_local = B_ptr + k_0_0 * 32 * (OC / 8);",
      "",
      "    for (int ax0_ax1_fused_0 = 0; ax0_ax1_fused_0 < N / 16; ++ax0_ax1_fused_0) {",
      "      // B: 32 x 136 (128+8) float16",
      "      // each warp: 32 x 4",
      "      // each thr: read 32 bit -> convert to 8xFP16 (a UINT4) -> scale and minus",
      "      // zero -> WB UINT4",
      "      // *(uint4*)(B_shared + ((((ax0_ax1_fused_0 * 544) + (((int)threadIdx.y) *",
      "      // 272)) + ((((int)threadIdx.x) >> 4) * 136)) + ((((int)threadIdx.x) & 15)",
      "      // * 8))) = *(uint4*)(B + ((((((k_0_0 * 163840) + (ax0_ax1_fused_0 *",
      "      // 20480)) + (((int)threadIdx.y) * 10240)) + ((((int)threadIdx.x) >> 4) *",
      "      // 5120)) + (((int)blockIdx_y) * 128)) + ((((int)threadIdx.x) & 15) *",
      "      // 8))); row stride in shared memory: (NWARPS * 32 * 8 / cta_N)",
      "      uint32_t B_loaded =",
      "          *(uint32_t*)(B_ptr_local + ax0_ax1_fused_0 * row_stride * (OC / 8));",
      "      uint4 B_loaded_fp16 = dequantize_s4_to_fp16x2(B_loaded);",
      "",
      "      // - zero and * scale",
      "      // TODO (Haotian): can save 4 assembly instructions if sormulate as deq =",
      "      // q * scale - zero * scale.",
      "      asm volatile(\"sub.f16x2 %0, %1, %2;\\n\"",
      "                   : \"=r\"(B_loaded_fp16.x)",
      "                   : \"r\"(B_loaded_fp16.x), \"r\"(B_loaded_zero.x));",
      "      asm volatile(\"fma.rn.f16x2 %0, %1, %2, %3;\\n\"",
      "                   : \"=r\"(B_loaded_fp16.x)",
      "                   : \"r\"(B_loaded_fp16.x), \"r\"(B_loaded_scale.x), \"r\"(ZERO));",
      "      asm volatile(\"sub.f16x2 %0, %1, %2;\\n\"",
      "                   : \"=r\"(B_loaded_fp16.y)",
      "                   : \"r\"(B_loaded_fp16.y), \"r\"(B_loaded_zero.y));",
      "      asm volatile(\"fma.rn.f16x2 %0, %1, %2, %3;\\n\"",
      "                   : \"=r\"(B_loaded_fp16.y)",
      "                   : \"r\"(B_loaded_fp16.y), \"r\"(B_loaded_scale.y), \"r\"(ZERO));",
      "      asm volatile(\"sub.f16x2 %0, %1, %2;\\n\"",
      "                   : \"=r\"(B_loaded_fp16.z)",
      "                   : \"r\"(B_loaded_fp16.z), \"r\"(B_loaded_zero.z));",
      "      asm volatile(\"fma.rn.f16x2 %0, %1, %2, %3;\\n\"",
      "                   : \"=r\"(B_loaded_fp16.z)",
      "                   : \"r\"(B_loaded_fp16.z), \"r\"(B_loaded_scale.z), \"r\"(ZERO));",
      "      asm volatile(\"sub.f16x2 %0, %1, %2;\\n\"",
      "                   : \"=r\"(B_loaded_fp16.w)",
      "                   : \"r\"(B_loaded_fp16.w), \"r\"(B_loaded_zero.w));",
      "      asm volatile(\"fma.rn.f16x2 %0, %1, %2, %3;\\n\"",
      "                   : \"=r\"(B_loaded_fp16.w)",
      "                   : \"r\"(B_loaded_fp16.w), \"r\"(B_loaded_scale.w), \"r\"(ZERO));",
      "      /*",
      "      if (ax0_ax1_fused_0 == 0 && blockIdx_z == 0 && blockIdx_y == 0 && k_0_0 ==",
      "      0 && threadIdx.x == 17 && threadIdx.y == 0){ printf(\"[x] %X %X %X %X\\n\",",
      "      B_loaded_fp16.x, B_loaded_fp16.y, B_loaded_fp16.z, B_loaded_fp16.w);",
      "      }",
      "      */",
      "",
      "      // write back",
      "      *(uint4*)(B_shared_ptr + ax0_ax1_fused_0 * row_stride * (N + 8)) =",
      "          B_loaded_fp16;",
      "    }",
      "    __syncthreads();",
      "",
      "    for (int k_0_1 = 0; k_0_1 < 2; ++k_0_1) {",
      "      {",
      "        unsigned int addr;",
      "        __asm__ __volatile__(",
      "            \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, \"",
      "            \"addr; }\\n\"",
      "            : \"=r\"(addr)",
      "            : \"l\"((void*)((&(A_shared[(k_0_1 * 16)])) +",
      "                          (((((int)threadIdx.x) & 15) * 40) +",
      "                           ((((int)threadIdx.x) >> 4) * 8)))));",
      "",
      "        __asm__ __volatile__(",
      "            \"ldmatrix.sync.aligned.m8n8.x4.shared.b16\"",
      "            \"{%0, %1, %2, %3}, [%4];\\n\"",
      "            : \"=r\"(((unsigned*)(A_shared_warp + 0))[0]),",
      "              \"=r\"(((unsigned*)(A_shared_warp + 0))[1]),",
      "              \"=r\"(((unsigned*)(A_shared_warp + 0))[2]),",
      "              \"=r\"(((unsigned*)(A_shared_warp + 0))[3])",
      "            : \"r\"(addr));",
      "      }",
      "",
      "      for (int ax1_0 = 0; ax1_0 < N / 32; ++ax1_0) {",
      "        {",
      "          unsigned int addr;",
      "          __asm__ __volatile__(",
      "              \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, \"",
      "              \"addr; }\\n\"",
      "              : \"=r\"(addr)",
      "              : \"l\"((void*)((&(B_shared[(((k_0_1 * (N * 16 + 128)) +",
      "                                          (((int)threadIdx.y) * (N / 2))) +",
      "                                         (ax1_0 * 16))])) +",
      "                            (((((int)threadIdx.x) & 15) * (N + 8)) +",
      "                             ((((int)threadIdx.x) >> 4) * 8)))));",
      "          __asm__ __volatile__(",
      "              \"ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16\"",
      "              \"{%0, %1, %2, %3}, [%4];\\n\"",
      "              : \"=r\"(((unsigned*)(B_shared_warp + (ax1_0 * 8)))[0]),",
      "                \"=r\"(((unsigned*)(B_shared_warp + (ax1_0 * 8)))[1]),",
      "                \"=r\"(((unsigned*)(B_shared_warp + (ax1_0 * 8)))[2]),",
      "                \"=r\"(((unsigned*)(B_shared_warp + (ax1_0 * 8)))[3])",
      "              : \"r\"(addr));",
      "        }",
      "      }",
      "      for (int j_0_4 = 0; j_0_4 < N / 32; ++j_0_4) {",
      "  #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ == 750",
      "        {",
      "          __asm__ __volatile__(",
      "              \"mma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\"",
      "              \"{%0, %1, %2, %3}, {%4, %5}, {%6}, {%7, %8, %9, %10};\\n\"",
      "              : \"=f\"(((float*)(C_warp + (j_0_4 * 8)))[0]),",
      "                \"=f\"(((float*)(C_warp + (j_0_4 * 8)))[1]),",
      "                \"=f\"(((float*)(C_warp + (j_0_4 * 8)))[2]),",
      "                \"=f\"(((float*)(C_warp + (j_0_4 * 8)))[3])",
      "              : \"r\"(((unsigned*)(A_shared_warp + 0))[0]),",
      "                \"r\"(((unsigned*)(A_shared_warp + 0))[1]),",
      "                \"r\"(((unsigned*)(B_shared_warp + (j_0_4 * 8)))[0]),",
      "                \"f\"(((float*)(C_warp + (j_0_4 * 8)))[0]),",
      "                \"f\"(((float*)(C_warp + (j_0_4 * 8)))[1]),",
      "                \"f\"(((float*)(C_warp + (j_0_4 * 8)))[2]),",
      "                \"f\"(((float*)(C_warp + (j_0_4 * 8)))[3]));",
      "        }",
      "",
      "        {",
      "          __asm__ __volatile__(",
      "              \"mma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\"",
      "              \"{%0, %1, %2, %3}, {%4, %5}, {%6}, {%7, %8, %9, %10};\\n\"",
      "              : \"=f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[0]),",
      "                \"=f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[1]),",
      "                \"=f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[2]),",
      "                \"=f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[3])",
      "              : \"r\"(((unsigned*)(A_shared_warp + 0))[0]),",
      "                \"r\"(((unsigned*)(A_shared_warp + 0))[1]),",
      "                \"r\"(((unsigned*)(B_shared_warp + ((j_0_4 * 8) + 4)))[0]),",
      "                \"f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[0]),",
      "                \"f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[1]),",
      "                \"f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[2]),",
      "                \"f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[3]));",
      "        }",
      "",
      "        {",
      "          __asm__ __volatile__(",
      "              \"mma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\"",
      "              \"{%0, %1, %2, %3}, {%4, %5}, {%6}, {%7, %8, %9, %10};\\n\"",
      "              : \"=f\"(((float*)(C_warp + (j_0_4 * 8)))[0]),",
      "                \"=f\"(((float*)(C_warp + (j_0_4 * 8)))[1]),",
      "                \"=f\"(((float*)(C_warp + (j_0_4 * 8)))[2]),",
      "                \"=f\"(((float*)(C_warp + (j_0_4 * 8)))[3])",
      "              : \"r\"(((unsigned*)(A_shared_warp + 0))[2]),",
      "                \"r\"(((unsigned*)(A_shared_warp + 0))[3]),",
      "                \"r\"(((unsigned*)(B_shared_warp + (j_0_4 * 8)))[1]),",
      "                \"f\"(((float*)(C_warp + (j_0_4 * 8)))[0]),",
      "                \"f\"(((float*)(C_warp + (j_0_4 * 8)))[1]),",
      "                \"f\"(((float*)(C_warp + (j_0_4 * 8)))[2]),",
      "                \"f\"(((float*)(C_warp + (j_0_4 * 8)))[3]));",
      "        }",
      "",
      "        {",
      "          __asm__ __volatile__(",
      "              \"mma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\"",
      "              \"{%0, %1, %2, %3}, {%4, %5}, {%6}, {%7, %8, %9, %10};\\n\"",
      "              : \"=f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[0]),",
      "                \"=f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[1]),",
      "                \"=f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[2]),",
      "                \"=f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[3])",
      "              : \"r\"(((unsigned*)(A_shared_warp + 0))[2]),",
      "                \"r\"(((unsigned*)(A_shared_warp + 0))[3]),",
      "                \"r\"(((unsigned*)(B_shared_warp + ((j_0_4 * 8) + 4)))[1]),",
      "                \"f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[0]),",
      "                \"f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[1]),",
      "                \"f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[2]),",
      "                \"f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[3]));",
      "        }",
      "  #else",
      "        {",
      "          __asm__ __volatile__(",
      "              \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"",
      "              \"{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, \"",
      "              \"%13};\\n\"",
      "              : \"=f\"(((float*)(C_warp + (j_0_4 * 8)))[0]),",
      "                \"=f\"(((float*)(C_warp + (j_0_4 * 8)))[1]),",
      "                \"=f\"(((float*)(C_warp + (j_0_4 * 8)))[2]),",
      "                \"=f\"(((float*)(C_warp + (j_0_4 * 8)))[3])",
      "              : \"r\"(((unsigned*)(A_shared_warp + 0))[0]),",
      "                \"r\"(((unsigned*)(A_shared_warp + 0))[1]),",
      "                \"r\"(((unsigned*)(A_shared_warp + 0))[2]),",
      "                \"r\"(((unsigned*)(A_shared_warp + 0))[3]),",
      "                \"r\"(((unsigned*)(B_shared_warp + (j_0_4 * 8)))[0]),",
      "                \"r\"(((unsigned*)(B_shared_warp + (j_0_4 * 8)))[1]),",
      "                \"f\"(((float*)(C_warp + (j_0_4 * 8)))[0]),",
      "                \"f\"(((float*)(C_warp + (j_0_4 * 8)))[1]),",
      "                \"f\"(((float*)(C_warp + (j_0_4 * 8)))[2]),",
      "                \"f\"(((float*)(C_warp + (j_0_4 * 8)))[3]));",
      "        }",
      "",
      "        {",
      "          __asm__ __volatile__(",
      "              \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"",
      "              \"{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, \"",
      "              \"%13};\\n\"",
      "              : \"=f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[0]),",
      "                \"=f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[1]),",
      "                \"=f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[2]),",
      "                \"=f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[3])",
      "              : \"r\"(((unsigned*)(A_shared_warp + 0))[0]),",
      "                \"r\"(((unsigned*)(A_shared_warp + 0))[1]),",
      "                \"r\"(((unsigned*)(A_shared_warp + 0))[2]),",
      "                \"r\"(((unsigned*)(A_shared_warp + 0))[3]),",
      "                \"r\"(((unsigned*)(B_shared_warp + ((j_0_4 * 8) + 4)))[0]),",
      "                \"r\"(((unsigned*)(B_shared_warp + ((j_0_4 * 8) + 4)))[1]),",
      "                \"f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[0]),",
      "                \"f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[1]),",
      "                \"f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[2]),",
      "                \"f\"(((float*)(C_warp + ((j_0_4 * 8) + 4)))[3]));",
      "        }",
      "",
      "  #endif",
      "      }",
      "    }",
      "  }",
      "",
      "  // TODO: Shang: Hoist loop invariance.",
      "  for (int ax1_0_1 = 0; ax1_0_1 < (N / 32); ++ax1_0_1) {",
      "    for (int local_id = 0; local_id < 8; ++local_id) {",
      "      int row_offset = (((int)blockIdx_y) / j_factors1) * 16 +",
      "                       ((int)threadIdx.x) / 4 + (local_id % 4) / 2 * 8;",
      "      if (row_offset < M) {",
      "        *(C_ptr + ax1_0_1 * 16 + row_offset * OC + (local_id / 4) * 8 +",
      "          local_id % 2) = __float2half(C_warp[(ax1_0_1 * 8) + local_id]);",
      "      }",
      "    }",
      "  }",
      "#endif",
      "}",
      "",
      "__global__ void __launch_bounds__(64)",
      "    dequantize_weights(int* __restrict__ B, half* __restrict__ scaling_factors,",
      "                       int* __restrict__ zeros, half* __restrict__ C, int G) {",
      "  static constexpr uint32_t ZERO = 0x0;",
      "  half B_shared[32 * (128 + 8)];",
      "",
      "  half* B_shared_ptr2 = B_shared;",
      "",
      "  int N = blockDim.x * gridDim.x;  // 2",
      "  int col = (blockIdx.x * blockDim.x + threadIdx.x);",
      "  int row = blockIdx.y * blockDim.y + threadIdx.y;",
      "  int index1 = 8 * col + 8 * row * N;",
      "  half* C_ptr2 = C + index1;",
      "",
      "  int index2 = col + row * N;",
      "  int* B_ptr2 = B + index2;",
      "",
      "  int index3 = col + (int)(row / G) * N;",
      "  int* zeros_ptr2 = zeros + index3;",
      "  int index4 = 8 * col + (int)(row / G) * N * 8;",
      "  half* scaling_factors_ptr2 = scaling_factors + index4;",
      "",
      "  uint32_t zeros_loaded = *(uint32_t*)(zeros_ptr2);",
      "  uint4 B_loaded_zero = dequantize_s4_to_fp16x2(zeros_loaded);",
      "  uint4 B_loaded_scale = *(uint4*)(scaling_factors_ptr2);",
      "",
      "  uint32_t B_loaded = *(uint32_t*)B_ptr2;",
      "  uint4 B_loaded_fp16 = dequantize_s4_to_fp16x2(B_loaded);",
      "  asm volatile(\"sub.f16x2 %0, %1, %2;\\n\"",
      "               : \"=r\"(B_loaded_fp16.x)",
      "               : \"r\"(B_loaded_fp16.x), \"r\"(B_loaded_zero.x));",
      "  asm volatile(\"fma.rn.f16x2 %0, %1, %2, %3;\\n\"",
      "               : \"=r\"(B_loaded_fp16.x)",
      "               : \"r\"(B_loaded_fp16.x), \"r\"(B_loaded_scale.x), \"r\"(ZERO));",
      "  asm volatile(\"sub.f16x2 %0, %1, %2;\\n\"",
      "               : \"=r\"(B_loaded_fp16.y)",
      "               : \"r\"(B_loaded_fp16.y), \"r\"(B_loaded_zero.y));",
      "  asm volatile(\"fma.rn.f16x2 %0, %1, %2, %3;\\n\"",
      "               : \"=r\"(B_loaded_fp16.y)",
      "               : \"r\"(B_loaded_fp16.y), \"r\"(B_loaded_scale.y), \"r\"(ZERO));",
      "  asm volatile(\"sub.f16x2 %0, %1, %2;\\n\"",
      "               : \"=r\"(B_loaded_fp16.z)",
      "               : \"r\"(B_loaded_fp16.z), \"r\"(B_loaded_zero.z));",
      "  asm volatile(\"fma.rn.f16x2 %0, %1, %2, %3;\\n\"",
      "               : \"=r\"(B_loaded_fp16.z)",
      "               : \"r\"(B_loaded_fp16.z), \"r\"(B_loaded_scale.z), \"r\"(ZERO));",
      "  asm volatile(\"sub.f16x2 %0, %1, %2;\\n\"",
      "               : \"=r\"(B_loaded_fp16.w)",
      "               : \"r\"(B_loaded_fp16.w), \"r\"(B_loaded_zero.w));",
      "  asm volatile(\"fma.rn.f16x2 %0, %1, %2, %3;\\n\"",
      "               : \"=r\"(B_loaded_fp16.w)",
      "               : \"r\"(B_loaded_fp16.w), \"r\"(B_loaded_scale.w), \"r\"(ZERO));",
      "",
      "  *(uint4*)B_shared_ptr2 = B_loaded_fp16;",
      "",
      "  for (int i = 0; i < 8; ++i) {",
      "    *(C_ptr2 + i) = B_shared[i];",
      "  }",
      "}",
      "",
      "}  // namespace awq",
      "}  // namespace vllm",
      "",
      "torch::Tensor awq_dequantize(torch::Tensor _kernel,",
      "                             torch::Tensor _scaling_factors,",
      "                             torch::Tensor _zeros, int64_t split_k_iters,",
      "                             int64_t thx, int64_t thy) {",
      "  int in_c = _kernel.size(0);",
      "  int qout_c = _kernel.size(1);",
      "  int out_c = qout_c * 8;",
      "  int G = in_c / _scaling_factors.size(0);",
      "",
      "  int x_thread = thx;",
      "  int y_thread = thy;",
      "",
      "  int x_blocks = 1;",
      "  int y_blocks = 1;",
      "  if (thx == 0) {",
      "    x_thread = qout_c;",
      "  }",
      "  if (thy == 0) {",
      "    y_thread = in_c;",
      "  }",
      "  if (thx == 0 && thy == 0) {",
      "    x_thread = 8;",
      "    y_thread = 8;",
      "    x_blocks = (int)(qout_c / 8);",
      "    y_blocks = (int)(in_c / 8);",
      "  }",
      "",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(_scaling_factors));",
      "",
      "  auto options = torch::TensorOptions()",
      "                     .dtype(_scaling_factors.dtype())",
      "                     .device(_scaling_factors.device());",
      "  at::Tensor _de_kernel = torch::empty({in_c, out_c}, options);",
      "",
      "  auto kernel = reinterpret_cast<int*>(_kernel.data_ptr<int>());",
      "  auto de_kernel = reinterpret_cast<half*>(_de_kernel.data_ptr<at::Half>());",
      "  auto scaling_factors =",
      "      reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());",
      "  auto zeros = reinterpret_cast<int*>(_zeros.data_ptr<int>());",
      "",
      "  dim3 num_blocks(x_blocks, y_blocks);",
      "  dim3 threads_per_block(x_thread, y_thread);",
      "",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  vllm::awq::dequantize_weights<<<num_blocks, threads_per_block, 0, stream>>>(",
      "      kernel, scaling_factors, zeros, de_kernel, G);",
      "",
      "  return _de_kernel;",
      "}",
      "",
      "// in_feats: M, IC [float16]",
      "// kernel: IC, OC // 8 [int32] -> cast to IC, OC [uint4b]",
      "// scaling_factors: IC // G, OC [float16]",
      "// zeros: IC // G, OC // 8 [int32] -> cast to IC // G, OC [uint4b]",
      "// assume that batch_size < 16 for now",
      "",
      "torch::Tensor awq_gemm(torch::Tensor _in_feats, torch::Tensor _kernel,",
      "                       torch::Tensor _scaling_factors, torch::Tensor _zeros,",
      "                       int64_t split_k_iters) {",
      "  int num_in_feats = _in_feats.size(0);",
      "  int num_in_channels = _in_feats.size(1);",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(_in_feats));",
      "",
      "  auto options = torch::TensorOptions()",
      "                     .dtype(_in_feats.dtype())",
      "                     .device(_in_feats.device());",
      "  at::Tensor _out_feats =",
      "      torch::empty({split_k_iters, num_in_feats, _kernel.size(1) * 8}, options);",
      "  int num_out_feats = _out_feats.size(-2);",
      "  int num_out_channels = _out_feats.size(-1);",
      "",
      "  auto in_feats = reinterpret_cast<half*>(_in_feats.data_ptr<at::Half>());",
      "  auto kernel = reinterpret_cast<int*>(_kernel.data_ptr<int>());",
      "  auto out_feats = reinterpret_cast<half*>(_out_feats.data_ptr<at::Half>());",
      "  auto scaling_factors =",
      "      reinterpret_cast<half*>(_scaling_factors.data_ptr<at::Half>());",
      "  auto zeros = reinterpret_cast<int*>(_zeros.data_ptr<int>());",
      "  int group_size = num_in_channels / _scaling_factors.size(0);",
      "",
      "  if (num_out_channels % 64 != 0)",
      "    throw std::invalid_argument(\"OC is not multiple of cta_N = 64\");",
      "  if (num_out_channels % 8 != 0)",
      "    throw std::invalid_argument(\"OC is not multiple of pack_num = 8\");",
      "  if (group_size % 32 != 0)",
      "    throw std::invalid_argument(\"Group size should be a multiple of 32\");",
      "  if (num_out_channels % group_size != 0)",
      "    throw std::invalid_argument(\"OC is not multiple of Group size\");",
      "",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  if (num_out_channels % 128 == 0) {",
      "    int j_factors1 = num_out_channels / 128 / 1;",
      "    dim3 num_blocks((num_out_feats + 16 - 1) / 16 * j_factors1 * split_k_iters);",
      "    // threadIdx.x: 32",
      "    // threadIdx.y: i_factors[2] * j_factors[2]",
      "    dim3 threads_per_block(32, 2);",
      "    vllm::awq::gemm_forward_4bit_cuda_m16nXk32<128>",
      "        <<<num_blocks, threads_per_block, 0, stream>>>(",
      "            group_size, split_k_iters, in_feats, kernel, scaling_factors, zeros,",
      "            num_in_feats, num_in_channels, num_out_channels, out_feats);",
      "  } else if (num_out_channels % 64 == 0) {",
      "    int j_factors1 = num_out_channels / 64 / 1;",
      "    dim3 num_blocks(1 * (num_out_feats + 16 - 1) / 16 * j_factors1 *",
      "                    split_k_iters);",
      "",
      "    // threadIdx.x: 32",
      "    // threadIdx.y: i_factors[2] * j_factors[2]",
      "    dim3 threads_per_block(32, 2);",
      "    vllm::awq::gemm_forward_4bit_cuda_m16nXk32<64>",
      "        <<<num_blocks, threads_per_block, 0, stream>>>(",
      "            group_size, split_k_iters, in_feats, kernel, scaling_factors, zeros,",
      "            num_in_feats, num_in_channels, num_out_channels, out_feats);",
      "  }",
      "  return _out_feats.sum(0);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/awq/dequantize.cuh",
    "source": [
      "/*",
      "Adapted from https://github.com/mit-han-lab/llm-awq",
      "Modified from NVIDIA FasterTransformer:",
      "https://github.com/NVIDIA/FasterTransformer/blob/main/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/interleaved_numeric_conversion.h",
      "@article{lin2023awq,",
      "  title={AWQ: Activation-aware Weight Quantization for LLM Compression and",
      "Acceleration}, author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang,",
      "Shang and Dang, Xingyu and Han, Song}, journal={arXiv}, year={2023}",
      "}",
      "*/",
      "",
      "#pragma once",
      "",
      "namespace vllm {",
      "namespace awq {",
      "",
      "__device__ uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 750",
      "  assert(false);",
      "#else",
      "  uint4 result;",
      "",
      "  uint32_t* h = reinterpret_cast<uint32_t*>(&result);",
      "  uint32_t const i4s = reinterpret_cast<uint32_t const&>(source);",
      "",
      "  // First, we extract the i4s and construct an intermediate fp16 number.",
      "  static constexpr uint32_t immLut = (0xf0 & 0xcc) | 0xaa;",
      "  static constexpr uint32_t BOTTOM_MASK = 0x000f000f;",
      "  static constexpr uint32_t TOP_MASK = 0x00f000f0;",
      "  static constexpr uint32_t I4s_TO_F16s_MAGIC_NUM = 0x64006400;",
      "",
      "  // Note that the entire sequence only requires 1 shift instruction. This is",
      "  // thanks to the register packing format and the fact that we force our",
      "  // integers to be unsigned, and account for this in the fp16 subtractions. In",
      "  // addition, I exploit the fact that sub and fma have the same throughput in",
      "  // order to convert elt_23 and elt_67 to fp16 without having to shift them to",
      "  // the bottom bits before hand.",
      "",
      "  // Shift right by 8 to now consider elt_45 and elt_67. Issue first to hide RAW",
      "  // dependency if we issue immediately before required.",
      "  const uint32_t top_i4s = i4s >> 8;",
      "  // Extract elt_01 - (i4s & 0x000f000f) | 0x64006400",
      "  asm volatile(\"lop3.b32 %0, %1, %2, %3, %4;\\n\"",
      "               : \"=r\"(h[0])",
      "               : \"r\"(i4s), \"n\"(BOTTOM_MASK), \"n\"(I4s_TO_F16s_MAGIC_NUM),",
      "                 \"n\"(immLut));",
      "  // Extract elt_23 (i4s & 0x00f000f0) | 0x64006400",
      "  asm volatile(\"lop3.b32 %0, %1, %2, %3, %4;\\n\"",
      "               : \"=r\"(h[1])",
      "               : \"r\"(i4s), \"n\"(TOP_MASK), \"n\"(I4s_TO_F16s_MAGIC_NUM),",
      "                 \"n\"(immLut));",
      "  // Extract elt_45 (top_i4s & 0x000f000f) | 0x64006400",
      "  asm volatile(\"lop3.b32 %0, %1, %2, %3, %4;\\n\"",
      "               : \"=r\"(h[2])",
      "               : \"r\"(top_i4s), \"n\"(BOTTOM_MASK), \"n\"(I4s_TO_F16s_MAGIC_NUM),",
      "                 \"n\"(immLut));",
      "  // Extract elt_67 (top_i4s & 0x00f000f0) | 0x64006400",
      "  asm volatile(\"lop3.b32 %0, %1, %2, %3, %4;\\n\"",
      "               : \"=r\"(h[3])",
      "               : \"r\"(top_i4s), \"n\"(TOP_MASK), \"n\"(I4s_TO_F16s_MAGIC_NUM),",
      "                 \"n\"(immLut));",
      "",
      "  // I use inline PTX below because I am not sure if the compiler will emit",
      "  // float2half instructions if I use the half2 ctor. In this case, I chose",
      "  // performance reliability over code readability.",
      "",
      "  // This is the half2 {1032, 1032} represented as an integer.",
      "  // static constexpr uint32_t FP16_TOP_MAGIC_NUM = 0x64086408;",
      "  // Haotian: subtract {1024, 1024} instead, we do not need to map to [-8, 7]",
      "  static constexpr uint32_t FP16_TOP_MAGIC_NUM = 0x64006400;",
      "  // This is the half2 {1 / 16, 1 / 16} represented as an integer.",
      "  static constexpr uint32_t ONE_SIXTEENTH = 0x2c002c00;",
      "  // This is the half2 {-72, -72} represented as an integer.",
      "  // static constexpr uint32_t NEG_72 = 0xd480d480;",
      "  // Haotian: Let's use {-64, -64}.",
      "  static constexpr uint32_t NEG_64 = 0xd400d400;",
      "",
      "  // Finally, we construct the output numbers.",
      "  // Convert elt_01",
      "  asm volatile(\"sub.f16x2 %0, %1, %2;\\n\"",
      "               : \"=r\"(h[0])",
      "               : \"r\"(h[0]), \"r\"(FP16_TOP_MAGIC_NUM));",
      "  // Convert elt_23",
      "  asm volatile(\"fma.rn.f16x2 %0, %1, %2, %3;\\n\"",
      "               : \"=r\"(h[1])",
      "               : \"r\"(h[1]), \"r\"(ONE_SIXTEENTH), \"r\"(NEG_64));",
      "  // Convert elt_45",
      "  asm volatile(\"sub.f16x2 %0, %1, %2;\\n\"",
      "               : \"=r\"(h[2])",
      "               : \"r\"(h[2]), \"r\"(FP16_TOP_MAGIC_NUM));",
      "  // Convert elt_67",
      "  asm volatile(\"fma.rn.f16x2 %0, %1, %2, %3;\\n\"",
      "               : \"=r\"(h[3])",
      "               : \"r\"(h[3]), \"r\"(ONE_SIXTEENTH), \"r\"(NEG_64));",
      "",
      "  return result;",
      "#endif",
      "  __builtin_unreachable();  // Suppress missing return statement warning",
      "}",
      "",
      "}  // namespace awq",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/compressed_tensors/int8_quant_kernels.cu",
    "source": [
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <torch/all.h>",
      "",
      "#ifndef USE_ROCM",
      "  #include \"../per_token_group_quant_8bit.h\"",
      "#endif",
      "",
      "#include <cmath>",
      "",
      "#include \"../../dispatch_utils.h\"",
      "#include \"../vectorization_utils.cuh\"",
      "",
      "#ifndef USE_ROCM",
      "  #include <cub/cub.cuh>",
      "  #include <cub/util_type.cuh>",
      "#else",
      "  #include <hipcub/hipcub.hpp>",
      "  #include <hipcub/util_type.hpp>",
      "#endif",
      "",
      "static inline __device__ int8_t float_to_int8_rn(float x) {",
      "#ifdef USE_ROCM",
      "  static constexpr auto i8_min =",
      "      static_cast<float>(std::numeric_limits<int8_t>::min());",
      "  static constexpr auto i8_max =",
      "      static_cast<float>(std::numeric_limits<int8_t>::max());",
      "",
      "  // To match the rounding mode of CUDA, we use nearbyint.",
      "  // It uses the current rounding mode, which is always FE_TONEAREST on HIP.",
      "  // If that changes in the future, we may need to set the rounding mode",
      "  // explicitly, either at runtime or compile time.",
      "  float dst = std::nearbyint(x);",
      "",
      "  // saturate",
      "",
      "  // See https://github.com/pytorch/pytorch/issues/127666",
      "  // See https://github.com/llvm/llvm-project/issues/95183",
      "  // hip-clang std::clamp __glibcxx_assert_fail host function when building on",
      "  // Arch/gcc14. The following replaces std::clamp usage with similar logic",
      "  // dst = std::clamp(dst, i8_min, i8_max);",
      "  dst = (dst < i8_min) ? i8_min : (dst > i8_max) ? i8_max : dst;",
      "  return static_cast<int8_t>(dst);",
      "#else",
      "  // CUDA path",
      "  uint32_t dst;",
      "  asm volatile(\"cvt.rni.sat.s8.f32 %0, %1;\" : \"=r\"(dst) : \"f\"(x));",
      "  return reinterpret_cast<const int8_t&>(dst);",
      "#endif",
      "}",
      "",
      "static inline __device__ int32_t float_to_int32_rn(float x) {",
      "#ifdef USE_ROCM",
      "  // int32_max is not exactly representable as float.",
      "  // Therefore, we need to be careful and manually return int32_max on overflow.",
      "  // For symmetry, we also do the same for int32_min, even though it is exactly",
      "  // representable as float and the conversion should be exact.",
      "  static constexpr auto i32_min = std::numeric_limits<int32_t>::min();",
      "  static constexpr auto i32_min_f = static_cast<float>(i32_min);",
      "  static constexpr auto i32_max = std::numeric_limits<int32_t>::max();",
      "  static constexpr auto i32_max_f = static_cast<float>(i32_max);",
      "",
      "  // To match the rounding mode of CUDA, we use nearbyint.",
      "  // It uses the current rounding mode, which is always FE_TONEAREST on HIP.",
      "  // If that changes in the future, we may need to set the rounding mode",
      "  // explicitly, either at runtime or compile time.",
      "  float dst = std::nearbyint(x);",
      "",
      "  // saturate on the higher end.",
      "  if (dst >= i32_max_f) {",
      "    return i32_max;",
      "  }",
      "  // saturate on the lower end.",
      "  if (dst <= i32_min_f) {",
      "    return i32_min;",
      "  }",
      "",
      "  return static_cast<int32_t>(dst);",
      "#else",
      "  // CUDA path",
      "  uint32_t dst;",
      "  asm volatile(\"cvt.rni.sat.s32.f32 %0, %1;\" : \"=r\"(dst) : \"f\"(x));",
      "  return reinterpret_cast<const int32_t&>(dst);",
      "#endif",
      "}",
      "",
      "static inline __device__ int8_t int32_to_int8(int32_t x) {",
      "#ifdef USE_ROCM",
      "  static constexpr auto i8_min =",
      "      static_cast<int32_t>(std::numeric_limits<int8_t>::min());",
      "  static constexpr auto i8_max =",
      "      static_cast<int32_t>(std::numeric_limits<int8_t>::max());",
      "",
      "  // saturate",
      "",
      "  // See https://github.com/pytorch/pytorch/issues/127666",
      "  // See https://github.com/llvm/llvm-project/issues/95183",
      "  // hip-clang std::clamp __glibcxx_assert_fail host function when building on",
      "  // Arch/gcc14. The following replaces std::clamp usage with similar logic",
      "  // int32_t dst = std::clamp(x, i8_min, i8_max);",
      "  int32_t dst = (x < i8_min) ? i8_min : (x > i8_max) ? i8_max : x;",
      "  return static_cast<int8_t>(dst);",
      "#else",
      "  // CUDA path",
      "  uint32_t dst;",
      "  asm volatile(\"cvt.sat.s8.s32 %0, %1;\" : \"=r\"(dst) : \"r\"(x));",
      "  return reinterpret_cast<const int8_t&>(dst);",
      "#endif",
      "}",
      "",
      "namespace vllm {",
      "",
      "template <typename scalar_t, typename scale_t>",
      "__global__ void static_scaled_int8_quant_kernel(",
      "    const scalar_t* __restrict__ input, int8_t* __restrict__ output,",
      "    const scale_t* scale_ptr, const int hidden_size) {",
      "  const int tid = threadIdx.x;",
      "  const int stride = blockDim.x;",
      "  const int64_t token_idx = blockIdx.x;",
      "  const float scale = *scale_ptr;",
      "",
      "  // Must be performed using 64-bit math to avoid integer overflow.",
      "  const scalar_t* row_in = input + token_idx * hidden_size;",
      "  int8_t* row_out = output + token_idx * hidden_size;",
      "",
      "  vectorize_with_alignment<16>(",
      "      row_in, row_out, hidden_size, tid, stride,",
      "      [=] __device__(int8_t& dst, const scalar_t& src) {",
      "        dst = float_to_int8_rn(static_cast<float>(src) / scale);",
      "      });",
      "}",
      "",
      "template <typename scalar_t, typename scale_t, typename azp_t>",
      "__global__ void static_scaled_int8_azp_quant_kernel(",
      "    const scalar_t* __restrict__ input, int8_t* __restrict__ output,",
      "    const scale_t* scale_ptr, const azp_t* azp_ptr, const int hidden_size) {",
      "  const int tid = threadIdx.x;",
      "  const int stride = blockDim.x;",
      "  const int64_t token_idx = blockIdx.x;",
      "  const float scale = *scale_ptr;",
      "  const azp_t azp = *azp_ptr;",
      "  const float inv_s = 1.0f / scale;",
      "",
      "  // Must be performed using 64-bit math to avoid integer overflow.",
      "  const scalar_t* row_in = input + token_idx * hidden_size;",
      "  int8_t* row_out = output + token_idx * hidden_size;",
      "",
      "  vectorize_with_alignment<16>(",
      "      row_in, row_out, hidden_size, tid, stride,",
      "      [=] __device__(int8_t& dst, const scalar_t& src) {",
      "        const auto v = static_cast<float>(src) * inv_s;",
      "        dst = int32_to_int8(float_to_int32_rn(v) + azp);",
      "      });",
      "}",
      "",
      "template <typename scalar_t, typename scale_t>",
      "__global__ void dynamic_scaled_int8_quant_kernel(",
      "    const scalar_t* __restrict__ input, int8_t* __restrict__ output,",
      "    scale_t* scale_out, const int hidden_size) {",
      "  const int tid = threadIdx.x;",
      "  const int stride = blockDim.x;",
      "  const int64_t token_idx = blockIdx.x;",
      "",
      "  // Must be performed using 64-bit math to avoid integer overflow.",
      "  const scalar_t* row_in = input + token_idx * hidden_size;",
      "  int8_t* row_out = output + token_idx * hidden_size;",
      "",
      "  // calculate for absmax",
      "  float thread_max = 0.f;",
      "  vectorize_read_with_alignment<16>(",
      "      row_in, hidden_size, tid, stride, [&] __device__(const scalar_t& src) {",
      "        const float v = fabsf(static_cast<float>(src));",
      "        thread_max = fmaxf(thread_max, v);",
      "      });",
      "  using BlockReduce = cub::BlockReduce<float, 256>;",
      "  __shared__ typename BlockReduce::TempStorage tmp;",
      "  float block_max = BlockReduce(tmp).Reduce(thread_max, cub::Max{}, blockDim.x);",
      "  __shared__ float absmax;",
      "  if (tid == 0) {",
      "    absmax = block_max;",
      "    scale_out[blockIdx.x] = absmax / 127.f;",
      "  }",
      "  __syncthreads();",
      "",
      "  float inv_s = (absmax == 0.f) ? 0.f : 127.f / absmax;",
      "",
      "  // 2. quantize",
      "  vectorize_with_alignment<16>(",
      "      row_in, row_out, hidden_size, tid, stride,",
      "      [=] __device__(int8_t& dst, const scalar_t& src) {",
      "        dst = float_to_int8_rn(static_cast<float>(src) * inv_s);",
      "      });",
      "}",
      "",
      "// MinMax structure to hold min and max values in one go",
      "struct MinMax {",
      "  float min, max;",
      "",
      "  __host__ __device__ MinMax()",
      "      : min(std::numeric_limits<float>::max()),",
      "        max(std::numeric_limits<float>::lowest()) {}",
      "",
      "  __host__ __device__ explicit MinMax(float v) : min(v), max(v) {}",
      "",
      "  // add a value to the MinMax",
      "  __host__ __device__ MinMax& operator+=(float v) {",
      "    min = fminf(min, v);",
      "    max = fmaxf(max, v);",
      "    return *this;",
      "  }",
      "",
      "  // merge two MinMax objects",
      "  __host__ __device__ MinMax& operator&=(const MinMax& other) {",
      "    min = fminf(min, other.min);",
      "    max = fmaxf(max, other.max);",
      "    return *this;",
      "  }",
      "};",
      "",
      "__host__ __device__ inline MinMax operator+(MinMax a, float v) {",
      "  return a += v;",
      "}",
      "__host__ __device__ inline MinMax operator&(MinMax a, const MinMax& b) {",
      "  return a &= b;",
      "}",
      "",
      "template <typename scalar_t, typename scale_t, typename azp_t>",
      "__global__ void dynamic_scaled_int8_azp_quant_kernel(",
      "    const scalar_t* __restrict__ input, int8_t* __restrict__ output,",
      "    scale_t* scale_out, azp_t* azp_out, const int hidden_size) {",
      "  const int tid = threadIdx.x;",
      "  const int stride = blockDim.x;",
      "  const int64_t token_idx = blockIdx.x;",
      "",
      "  // Must be performed using 64-bit math to avoid integer overflow.",
      "  const scalar_t* row_in = input + token_idx * hidden_size;",
      "  int8_t* row_out = output + token_idx * hidden_size;",
      "",
      "  // 1. calculate min & max",
      "  MinMax thread_mm;",
      "  vectorize_read_with_alignment<16>(row_in, hidden_size, tid, stride,",
      "                                    [&] __device__(const scalar_t& src) {",
      "                                      thread_mm += static_cast<float>(src);",
      "                                    });",
      "",
      "  using BlockReduce = cub::BlockReduce<MinMax, 256>;",
      "  __shared__ typename BlockReduce::TempStorage tmp;",
      "",
      "  MinMax mm = BlockReduce(tmp).Reduce(",
      "      thread_mm,",
      "      [] __device__(MinMax a, const MinMax& b) {",
      "        a &= b;",
      "        return a;",
      "      },",
      "      blockDim.x);",
      "",
      "  __shared__ float scale_sh;",
      "  __shared__ azp_t azp_sh;",
      "  if (tid == 0) {",
      "    float s = (mm.max - mm.min) / 255.f;",
      "    float zp = nearbyintf(-128.f - mm.min / s);  // round-to-even",
      "    scale_sh = s;",
      "    azp_sh = azp_t(zp);",
      "    scale_out[blockIdx.x] = s;",
      "    azp_out[blockIdx.x] = azp_sh;",
      "  }",
      "  __syncthreads();",
      "",
      "  const float inv_s = 1.f / scale_sh;",
      "  const azp_t azp = azp_sh;",
      "",
      "  // 2. quantize",
      "  vectorize_with_alignment<16>(",
      "      row_in, row_out, hidden_size, tid, stride,",
      "      [=] __device__(int8_t& dst, const scalar_t& src) {",
      "        const auto v = static_cast<float>(src) * inv_s;",
      "        dst = int32_to_int8(float_to_int32_rn(v) + azp);",
      "      });",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "void static_scaled_int8_quant(torch::Tensor& out,          // [..., hidden_size]",
      "                              torch::Tensor const& input,  // [..., hidden_size]",
      "                              torch::Tensor const& scale,",
      "                              std::optional<torch::Tensor> const& azp) {",
      "  TORCH_CHECK(input.is_contiguous());",
      "  TORCH_CHECK(out.is_contiguous());",
      "  TORCH_CHECK(scale.numel() == 1);",
      "  TORCH_CHECK(!azp || azp->numel() == 1);",
      "",
      "  int const hidden_size = input.size(-1);",
      "  int const num_tokens = input.numel() / hidden_size;",
      "  dim3 const grid(num_tokens);",
      "  dim3 const block(std::min(hidden_size, 256));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  VLLM_DISPATCH_FLOATING_TYPES(",
      "      input.scalar_type(), \"static_scaled_int8_quant_kernel\", [&] {",
      "        if (!azp) {",
      "          vllm::static_scaled_int8_quant_kernel<scalar_t, float>",
      "              <<<grid, block, 0, stream>>>(",
      "                  input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),",
      "                  scale.data_ptr<float>(), hidden_size);",
      "        } else {",
      "          vllm::static_scaled_int8_azp_quant_kernel<scalar_t, float, int32_t>",
      "              <<<grid, block, 0, stream>>>(",
      "                  input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),",
      "                  scale.data_ptr<float>(), azp->data_ptr<int32_t>(),",
      "                  hidden_size);",
      "        }",
      "      });",
      "}",
      "",
      "void dynamic_scaled_int8_quant(",
      "    torch::Tensor& out,          // [..., hidden_size]",
      "    torch::Tensor const& input,  // [..., hidden_size]",
      "    torch::Tensor& scales, std::optional<torch::Tensor> const& azp) {",
      "  TORCH_CHECK(input.is_contiguous());",
      "  TORCH_CHECK(out.is_contiguous());",
      "  TORCH_CHECK(scales.is_contiguous());",
      "  TORCH_CHECK(!azp || azp->is_contiguous());",
      "",
      "  int const hidden_size = input.size(-1);",
      "  int const num_tokens = input.numel() / hidden_size;",
      "  dim3 const grid(num_tokens);",
      "  dim3 const block(std::min(hidden_size, 256));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  VLLM_DISPATCH_FLOATING_TYPES(",
      "      input.scalar_type(), \"dynamic_scaled_int8_quant_kernel\", [&] {",
      "        if (!azp) {",
      "          vllm::dynamic_scaled_int8_quant_kernel<scalar_t, float>",
      "              <<<grid, block, 0, stream>>>(",
      "                  input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),",
      "                  scales.data_ptr<float>(), hidden_size);",
      "        } else {",
      "          vllm::dynamic_scaled_int8_azp_quant_kernel<scalar_t, float, int32_t>",
      "              <<<grid, block, 0, stream>>>(",
      "                  input.data_ptr<scalar_t>(), out.data_ptr<int8_t>(),",
      "                  scales.data_ptr<float>(), azp->data_ptr<int32_t>(),",
      "                  hidden_size);",
      "        }",
      "      });",
      "}",
      "",
      "#ifndef USE_ROCM",
      "void per_token_group_quant_int8(const torch::Tensor& input,",
      "                                torch::Tensor& output_q,",
      "                                torch::Tensor& output_s, int64_t group_size,",
      "                                double eps, double int8_min, double int8_max) {",
      "  per_token_group_quant_8bit(input, output_q, output_s, group_size, eps,",
      "                             int8_min, int8_max);",
      "}",
      "#endif"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq_marlin/marlin_dtypes.cuh",
    "source": [
      "",
      "#ifndef _data_types_cuh",
      "#define _data_types_cuh",
      "#include \"marlin.cuh\"",
      "#include <cuda_fp16.h>",
      "#include <cuda_bf16.h>",
      "",
      "#ifndef MARLIN_NAMESPACE_NAME",
      "  #define MARLIN_NAMESPACE_NAME marlin",
      "#endif",
      "",
      "namespace MARLIN_NAMESPACE_NAME {",
      "",
      "template <typename scalar_t>",
      "class ScalarType {};",
      "",
      "template <>",
      "class ScalarType<half> {",
      " public:",
      "  using scalar_t = half;",
      "  using scalar_t2 = half2;",
      "",
      "  // Matrix fragments for tensor core instructions; their precise layout is",
      "  // documented here:",
      "  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#matrix-fragments-for-mma-m16n8k16-with-floating-point-type",
      "  using FragA = Vec<half2, 4>;",
      "  using FragB = Vec<half2, 2>;",
      "  using FragC = Vec<float, 4>;",
      "  using FragS = Vec<half2, 1>;",
      "  using FragZP = Vec<half2, 4>;",
      "",
      "  static __device__ float inline num2float(const half x) {",
      "    return __half2float(x);",
      "  }",
      "",
      "  static __device__ half2 inline num2num2(const half x) {",
      "    return __half2half2(x);",
      "  }",
      "",
      "  static __device__ half2 inline nums2num2(const half x1, const half x2) {",
      "    return __halves2half2(x1, x2);",
      "  }",
      "",
      "  static __host__ __device__ half inline float2num(const float x) {",
      "    return __float2half(x);",
      "  }",
      "};",
      "",
      "template <>",
      "class ScalarType<nv_bfloat16> {",
      " public:",
      "  using scalar_t = nv_bfloat16;",
      "  using scalar_t2 = nv_bfloat162;",
      "",
      "  using FragA = Vec<nv_bfloat162, 4>;",
      "  using FragB = Vec<nv_bfloat162, 2>;",
      "  using FragC = Vec<float, 4>;",
      "  using FragS = Vec<nv_bfloat162, 1>;",
      "  using FragZP = Vec<nv_bfloat162, 4>;",
      "",
      "#if !defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 800",
      "  static __device__ float inline num2float(const nv_bfloat16 x) {",
      "    return __bfloat162float(x);",
      "  }",
      "",
      "  static __device__ nv_bfloat162 inline num2num2(const nv_bfloat16 x) {",
      "    return __bfloat162bfloat162(x);",
      "  }",
      "",
      "  static __device__ nv_bfloat162 inline nums2num2(const nv_bfloat16 x1,",
      "                                                  const nv_bfloat16 x2) {",
      "    return __halves2bfloat162(x1, x2);",
      "  }",
      "",
      "  static __host__ __device__ nv_bfloat16 inline float2num(const float x) {",
      "    return __float2bfloat16(x);",
      "  }",
      "#endif",
      "};",
      "",
      "}  // namespace MARLIN_NAMESPACE_NAME",
      "",
      "#endif"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq_marlin/gptq_marlin.cu",
    "source": [
      "/*",
      " * Modified by Neural Magic",
      " * Copyright (C) Marlin.2024 Elias Frantar",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *         http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "",
      "/*",
      " * Adapted from https://github.com/IST-DASLab/marlin",
      " */",
      "",
      "#ifndef MARLIN_NAMESPACE_NAME",
      "  #define MARLIN_NAMESPACE_NAME marlin",
      "#endif",
      "",
      "#include \"kernel.h\"",
      "#include \"core/registration.h\"",
      "",
      "#define STATIC_ASSERT_SCALAR_TYPE_VALID(scalar_t)               \\",
      "  static_assert(std::is_same<scalar_t, half>::value ||          \\",
      "                    std::is_same<scalar_t, nv_bfloat16>::value, \\",
      "                \"only float16 and bfloat16 is supported\");",
      "",
      "namespace marlin {",
      "",
      "__global__ void MarlinDefault(MARLIN_KERNEL_PARAMS){};",
      "",
      "using MarlinFuncPtr = void (*)(MARLIN_KERNEL_PARAMS);",
      "",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "",
      "__global__ void permute_cols_kernel(int4 const* __restrict__ a_int4_ptr,",
      "                                    int const* __restrict__ perm_int_ptr,",
      "                                    int4* __restrict__ out_int4_ptr, int size_m,",
      "                                    int size_k, int lda, int block_rows) {}",
      "",
      "}  // namespace marlin",
      "",
      "torch::Tensor gptq_marlin_gemm(",
      "    torch::Tensor& a, std::optional<torch::Tensor> c_or_none,",
      "    torch::Tensor& b_q_weight,",
      "    std::optional<torch::Tensor> const& b_bias_or_none, torch::Tensor& b_scales,",
      "    std::optional<torch::Tensor> const& b_zeros_or_none,",
      "    std::optional<torch::Tensor> const& g_idx_or_none,",
      "    std::optional<torch::Tensor> const& perm_or_none, torch::Tensor& workspace,",
      "    vllm::ScalarTypeId const& b_q_type_id, int64_t size_m, int64_t size_n,",
      "    int64_t size_k, bool is_k_full, bool use_atomic_add, bool use_fp32_reduce,",
      "    bool is_zp_float) {",
      "  TORCH_CHECK_NOT_IMPLEMENTED(false,",
      "                              \"marlin_gemm(..) requires CUDA_ARCH >= 8.0\");",
      "  return torch::empty({1, 1});",
      "}",
      "",
      "#else",
      "",
      "// For a given \"a\" of size [M,K] performs a permutation of the K columns based",
      "// on the given \"perm\" indices.",
      "__global__ void permute_cols_kernel(int4 const* __restrict__ a_int4_ptr,",
      "                                    int const* __restrict__ perm_int_ptr,",
      "                                    int4* __restrict__ out_int4_ptr, int size_m,",
      "                                    int size_k, int lda, int block_rows) {",
      "  auto start_row = block_rows * blockIdx.x;",
      "  int finish_row = start_row + block_rows;",
      "  if (finish_row > size_m) {",
      "    finish_row = size_m;",
      "  }",
      "  int cur_block_rows = finish_row - start_row;",
      "",
      "  int input_row_stride = lda * sizeof(half) / 16;",
      "  int output_row_stride = size_k * sizeof(half) / 16;",
      "",
      "  auto permute_row = [&](int row) {",
      "    int iters = size_k / default_threads;",
      "    int rest = size_k % default_threads;",
      "",
      "    int input_offset = row * input_row_stride;",
      "    int output_offset = row * output_row_stride;",
      "",
      "    half const* a_row_half =",
      "        reinterpret_cast<half const*>(a_int4_ptr + input_offset);",
      "    half* out_half = reinterpret_cast<half*>(out_int4_ptr + output_offset);",
      "",
      "    int base_k = 0;",
      "",
      "    for (int i = 0; i < iters; i++) {",
      "      auto cur_k = base_k + threadIdx.x;",
      "      int src_pos = perm_int_ptr[cur_k];",
      "",
      "      out_half[cur_k] = a_row_half[src_pos];",
      "",
      "      base_k += default_threads;",
      "    }",
      "",
      "    if (rest) {",
      "      if (threadIdx.x < rest) {",
      "        auto cur_k = base_k + threadIdx.x;",
      "        int src_pos = perm_int_ptr[cur_k];",
      "",
      "        out_half[cur_k] = a_row_half[src_pos];",
      "      }",
      "    }",
      "  };",
      "",
      "  for (int i = 0; i < cur_block_rows; i++) {",
      "    int cur_row = start_row + i;",
      "    if (cur_row < size_m) {",
      "      permute_row(cur_row);",
      "    }",
      "  }",
      "}",
      "",
      "typedef struct {",
      "  int thread_k;",
      "  int thread_n;",
      "  int num_threads;",
      "} thread_config_t;",
      "",
      "thread_config_t small_batch_thread_configs[] = {",
      "    // Ordered by priority",
      "",
      "    // thread_k, thread_n, num_threads",
      "    {128, 128, 256},",
      "    {64, 128, 128},",
      "    {128, 64, 128}};",
      "",
      "thread_config_t large_batch_thread_configs[] = {",
      "    // Ordered by priority",
      "",
      "    // thread_k, thread_n, num_threads",
      "    {64, 256, 256},",
      "    {64, 128, 128},",
      "    {128, 64, 128}};",
      "",
      "typedef struct {",
      "  int blocks_per_sm;",
      "  thread_config_t tb_cfg;",
      "} exec_config_t;",
      "",
      "int get_scales_cache_size(thread_config_t const& th_config, int prob_m,",
      "                          int prob_n, int prob_k, int num_bits, int group_size,",
      "                          bool has_act_order, bool is_k_full) {",
      "  bool cache_scales_chunk = has_act_order && !is_k_full;",
      "",
      "  int tb_n = th_config.thread_n;",
      "  int tb_k = th_config.thread_k;",
      "",
      "  // Get max scale groups per thread-block",
      "  int tb_groups;",
      "  if (group_size == -1) {",
      "    tb_groups = 1;",
      "  } else if (group_size == 0) {",
      "    tb_groups = div_ceil(tb_k, 32);  // Worst case is 32 group size",
      "  } else {",
      "    tb_groups = div_ceil(tb_k, group_size);",
      "  }",
      "",
      "  if (cache_scales_chunk) {",
      "    int load_groups =",
      "        tb_groups * pipe_stages * 2;     // Chunk size is 2x pipeline over dim K",
      "    load_groups = max(load_groups, 32);  // We load at least 32 scale groups",
      "    return load_groups * tb_n * 2;",
      "  } else {",
      "    int tb_scales = tb_groups * tb_n * 2;",
      "",
      "    return tb_scales * pipe_stages;",
      "  }",
      "}",
      "",
      "int get_kernel_cache_size(thread_config_t const& th_config, int thread_m_blocks,",
      "                          int prob_m, int prob_n, int prob_k, int num_bits,",
      "                          int group_size, bool has_act_order, bool is_k_full,",
      "                          int has_zp, int is_zp_float) {",
      "  int pack_factor = 32 / num_bits;",
      "",
      "  // Get B size",
      "  int tb_k = th_config.thread_k;",
      "  int tb_n = th_config.thread_n;",
      "  int tb_m = thread_m_blocks * 16;",
      "  int sh_a_size = pipe_stages * (tb_m * tb_k) * 2;",
      "  int sh_b_size = pipe_stages * (tb_k * tb_n / pack_factor) * 4;",
      "  int sh_red_size = tb_m * (tb_n + 8) * 2;",
      "  int sh_bias_size = tb_n * 2;",
      "  int tmp_size =",
      "      (sh_b_size > sh_red_size ? sh_red_size : sh_b_size) + sh_bias_size;",
      "  tmp_size = max(max(sh_b_size, sh_red_size), tmp_size);",
      "",
      "  int sh_s_size =",
      "      get_scales_cache_size(th_config, prob_m, prob_n, prob_k, num_bits,",
      "                            group_size, has_act_order, is_k_full);",
      "  int sh_g_idx_size = has_act_order && !is_k_full ? pipe_stages * tb_k / 4 : 0;",
      "  int sh_zp_size = 0;",
      "  if (has_zp) {",
      "    if (is_zp_float)",
      "      sh_zp_size = sh_s_size;",
      "    else if (num_bits == 4)",
      "      sh_zp_size = sh_s_size / 4;",
      "    else if (num_bits == 8)",
      "      sh_zp_size = sh_s_size / 2;",
      "  }",
      "",
      "  int total_size =",
      "      tmp_size + sh_a_size + sh_s_size + sh_zp_size + sh_g_idx_size;",
      "",
      "  return total_size;",
      "}",
      "",
      "bool is_valid_config(thread_config_t const& th_config, int thread_m_blocks,",
      "                     int prob_m, int prob_n, int prob_k, int num_bits,",
      "                     int group_size, bool has_act_order, bool is_k_full,",
      "                     int has_zp, int is_zp_float, int max_shared_mem) {",
      "  // Sanity",
      "  if (th_config.thread_k == -1 || th_config.thread_n == -1 ||",
      "      th_config.num_threads == -1) {",
      "    return false;",
      "  }",
      "",
      "  // Verify K/N are divisible by thread K/N",
      "  if (prob_k % th_config.thread_k != 0 || prob_n % th_config.thread_n != 0) {",
      "    return false;",
      "  }",
      "",
      "  // Verify min for thread K/N",
      "  if (th_config.thread_n < min_thread_n || th_config.thread_k < min_thread_k) {",
      "    return false;",
      "  }",
      "",
      "  // num_threads must be at least 128 (= 4 warps)",
      "  if (th_config.num_threads < 128) {",
      "    return false;",
      "  }",
      "",
      "  // Check that pipeline fits into cache",
      "  int cache_size = get_kernel_cache_size(",
      "      th_config, thread_m_blocks, prob_m, prob_n, prob_k, num_bits, group_size,",
      "      has_act_order, is_k_full, has_zp, is_zp_float);",
      "  return cache_size + 512 <= max_shared_mem;",
      "}",
      "",
      "  #define _GET_IF(W_TYPE, THREAD_M_BLOCKS, THREAD_N_BLOCKS, THREAD_K_BLOCKS,   \\",
      "                  M_BLOCK_SIZE_8, GROUP_BLOCKS, NUM_THREADS, IS_ZP_FLOAT)      \\",
      "    else if (q_type == W_TYPE && thread_m_blocks == THREAD_M_BLOCKS &&         \\",
      "             thread_n_blocks == THREAD_N_BLOCKS &&                             \\",
      "             thread_k_blocks == THREAD_K_BLOCKS &&                             \\",
      "             m_block_size_8 == M_BLOCK_SIZE_8 &&                               \\",
      "             group_blocks == GROUP_BLOCKS && num_threads == NUM_THREADS &&     \\",
      "             is_zp_float == IS_ZP_FLOAT) {                                     \\",
      "      constexpr auto S_TYPE =                                                  \\",
      "          W_TYPE == vllm::kFE2M1f                                              \\",
      "              ? (GROUP_BLOCKS == 1 ? vllm::kFE4M3fn : vllm::kFE8M0fnu)         \\",
      "              : (std::is_same<scalar_t, half>::value ? vllm::kFloat16          \\",
      "                                                     : vllm::kBFloat16);       \\",
      "      kernel = Marlin<scalar_t, W_TYPE.id(), S_TYPE.id(), NUM_THREADS,         \\",
      "                      THREAD_M_BLOCKS, THREAD_N_BLOCKS, THREAD_K_BLOCKS,       \\",
      "                      M_BLOCK_SIZE_8, pipe_stages, GROUP_BLOCKS, IS_ZP_FLOAT>; \\",
      "    }",
      "",
      "  // COMMON: cases for (group_blocks in [-1, 2, 4, 8] and is_zp_float == false)",
      "  //         this is the most common cases",
      "  // BIGGROUP: cases for big group size (group_blocks in [-1, 8])",
      "  // FZP: cases for float-zero-point (is_zp_float = true)",
      "  // ACT: cases for act order case (group_blocks == 0)",
      "  // FP4: cases for nvfp4(e2m1) (group_blocks == 1)",
      "  #define COMMON_GET_IF_M1(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)       \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, -1, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 2, NUM_THREADS, false)   \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 4, NUM_THREADS, false)   \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 8, NUM_THREADS, false)   \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)",
      "",
      "  #define COMMON_GET_IF_M234(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)     \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)  \\",
      "                                                                          \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)  \\",
      "                                                                          \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)",
      "",
      "  #define COMMON_GET_IF(W_TYPE)            \\",
      "    COMMON_GET_IF_M1(W_TYPE, 8, 8, 256)    \\",
      "    COMMON_GET_IF_M1(W_TYPE, 8, 4, 128)    \\",
      "    COMMON_GET_IF_M1(W_TYPE, 4, 8, 128)    \\",
      "    COMMON_GET_IF_M234(W_TYPE, 16, 4, 256) \\",
      "    COMMON_GET_IF_M234(W_TYPE, 8, 4, 128)  \\",
      "    COMMON_GET_IF_M234(W_TYPE, 4, 8, 128)",
      "",
      "  #define BIGGROUP_GET_IF_M1(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)     \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, -1, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 8, NUM_THREADS, false)   \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)",
      "",
      "  #define BIGGROUP_GET_IF_M234(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)   \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)  \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, -1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 8, NUM_THREADS, false)",
      "",
      "  #define BIGGROUP_GET_IF(W_TYPE)            \\",
      "    BIGGROUP_GET_IF_M1(W_TYPE, 8, 8, 256)    \\",
      "    BIGGROUP_GET_IF_M1(W_TYPE, 8, 4, 128)    \\",
      "    BIGGROUP_GET_IF_M1(W_TYPE, 4, 8, 128)    \\",
      "    BIGGROUP_GET_IF_M234(W_TYPE, 16, 4, 256) \\",
      "    BIGGROUP_GET_IF_M234(W_TYPE, 8, 4, 128)  \\",
      "    BIGGROUP_GET_IF_M234(W_TYPE, 4, 8, 128)",
      "",
      "  #define NVFP4_GET_IF_M1(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)      \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 1, NUM_THREADS, false)",
      "",
      "  #define NVFP4_GET_IF_M234(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)     \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 1, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 1, NUM_THREADS, false)",
      "",
      "  #define NVFP4_GET_IF(W_TYPE)            \\",
      "    NVFP4_GET_IF_M1(W_TYPE, 8, 8, 256)    \\",
      "    NVFP4_GET_IF_M1(W_TYPE, 8, 4, 128)    \\",
      "    NVFP4_GET_IF_M1(W_TYPE, 4, 8, 128)    \\",
      "    NVFP4_GET_IF_M234(W_TYPE, 16, 4, 256) \\",
      "    NVFP4_GET_IF_M234(W_TYPE, 8, 4, 128)  \\",
      "    NVFP4_GET_IF_M234(W_TYPE, 4, 8, 128)",
      "",
      "  #define MXFP4_GET_IF_M1(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)      \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 2, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false)",
      "",
      "  #define MXFP4_GET_IF_M234(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)     \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 2, NUM_THREADS, false)",
      "",
      "  #define MXFP4_GET_IF(W_TYPE)            \\",
      "    MXFP4_GET_IF_M1(W_TYPE, 8, 8, 256)    \\",
      "    MXFP4_GET_IF_M1(W_TYPE, 8, 4, 128)    \\",
      "    MXFP4_GET_IF_M1(W_TYPE, 4, 8, 128)    \\",
      "    MXFP4_GET_IF_M234(W_TYPE, 16, 4, 256) \\",
      "    MXFP4_GET_IF_M234(W_TYPE, 8, 4, 128)  \\",
      "    MXFP4_GET_IF_M234(W_TYPE, 4, 8, 128)",
      "",
      "  // We currently have 4-bit models only with group_blocks == 4",
      "  #define FZP_GET_IF_M1(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)       \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 4, NUM_THREADS, true) \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, true)",
      "",
      "  #define FZP_GET_IF_M234(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)      \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, true) \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, true) \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 4, NUM_THREADS, true)",
      "",
      "  #define FZP_GET_IF(W_TYPE)            \\",
      "    FZP_GET_IF_M1(W_TYPE, 8, 8, 256)    \\",
      "    FZP_GET_IF_M1(W_TYPE, 8, 4, 128)    \\",
      "    FZP_GET_IF_M1(W_TYPE, 4, 8, 128)    \\",
      "    FZP_GET_IF_M234(W_TYPE, 16, 4, 256) \\",
      "    FZP_GET_IF_M234(W_TYPE, 8, 4, 128)  \\",
      "    FZP_GET_IF_M234(W_TYPE, 4, 8, 128)",
      "",
      "  // We currently have 4-bit models only with group_blocks == 4",
      "  #define ACT_GET_IF_M1(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)        \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, true, 0, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 1, N_BLOCKS, K_BLOCKS, false, 0, NUM_THREADS, false)",
      "",
      "  #define ACT_GET_IF_M234(W_TYPE, N_BLOCKS, K_BLOCKS, NUM_THREADS)       \\",
      "    _GET_IF(W_TYPE, 2, N_BLOCKS, K_BLOCKS, false, 0, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 3, N_BLOCKS, K_BLOCKS, false, 0, NUM_THREADS, false) \\",
      "    _GET_IF(W_TYPE, 4, N_BLOCKS, K_BLOCKS, false, 0, NUM_THREADS, false)",
      "",
      "  #define ACT_GET_IF(W_TYPE)            \\",
      "    ACT_GET_IF_M1(W_TYPE, 8, 8, 256)    \\",
      "    ACT_GET_IF_M1(W_TYPE, 8, 4, 128)    \\",
      "    ACT_GET_IF_M1(W_TYPE, 4, 8, 128)    \\",
      "    ACT_GET_IF_M234(W_TYPE, 16, 4, 256) \\",
      "    ACT_GET_IF_M234(W_TYPE, 8, 4, 128)  \\",
      "    ACT_GET_IF_M234(W_TYPE, 4, 8, 128)",
      "",
      "template <typename scalar_t>",
      "MarlinFuncPtr get_marlin_kernel(const vllm::ScalarType q_type,",
      "                                int thread_m_blocks, int thread_n_blocks,",
      "                                int thread_k_blocks, bool m_block_size_8,",
      "                                bool has_act_order, bool has_zp,",
      "                                int group_blocks, int num_threads,",
      "                                bool is_zp_float) {",
      "  int num_bits = q_type.size_bits();",
      "  auto kernel = MarlinDefault;",
      "  if (false) {",
      "  }",
      "",
      "  COMMON_GET_IF(vllm::kU4)",
      "  COMMON_GET_IF(vllm::kU4B8)",
      "  COMMON_GET_IF(vllm::kU8B128)",
      "",
      "  NVFP4_GET_IF(vllm::kFE2M1f)",
      "",
      "  BIGGROUP_GET_IF(vllm::kFE4M3fn)",
      "",
      "  ACT_GET_IF(vllm::kU4B8)",
      "  ACT_GET_IF(vllm::kU8B128)",
      "",
      "  if (std::is_same<scalar_t, half>::value) {",
      "    if (false) {",
      "    }",
      "    FZP_GET_IF(vllm::kU4)",
      "  }",
      "  if (std::is_same<scalar_t, nv_bfloat16>::value) {",
      "    if (false) {",
      "    }",
      "    MXFP4_GET_IF(vllm::kFE2M1f)",
      "  }",
      "",
      "  return kernel;",
      "}",
      "",
      "template <typename scalar_t>",
      "exec_config_t determine_exec_config(const vllm::ScalarType& q_type, int prob_m,",
      "                                    int prob_n, int prob_k, int thread_m_blocks,",
      "                                    bool m_block_size_8, int num_bits,",
      "                                    int group_size, bool has_act_order,",
      "                                    bool is_k_full, bool has_zp,",
      "                                    bool is_zp_float, int max_shared_mem,",
      "                                    int sms) {",
      "  exec_config_t exec_cfg = exec_config_t{1, thread_config_t{-1, -1, -1}};",
      "  thread_config_t* thread_configs = thread_m_blocks > 1",
      "                                        ? large_batch_thread_configs",
      "                                        : small_batch_thread_configs;",
      "  int thread_configs_size =",
      "      thread_m_blocks > 1",
      "          ? sizeof(large_batch_thread_configs) / sizeof(thread_config_t)",
      "          : sizeof(small_batch_thread_configs) / sizeof(thread_config_t);",
      "",
      "  for (int i = 0; i < thread_configs_size; i++) {",
      "    thread_config_t th_config = thread_configs[i];",
      "",
      "    if (!is_valid_config(th_config, thread_m_blocks, prob_m, prob_n, prob_k,",
      "                         num_bits, group_size, has_act_order, is_k_full, has_zp,",
      "                         is_zp_float, max_shared_mem)) {",
      "      continue;",
      "    }",
      "",
      "    int cache_size = get_kernel_cache_size(",
      "        th_config, thread_m_blocks, prob_m, prob_n, prob_k, num_bits,",
      "        group_size, has_act_order, is_k_full, has_zp, is_zp_float);",
      "",
      "    int group_blocks = 0;",
      "    if (!has_act_order) {",
      "      group_blocks = group_size == -1 ? -1 : group_size / 16;",
      "    }",
      "",
      "    auto kernel = get_marlin_kernel<scalar_t>(",
      "        q_type, thread_m_blocks, th_config.thread_n / 16,",
      "        th_config.thread_k / 16, m_block_size_8, has_act_order, has_zp,",
      "        group_blocks, th_config.num_threads, is_zp_float);",
      "",
      "    if (kernel == MarlinDefault) continue;",
      "",
      "    // int m_tiles = div_ceil(prob_m, thread_m_blocks * 16);",
      "    // int n_tiles = prob_n / th_config.thread_n;",
      "    // int k_tiles = prob_k / th_config.thread_k;",
      "",
      "    return {1, th_config};",
      "  }",
      "",
      "  return exec_cfg;",
      "}",
      "",
      "template <typename scalar_t>",
      "void marlin_mm(const void* A, const void* B, void* C, void* C_tmp, void* b_bias,",
      "               void* s, void* s2, void* zp, void* g_idx, void* perm,",
      "               void* a_tmp, int prob_m, int prob_n, int prob_k, int lda,",
      "               void* workspace, vllm::ScalarType const& q_type, bool has_bias,",
      "               bool has_act_order, bool is_k_full, bool has_zp, int num_groups,",
      "               int group_size, int dev, cudaStream_t stream, int thread_k_init,",
      "               int thread_n_init, int sms, bool use_atomic_add,",
      "               bool use_fp32_reduce, bool is_zp_float) {",
      "  if (has_zp) {",
      "    TORCH_CHECK(",
      "        q_type == vllm::kU4 || q_type == vllm::kU8,",
      "        \"q_type must be u4 or u8 when has_zp = True. Got = \", q_type.str());",
      "  } else {",
      "    TORCH_CHECK(",
      "        q_type == vllm::kU4B8 || q_type == vllm::kU8B128 ||",
      "            q_type == vllm::kFE4M3fn || q_type == vllm::kFE2M1f,",
      "        \"q_type must be uint4b8, uint8b128, float8_e4m3fn or float4_e2m1f when \"",
      "        \"has_zp = False. Got = \",",
      "        q_type.str());",
      "  }",
      "",
      "  TORCH_CHECK(prob_m > 0 && prob_n > 0 && prob_k > 0, \"Invalid MNK = [\", prob_m,",
      "              \", \", prob_n, \", \", prob_k, \"]\");",
      "",
      "  int group_blocks = 0;",
      "  if (has_act_order) {",
      "    if (is_k_full) {",
      "      TORCH_CHECK(group_size != -1);",
      "      group_blocks = group_size / 16;",
      "      TORCH_CHECK(prob_k % group_blocks == 0, \"prob_k = \", prob_k,",
      "                  \" is not divisible by group_blocks = \", group_blocks);",
      "    } else {",
      "      TORCH_CHECK(group_size == 0);",
      "      group_blocks = 0;",
      "    }",
      "  } else {",
      "    if (group_size == -1) {",
      "      group_blocks = -1;",
      "    } else {",
      "      group_blocks = group_size / 16;",
      "      TORCH_CHECK(prob_k % group_blocks == 0, \"prob_k = \", prob_k,",
      "                  \" is not divisible by group_blocks = \", group_blocks);",
      "    }",
      "  }",
      "",
      "  int num_bits = q_type.size_bits();",
      "  const int4* A_ptr = (const int4*)A;",
      "  const int4* B_ptr = (const int4*)B;",
      "  int4* C_ptr = (int4*)C;",
      "  int4* C_tmp_ptr = (int4*)C_tmp;",
      "  const int4* bias_ptr = (const int4*)b_bias;",
      "  const int4* s_ptr = (const int4*)s;",
      "  const uint16_t* s2_ptr = (const uint16_t*)s2;",
      "  const int4* zp_ptr = (const int4*)zp;",
      "  const int* g_idx_ptr = (const int*)g_idx;",
      "  const int* perm_ptr = (const int*)perm;",
      "  int4* a_tmp_ptr = (int4*)a_tmp;",
      "",
      "  int* locks = (int*)workspace;",
      "",
      "  if (has_act_order) {",
      "    // Permute A columns",
      "    int block_rows = div_ceil(prob_m, sms);",
      "    // avoid \">>>\" being formatted to \"> > >\"",
      "    // clang-format off",
      "    permute_cols_kernel<<<sms, default_threads, 0, stream>>>(",
      "        A_ptr, perm_ptr, a_tmp_ptr, prob_m, prob_k, lda, block_rows);",
      "    // clang-format on",
      "    A_ptr = a_tmp_ptr;",
      "    lda = prob_k;",
      "",
      "    // If we have a full K, then we can run the non-act-order version of Marlin",
      "    // (since the weight rows are reordered by increasing group ids, and by",
      "    // having a full K, we have full original groups)",
      "    if (is_k_full) has_act_order = false;",
      "  }",
      "",
      "  int max_shared_mem = 0;",
      "  cudaDeviceGetAttribute(&max_shared_mem,",
      "                         cudaDevAttrMaxSharedMemoryPerBlockOptin, dev);",
      "  TORCH_CHECK(max_shared_mem > 0);",
      "",
      "  int max_par = 16;",
      "  if (prob_n <= 4096) max_par = 16 * 8;",
      "  int max_shared_mem_new = max_shared_mem;",
      "  int rest_m = prob_m;",
      "  int max_thread_m_blocks = 4;",
      "  while (rest_m) {",
      "    int par_count = rest_m / (max_thread_m_blocks * 16);",
      "    if (par_count > max_par) par_count = max_par;",
      "    int prob_m_split =",
      "        par_count > 0 ? (par_count * (max_thread_m_blocks * 16)) : rest_m;",
      "",
      "    int thread_k = thread_k_init;",
      "    int thread_n = thread_n_init;",
      "",
      "    int thread_m_blocks = min(div_ceil(prob_m_split, 16), max_thread_m_blocks);",
      "    int m_block_size_8 = prob_m_split <= 8;",
      "",
      "    // Set thread config",
      "    exec_config_t exec_cfg;",
      "    thread_config_t thread_tfg;",
      "    if (thread_k != -1 && thread_n != -1) {",
      "      thread_tfg = thread_config_t{thread_k, thread_n, default_threads};",
      "      exec_cfg = exec_config_t{1, thread_tfg};",
      "      TORCH_CHECK(prob_n % thread_n == 0, \"prob_n = \", prob_n,",
      "                  \" is not divisible by thread_n = \", thread_n);",
      "      TORCH_CHECK(prob_k % thread_k == 0, \"prob_k = \", prob_k,",
      "                  \" is not divisible by thread_k = \", thread_k);",
      "    } else {",
      "      // Auto config",
      "      exec_cfg = determine_exec_config<scalar_t>(",
      "          q_type, prob_m_split, prob_n, prob_k, thread_m_blocks, m_block_size_8,",
      "          num_bits, group_size, has_act_order, is_k_full, has_zp, is_zp_float,",
      "          max_shared_mem, sms);",
      "      thread_tfg = exec_cfg.tb_cfg;",
      "      if (thread_tfg.thread_k == -1 && max_thread_m_blocks > 1) {",
      "        max_thread_m_blocks--;",
      "        continue;",
      "      }",
      "    }",
      "",
      "    int num_threads = thread_tfg.num_threads;",
      "    thread_k = thread_tfg.thread_k;",
      "    thread_n = thread_tfg.thread_n;",
      "    int blocks = sms * exec_cfg.blocks_per_sm;",
      "    if (exec_cfg.blocks_per_sm > 1)",
      "      max_shared_mem_new = max_shared_mem / exec_cfg.blocks_per_sm - 1024;",
      "",
      "    int thread_k_blocks = thread_k / 16;",
      "    int thread_n_blocks = thread_n / 16;",
      "",
      "    TORCH_CHECK(",
      "        is_valid_config(thread_tfg, thread_m_blocks, prob_m_split, prob_n,",
      "                        prob_k, num_bits, group_size, has_act_order, is_k_full,",
      "                        has_zp, is_zp_float, max_shared_mem_new),",
      "        \"Invalid thread config: thread_m_blocks = \", thread_m_blocks,",
      "        \", thread_k = \", thread_tfg.thread_k,",
      "        \", thread_n = \", thread_tfg.thread_n,",
      "        \", num_threads = \", thread_tfg.num_threads, \" for MKN = [\", prob_m,",
      "        \", \", prob_k, \", \", prob_n, \"] and num_bits = \", num_bits,",
      "        \", prob_m_split = \", prob_m_split, \", group_size = \", group_size,",
      "        \", has_act_order = \", has_act_order, \", is_k_full = \", is_k_full,",
      "        \", has_zp = \", has_zp, \", is_zp_float = \", is_zp_float,",
      "        \", max_shared_mem_new = \", max_shared_mem_new);",
      "",
      "    auto kernel = get_marlin_kernel<scalar_t>(",
      "        q_type, thread_m_blocks, thread_n_blocks, thread_k_blocks,",
      "        m_block_size_8, has_act_order, has_zp, group_blocks, num_threads,",
      "        is_zp_float);",
      "",
      "    if (kernel == MarlinDefault) {",
      "      TORCH_CHECK(false, \"Unsupported shapes: MNK = [\", prob_m, \", \", prob_n,",
      "                  \", \", prob_k, \"]\", \", has_act_order = \", has_act_order,",
      "                  \", num_groups = \", num_groups, \", group_size = \", group_size,",
      "                  \", prob_m_split = \", prob_m_split,",
      "                  \", thread_m_blocks = \", thread_m_blocks,",
      "                  \", thread_n_blocks = \", thread_n_blocks,",
      "                  \", thread_k_blocks = \", thread_k_blocks,",
      "                  \", num_threads = \", num_threads, \", num_bits = \", num_bits);",
      "    }",
      "",
      "    cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize,",
      "                         max_shared_mem_new);",
      "",
      "    bool part_use_atomic_add =",
      "        use_atomic_add && div_ceil(prob_m_split, 64) * prob_n <= 2048;",
      "",
      "    // avoid \">>>\" being formatted to \"> > >\"",
      "    // clang-format off",
      "    kernel<<<blocks, num_threads, max_shared_mem_new, stream>>>(",
      "        A_ptr, B_ptr, C_ptr, C_tmp_ptr, bias_ptr, s_ptr, s2_ptr, zp_ptr,",
      "        g_idx_ptr, num_groups,",
      "        prob_m_split, prob_n, prob_k, lda, locks, has_bias, part_use_atomic_add,",
      "        use_fp32_reduce, max_shared_mem_new);",
      "    // clang-format on",
      "",
      "    A_ptr += prob_m_split * (lda / 8);",
      "    C_ptr += prob_m_split * (prob_n / 8);",
      "    rest_m -= prob_m_split;",
      "  }",
      "}",
      "",
      "}  // namespace marlin",
      "",
      "torch::Tensor gptq_marlin_gemm(",
      "    torch::Tensor& a, std::optional<torch::Tensor> c_or_none,",
      "    torch::Tensor& b_q_weight,",
      "    std::optional<torch::Tensor> const& b_bias_or_none, torch::Tensor& b_scales,",
      "    std::optional<torch::Tensor> const& global_scale_or_none,",
      "    std::optional<torch::Tensor> const& b_zeros_or_none,",
      "    std::optional<torch::Tensor> const& g_idx_or_none,",
      "    std::optional<torch::Tensor> const& perm_or_none, torch::Tensor& workspace,",
      "    vllm::ScalarTypeId const& b_q_type_id, int64_t size_m, int64_t size_n,",
      "    int64_t size_k, bool is_k_full, bool use_atomic_add, bool use_fp32_reduce,",
      "    bool is_zp_float) {",
      "  vllm::ScalarType const b_q_type = vllm::ScalarType::from_id(b_q_type_id);",
      "  int pack_factor = 32 / b_q_type.size_bits();",
      "",
      "  // Verify A",
      "  TORCH_CHECK(a.size(0) == size_m, \"Shape mismatch: a.size(0) = \", a.size(0),",
      "              \", size_m = \", size_m);",
      "  TORCH_CHECK(a.size(1) == size_k, \"Shape mismatch: a.size(1) = \", a.size(1),",
      "              \", size_k = \", size_k);",
      "",
      "  // Verify B",
      "  TORCH_CHECK(",
      "      size_k % MARLIN_NAMESPACE_NAME::tile_size == 0, \"size_k = \", size_k,",
      "      \" is not divisible by tile_size = \", MARLIN_NAMESPACE_NAME::tile_size);",
      "  TORCH_CHECK((size_k / MARLIN_NAMESPACE_NAME::tile_size) == b_q_weight.size(0),",
      "              \"Shape mismatch: b_q_weight.size(0) = \", b_q_weight.size(0),",
      "              \", size_k = \", size_k,",
      "              \", tile_size = \", MARLIN_NAMESPACE_NAME::tile_size);",
      "  TORCH_CHECK(",
      "      b_q_weight.size(1) % MARLIN_NAMESPACE_NAME::tile_size == 0,",
      "      \"b_q_weight.size(1) = \", b_q_weight.size(1),",
      "      \" is not divisible by tile_size = \", MARLIN_NAMESPACE_NAME::tile_size);",
      "  int actual_size_n =",
      "      (b_q_weight.size(1) / MARLIN_NAMESPACE_NAME::tile_size) * pack_factor;",
      "  TORCH_CHECK(size_n == actual_size_n, \"size_n = \", size_n,",
      "              \", actual_size_n = \", actual_size_n);",
      "",
      "  // Verify device and strides",
      "  TORCH_CHECK(a.device().is_cuda(), \"A is not on GPU\");",
      "  TORCH_CHECK(a.stride(1) == 1, \"A.stride(1) is not 1\");",
      "  // We use int4 (16 bytes) to load A, so A must aligned to 16 bytes",
      "  TORCH_CHECK(a.stride(0) % 8 == 0, \"A.stride(0) must divisible by 8\");",
      "  TORCH_CHECK(((uint64_t)a.data_ptr()) % 16 == 0, \"A must aligned to 16 bytes\");",
      "",
      "  TORCH_CHECK(b_q_weight.device().is_cuda(), \"b_q_weight is not on GPU\");",
      "  TORCH_CHECK(b_q_weight.is_contiguous(), \"b_q_weight is not contiguous\");",
      "",
      "  TORCH_CHECK(b_scales.device().is_cuda(), \"b_scales is not on GPU\");",
      "  TORCH_CHECK(b_scales.is_contiguous(), \"b_scales is not contiguous\");",
      "",
      "  // thread_k: `k` size of a thread_tile in `weights` (can usually be left as",
      "  // auto -1)",
      "  int thread_k = -1;",
      "  // thread_n: `n` size of a thread_tile in `weights` (can usually be left as",
      "  // auto -1)",
      "  int thread_n = -1;",
      "  // sms: number of SMs to use for the kernel",
      "  int sms = -1;",
      "  cudaDeviceGetAttribute(&sms, cudaDevAttrMultiProcessorCount, a.get_device());",
      "",
      "  // Alloc buffers",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(a));",
      "  auto options = torch::TensorOptions().dtype(a.dtype()).device(a.device());",
      "  torch::Tensor c;",
      "  if (c_or_none.has_value()) {",
      "    c = c_or_none.value();",
      "    TORCH_CHECK(c.device().is_cuda(), \"c is not on GPU\");",
      "    TORCH_CHECK(c.is_contiguous(), \"c is not contiguous\");",
      "    TORCH_CHECK(c.size(0) == size_m, \"Shape mismatch: c.size(0) = \", c.size(0),",
      "                \", size_m = \", size_m);",
      "    TORCH_CHECK(c.size(1) == size_n, \"Shape mismatch: c.size(1) = \", c.size(1),",
      "                \", size_n = \", size_n);",
      "  } else {",
      "    c = torch::empty({size_m, size_n}, options);",
      "  }",
      "  if (size_m == 0) return c;",
      "",
      "  // Alloc C tmp buffer that is going to be used for the global reduce",
      "  torch::Tensor c_tmp;",
      "  auto options_fp32 =",
      "      torch::TensorOptions().dtype(at::kFloat).device(a.device());",
      "  if (use_fp32_reduce) {",
      "    int max_m_block_size = (size_m + 16 - 1) / 16 * 16;",
      "    max_m_block_size = min(max_m_block_size, 64);",
      "    int max_c_tmp_size =",
      "        sms * max_m_block_size * MARLIN_NAMESPACE_NAME::max_thread_n;",
      "    c_tmp = torch::empty({max_c_tmp_size}, options_fp32);",
      "  } else {",
      "    c_tmp = torch::empty({0}, options_fp32);",
      "  }",
      "",
      "  // Detect groupsize and act_order",
      "  int num_groups = -1;",
      "  int group_size = -1;",
      "",
      "  int rank = b_scales.sizes().size();",
      "  TORCH_CHECK(rank == 2, \"b_scales rank = \", rank, \" is not 2\");",
      "  TORCH_CHECK(b_scales.size(1) == size_n, \"b_scales dim 1 = \", b_scales.size(1),",
      "              \" is not size_n = \", size_n);",
      "  num_groups = b_scales.size(0);",
      "",
      "  torch::Tensor g_idx, perm, a_tmp;",
      "  if (g_idx_or_none.has_value() && perm_or_none.has_value()) {",
      "    g_idx = g_idx_or_none.value();",
      "    perm = perm_or_none.value();",
      "",
      "    TORCH_CHECK(g_idx.device().is_cuda(), \"g_idx is not on GPU\");",
      "    TORCH_CHECK(g_idx.is_contiguous(), \"g_idx is not contiguous\");",
      "    TORCH_CHECK(perm.device().is_cuda(), \"perm is not on GPU\");",
      "    TORCH_CHECK(perm.is_contiguous(), \"perm is not contiguous\");",
      "",
      "    // Verify g_idx and perm",
      "    TORCH_CHECK((g_idx.size(-1) == 0 && perm.size(-1) == 0) ||",
      "                    (g_idx.size(-1) == size_k && perm.size(-1) == size_k),",
      "                \"Unexpected g_idx.size(-1) = \", g_idx.size(-1),",
      "                \" and perm.size(-1) = \", perm.size(-1),",
      "                \", where size_k = \", size_k);",
      "  } else {",
      "    g_idx = torch::empty({0}, options);",
      "    perm = torch::empty({0}, options);",
      "    a_tmp = torch::empty({0}, options);",
      "  }",
      "  bool has_act_order = g_idx.size(-1) > 0 && perm.size(-1) > 0;",
      "",
      "  if (has_act_order) {",
      "    a_tmp = torch::empty({size_m, size_k}, options);",
      "    if (is_k_full) {",
      "      TORCH_CHECK(num_groups > 1, \"For act_order, num_groups must be > 1\");",
      "      TORCH_CHECK(size_k % num_groups == 0, \"size_k = \", size_k,",
      "                  \", is not divisible by num_groups = \", num_groups);",
      "      group_size = size_k / num_groups;",
      "    } else {",
      "      group_size = 0;",
      "    }",
      "",
      "  } else {",
      "    a_tmp = torch::empty({0}, options);",
      "    if (num_groups > 1) {",
      "      TORCH_CHECK(",
      "          size_k % num_groups == 0, \"size_k = \", size_k,",
      "          \", is not divisible by b_scales.size(0) = \", b_scales.size(0));",
      "      group_size = size_k / num_groups;",
      "    } else {",
      "      group_size = -1;",
      "    }",
      "  }",
      "",
      "  torch::Tensor global_scale;",
      "  if (global_scale_or_none.has_value()) {",
      "    global_scale = global_scale_or_none.value();",
      "    TORCH_CHECK(b_q_type == vllm::kFE2M1f && group_size == 16,",
      "                \"global_scale can only be used for nvfp4 format.\");",
      "  } else {",
      "    global_scale = torch::empty({0}, options);",
      "    TORCH_CHECK(!(b_q_type == vllm::kFE2M1f && group_size == 16),",
      "                \"the global_scale parameter must be passed for nvfp4 format.\");",
      "  }",
      "",
      "  bool has_bias = b_bias_or_none.has_value();",
      "  torch::Tensor b_bias;",
      "  if (has_bias) {",
      "    b_bias = b_bias_or_none.value();",
      "    TORCH_CHECK(b_bias.device().is_cuda(), \"b_bias is not on GPU\");",
      "    TORCH_CHECK(b_bias.is_contiguous(), \"b_bias is not contiguous\");",
      "    TORCH_CHECK(b_bias.size(0) == size_n, \"b_bias.size(0) != size_n\");",
      "    TORCH_CHECK(b_bias.stride(0) == 1, \"b_bias.stride(0) != 1\");",
      "  } else {",
      "    b_bias = torch::empty({0}, options);",
      "  }",
      "",
      "  torch::Tensor b_zeros;",
      "  if (b_zeros_or_none.has_value()) {",
      "    b_zeros = b_zeros_or_none.value();",
      "    TORCH_CHECK(b_zeros.device().is_cuda(), \"b_zeros is not on GPU\");",
      "    TORCH_CHECK(b_zeros.is_contiguous(), \"b_zeros is not contiguous\");",
      "  } else {",
      "    b_zeros = torch::empty({0}, options);",
      "  }",
      "  bool has_zp = b_zeros.size(-1) > 0;",
      "  if (has_zp) {",
      "    TORCH_CHECK(",
      "        b_q_type == vllm::kU4 || b_q_type == vllm::kU8,",
      "        \"b_q_type must be u4 or u8 when has_zp = True. Got = \", b_q_type.str());",
      "  } else {",
      "    TORCH_CHECK(b_q_type == vllm::kU4B8 || b_q_type == vllm::kU8B128 ||",
      "                    b_q_type == vllm::kFE4M3fn || b_q_type == vllm::kFE2M1f,",
      "                \"b_q_type must be uint4b8, uint8b128, float8_e4m3fn or \"",
      "                \"float4_e2m1f when \"",
      "                \"has_zp = False. Got = \",",
      "                b_q_type.str());",
      "  }",
      "",
      "  if (has_zp && is_zp_float) {",
      "    TORCH_CHECK(a.scalar_type() == at::ScalarType::Half,",
      "                \"Computation type must be float16 (half) when using float zero \"",
      "                \"points.\");",
      "  }",
      "",
      "  // Verify b_zeros",
      "  if (has_zp) {",
      "    int rank = b_zeros.sizes().size();",
      "    TORCH_CHECK(rank == 2, \"b_zeros rank = \", rank, \" is not 2\");",
      "    if (is_zp_float) {",
      "      TORCH_CHECK(b_zeros.size(1) == size_n,",
      "                  \"b_zeros dim 1 = \", b_zeros.size(1),",
      "                  \" is not size_n = \", size_n);",
      "      TORCH_CHECK(num_groups == b_zeros.size(0),",
      "                  \"b_zeros dim 0 = \", b_zeros.size(0),",
      "                  \" is not num_groups = \", num_groups);",
      "      TORCH_CHECK(num_groups != -1, \"num_groups must be != -1\");",
      "    } else {",
      "      TORCH_CHECK(b_zeros.size(0) == num_groups,",
      "                  \"b_zeros dim 0 = \", b_zeros.size(0),",
      "                  \" is not num_groups = \", num_groups);",
      "      TORCH_CHECK(b_zeros.size(1) == size_n / pack_factor,",
      "                  \"b_zeros dim 1 = \", b_zeros.size(1),",
      "                  \" is not size_n / pack_factor = \", size_n / pack_factor);",
      "    }",
      "  }",
      "",
      "  // Verify workspace size",
      "  TORCH_CHECK(size_n % MARLIN_NAMESPACE_NAME::min_thread_n == 0,",
      "              \"size_n = \", size_n, \", is not divisible by min_thread_n = \",",
      "              MARLIN_NAMESPACE_NAME::min_thread_n);",
      "",
      "  int min_workspace_size = sms;",
      "  TORCH_CHECK(workspace.numel() >= min_workspace_size,",
      "              \"workspace.numel = \", workspace.numel(),",
      "              \" is below min_workspace_size = \", min_workspace_size);",
      "",
      "  int dev = a.get_device();",
      "  if (a.scalar_type() == at::ScalarType::Half) {",
      "    void* scales_ptr;",
      "    if (b_q_type == vllm::kFE2M1f) {",
      "      if (group_size == 16)",
      "        scales_ptr = b_scales.data_ptr<at::Float8_e4m3fn>();",
      "      else if (group_size == 32)",
      "        scales_ptr = b_scales.data_ptr<at::Float8_e8m0fnu>();",
      "      else",
      "        TORCH_CHECK(false,",
      "                    \"float4_e2m1f only supports group_size == 16 (NVFP4) \",",
      "                    \"and group_size == 32 (MXFP4)\");",
      "    } else {",
      "      scales_ptr = b_scales.data_ptr<at::Half>();",
      "    }",
      "",
      "    marlin::marlin_mm<half>(",
      "        a.data_ptr<at::Half>(), b_q_weight.data_ptr(), c.data_ptr<at::Half>(),",
      "        c_tmp.data_ptr<float>(), b_bias.data_ptr<at::Half>(), scales_ptr,",
      "        global_scale.data_ptr<at::Half>(), b_zeros.data_ptr(), g_idx.data_ptr(),",
      "        perm.data_ptr(), a_tmp.data_ptr<at::Half>(), size_m, size_n, size_k,",
      "        a.stride(0), workspace.data_ptr(), b_q_type, has_bias, has_act_order,",
      "        is_k_full, has_zp, num_groups, group_size, dev,",
      "        at::cuda::getCurrentCUDAStream(dev), thread_k, thread_n, sms,",
      "        use_atomic_add, use_fp32_reduce, is_zp_float);",
      "  } else if (a.scalar_type() == at::ScalarType::BFloat16) {",
      "    void* scales_ptr;",
      "    if (b_q_type == vllm::kFE2M1f) {",
      "      if (group_size == 16)",
      "        scales_ptr = b_scales.data_ptr<at::Float8_e4m3fn>();",
      "      else if (group_size == 32)",
      "        scales_ptr = b_scales.data_ptr<at::Float8_e8m0fnu>();",
      "      else",
      "        TORCH_CHECK(false,",
      "                    \"float4_e2m1f only supports group_size == 16 (NVFP4) \",",
      "                    \"and group_size == 32 (MXFP4)\");",
      "    } else {",
      "      scales_ptr = b_scales.data_ptr<at::BFloat16>();",
      "    }",
      "",
      "    marlin::marlin_mm<nv_bfloat16>(",
      "        a.data_ptr<at::BFloat16>(), b_q_weight.data_ptr(),",
      "        c.data_ptr<at::BFloat16>(), c_tmp.data_ptr<float>(),",
      "        b_bias.data_ptr<at::BFloat16>(), scales_ptr,",
      "        global_scale.data_ptr<at::BFloat16>(), b_zeros.data_ptr(),",
      "        g_idx.data_ptr(), perm.data_ptr(), a_tmp.data_ptr<at::BFloat16>(),",
      "        size_m, size_n, size_k, a.stride(0), workspace.data_ptr(), b_q_type,",
      "        has_bias, has_act_order, is_k_full, has_zp, num_groups, group_size, dev,",
      "        at::cuda::getCurrentCUDAStream(dev), thread_k, thread_n, sms,",
      "        use_atomic_add, use_fp32_reduce, is_zp_float);",
      "  } else {",
      "    TORCH_CHECK(false, \"gpt_marlin_gemm only supports bfloat16 and float16\");",
      "  }",
      "",
      "  return c;",
      "}",
      "",
      "#endif",
      "",
      "TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, CUDA, m) {",
      "  m.impl(\"gptq_marlin_gemm\", &gptq_marlin_gemm);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq_marlin/awq_marlin_repack.cu",
    "source": [
      "#include \"marlin.cuh\"",
      "",
      "#include \"core/registration.h\"",
      "",
      "namespace marlin {",
      "",
      "template <int const num_threads, int const num_bits>",
      "__global__ void awq_marlin_repack_kernel(",
      "    uint32_t const* __restrict__ b_q_weight_ptr, uint32_t* __restrict__ out_ptr,",
      "    int size_k, int size_n) {",
      "  constexpr int pack_factor = 32 / num_bits;",
      "",
      "  int k_tiles = size_k / tile_k_size;",
      "  int n_tiles = size_n / tile_n_size;",
      "  int block_k_tiles = div_ceil(k_tiles, gridDim.x);",
      "",
      "  auto start_k_tile = blockIdx.x * block_k_tiles;",
      "  if (start_k_tile >= k_tiles) {",
      "    return;",
      "  }",
      "",
      "  int finish_k_tile = min(start_k_tile + block_k_tiles, k_tiles);",
      "",
      "  // Wait until the next thread tile has been loaded to shared memory.",
      "  auto wait_for_stage = [&]() {",
      "    // We only have `stages - 2` active fetches since we are double buffering",
      "    // and can only issue the next fetch when it is guaranteed that the previous",
      "    // shared memory load is fully complete (as it may otherwise be",
      "    // overwritten).",
      "    cp_async_wait<repack_stages - 2>();",
      "    __syncthreads();",
      "  };",
      "",
      "  extern __shared__ int4 sh[];",
      "",
      "  constexpr int tile_n_ints = tile_n_size / pack_factor;",
      "",
      "  constexpr int stage_n_threads = tile_n_ints / 4;",
      "  constexpr int stage_k_threads = tile_k_size;",
      "  constexpr int stage_size = stage_k_threads * stage_n_threads;",
      "",
      "  auto fetch_to_shared = [&](int pipe, int k_tile_id, int n_tile_id) {",
      "    if (n_tile_id >= n_tiles) {",
      "      cp_async_fence();",
      "      return;",
      "    }",
      "",
      "    int first_n = n_tile_id * tile_n_size;",
      "    int first_n_packed = first_n / pack_factor;",
      "",
      "    int4* sh_ptr = sh + stage_size * pipe;",
      "",
      "    if (threadIdx.x < stage_size) {",
      "      auto k_id = threadIdx.x / stage_n_threads;",
      "      auto n_id = threadIdx.x % stage_n_threads;",
      "",
      "      int first_k = k_tile_id * tile_k_size;",
      "",
      "      cp_async4(&sh_ptr[k_id * stage_n_threads + n_id],",
      "                reinterpret_cast<int4 const*>(",
      "                    &(b_q_weight_ptr[(first_k + k_id) * (size_n / pack_factor) +",
      "                                     first_n_packed + (n_id * 4)])));",
      "    }",
      "",
      "    cp_async_fence();",
      "  };",
      "",
      "  auto repack_tile = [&](int pipe, int k_tile_id, int n_tile_id) {",
      "    if (n_tile_id >= n_tiles) {",
      "      return;",
      "    }",
      "",
      "    auto warp_id = threadIdx.x / 32;",
      "    auto th_id = threadIdx.x % 32;",
      "",
      "    if (warp_id >= 4) {",
      "      return;",
      "    }",
      "",
      "    int tc_col = th_id / 4;",
      "    int tc_row = (th_id % 4) * 2;",
      "",
      "    constexpr int tc_offsets[4] = {0, 1, 8, 9};",
      "",
      "    int cur_n = warp_id * 16 + tc_col;",
      "    int cur_n_packed = cur_n / pack_factor;",
      "    int cur_n_pos = cur_n % pack_factor;",
      "",
      "    constexpr int sh_stride = tile_n_ints;",
      "    constexpr uint32_t mask = (1 << num_bits) - 1;",
      "",
      "    int4* sh_stage_ptr = sh + stage_size * pipe;",
      "    uint32_t* sh_stage_int_ptr = reinterpret_cast<uint32_t*>(sh_stage_ptr);",
      "",
      "    // Undo interleaving",
      "    int cur_n_pos_unpacked;",
      "    if constexpr (num_bits == 4) {",
      "      constexpr int undo_pack[8] = {0, 4, 1, 5, 2, 6, 3, 7};",
      "      cur_n_pos_unpacked = undo_pack[cur_n_pos];",
      "    } else {",
      "      constexpr int undo_pack[4] = {0, 2, 1, 3};",
      "      cur_n_pos_unpacked = undo_pack[cur_n_pos];",
      "    }",
      "",
      "    uint32_t vals[8];",
      "#pragma unroll",
      "    for (int i = 0; i < 4; i++) {",
      "      int cur_elem = tc_row + tc_offsets[i];",
      "",
      "      int packed_src_0 = sh_stage_int_ptr[cur_n_packed + sh_stride * cur_elem];",
      "      int packed_src_1 = sh_stage_int_ptr[cur_n_packed + (8 / pack_factor) +",
      "                                          sh_stride * cur_elem];",
      "",
      "      vals[i] = (packed_src_0 >> (cur_n_pos_unpacked * num_bits)) & mask;",
      "      vals[4 + i] = (packed_src_1 >> (cur_n_pos_unpacked * num_bits)) & mask;",
      "    }",
      "",
      "    constexpr int tile_size = tile_k_size * tile_n_size / pack_factor;",
      "    int out_offset = (k_tile_id * n_tiles + n_tile_id) * tile_size;",
      "",
      "    // Result of:",
      "    // https://github.com/NVIDIA/FasterTransformer/blob/main/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/interleaved_numeric_conversion.h",
      "    if constexpr (num_bits == 4) {",
      "      constexpr int pack_idx[8] = {0, 2, 4, 6, 1, 3, 5, 7};",
      "",
      "      uint32_t res = 0;",
      "#pragma unroll",
      "      for (int i = 0; i < 8; i++) {",
      "        res |= vals[pack_idx[i]] << (i * 4);",
      "      }",
      "",
      "      out_ptr[out_offset + th_id * 4 + warp_id] = res;",
      "",
      "    } else {",
      "      constexpr int pack_idx[4] = {0, 2, 1, 3};",
      "",
      "      uint32_t res1 = 0;",
      "      uint32_t res2 = 0;",
      "#pragma unroll",
      "      for (int i = 0; i < 4; i++) {",
      "        res1 |= vals[pack_idx[i]] << (i * 8);",
      "        res2 |= vals[4 + pack_idx[i]] << (i * 8);",
      "      }",
      "",
      "      out_ptr[out_offset + th_id * 8 + (warp_id * 2) + 0] = res1;",
      "      out_ptr[out_offset + th_id * 8 + (warp_id * 2) + 1] = res2;",
      "    }",
      "  };",
      "",
      "  auto start_pipes = [&](int k_tile_id, int n_tile_id) {",
      "#pragma unroll",
      "    for (int pipe = 0; pipe < repack_stages - 1; pipe++) {",
      "      fetch_to_shared(pipe, k_tile_id, n_tile_id + pipe);",
      "    }",
      "",
      "    wait_for_stage();",
      "  };",
      "#pragma unroll",
      "  for (int k_tile_id = start_k_tile; k_tile_id < finish_k_tile; k_tile_id++) {",
      "    int n_tile_id = 0;",
      "",
      "    start_pipes(k_tile_id, n_tile_id);",
      "",
      "    while (n_tile_id < n_tiles) {",
      "#pragma unroll",
      "      for (int pipe = 0; pipe < repack_stages; pipe++) {",
      "        fetch_to_shared((pipe + repack_stages - 1) % repack_stages, k_tile_id,",
      "                        n_tile_id + pipe + repack_stages - 1);",
      "        repack_tile(pipe, k_tile_id, n_tile_id + pipe);",
      "        wait_for_stage();",
      "      }",
      "      n_tile_id += repack_stages;",
      "    }",
      "  }",
      "}",
      "",
      "}  // namespace marlin",
      "",
      "#define CALL_IF(NUM_BITS)                                                   \\",
      "  else if (num_bits == NUM_BITS) {                                          \\",
      "    cudaFuncSetAttribute(                                                   \\",
      "        marlin::awq_marlin_repack_kernel<marlin::repack_threads, NUM_BITS>, \\",
      "        cudaFuncAttributeMaxDynamicSharedMemorySize, max_shared_mem);       \\",
      "    marlin::awq_marlin_repack_kernel<marlin::repack_threads, NUM_BITS>      \\",
      "        <<<blocks, marlin::repack_threads, max_shared_mem, stream>>>(       \\",
      "            b_q_weight_ptr, out_ptr, size_k, size_n);                       \\",
      "  }",
      "",
      "torch::Tensor awq_marlin_repack(torch::Tensor& b_q_weight, int64_t size_k,",
      "                                int64_t size_n, int64_t num_bits) {",
      "  // Verify compatibility with marlin tile of 16x64",
      "  TORCH_CHECK(size_k % marlin::tile_k_size == 0, \"size_k = \", size_k,",
      "              \" is not divisible by tile_k_size = \", marlin::tile_k_size);",
      "  TORCH_CHECK(size_n % marlin::tile_n_size == 0, \"size_n = \", size_n,",
      "              \" is not divisible by tile_n_size = \", marlin::tile_n_size);",
      "",
      "  TORCH_CHECK(num_bits == 4 || num_bits == 8,",
      "              \"num_bits must be 4 or 8. Got = \", num_bits);",
      "  int const pack_factor = 32 / num_bits;",
      "",
      "  // Verify B",
      "  TORCH_CHECK(b_q_weight.size(0) == size_k,",
      "              \"b_q_weight.size(0) = \", b_q_weight.size(0),",
      "              \" is not size_k = \", size_k);",
      "  TORCH_CHECK((size_n / pack_factor) == b_q_weight.size(1),",
      "              \"Shape mismatch: b_q_weight.size(1) = \", b_q_weight.size(1),",
      "              \", size_n = \", size_n, \", pack_factor = \", pack_factor);",
      "",
      "  // Verify device and strides",
      "  TORCH_CHECK(b_q_weight.device().is_cuda(), \"b_q_weight is not on GPU\");",
      "  TORCH_CHECK(b_q_weight.is_contiguous(), \"b_q_weight is not contiguous\");",
      "  TORCH_CHECK(b_q_weight.dtype() == at::kInt, \"b_q_weight type is not kInt\");",
      "",
      "  // Alloc buffers",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(b_q_weight));",
      "  auto options = torch::TensorOptions()",
      "                     .dtype(b_q_weight.dtype())",
      "                     .device(b_q_weight.device());",
      "  torch::Tensor out = torch::empty(",
      "      {size_k / marlin::tile_size, size_n * marlin::tile_size / pack_factor},",
      "      options);",
      "",
      "  // Get ptrs",
      "  uint32_t const* b_q_weight_ptr =",
      "      reinterpret_cast<uint32_t const*>(b_q_weight.data_ptr());",
      "  uint32_t* out_ptr = reinterpret_cast<uint32_t*>(out.data_ptr());",
      "",
      "  // Get dev info",
      "  int dev = b_q_weight.get_device();",
      "  cudaStream_t stream = at::cuda::getCurrentCUDAStream(dev);",
      "  int blocks;",
      "  cudaDeviceGetAttribute(&blocks, cudaDevAttrMultiProcessorCount, dev);",
      "",
      "  int max_shared_mem = 0;",
      "  cudaDeviceGetAttribute(&max_shared_mem,",
      "                         cudaDevAttrMaxSharedMemoryPerBlockOptin, dev);",
      "  TORCH_CHECK(max_shared_mem > 0);",
      "",
      "  if (false) {",
      "  }",
      "  CALL_IF(4)",
      "  CALL_IF(8)",
      "  else {",
      "    TORCH_CHECK(false, \"Unsupported repack config: num_bits = \", num_bits);",
      "  }",
      "",
      "  return out;",
      "}",
      "",
      "torch::Tensor awq_marlin_repack_meta(torch::Tensor& b_q_weight,",
      "                                     c10::SymInt size_k, c10::SymInt size_n,",
      "                                     int64_t num_bits) {",
      "  int const pack_factor = 32 / num_bits;",
      "  auto options = torch::TensorOptions()",
      "                     .dtype(b_q_weight.dtype())",
      "                     .device(b_q_weight.device());",
      "  return torch::empty_symint(",
      "      {size_k / marlin::tile_size, size_n * marlin::tile_size / pack_factor},",
      "      options);",
      "}",
      "",
      "TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, CUDA, m) {",
      "  m.impl(\"awq_marlin_repack\", &awq_marlin_repack);",
      "}",
      "",
      "TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, Meta, m) {",
      "  m.impl(\"awq_marlin_repack\", &awq_marlin_repack_meta);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq_marlin/marlin.cuh",
    "source": [
      "#pragma once",
      "",
      "#include <torch/all.h>",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <cuda.h>",
      "#include <cuda_fp16.h>",
      "#include <cuda_runtime.h>",
      "#include <iostream>",
      "",
      "#ifndef MARLIN_NAMESPACE_NAME",
      "  #define MARLIN_NAMESPACE_NAME marlin",
      "#endif",
      "",
      "namespace MARLIN_NAMESPACE_NAME {",
      "",
      "// Marlin params",
      "",
      "// 8 warps are a good choice since every SM has 4 schedulers and having more",
      "// than 1 warp per schedule allows some more latency hiding. At the same time,",
      "// we want relatively few warps to have many registers per warp and small tiles.",
      "static constexpr int default_threads = 256;",
      "",
      "static constexpr int pipe_stages =",
      "    4;  // 4 pipeline stages fit into shared memory",
      "",
      "static constexpr int min_thread_n = 64;",
      "static constexpr int min_thread_k = 64;",
      "static constexpr int max_thread_n = 256;",
      "",
      "static constexpr int tile_size = 16;",
      "static constexpr int max_par = 16;",
      "",
      "// Repack params",
      "static constexpr int repack_stages = 8;",
      "",
      "static constexpr int repack_threads = 256;",
      "",
      "static constexpr int tile_k_size = tile_size;",
      "static constexpr int tile_n_size = tile_k_size * 4;",
      "",
      "// Helpers",
      "template <typename T, int n>",
      "struct Vec {",
      "  T elems[n];",
      "  __device__ T& operator[](int i) { return elems[i]; }",
      "};",
      "",
      "using I4 = Vec<int, 4>;",
      "",
      "constexpr int div_ceil(int a, int b) { return (a + b - 1) / b; }",
      "",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "// No support for async",
      "#else",
      "",
      "__device__ inline void cp_async4_pred(void* smem_ptr, const void* glob_ptr,",
      "                                      bool pred = true) {",
      "  const int BYTES = 16;",
      "  uint32_t smem = static_cast<uint32_t>(__cvta_generic_to_shared(smem_ptr));",
      "  asm volatile(",
      "      \"{\\n\"",
      "      \"   .reg .pred p;\\n\"",
      "      \"   setp.ne.b32 p, %0, 0;\\n\"",
      "      \"   @p cp.async.cg.shared.global [%1], [%2], %3;\\n\"",
      "      \"}\\n\" ::\"r\"((int)pred),",
      "      \"r\"(smem), \"l\"(glob_ptr), \"n\"(BYTES));",
      "}",
      "",
      "__device__ inline void cp_async4(void* smem_ptr, const void* glob_ptr) {",
      "  const int BYTES = 16;",
      "  uint32_t smem = static_cast<uint32_t>(__cvta_generic_to_shared(smem_ptr));",
      "  asm volatile(",
      "      \"{\\n\"",
      "      \"   cp.async.cg.shared.global [%0], [%1], %2;\\n\"",
      "      \"}\\n\" ::\"r\"(smem),",
      "      \"l\"(glob_ptr), \"n\"(BYTES));",
      "}",
      "",
      "__device__ inline void cp_async_fence() {",
      "  asm volatile(\"cp.async.commit_group;\\n\" ::);",
      "}",
      "",
      "template <int n>",
      "__device__ inline void cp_async_wait() {",
      "  asm volatile(\"cp.async.wait_group %0;\\n\" ::\"n\"(n));",
      "}",
      "",
      "#endif",
      "",
      "}  // namespace MARLIN_NAMESPACE_NAME"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq_marlin/gptq_marlin_repack.cu",
    "source": [
      "#include \"marlin.cuh\"",
      "",
      "#include \"core/registration.h\"",
      "",
      "namespace marlin {",
      "",
      "template <int const num_threads, int const num_bits, bool const has_perm>",
      "__global__ void gptq_marlin_repack_kernel(",
      "    uint32_t const* __restrict__ b_q_weight_ptr,",
      "    uint32_t const* __restrict__ perm_ptr, uint32_t* __restrict__ out_ptr,",
      "    int size_k, int size_n) {",
      "  constexpr int pack_factor = 32 / num_bits;",
      "",
      "  int k_tiles = size_k / tile_k_size;",
      "  int n_tiles = size_n / tile_n_size;",
      "  int block_k_tiles = div_ceil(k_tiles, gridDim.x);",
      "",
      "  auto start_k_tile = blockIdx.x * block_k_tiles;",
      "  if (start_k_tile >= k_tiles) {",
      "    return;",
      "  }",
      "",
      "  int finish_k_tile = min(start_k_tile + block_k_tiles, k_tiles);",
      "",
      "  // Wait until the next thread tile has been loaded to shared memory.",
      "  auto wait_for_stage = [&]() {",
      "    // We only have `stages - 2` active fetches since we are double buffering",
      "    // and can only issue the next fetch when it is guaranteed that the previous",
      "    // shared memory load is fully complete (as it may otherwise be",
      "    // overwritten).",
      "    cp_async_wait<repack_stages - 2>();",
      "    __syncthreads();",
      "  };",
      "",
      "  extern __shared__ int4 sh[];",
      "",
      "  constexpr int perm_size = tile_k_size / 4;",
      "",
      "  int4* sh_perm_ptr = sh;",
      "  int4* sh_pipe_ptr = sh_perm_ptr;",
      "  if constexpr (has_perm) {",
      "    sh_pipe_ptr += perm_size;",
      "  }",
      "",
      "  constexpr int tile_ints = tile_k_size / pack_factor;",
      "",
      "  constexpr int stage_n_threads = tile_n_size / 4;",
      "  constexpr int stage_k_threads = has_perm ? tile_k_size : tile_ints;",
      "  constexpr int stage_size = stage_k_threads * stage_n_threads;",
      "",
      "  auto load_perm_to_shared = [&](int k_tile_id) {",
      "    int first_k_int4 = (k_tile_id * tile_k_size) / 4;",
      "",
      "    int4 const* perm_int4_ptr = reinterpret_cast<int4 const*>(perm_ptr);",
      "",
      "    if (threadIdx.x < perm_size) {",
      "      sh_perm_ptr[threadIdx.x] = perm_int4_ptr[first_k_int4 + threadIdx.x];",
      "    }",
      "    __syncthreads();",
      "  };",
      "",
      "  auto fetch_to_shared = [&](int pipe, int k_tile_id, int n_tile_id) {",
      "    if (n_tile_id >= n_tiles) {",
      "      cp_async_fence();",
      "      return;",
      "    }",
      "",
      "    int first_n = n_tile_id * tile_n_size;",
      "",
      "    int4* sh_ptr = sh_pipe_ptr + stage_size * pipe;",
      "",
      "    if constexpr (has_perm) {",
      "      if (threadIdx.x < stage_size) {",
      "        auto k_id = threadIdx.x / stage_n_threads;",
      "        auto n_id = threadIdx.x % stage_n_threads;",
      "",
      "        uint32_t const* sh_perm_int_ptr =",
      "            reinterpret_cast<uint32_t const*>(sh_perm_ptr);",
      "",
      "        int src_k = sh_perm_int_ptr[k_id];",
      "        int src_k_packed = src_k / pack_factor;",
      "",
      "        cp_async4(",
      "            &sh_ptr[k_id * stage_n_threads + n_id],",
      "            reinterpret_cast<int4 const*>(&(",
      "                b_q_weight_ptr[src_k_packed * size_n + first_n + (n_id * 4)])));",
      "      }",
      "",
      "    } else {",
      "      if (threadIdx.x < stage_size) {",
      "        auto k_id = threadIdx.x / stage_n_threads;",
      "        auto n_id = threadIdx.x % stage_n_threads;",
      "",
      "        int first_k = k_tile_id * tile_k_size;",
      "        int first_k_packed = first_k / pack_factor;",
      "",
      "        cp_async4(&sh_ptr[k_id * stage_n_threads + n_id],",
      "                  reinterpret_cast<int4 const*>(",
      "                      &(b_q_weight_ptr[(first_k_packed + k_id) * size_n +",
      "                                       first_n + (n_id * 4)])));",
      "      }",
      "    }",
      "",
      "    cp_async_fence();",
      "  };",
      "",
      "  auto repack_tile = [&](int pipe, int k_tile_id, int n_tile_id) {",
      "    if (n_tile_id >= n_tiles) {",
      "      return;",
      "    }",
      "",
      "    auto warp_id = threadIdx.x / 32;",
      "    auto th_id = threadIdx.x % 32;",
      "",
      "    if (warp_id >= 4) {",
      "      return;",
      "    }",
      "",
      "    int tc_col = th_id / 4;",
      "    int tc_row = (th_id % 4) * 2;",
      "",
      "    constexpr int tc_offsets[4] = {0, 1, 8, 9};",
      "",
      "    int cur_n = warp_id * 16 + tc_col;",
      "",
      "    constexpr int sh_stride = 64;",
      "    constexpr uint32_t mask = (1 << num_bits) - 1;",
      "",
      "    int4* sh_stage_ptr = sh_pipe_ptr + stage_size * pipe;",
      "    uint32_t* sh_stage_int_ptr = reinterpret_cast<uint32_t*>(sh_stage_ptr);",
      "",
      "    uint32_t* sh_perm_int_ptr = reinterpret_cast<uint32_t*>(sh_perm_ptr);",
      "",
      "    uint32_t vals[8];",
      "",
      "    if constexpr (has_perm) {",
      "      for (int i = 0; i < 4; i++) {",
      "        int k_idx = tc_row + tc_offsets[i];",
      "",
      "        uint32_t src_k = sh_perm_int_ptr[k_idx];",
      "        uint32_t src_k_pos = src_k % pack_factor;",
      "",
      "        uint32_t b1_val = sh_stage_int_ptr[k_idx * sh_stride + cur_n];",
      "        uint32_t b1_cur_val = (b1_val >> (src_k_pos * num_bits)) & mask;",
      "",
      "        uint32_t b2_val = sh_stage_int_ptr[k_idx * sh_stride + cur_n + 8];",
      "        uint32_t b2_cur_val = (b2_val >> (src_k_pos * num_bits)) & mask;",
      "",
      "        vals[i] = b1_cur_val;",
      "        vals[4 + i] = b2_cur_val;",
      "      }",
      "",
      "    } else {",
      "      uint32_t b1_vals[tile_ints];",
      "      uint32_t b2_vals[tile_ints];",
      "",
      "#pragma unroll",
      "      for (int i = 0; i < tile_ints; i++) {",
      "        b1_vals[i] = sh_stage_int_ptr[cur_n + sh_stride * i];",
      "        b2_vals[i] = sh_stage_int_ptr[cur_n + 8 + sh_stride * i];",
      "      }",
      "",
      "#pragma unroll",
      "      for (int i = 0; i < 4; i++) {",
      "        int cur_elem = tc_row + tc_offsets[i];",
      "        int cur_int = cur_elem / pack_factor;",
      "        int cur_pos = cur_elem % pack_factor;",
      "",
      "        vals[i] = (b1_vals[cur_int] >> (cur_pos * num_bits)) & mask;",
      "        vals[4 + i] = (b2_vals[cur_int] >> (cur_pos * num_bits)) & mask;",
      "      }",
      "    }",
      "",
      "    constexpr int tile_size = tile_k_size * tile_n_size / pack_factor;",
      "    int out_offset = (k_tile_id * n_tiles + n_tile_id) * tile_size;",
      "",
      "    // Result of:",
      "    // https://github.com/NVIDIA/FasterTransformer/blob/main/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/interleaved_numeric_conversion.h",
      "    if constexpr (num_bits == 4) {",
      "      constexpr int pack_idx[8] = {0, 2, 4, 6, 1, 3, 5, 7};",
      "",
      "      uint32_t res = 0;",
      "#pragma unroll",
      "      for (int i = 0; i < 8; i++) {",
      "        res |= vals[pack_idx[i]] << (i * 4);",
      "      }",
      "",
      "      out_ptr[out_offset + th_id * 4 + warp_id] = res;",
      "",
      "    } else {",
      "      constexpr int pack_idx[4] = {0, 2, 1, 3};",
      "",
      "      uint32_t res1 = 0;",
      "      uint32_t res2 = 0;",
      "#pragma unroll",
      "      for (int i = 0; i < 4; i++) {",
      "        res1 |= vals[pack_idx[i]] << (i * 8);",
      "        res2 |= vals[4 + pack_idx[i]] << (i * 8);",
      "      }",
      "",
      "      out_ptr[out_offset + th_id * 8 + (warp_id * 2) + 0] = res1;",
      "      out_ptr[out_offset + th_id * 8 + (warp_id * 2) + 1] = res2;",
      "    }",
      "  };",
      "",
      "  auto start_pipes = [&](int k_tile_id, int n_tile_id) {",
      "#pragma unroll",
      "    for (int pipe = 0; pipe < repack_stages - 1; pipe++) {",
      "      fetch_to_shared(pipe, k_tile_id, n_tile_id + pipe);",
      "    }",
      "",
      "    wait_for_stage();",
      "  };",
      "#pragma unroll",
      "  for (int k_tile_id = start_k_tile; k_tile_id < finish_k_tile; k_tile_id++) {",
      "    int n_tile_id = 0;",
      "",
      "    if constexpr (has_perm) {",
      "      load_perm_to_shared(k_tile_id);",
      "    }",
      "",
      "    start_pipes(k_tile_id, n_tile_id);",
      "",
      "    while (n_tile_id < n_tiles) {",
      "#pragma unroll",
      "      for (int pipe = 0; pipe < repack_stages; pipe++) {",
      "        fetch_to_shared((pipe + repack_stages - 1) % repack_stages, k_tile_id,",
      "                        n_tile_id + pipe + repack_stages - 1);",
      "        repack_tile(pipe, k_tile_id, n_tile_id + pipe);",
      "        wait_for_stage();",
      "      }",
      "      n_tile_id += repack_stages;",
      "    }",
      "  }",
      "}",
      "",
      "}  // namespace marlin",
      "",
      "#define CALL_IF(NUM_BITS, HAS_PERM)                                         \\",
      "  else if (num_bits == NUM_BITS && has_perm == HAS_PERM) {                  \\",
      "    cudaFuncSetAttribute(                                                   \\",
      "        marlin::gptq_marlin_repack_kernel<marlin::repack_threads, NUM_BITS, \\",
      "                                          HAS_PERM>,                        \\",
      "        cudaFuncAttributeMaxDynamicSharedMemorySize, max_shared_mem);       \\",
      "    marlin::gptq_marlin_repack_kernel<marlin::repack_threads, NUM_BITS,     \\",
      "                                      HAS_PERM>                             \\",
      "        <<<blocks, marlin::repack_threads, max_shared_mem, stream>>>(       \\",
      "            b_q_weight_ptr, perm_ptr, out_ptr, size_k, size_n);             \\",
      "  }",
      "",
      "torch::Tensor gptq_marlin_repack(torch::Tensor& b_q_weight, torch::Tensor& perm,",
      "                                 int64_t size_k, int64_t size_n,",
      "                                 int64_t num_bits) {",
      "  // Verify compatibility with marlin tile of 16x64",
      "  TORCH_CHECK(size_k % marlin::tile_k_size == 0, \"size_k = \", size_k,",
      "              \" is not divisible by tile_k_size = \", marlin::tile_k_size);",
      "  TORCH_CHECK(size_n % marlin::tile_n_size == 0, \"size_n = \", size_n,",
      "              \" is not divisible by tile_n_size = \", marlin::tile_n_size);",
      "",
      "  TORCH_CHECK(num_bits == 4 || num_bits == 8,",
      "              \"num_bits must be 4 or 8. Got = \", num_bits);",
      "  int const pack_factor = 32 / num_bits;",
      "",
      "  // Verify B",
      "  TORCH_CHECK((size_k / pack_factor) == b_q_weight.size(0),",
      "              \"Shape mismatch: b_q_weight.size(0) = \", b_q_weight.size(0),",
      "              \", size_k = \", size_k, \", pack_factor = \", pack_factor);",
      "  TORCH_CHECK(b_q_weight.size(1) == size_n,",
      "              \"b_q_weight.size(1) = \", b_q_weight.size(1),",
      "              \" is not size_n = \", size_n);",
      "",
      "  // Verify device and strides",
      "  TORCH_CHECK(b_q_weight.device().is_cuda(), \"b_q_weight is not on GPU\");",
      "  TORCH_CHECK(b_q_weight.is_contiguous(), \"b_q_weight is not contiguous\");",
      "  TORCH_CHECK(b_q_weight.dtype() == at::kInt, \"b_q_weight type is not kInt\");",
      "",
      "  TORCH_CHECK(perm.device().is_cuda(), \"perm is not on GPU\");",
      "  TORCH_CHECK(perm.is_contiguous(), \"perm is not contiguous\");",
      "  TORCH_CHECK(perm.dtype() == at::kInt, \"perm type is not at::kInt\");",
      "",
      "  // Alloc buffers",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(b_q_weight));",
      "  auto options = torch::TensorOptions()",
      "                     .dtype(b_q_weight.dtype())",
      "                     .device(b_q_weight.device());",
      "  torch::Tensor out = torch::empty(",
      "      {size_k / marlin::tile_size, size_n * marlin::tile_size / pack_factor},",
      "      options);",
      "",
      "  // Detect if there is act_order",
      "  bool has_perm = perm.size(0) != 0;",
      "",
      "  // Get ptrs",
      "  uint32_t const* b_q_weight_ptr =",
      "      reinterpret_cast<uint32_t const*>(b_q_weight.data_ptr());",
      "  uint32_t const* perm_ptr = reinterpret_cast<uint32_t const*>(perm.data_ptr());",
      "  uint32_t* out_ptr = reinterpret_cast<uint32_t*>(out.data_ptr());",
      "",
      "  // Get dev info",
      "  int dev = b_q_weight.get_device();",
      "  cudaStream_t stream = at::cuda::getCurrentCUDAStream(dev);",
      "  int blocks;",
      "  cudaDeviceGetAttribute(&blocks, cudaDevAttrMultiProcessorCount, dev);",
      "",
      "  int max_shared_mem = 0;",
      "  cudaDeviceGetAttribute(&max_shared_mem,",
      "                         cudaDevAttrMaxSharedMemoryPerBlockOptin, dev);",
      "  TORCH_CHECK(max_shared_mem > 0);",
      "",
      "  if (false) {",
      "  }",
      "  CALL_IF(4, false)",
      "  CALL_IF(4, true)",
      "  CALL_IF(8, false)",
      "  CALL_IF(8, true)",
      "  else {",
      "    TORCH_CHECK(false, \"Unsupported repack config: num_bits = \", num_bits,",
      "                \", has_perm = \", has_perm);",
      "  }",
      "",
      "  return out;",
      "}",
      "",
      "torch::Tensor gptq_marlin_repack_meta(torch::Tensor& b_q_weight,",
      "                                      torch::Tensor& perm, c10::SymInt size_k,",
      "                                      c10::SymInt size_n, int64_t num_bits) {",
      "  int const pack_factor = 32 / num_bits;",
      "  auto options = torch::TensorOptions()",
      "                     .dtype(b_q_weight.dtype())",
      "                     .device(b_q_weight.device());",
      "  return torch::empty_symint(",
      "      {size_k / marlin::tile_size, size_n * marlin::tile_size / pack_factor},",
      "      options);",
      "}",
      "",
      "TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, CUDA, m) {",
      "  m.impl(\"gptq_marlin_repack\", &gptq_marlin_repack);",
      "}",
      "",
      "TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, Meta, m) {",
      "  m.impl(\"gptq_marlin_repack\", &gptq_marlin_repack_meta);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/machete/machete_mm_launcher.cuh",
    "source": [
      "#pragma once",
      "",
      "#include <torch/all.h>",
      "#include <Python.h>",
      "",
      "#include \"machete_mm_kernel.cuh\"",
      "#include \"cutlass_extensions/torch_utils.hpp\"",
      "#include \"core/scalar_type.hpp\"",
      "",
      "namespace machete {",
      "",
      "struct MMArgs {",
      "  torch::Tensor const& A;",
      "  torch::Tensor const& B;",
      "  vllm::ScalarType const& b_type;",
      "  std::optional<at::ScalarType> const& maybe_out_type;",
      "  std::optional<torch::Tensor> const& maybe_group_scales;",
      "  std::optional<torch::Tensor> const& maybe_group_zeros;",
      "  std::optional<int64_t> maybe_group_size;",
      "  std::optional<torch::Tensor> const& maybe_channel_scales;",
      "  std::optional<torch::Tensor> const& maybe_token_scales;",
      "  std::optional<std::string> maybe_schedule;",
      "};",
      "",
      "struct SupportedSchedulesArgs {",
      "  at::ScalarType a_type;",
      "  vllm::ScalarType b_type;",
      "  std::optional<at::ScalarType> maybe_group_scales_type;",
      "  std::optional<at::ScalarType> maybe_group_zeros_type;",
      "  std::optional<at::ScalarType> maybe_channel_scales_type;",
      "  std::optional<at::ScalarType> maybe_token_scales_type;",
      "  std::optional<at::ScalarType> maybe_out_type;",
      "};",
      "",
      "torch::Tensor mm_dispatch(MMArgs args);",
      "",
      "std::vector<std::string> supported_schedules_dispatch(",
      "    SupportedSchedulesArgs args);",
      "",
      "template <typename MacheteKernel>",
      "torch::Tensor run_impl(MMArgs args) {",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(args.A));",
      "",
      "  auto device = args.A.device();",
      "  auto stream = at::cuda::getCurrentCUDAStream(device.index());",
      "",
      "  int M = args.A.size(0);",
      "  int N = args.B.size(1);",
      "  int K = args.A.size(1);",
      "",
      "  // Allocate output",
      "  torch::Tensor D = torch::empty(",
      "      {M, N},",
      "      torch::TensorOptions()",
      "          .dtype(equivalent_scalar_type_v<typename MacheteKernel::ElementD>)",
      "          .device(device));",
      "",
      "  auto arguments = MacheteKernel::create_arguments(",
      "      stream,  //",
      "      args.A, args.B, D, args.maybe_group_scales, args.maybe_group_zeros,",
      "      args.maybe_group_size, args.maybe_channel_scales,",
      "      args.maybe_token_scales);",
      "  TORCH_CHECK(MacheteKernel::can_implement(arguments),",
      "              \"Machete kernel cannot be run with these arguments\");",
      "",
      "  size_t workspace_size = MacheteKernel::get_workspace_size(arguments);",
      "  torch::Tensor workspace = torch::empty(",
      "      workspace_size, torch::TensorOptions().dtype(torch::kU8).device(device));",
      "",
      "  MacheteKernel::run(arguments, workspace.mutable_data_ptr(), stream);",
      "",
      "  return D;",
      "};",
      "",
      "};  // namespace machete"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/machete/machete_prepack_kernel.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"machete_mm_kernel.cuh\"",
      "#include \"cutlass_extensions/cute_utils.cuh\"",
      "#include \"cutlass_extensions/torch_utils.hpp\"",
      "",
      "namespace machete {",
      "",
      "template <int threads, typename PrepackedLayoutB, typename BInTensor,",
      "          typename ElementB>",
      "static __global__ void prepack_B_kernel(BInTensor B_in, ElementB* B_out_ptr) {",
      "  auto constexpr block_size =",
      "      Int<size(typename PrepackedLayoutB::PPBlockShape_NK{})>{};",
      "  auto constexpr eles_per_thread = Int<block_size / threads>{};",
      "  static_assert(block_size % threads == 0,",
      "                \"block_size must be divisible by the number of threads\");",
      "",
      "  // Which pre-packed are we responsible for",
      "  auto blk_coord = make_coord(blockIdx.x, blockIdx.y, blockIdx.z);",
      "  auto tB_in = local_tile(",
      "      B_in, append(typename PrepackedLayoutB::PPBlockShape_NK{}, _1{}),",
      "      blk_coord);",
      "",
      "  // Find the start offset in the output for this pre-packed block",
      "  auto bNbKL_to_offset = PrepackedLayoutB::bNbKL_to_offset(shape(B_in));",
      "",
      "  // Tensor representing a 1:1 mapping to the output space in 1D",
      "  auto tB_out_linear =",
      "      make_tensor(get_logical_ptr(B_out_ptr) + bNbKL_to_offset(blk_coord),",
      "                  make_layout(make_shape(block_size)));",
      "  // Mapping from output space (1D) to input space",
      "  auto tB_in_linear = make_tensor(",
      "      tB_in.data(),",
      "      tB_in.layout()",
      "          .compose(right_inverse(PrepackedLayoutB::ppblock_ilvd_NK_to_offset()))",
      "          .with_shape(make_shape(block_size)));",
      "",
      "  // Tile for this specific thread (could have used a TiledCopy but these work",
      "  // best with 2d layouts, this is a simple 1d layout so local_tile is enough,",
      "  // we are also not that concerned with performance for this kernel)",
      "  auto thr_tB_in_linear =",
      "      local_tile(tB_in_linear, make_shape(eles_per_thread), threadIdx.x);",
      "  auto thr_tB_out_linear =",
      "      local_tile(tB_out_linear, make_shape(eles_per_thread), threadIdx.x);",
      "",
      "  // Construct a register-backed Tensor with the same shape as each thread's",
      "  // partition",
      "  auto fragment = make_tensor<ElementB>(shape(thr_tB_in_linear));",
      "",
      "  copy(thr_tB_in_linear, fragment);",
      "  copy(Copy_Atom<DefaultCopy, uint8_t>{}, fragment, thr_tB_out_linear);",
      "}",
      "",
      "template <typename PrepackedLayoutB, typename InLayout>",
      "static void prepack_B_template(",
      "    cudaStream_t stream, typename PrepackedLayoutB::ElementB const* B_in_ptr,",
      "    InLayout B_layout, typename PrepackedLayoutB::ElementB* B_out_ptr) {",
      "  using TileShapeNKL =",
      "      decltype(append(typename PrepackedLayoutB::PPBlockShape_NK{}, _1{}));",
      "  auto ilvd_NKbNbKL_to_offset =",
      "      PrepackedLayoutB::ilvd_NKbNbKL_to_offset(shape(B_layout));",
      "",
      "  TORCH_CHECK(size<0>(B_layout) % size<0>(TileShapeNKL{}) == 0);",
      "  TORCH_CHECK(size<1>(B_layout) % size<1>(TileShapeNKL{}) == 0);",
      "",
      "  auto N_tiles = size<0>(B_layout) / size<0>(TileShapeNKL{});",
      "  auto K_tiles = size<1>(B_layout) / size<1>(TileShapeNKL{});",
      "  auto L_tiles = size<2>(B_layout);",
      "",
      "  auto B_in = make_tensor(get_logical_ptr(B_in_ptr), B_layout);",
      "",
      "  prepack_B_kernel<128, PrepackedLayoutB>",
      "      <<<dim3(N_tiles, K_tiles, L_tiles), 128, 0, stream>>>(B_in, B_out_ptr);",
      "}",
      "",
      "};  // namespace machete"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/machete/machete_prepack_launcher.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"machete_prepack_kernel.cuh\"",
      "#include \"cutlass_extensions/torch_utils.hpp\"",
      "#include \"core/scalar_type.hpp\"",
      "",
      "namespace machete {",
      "",
      "struct PrepackBArgs {",
      "  torch::Tensor const& B;",
      "  at::ScalarType a_type;",
      "  vllm::ScalarType b_type;",
      "  std::optional<at::ScalarType> maybe_group_scales_type;",
      "};",
      "",
      "template <typename PrepackedLayoutB>",
      "torch::Tensor prepack_impl(torch::Tensor const B) {",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(B));",
      "  using ElementB = typename PrepackedLayoutB::ElementB;",
      "  using PPBlockShape_NK = typename PrepackedLayoutB::PPBlockShape_NK;",
      "",
      "  auto device = B.device();",
      "  auto stream = at::cuda::getCurrentCUDAStream(device.index());",
      "  auto B_ptr = static_cast<ElementB const*>(B.const_data_ptr());",
      "  // elements per storage item for B",
      "  auto eles_per_storage =",
      "      (B.dtype().itemsize() * 8) / cute::sizeof_bits_v<ElementB>;",
      "",
      "  // torch B passed in is/should be (packed_K,N), the kernel expects (N,K,L) (to",
      "  // match cutlass using (N,K,L) for B), so we transpose B to (N,packed_K,L)",
      "  auto Bt_packed = B.t();",
      "",
      "  TORCH_CHECK(",
      "      (B.size(0) * eles_per_storage) % size<1>(PPBlockShape_NK{}) == 0,",
      "      \"B.shape[0] (in terms of unpacked elements) must be a multiple of \",",
      "      size<1>(PPBlockShape_NK{}));",
      "  TORCH_CHECK(B.size(1) % size<0>(PPBlockShape_NK{}) == 0,",
      "              \"B.shape[1] must be a multiple of \", size<0>(PPBlockShape_NK{}));",
      "",
      "  using StrideB = cutlass::detail::TagToStrideB_t<cutlass::layout::ColumnMajor>;",
      "  auto const l_Bt_packed = make_cute_layout<StrideB>(Bt_packed, \"B\");",
      "",
      "  // convert (N,packed_K,L) layout to (N,K,L) layout",
      "  //  in effect we want to do: blocked_product(layout_Bt_packed,",
      "  //      make_ordered_layout(make_shape(_1{}, eles_per_storage, _1{}),",
      "  //                          Step<_1, _0, _2>{}));",
      "  // but blocked_product does not support dynamic strides so we implement the",
      "  // equivalent manually,",
      "  //   new_shape = (N, packed_K, L) * (1, eles_per_storage, 1) -> (N, K, L)",
      "  //   new_stride = (s0, s1, s2) * (eles_per_storage, 1, eles_per_storage)",
      "  //                 when s1 == 1",
      "  TORCH_CHECK(stride<1>(l_Bt_packed) == 1);",
      "  // clang-format off",
      "  auto const layout_Bt = make_layout(",
      "      transform_with_idx(l_Bt_packed.shape(), [&](auto ele, auto idx) {",
      "        return idx == 1 ? ele * eles_per_storage : ele;",
      "      }), ",
      "      transform_with_idx(l_Bt_packed.stride(), [&](auto ele, auto idx) {",
      "        return idx != 1 ? ele * eles_per_storage : ele;",
      "      }));",
      "  // clang-format on",
      "",
      "  // Allocate output",
      "  torch::Tensor D = torch::empty_like(B, {}, at::MemoryFormat::Contiguous);",
      "",
      "  prepack_B_template<PrepackedLayoutB>(",
      "      stream, B_ptr, layout_Bt, static_cast<ElementB*>(D.mutable_data_ptr()));",
      "",
      "  return D;",
      "};",
      "",
      "torch::Tensor prepack_B_dispatch(PrepackBArgs args);",
      "",
      "};  // namespace machete"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/machete/machete_mm_kernel.cuh",
    "source": [
      "#pragma once",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <torch/all.h>",
      "",
      "// clang-format off",
      "// The cutlass include order matters (annoyingly)",
      "#include \"cutlass/cutlass.h\"",
      "",
      "#include \"cute/tensor.hpp\"",
      "#include \"cutlass/tensor_ref.h\"",
      "#include \"cutlass/epilogue/collective/default_epilogue.hpp\"",
      "#include \"cutlass/epilogue/thread/linear_combination.h\"",
      "#include \"cutlass/gemm/dispatch_policy.hpp\"",
      "#include \"cutlass/gemm/collective/collective_builder.hpp\"",
      "#include \"cutlass/epilogue/collective/collective_builder.hpp\"",
      "#include \"cutlass/gemm/device/gemm_universal_adapter.h\"",
      "#include \"cutlass/gemm/kernel/gemm_universal.hpp\"",
      "// clang-format on",
      "",
      "#include \"cutlass_extensions/cute_utils.cuh\"",
      "#include \"cutlass_extensions/vllm_numeric_conversion.cuh\"",
      "#include \"cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp\"",
      "#include \"cutlass_extensions/torch_utils.hpp\"",
      "#include \"machete_collective_builder.cuh\"",
      "#include \"machete_prepacked_layout.cuh\"",
      "#include \"machete_interleaving_utils.cuh\"",
      "",
      "namespace machete {",
      "",
      "using namespace cute;",
      "",
      "// NOTE This kernel computes D = alpha * A * B + beta * C by computing",
      "//   D^t = alpha * B^t * A^t + beta * C^t, this is because the wgmma",
      "//   instructions only support sourcing from registers for the left-hand",
      "//   operand, we want to upconvert/decompress the quantized operand in",
      "//   register. Since the primary use case we want to support is Y = XW^t where",
      "//   W is quantized, in this situation or right-hand operand is quantized so",
      "//   we compute the transpose to move it to the left-hand side.",
      "template <typename ElementA_, typename ElementB_, typename ElementD_,",
      "          typename AccumulatorT, typename GroupScaleT, typename GroupZeroT,",
      "          typename ChannelScaleT, typename TokenScaleT, class KernelSchedule,",
      "          typename ScheduleConfig>",
      "struct MacheteKernelTemplate {",
      "  static constexpr bool with_C = false;  // not ever used",
      "  static constexpr bool with_group_scales = !std::is_same_v<GroupScaleT, void>;",
      "  static constexpr bool with_group_zeropoints =",
      "      !std::is_same_v<GroupZeroT, void>;",
      "  static constexpr bool with_channel_scales =",
      "      !std::is_same_v<ChannelScaleT, void>;",
      "  static constexpr bool with_token_scales = !std::is_same_v<TokenScaleT, void>;",
      "",
      "  using MmaType = ElementA_;",
      "  using ElementA = ElementA_;",
      "  using ElementB = ElementB_;",
      "  using ElementD = ElementD_;",
      "  using ElementC = cute::conditional_t<with_C, ElementD, void>;",
      "  using ElementAccumulator = AccumulatorT;",
      "  using ElementCompute = AccumulatorT;  // For Epilogue",
      "  // Use dummy values when we don't have scales or zeropoints",
      "  using ElementZGroup =",
      "      cute::conditional_t<with_group_zeropoints, GroupZeroT, MmaType>;",
      "  using ElementSGroup =",
      "      cute::conditional_t<with_group_scales, GroupScaleT, MmaType>;",
      "  using ElementConvertGroup =",
      "      cute::conditional_t<with_group_scales, GroupScaleT, MmaType>;",
      "  using ElementSChannel =",
      "      cute::conditional_t<with_channel_scales, ChannelScaleT, AccumulatorT>;",
      "  using ElementSToken =",
      "      cute::conditional_t<with_token_scales, TokenScaleT, AccumulatorT>;",
      "",
      "  using BTypeTuple = cute::conditional_t<",
      "      with_group_scales,",
      "      cute::conditional_t<with_group_zeropoints,",
      "                          cute::tuple<ElementB, ElementSGroup, ElementZGroup>,",
      "                          cute::tuple<ElementB, ElementSGroup>>,",
      "      ElementB>;",
      "",
      "  using LayoutA = cutlass::layout::RowMajor;",
      "  using LayoutC = cutlass::layout::RowMajor;",
      "  using LayoutD = LayoutC;",
      "  using LayoutScale = cutlass::layout::RowMajor;",
      "  // not actually used since B has the prepacked layout, but required by cutlass",
      "  using _LayoutB = cutlass::layout::ColumnMajor;",
      "",
      "  // Interface strides expected by create_arguments (will get transposed)",
      "  using StrideA = cutlass::detail::TagToStrideA_t<LayoutA>;",
      "  using StrideC = cutlass::detail::TagToStrideA_t<LayoutC>;",
      "  using StrideD = cutlass::detail::TagToStrideA_t<LayoutD>;",
      "  using StrideSGroup = cutlass::detail::TagToStrideA_t<LayoutScale>;",
      "  using StrideZGroup = StrideSGroup;",
      "",
      "  using LayoutA_Transpose =",
      "      typename cutlass::layout::LayoutTranspose<LayoutA>::type;",
      "  using LayoutC_Transpose =",
      "      typename cutlass::layout::LayoutTranspose<LayoutC>::type;",
      "  using LayoutD_Transpose =",
      "      typename cutlass::layout::LayoutTranspose<LayoutD>::type;",
      "",
      "  using ArchTag = cutlass::arch::Sm90;",
      "  using OperatorClass = cutlass::arch::OpClassTensorOp;",
      "",
      "  using PrepackedLayoutB =",
      "      PrepackedLayoutBTemplate<ElementA_, ElementB_, ElementConvertGroup,",
      "                               AccumulatorT, LayoutA_Transpose, KernelSchedule>;",
      "",
      "  static int constexpr TileShapeK =",
      "      128 * 8 / cutlass::sizeof_bits<MmaType>::value;",
      "  static int constexpr AlignmentA = 128 / cutlass::sizeof_bits_v<ElementA>;",
      "  static int constexpr AlignmentB = 128 / cutlass::sizeof_bits_v<ElementB>;",
      "  static int constexpr AlignmentC =",
      "      (with_C) ? 128 / cutlass::sizeof_bits_v<ElementC> : 0;",
      "  static int constexpr AlignmentD = 128 / cutlass::sizeof_bits_v<ElementD>;",
      "",
      "  using TileShape = decltype(append(typename ScheduleConfig::TileShapeNM{},",
      "                                    cute::Int<TileShapeK>{}));",
      "  using ClusterShape = typename ScheduleConfig::ClusterShape;",
      "  using EpilogueSchedule = typename ScheduleConfig::EpilogueSchedule;",
      "  using EpilogueTileType = typename ScheduleConfig::EpilogueTileType;",
      "  using TileScheduler = typename ScheduleConfig::TileScheduler;",
      "",
      "  static_assert(",
      "      (!with_channel_scales && !with_token_scales) ||",
      "          ((with_channel_scales && with_token_scales) &&",
      "           std::is_same_v<ElementSChannel, ElementSToken>),",
      "      \"Currently token and channel scales (if present) must be the same type\");",
      "",
      "  // Currently only supports float scales",
      "  using ChTokScalesEpilogue =",
      "      typename vllm::c3x::ScaledEpilogue<ElementAccumulator, ElementD,",
      "                                         TileShape>;",
      "  static_assert((with_channel_scales || with_token_scales) ||",
      "                    (std::is_same_v<ElementSChannel, float> &&",
      "                     std::is_same_v<ElementSToken, float>),",
      "                \"Currently token and channel scales (if present) must be float \"",
      "                \"(and if one is present the other must be too)\");",
      "",
      "  using StoreEpilogueCompute = typename cutlass::epilogue::fusion::Sm90EVT<",
      "      cutlass::epilogue::fusion::Sm90AccFetch>;",
      "",
      "  using EVTCompute =",
      "      std::conditional_t<with_channel_scales || with_token_scales,",
      "                         typename ChTokScalesEpilogue::EVTCompute,",
      "                         StoreEpilogueCompute>;",
      "",
      "  // EVTCompute",
      "  using CollectiveEpilogue =",
      "      typename cutlass::epilogue::collective::CollectiveBuilder<",
      "          ArchTag, OperatorClass, TileShape, ClusterShape, EpilogueTileType,",
      "          ElementAccumulator, ElementSChannel, ElementC, LayoutC_Transpose,",
      "          AlignmentC, ElementD, LayoutD_Transpose, AlignmentD, EpilogueSchedule,",
      "          EVTCompute>::CollectiveOp;",
      "",
      "  using CollectiveMainloop =",
      "      typename cutlass::gemm::collective::VLLMCollectiveBuilder<",
      "          cutlass::gemm::collective::MacheteKernelTag, ArchTag, OperatorClass,",
      "          BTypeTuple, PrepackedLayoutB, AlignmentB, ElementA, LayoutA_Transpose,",
      "          AlignmentA, ElementAccumulator, TileShape, ClusterShape,",
      "          cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(",
      "              sizeof(typename CollectiveEpilogue::SharedStorage))>,",
      "          KernelSchedule>::CollectiveOp;",
      "",
      "  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<",
      "      Shape<int, int, int, int>,  // Indicates ProblemShape",
      "      CollectiveMainloop, CollectiveEpilogue, TileScheduler>;",
      "  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;",
      "",
      "  // stride_B is unused (since B is prepacked), but still required by cutlass",
      "  using _StrideB = cutlass::detail::TagToStrideB_t<_LayoutB>;",
      "",
      "  using Arguments = typename Gemm::Arguments;",
      "  using MainloopArguments = typename GemmKernel::MainloopArguments;",
      "  using EpilogueArguments = typename GemmKernel::EpilogueArguments;",
      "",
      "  static Arguments create_arguments(",
      "      cudaStream_t stream,",
      "      torch::Tensor const& A,  // MxK matrix",
      "      torch::Tensor const& B,  // KxN prepacked matrix",
      "      torch::Tensor& D,        // MxN matrix",
      "      std::optional<torch::Tensor> const& maybe_g_scales,  // scale_KxN matrix",
      "      std::optional<torch::Tensor> const& maybe_g_zeros,   // scale_KxN matrix",
      "      std::optional<int64_t> maybe_group_size,",
      "      std::optional<torch::Tensor> const& maybe_ch_scales,   // len N vector",
      "      std::optional<torch::Tensor> const& maybe_tok_scales)  // len M vector",
      "  {",
      "    static_assert(!with_group_zeropoints || with_group_scales);",
      "",
      "    int M = A.size(0), N = B.size(1), K = A.size(1);",
      "    TORCH_CHECK(D.size(0) == M && D.size(1) == N);",
      "",
      "    auto layout_A = make_cute_layout<StrideA>(A, \"A\");",
      "    auto layout_D = make_cute_layout<StrideD>(D, \"D\");",
      "    auto layout_S_group =",
      "        maybe_make_cute_layout<StrideSGroup>(maybe_g_scales, \"group_scales\");",
      "    auto layout_Z_group =",
      "        maybe_make_cute_layout<StrideZGroup>(maybe_g_zeros, \"group_zeros\");",
      "    int64_t numel_S_channel = maybe_ch_scales ? maybe_ch_scales->numel() : 0;",
      "    int64_t numel_S_token = maybe_tok_scales ? maybe_tok_scales->numel() : 0;",
      "",
      "    auto unwrap = [](auto const& t) {",
      "      return t ? t->const_data_ptr() : nullptr;",
      "    };",
      "    auto A_ptr = static_cast<ElementA const*>(A.const_data_ptr());",
      "    auto B_ptr = static_cast<ElementB const*>(B.const_data_ptr());",
      "    auto D_ptr = static_cast<ElementD*>(D.mutable_data_ptr());",
      "    auto S_group_ptr =",
      "        static_cast<ElementSGroup const*>(unwrap(maybe_g_scales));",
      "    auto Z_group_ptr = static_cast<ElementZGroup const*>(unwrap(maybe_g_zeros));",
      "    auto S_channel_ptr =",
      "        static_cast<ElementSChannel const*>(unwrap(maybe_ch_scales));",
      "    auto S_token_ptr =",
      "        static_cast<ElementSToken const*>(unwrap(maybe_tok_scales));",
      "",
      "    int const group_size =",
      "        maybe_group_size == -1 ? K : maybe_group_size.value_or(K);",
      "    int const scale_k = (K + group_size - 1) / group_size;",
      "",
      "    TORCH_CHECK(size<0>(layout_A) == M && size<1>(layout_A) == K);",
      "    TORCH_CHECK(size<0>(layout_D) == M && size<1>(layout_D) == N);",
      "",
      "    if constexpr (with_group_scales) {",
      "      TORCH_CHECK(S_group_ptr && layout_S_group);",
      "      TORCH_CHECK((size<0>(*layout_S_group) == scale_k &&",
      "                   size<1>(*layout_S_group) == N));",
      "    } else {",
      "      TORCH_CHECK(!S_group_ptr, \"Scales not supported\");",
      "    }",
      "",
      "    if constexpr (with_group_zeropoints) {",
      "      TORCH_CHECK(Z_group_ptr && layout_Z_group);",
      "      TORCH_CHECK((size<0>(*layout_Z_group) == scale_k &&",
      "                   size<1>(*layout_Z_group) == N));",
      "      TORCH_CHECK(layout_S_group && *layout_Z_group == *layout_S_group,",
      "                  \"Scales and zeros must have the same layout\");",
      "    } else {",
      "      TORCH_CHECK(!Z_group_ptr, \"Zeropoints not supported\");",
      "    }",
      "",
      "    if constexpr (with_channel_scales || with_token_scales) {",
      "      TORCH_CHECK(",
      "          (maybe_ch_scales->numel() == N || maybe_ch_scales->numel() == 1) &&",
      "          (maybe_tok_scales->numel() == M || maybe_tok_scales->numel() == 1));",
      "    }",
      "",
      "    // Transpose A and D",
      "    // A doesn't need to be transposed since cutlass expects a NxK matrix",
      "    //  for B (which is At)",
      "    auto stride_At = layout_A.stride();",
      "    auto stride_Dt = permute_layout<1, 0, 2>(layout_D).stride();",
      "",
      "    MainloopArguments mainloop_arguments{};",
      "    // {Accum, C, C_layout, D, D}",
      "    EpilogueArguments epilogue_arguments{};",
      "",
      "    if constexpr (with_channel_scales || with_token_scales) {",
      "      epilogue_arguments =",
      "          EpilogueArguments{ChTokScalesEpilogue::prepare_args(",
      "                                *maybe_ch_scales, *maybe_tok_scales),",
      "                            nullptr,",
      "                            {},",
      "                            D_ptr,",
      "                            stride_Dt};",
      "    } else {",
      "      epilogue_arguments = EpilogueArguments{{}, nullptr, {}, D_ptr, stride_Dt};",
      "    }",
      "",
      "    if constexpr (with_group_scales && with_group_zeropoints) {",
      "      auto stride_S_group = permute_layout<1, 0, 2>(*layout_S_group).stride();",
      "      mainloop_arguments = MainloopArguments{",
      "          B_ptr,       _StrideB{},     A_ptr,      stride_At,",
      "          S_group_ptr, stride_S_group, group_size, Z_group_ptr};",
      "    } else if constexpr (with_group_scales) {",
      "      auto stride_S_group = permute_layout<1, 0, 2>(*layout_S_group).stride();",
      "      mainloop_arguments =",
      "          MainloopArguments{B_ptr,       _StrideB{},     A_ptr,     stride_At,",
      "                            S_group_ptr, stride_S_group, group_size};",
      "    } else {",
      "      mainloop_arguments =",
      "          MainloopArguments{B_ptr, _StrideB{}, A_ptr, stride_At};",
      "    }",
      "",
      "    return Arguments{cutlass::gemm::GemmUniversalMode::kGemm,",
      "                     {N, M, K, 1},",
      "                     mainloop_arguments,",
      "                     epilogue_arguments};",
      "  };",
      "",
      "  static size_t get_workspace_size(Arguments const& args) {",
      "    return Gemm::get_workspace_size(args);",
      "  }",
      "",
      "  static bool can_implement(Arguments const& args) {",
      "    return Gemm::can_implement(args) == cutlass::Status::kSuccess;",
      "  }",
      "",
      "  static void run(Arguments const& args, void* workspace, cudaStream_t stream) {",
      "    Gemm gemm_op;",
      "",
      "    cutlass::Status status = gemm_op.initialize(args, workspace, stream);",
      "    TORCH_CHECK(status == cutlass::Status::kSuccess,",
      "                \"Machete kernel failed to initialize workspace\");",
      "",
      "    status = gemm_op.run(stream);",
      "    TORCH_CHECK(status == cutlass::Status::kSuccess, \"Machete kernel failed\");",
      "  }",
      "};",
      "",
      "};  // namespace machete"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/machete/machete_prepacked_layout.cuh",
    "source": [
      "#pragma once",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <torch/all.h>",
      "",
      "// clang-format off",
      "// The cutlass include order matters (annoyingly)",
      "",
      "#include \"cutlass/cutlass.h\"",
      "",
      "#include \"cute/tensor.hpp\"",
      "#include \"cutlass/tensor_ref.h\"",
      "#include \"cutlass/epilogue/collective/default_epilogue.hpp\"",
      "#include \"cutlass/epilogue/thread/linear_combination.h\"",
      "#include \"cutlass/gemm/dispatch_policy.hpp\"",
      "#include \"cutlass/gemm/collective/collective_builder.hpp\"",
      "#include \"cutlass/epilogue/collective/collective_builder.hpp\"",
      "#include \"cutlass/gemm/device/gemm_universal_adapter.h\"",
      "#include \"cutlass/gemm/kernel/gemm_universal.hpp\"",
      "// clang-format on",
      "",
      "#include \"cutlass_extensions/cute_utils.cuh\"",
      "#include \"machete_collective_builder.cuh\"",
      "#include \"machete_interleaving_utils.cuh\"",
      "",
      "namespace machete {",
      "",
      "using namespace cute;",
      "",
      "struct IlvBlkLayoutAuto {};",
      "",
      "// This defines a prepacked layout for the B matrix, where the matrix is broken",
      "// up into PPBlockShape_NK blocks. The data within each block is then compactly",
      "// stored in memory such that when performing a TiledMMA operation with the same",
      "// shape as prepacked block, all the data for a given thread is contiguous in",
      "// memory. This allows us to use wider shared memory loads when loading B from",
      "// shared memory. The values within a thread are also potentially interlaeved",
      "// inorder to allow for more efficient upconverting.",
      "//",
      "// The contract here is that the `TiledMma` determined below matches the one",
      "// ultimately used in the kernel. (this is also why the other element types are",
      "// required along with the kernel schedule)",
      "template <typename ElementA_, typename ElementB_, typename ElementConvert_,",
      "          typename AccumulatorT, class LayoutB, class KernelSchedule,",
      "          typename IlvBlkLayout_ = IlvBlkLayoutAuto>",
      "// clang-format on",
      "struct PrepackedLayoutBTemplate {",
      "  using MmaType = ElementA_;",
      "  using ElementA = ElementA_;",
      "  using ElementB = ElementB_;",
      "  using ElementAccumulator = AccumulatorT;",
      "  using ElementMma = MmaType;",
      "",
      "  // Interleave for 4bit bit types when we are not upconverting to fp8 or int8,",
      "  // in those cases case we use a LUT using prmt instructions to upconvert and",
      "  // is more efficient if the data is not interleaved For 8bit+ prmt",
      "  // instructions makes non-interleaved layouts efficient enough we don't need",
      "  // iterleaved layouts (and can reuse more of the existing cutlass converts)",
      "  static constexpr bool should_interleave =",
      "      sizeof_bits_v<ElementB> <= 4 &&",
      "      !std::is_same_v<ElementConvert_, cutlass::float_e4m3_t> &&",
      "      !std::is_same_v<ElementConvert_, int8_t>;",
      "",
      "  // Only use interleaved layouts for subbyte weights,",
      "  using IlvdBlkLayout = std::conditional_t<",
      "      std::is_same_v<IlvBlkLayout_, IlvBlkLayoutAuto>,",
      "      std::conditional_t<",
      "          should_interleave,",
      "          decltype(get_interleaved_blk_layout<",
      "                   ElementB, sizeof_bits_v<ElementConvert_>, 32>()),",
      "          void>,",
      "      IlvBlkLayout_>;",
      "",
      "  // TODO (LucasWilkinson): compare the performance for other sizes",
      "  // Prepacked block shape, smallest layout atom for loading into registers",
      "  //   (can contain multiple wgmma instructions worth of data in one block)",
      "  // We ideally want this to be configured such that a thread can perform 128bit",
      "  // loads, i.e. we amount of data associated with each thread within a",
      "  // prepacked block is a multiple of 128bits, when using a cooperative sechdule",
      "  // we have 256 threads working a single block at a time, this means each",
      "  // thread works on `sizeof_bits_v<ElementB> * (128*64) / 256` bits of data,",
      "  // for a 4bit type this would be 128bits",
      "  using PPBlockShape_NK = Shape<_128, _64>;",
      "",
      "  // Create the shape of the tile anticipated to be used by the GEMM kernel,",
      "  //  when the kernel executes we will compute `Ct = Bt * At` since the",
      "  //  quantized weights (B), must be the lhs operand so the flow through",
      "  //  registers.",
      "  // The _128 here doesn't actually impact the shape of the stored tile directly",
      "  //  but may impact the op selected by rs_op_selector",
      "  using GemmTileShape = decltype(make_shape(size<0>(PPBlockShape_NK{}), _128{},",
      "                                            size<1>(PPBlockShape_NK{})));",
      "",
      "  static constexpr cute::GMMA::Major GmmaMajorB =",
      "      gmma_rs_tag_to_major_B<LayoutB>();",
      "",
      "  // For coop schedules we have two warp groups cooperatively issuing wgmma",
      "  // instructions so we use 2 atoms along the M dim (one for each warpgroup)",
      "  using AtomLayoutMNK = cute::conditional_t<",
      "      cute::is_same_v<KernelSchedule, KernelTmaWarpSpecializedCooperative>,",
      "      Layout<Shape<_2, _1, _1>>, Layout<Shape<_1, _1, _1>>>;",
      "",
      "  using TiledMma = decltype(cute::make_tiled_mma(",
      "      cute::GMMA::rs_op_selector<ElementMma, ElementMma, ElementAccumulator,",
      "                                 GemmTileShape, GMMA::Major::K, GmmaMajorB>(),",
      "      AtomLayoutMNK{}));",
      "",
      "  // Prepacked block, (athrid, val) -> (N,K)",
      "  // i.e. ((ThrV,(ThrN,ThrK)),(FrgV,(RestN,RestK,...))) -> (N,K)",
      "  CUTE_HOST_DEVICE static constexpr auto ppblock_TV_to_NK() {",
      "    return TiledMma{}.thrfrg_A(make_layout(PPBlockShape_NK{}));",
      "  }",
      "",
      "  // Prepacked block, (N,K) -> (athrid, val)",
      "  // i.e. (N,K) -> ((ThrV,(ThrN,ThrK)),(FrgV,(RestN,RestK,...)))",
      "  CUTE_HOST_DEVICE static constexpr auto ppblock_NK_to_TV() {",
      "    return right_inverse(ppblock_TV_to_NK()).with_shape(PPBlockShape_NK{});",
      "  }",
      "",
      "  // Prepacked block, (athrid, val) -> (storage_offset)",
      "  // i.e. ((ThrV,(ThrN,ThrK)),(FrgV,(RestN,RestK,...))) -> (storage_idx)",
      "  CUTE_HOST_DEVICE static constexpr auto ppblock_TV_to_offset() {",
      "    // Return iterleaved layout",
      "    return make_ordered_layout(shape(ppblock_TV_to_NK()), Step<_1, _0>{});",
      "  }",
      "",
      "  // Prepacked block, (athrid, val) -> (storage_offset)",
      "  // i.e. ((ThrV,(ThrM,ThrK)),(IlvdFrgV,(RestM,RestK,...))) -> (storage_idx)",
      "  CUTE_HOST_DEVICE static constexpr auto ppblock_ilvd_TV_to_offset() {",
      "    auto layout_no_interleave =",
      "        make_ordered_layout(shape(ppblock_TV_to_NK()), Step<_1, _0>{});",
      "",
      "    if constexpr (std::is_same_v<IlvdBlkLayout, void>) {",
      "      return layout_no_interleave;",
      "    } else {",
      "      // interleave by transforming FrgV into interleaved blocks where each",
      "      // block has the layout IlvdBlkLayout, for example if IlvdBlkLayout is",
      "      // (2, 2) : (2, 1) then we get: ((2, 2), size(FrgV) / 4) : ((2, 1), 4)",
      "      //   if FrgV is {A, B, C, D, E, F, G, H}",
      "      //   then ((IlvBlk), FrgB) is {A, C, B, D, C, G, D, H}",
      "      auto frgV = get<1, 0>(layout_no_interleave);",
      "      auto ilvdBlk = IlvdBlkLayout{};",
      "      static_assert(size(frgV) % size(ilvdBlk) == 0,",
      "                    \"FrgV must be divisible by size(ilvdBlk)\");",
      "      auto ilvd_FrgV = make_layout(",
      "          make_shape(shape(ilvdBlk), Int<size(frgV) / size(ilvdBlk)>{}),",
      "          make_stride(stride(ilvdBlk), size(ilvdBlk)));",
      "",
      "      // Return iterleaved layout",
      "      return make_layout(",
      "          get<0>(layout_no_interleave),",
      "          make_layout(ilvd_FrgV, get<1, 1>(layout_no_interleave)));",
      "    }",
      "  }",
      "",
      "  // Prepacked block, (M,K) -> (storage_offset)",
      "  CUTE_HOST_DEVICE static constexpr auto ppblock_ilvd_NK_to_offset() {",
      "    // do (M,K) -> (athrid, val) -> (storage_idx)",
      "    return ppblock_ilvd_TV_to_offset().compose(ppblock_NK_to_TV());",
      "  }",
      "",
      "  // ((athrid, val), (BlocksN, BlocksK), L) -> (storage_idx)",
      "  template <typename Shape_NKL>",
      "  CUTE_HOST_DEVICE static constexpr auto TVbNbKL_to_offset(",
      "      Shape_NKL shape_mkl) {",
      "    constexpr auto block_layout = ppblock_TV_to_offset();",
      "",
      "    // (BlocksN, BlocksK, L)",
      "    auto blocks_shape =",
      "        cute::transform(shape_mkl, append(PPBlockShape_NK{}, _1{}),",
      "                        [](auto x, auto y) { return x / y; });",
      "",
      "    // ((athrid, val), (BlocksN, BlocksK, L)) -> (storage_idx)",
      "    auto result = make_layout(",
      "        block_layout,",
      "        make_layout(blocks_shape,",
      "                    compact_col_major(blocks_shape, size(block_layout))));",
      "",
      "    // ((athrid, val), (BlocksN, BlocksK, L))",
      "    //   => ((athrid, val), (BlocksN, BlocksK), L)",
      "    return group<1, 3>(result(_, repeat<rank<1>(result)>(_)));",
      "  }",
      "",
      "  // ((athrid_val), (BlocksN, BlocksK, L)) -> (N, K, L)",
      "  template <typename Shape_NKL>",
      "  CUTE_HOST_DEVICE static constexpr auto TVbNbKL_to_offset_copy(",
      "      Shape_NKL shape_mkl) {",
      "    auto layout = TVbNbKL_to_offset(shape_mkl);",
      "    // for 4-bit elements, having >= 64 values per column",
      "    // allows TMA to load full 32-byte sectors",
      "    auto inner_layout =",
      "        make_layout(make_shape(_256{}, size<0>(layout) / _256{}));",
      "",
      "    return make_layout(inner_layout, get<1>(layout), get<2>(layout));",
      "  }",
      "",
      "  // ((BlockN, BlockK), (BlocksN, BlocksK), L) -> (storage_idx)",
      "  template <typename Shape_NKL>",
      "  CUTE_HOST_DEVICE static constexpr auto ilvd_NKbNbKL_to_offset(",
      "      Shape_NKL shape_mkl) {",
      "    constexpr auto block_layout = ppblock_ilvd_NK_to_offset();",
      "",
      "    // (BlocksN, BlocksK, L)",
      "    auto blocks_shape =",
      "        cute::transform(shape_mkl, append(PPBlockShape_NK{}, _1{}),",
      "                        [](auto x, auto y) { return x / y; });",
      "",
      "    // ((athrid, val), (BlocksN, BlocksK, L)) -> (storage_idx)",
      "    auto result = make_layout(",
      "        block_layout,",
      "        make_layout(blocks_shape,",
      "                    compact_col_major(blocks_shape, size(block_layout))));",
      "",
      "    // ((athrid, val), (BlocksN, BlocksK, L)) => ((athrid, val), (BlocksN,",
      "    // BlocksK), L)",
      "    return group<1, 3>(result(_, repeat<rank<1>(result)>(_)));",
      "  }",
      "",
      "  // (BlocksN, BlocksK, L) -> (storage_idx)",
      "  template <typename Shape_NKL>",
      "  CUTE_HOST_DEVICE static constexpr auto bNbKL_to_offset(Shape_NKL shape_mkl) {",
      "    // (BlocksN, BlocksK, L)",
      "    auto blocks_shape =",
      "        cute::transform(shape_mkl, append(PPBlockShape_NK{}, _1{}),",
      "                        [](auto x, auto y) { return x / y; });",
      "    auto stride = size(PPBlockShape_NK{});",
      "",
      "    // (BlocksN, BlocksK, L) -> (storage_idx)",
      "    return make_layout(blocks_shape, compact_col_major(blocks_shape, stride));",
      "  }",
      "",
      "  // ((athrid, val), (BlocksN, BlocksK, L)) -> (N, K, L)",
      "  template <class Shape_NKL>",
      "  CUTE_HOST_DEVICE static auto TVbNbK_to_NKL(Shape_NKL shape_mkl) {",
      "    auto tile = make_tile(make_layout(size<0>(PPBlockShape_NK{})),",
      "                          make_layout(size<1>(PPBlockShape_NK{})));",
      "",
      "    // ((BlockN, BlockK), (BlocksN, BlocksK, L)) -> (N, K, L)",
      "    auto tiled_A = zipped_divide(make_layout(shape_mkl), tile);",
      "    return tiled_A.compose(ppblock_TV_to_NK(), _);",
      "  }",
      "",
      "  // (N, K, L) -> ((athrid, val), (BlocksN, BlocksK), L)",
      "  template <class Shape_NKL>",
      "  CUTE_HOST_DEVICE static auto NKL_to_TVbNbK(Shape_NKL shape_mkl) {",
      "    auto TVbNbK_to_NKL_layout = TVbNbK_to_NKL(shape_mkl);",
      "    return blocked_product(ppblock_NK_to_TV(),",
      "                           make_layout(shape<1>(TVbNbK_to_NKL_layout)));",
      "  }",
      "};",
      "",
      "};  // namespace machete"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/machete/machete_collective_builder.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"cutlass_extensions/vllm_collective_builder.cuh\"",
      "#include \"machete_mainloop.cuh\"",
      "",
      "namespace cutlass::gemm::collective {",
      "using namespace cute;",
      "",
      "struct MacheteKernelTag {};",
      "",
      "template <class ElementPairA_, class GmemLayoutA_, int AlignmentA,",
      "          class ElementPairB_, class GmemLayoutB_, int AlignmentB,",
      "          class ElementAccumulator, class TileShape_MNK, class ClusterShape_MNK,",
      "          class StageCountType, class KernelScheduleType>",
      "struct VLLMCollectiveBuilder<",
      "    MacheteKernelTag, arch::Sm90, arch::OpClassTensorOp, ElementPairA_,",
      "    GmemLayoutA_, AlignmentA, ElementPairB_, GmemLayoutB_, AlignmentB,",
      "    ElementAccumulator, TileShape_MNK, ClusterShape_MNK, StageCountType,",
      "    KernelScheduleType,",
      "    cute::enable_if_t<(",
      "        cute::is_same_v<KernelScheduleType, KernelTmaWarpSpecialized> ||",
      "        cute::is_same_v<KernelScheduleType, KernelTmaWarpSpecializedPingpong> ||",
      "        cute::is_same_v<KernelScheduleType,",
      "                        KernelTmaWarpSpecializedCooperative>)>> {",
      "  using CollectiveOp = machete::MacheteCollectiveMma<",
      "      ElementPairA_, GmemLayoutA_, AlignmentA, ElementPairB_, GmemLayoutB_,",
      "      AlignmentB, ElementAccumulator, TileShape_MNK, ClusterShape_MNK,",
      "      StageCountType, KernelScheduleType>;",
      "};",
      "",
      "};  // namespace cutlass::gemm::collective"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/machete/machete_interleaving_utils.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"cutlass/cutlass.h\"",
      "#include \"cute/layout.hpp\"",
      "",
      "namespace machete {",
      "",
      "using namespace cute;",
      "",
      "// get an interleaved block layout where each element consecutive element has a",
      "// stride of bit_stride and the block width is blk_bit_width,",
      "// examples:",
      "//  size_bits<T> = 8, bit_stride = 8,  blk_bit_width = 32 -> 4:1",
      "//  size_bits<T> = 8, bit_stride = 16, blk_bit_width = 32 -> (2, 2):(2, 1)",
      "//  size_bits<T> = 4, bit_stride = 8,  blk_bit_width = 32 -> (4, 2):(2, 1)",
      "//  size_bits<T> = 4, bit_stride = 16, blk_bit_width = 32 -> (2, 4):(4, 1)",
      "template <typename T, int bit_stride, int blk_bit_width>",
      "CUTE_HOST_DEVICE static constexpr auto get_interleaved_blk_layout() {",
      "  static_assert(blk_bit_width % bit_stride == 0);",
      "  static_assert(bit_stride % cute::sizeof_bits_v<T> == 0);",
      "",
      "  constexpr auto elems_per_blk = blk_bit_width / cute::sizeof_bits_v<T>;",
      "",
      "  if constexpr (cute::sizeof_bits_v<T> == bit_stride) {",
      "    // identity layout",
      "    return Layout<Shape<Int<elems_per_blk>>>{};",
      "  } else {",
      "    constexpr auto elems_per_stride = bit_stride / cute::sizeof_bits_v<T>;",
      "    constexpr auto num_strides = elems_per_blk / elems_per_stride;",
      "    return Layout<Shape<Int<num_strides>, Int<elems_per_stride>>,",
      "                  Stride<Int<elems_per_stride>, Int<1>>>{};",
      "  }",
      "}",
      "",
      "};  // namespace machete"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/machete/machete_mainloop.cuh",
    "source": [
      "//",
      "// Based off of:",
      "//   cutlass/gemm/collective/sm90_mma_tma_gmma_rs_warpspecialized_mixed_input.hpp",
      "// Specifically:",
      "//   https://github.com/NVIDIA/cutlass/tree/06b21349bcf6ddf6a1686a47a137ad1446579db9/include/cutlass/gemm/collective/sm90_mma_tma_gmma_rs_warpspecialized_mixed_input.hpp",
      "// Referred to as upstream from in the comments",
      "//",
      "// The main optimization machete implements compared to upstream is to prepack",
      "// the weight matrix to more closely match the shape of the wgmma instructions",
      "// allowing for wider (ideally 128bit) shared memory loads. For subbyte types",
      "// this is done by packing values from multiple wgmma loads (for a single",
      "// thread) into a single 128bit load. This is very similar to layout used in",
      "// Marlin, although specific to the wgmma instructions.",
      "//",
      "// Since the wgmma instructions only support sourcing from registers for the A",
      "// operand, and we want to upconvert/decompress the weight values/elements",
      "// before feeding them into the tensor cores in registers, we need the weight",
      "// matrix to be A. To achieve this we compute the transpose of Y = XW^t as",
      "// Y^t = W^tX^t. This is mostly done outside of this file in",
      "// csrc/quantization/machete/machete_mm_kernel.cuh, but this why A is the",
      "// quantized/narrow type and has the prepacked layout despite the API being:",
      "//   B_prepacked = machete_prepack_B(B)",
      "//   Y = machete_mm(A, B_prepacked)",
      "//",
      "#pragma once",
      "",
      "// clang-format off",
      "#include \"cutlass/cutlass.h\"",
      "#include \"cutlass/numeric_conversion.h\"",
      "#include \"cute/arch/cluster_sm90.hpp\"",
      "#include \"cute/arch/copy_sm90.hpp\"",
      "#include \"cutlass/gemm/gemm.h\"",
      "#include \"cutlass/detail/dependent_false.hpp\"",
      "#include \"cutlass/gemm/dispatch_policy.hpp\"",
      "#include \"cutlass/detail/layout.hpp\"",
      "",
      "#include \"cute/algorithm/functional.hpp\"",
      "#include \"cute/atom/mma_atom.hpp\"",
      "#include \"cute/atom/copy_traits_sm90_tma.hpp\"",
      "#include \"cute/algorithm/gemm.hpp\"",
      "#include \"cute/numeric/arithmetic_tuple.hpp\"",
      "#include \"cutlass/pipeline/pipeline.hpp\"",
      "#include \"cutlass/transform/collective/sm90_wgmma_transpose.hpp\"",
      "#include \"cutlass/trace.h\"",
      "",
      "#include \"cutlass/detail/collective.hpp\"",
      "// clang-format on",
      "",
      "#include \"cutlass_extensions/cute_utils.cuh\"",
      "",
      "namespace machete {",
      "",
      "using namespace cute;",
      "using namespace cutlass;",
      "using namespace cutlass::gemm;",
      "using namespace cutlass::gemm::collective;",
      "using namespace cutlass::gemm::collective::detail;",
      "",
      "template <class ElementATuple_, class GmemLayoutA, int AlignmentA,",
      "          class ElementB_, class GmemLayoutB, int AlignmentB,",
      "          class ElementAccumulator_, class TileShape_MNK,",
      "          class ClusterShape_MNK, class StageCountType,",
      "          class KernelScheduleType>",
      "struct MacheteCollectiveMma {",
      "  using Schedule = KernelScheduleType;",
      "  static_assert(",
      "      cute::is_same_v<Schedule, KernelTmaWarpSpecialized> ||",
      "          cute::is_same_v<Schedule, KernelTmaWarpSpecialized> ||",
      "          cute::is_same_v<Schedule, KernelTmaWarpSpecializedPingpong> ||",
      "          cute::is_same_v<Schedule, KernelTmaWarpSpecializedPingpong> ||",
      "          cute::is_same_v<Schedule, KernelTmaWarpSpecializedCooperative> ||",
      "          cute::is_same_v<Schedule, KernelTmaWarpSpecializedCooperative>,",
      "      \"KernelSchedule must be one of the warp specialized policies\");",
      "",
      " public:",
      "  static constexpr bool ALayoutIsPrepacked = true;",
      "",
      "  // Prepacked block shape (N is M in the transposed problem)",
      "  using PPBlockShape_MK = typename GmemLayoutA::PPBlockShape_NK;",
      "  // Prepacked blocks per dim for a single MMA tile",
      "  using PPBlocksPerTile_MK = decltype(make_shape(",
      "      size<0>(TileShape_MNK{}) / size<0>(PPBlockShape_MK{}),",
      "      size<2>(TileShape_MNK{}) / size<1>(PPBlockShape_MK{})));",
      "",
      "  using IlvdBlkLayout = typename GmemLayoutA::IlvdBlkLayout;",
      "",
      "  static_assert(size<0>(TileShape_MNK{}) % size<0>(PPBlockShape_MK{}) == 0,",
      "                \"M in PPBlockShape_MK must evenly divide M TileShape_MNK\");",
      "  static_assert(size<2>(TileShape_MNK{}) % size<1>(PPBlockShape_MK{}) == 0,",
      "                \"K in PPBlockShape_MK must evenly divide K TileShape_MNK\");",
      "",
      "  using ArchTag = arch::Sm90;",
      "  using TileShape = TileShape_MNK;",
      "  using ClusterShape = ClusterShape_MNK;",
      "  using ElementA = deduce_mixed_width_dtype_t<0, ElementATuple_>;",
      "  using StrideA = TagToStrideA_t<layout::RowMajor>;",
      "  using ElementB = ElementB_;",
      "  using StrideB = TagToStrideB_t<GmemLayoutB>;",
      "  using ElementAccumulator = ElementAccumulator_;",
      "  using ElementMma = ElementB;",
      "  using ElementATuple =",
      "      cute::conditional_t<!cute::is_tuple<ElementATuple_>::value,",
      "                          cute::tuple<ElementA>, ElementATuple_>;",
      "",
      "  static constexpr cute::GMMA::Major GmmaMajorA =",
      "      gmma_rs_tag_to_major_A<layout::RowMajor>();",
      "  static constexpr cute::GMMA::Major GmmaMajorB =",
      "      gmma_rs_tag_to_major_B<GmemLayoutB>();",
      "",
      "  // For coop schedules we have two warp groups cooperatively issuing wgmma",
      "  // instructions so we use 2 atoms along the M dim (one for each warpgroup)",
      "  using AtomLayoutMNK = cute::conditional_t<",
      "      cute::is_same_v<KernelScheduleType, KernelTmaWarpSpecializedCooperative>,",
      "      Layout<Shape<_2, _1, _1>>, Layout<Shape<_1, _1, _1>>>;",
      "",
      "  using TiledMma = decltype(cute::make_tiled_mma(",
      "      cute::GMMA::rs_op_selector<ElementMma, ElementMma, ElementAccumulator,",
      "                                 TileShape_MNK, GMMA::Major::K, GmmaMajorB>(),",
      "      AtomLayoutMNK{}));",
      "",
      " private:",
      "  //",
      "  // the setup section (until \"section setup end\") contains a combination of",
      "  // modified code from (used as a starting point):",
      "  //   `cutlass/gemm/collective/builders/sm90_gmma_builder.inl`",
      "  //   `cutlass/gemm/collective/sm90_mma_tma_gmma_rs_warpspecialized_mixed_input.hpp`",
      "  //   (upstream)",
      "  //",
      "  // however in-order to simplify the code we combine a lot of the logic from",
      "  // `CollectiveMma` and `CollectiveBuilder` into this class, this also makes",
      "  // sense given that we have flexibility on layouts here. We also simplify the",
      "  // code by only supporting scales and zeros for A (in the transposed problem,",
      "  // B from an API perspective), also since we force A to be the narrow type",
      "  // (i.e. the type to be upconverted) we can remove all the `SwapAB` logic in",
      "  // the upstream also simplifying the code. This section includes new logic",
      "  // (compared ustream) for handling the prepacked-A layouts (in the transposed",
      "  // problem, B from an API perspective)",
      "  //",
      "  using ElementScale = deduce_mixed_width_dtype_t<1, ElementATuple_>;",
      "  using ElementZero = deduce_mixed_width_dtype_t<2, ElementATuple_>;",
      "",
      "  static constexpr bool IsANarrow = cutlass::sizeof_bits<ElementA>::value <",
      "                                    cutlass::sizeof_bits<ElementB>::value;",
      "  static_assert(IsANarrow,",
      "                \"A must be the narrow one since its the one that flows through \"",
      "                \"registers.\");",
      "",
      " public:",
      "  static constexpr int PipelineStages =",
      "      compute_stage_count_or_override_single_affine_transformed_input<",
      "          sm90_smem_capacity_bytes, ElementA, ElementB, ElementScale,",
      "          ElementZero, TileShape_MNK>(StageCountType{});",
      "",
      "  struct DispatchPolicy {",
      "    constexpr static int Stages = PipelineStages;",
      "    using ClusterShape = ClusterShape_MNK;",
      "    using Schedule = KernelScheduleType;",
      "  };",
      "",
      "  using GmemTiledCopyA =",
      "      decltype(sm90_cluster_shape_to_tma_atom(shape<1>(ClusterShape_MNK{})));",
      "  using GmemTiledCopyB =",
      "      decltype(sm90_cluster_shape_to_tma_atom(shape<0>(ClusterShape_MNK{})));",
      "",
      "  // ((T, V), (BlocksM, BlocksK), pipe) -> offset",
      "  using SmemLayoutA = decltype(GmemLayoutA::TVbNbKL_to_offset(",
      "      make_shape(size<0>(TileShape_MNK{}), size<2>(TileShape_MNK{}),",
      "                 Int<DispatchPolicy::Stages>{})));",
      "",
      "  using SmemLayoutACopy = decltype(GmemLayoutA::TVbNbKL_to_offset_copy(",
      "      make_shape(size<0>(TileShape_MNK{}), size<2>(TileShape_MNK{}),",
      "                 Int<DispatchPolicy::Stages>{})));",
      "",
      "  using SmemLayoutAtomARowMajor =",
      "      decltype(rs_smem_selector<GmmaMajorA, ElementA,",
      "                                decltype(cute::get<0>(TileShape_MNK{})),",
      "                                decltype(cute::get<2>(TileShape_MNK{}))>());",
      "",
      "  using SmemLayoutAtomScale = Layout<",
      "      Shape<decltype(cute::shape<0>(SmemLayoutAtomARowMajor{})), cute::Int<1>>>;",
      "",
      "  using SmemLayoutAtomB =",
      "      decltype(rs_smem_selector<GmmaMajorB, ElementB,",
      "                                decltype(cute::get<1>(TileShape_MNK{})),",
      "                                decltype(cute::get<2>(TileShape_MNK{}))>());",
      "",
      "  using SmemCopyAtomA = Copy_Atom<cute::DefaultCopy, ElementA>;",
      "  using SmemCopyAtomB = void;",
      "",
      "  //",
      "  //  Validity checks",
      "  //",
      "  static_assert(is_static<TileShape_MNK>::value);",
      "  static_assert(is_static<ClusterShape_MNK>::value);",
      "  static_assert(is_aligned<ElementA, AlignmentA, ElementB, AlignmentB,",
      "                           tma_alignment_bytes>(),",
      "                \"Should meet TMA alignment requirement\\n\");",
      "#ifndef CUTLASS_SM90_COLLECTIVE_BUILDER_SUPPORTED",
      "  static_assert(cutlass::detail::dependent_false<ElementA>,",
      "                \"Unsupported Toolkit for SM90 Collective Builder\\n\");",
      "#endif",
      "",
      " private:",
      "  enum class ConversionMode {",
      "    DirectConvert,",
      "    ConvertAndScale,",
      "    ConvertAndScaleWithZero",
      "  };",
      "",
      " public:",
      "  //",
      "  // Type Aliases",
      "  //",
      "  using KernelSchedule = KernelScheduleType;",
      "",
      "  // For cases where we can't have a void type, we can use this to allow the",
      "  // code to compile when the scale / zero is void.",
      "  using NonVoidElementScale =",
      "      cute::conditional_t<cute::is_void_v<ElementScale>, float, ElementScale>;",
      "  using NonVoidElementZero =",
      "      cute::conditional_t<cute::is_void_v<ElementZero>, float, ElementZero>;",
      "",
      "  // These are always MN major",
      "  using StrideScale = cute::Stride<cute::Int<1>, int64_t, int64_t>;",
      "  // For cases where we can't have a void scale, we can use this to allow the",
      "  // code to compile when the scale is void.",
      "  using NonVoidStrideScale =",
      "      cute::conditional_t<cute::is_void_v<StrideScale>,",
      "                          cute::Stride<_1, int64_t, int64_t>, StrideScale>;",
      "",
      "  static_assert((cutlass::gemm::detail::is_k_major<StrideA>()),",
      "                \"The transformed matrix (A) must be K-major.\");",
      "",
      "  static_assert((sizeof(ElementB) == 2) ||",
      "                    (cutlass::gemm::detail::is_k_major<StrideA>() &&",
      "                     cutlass::gemm::detail::is_k_major<StrideB>()),",
      "                \"The unscaled element (matrix B) must be 2 bytes OR both \"",
      "                \"inputs must be K-major\");",
      "",
      "  static_assert(cutlass::gemm::detail::is_mn_major<NonVoidStrideScale>(),",
      "                \"Scale must be MN major [Col Major if A is scaled, Row Major \"",
      "                \"if B is scaled].\");",
      "",
      "  static_assert(std::is_same_v<typename TiledMma::ValTypeC, ElementAccumulator>,",
      "                \"TiledMma::ValTypeC must be the same as ElementAccumulator.\");",
      "",
      "  using GmemTiledCopyScale = cute::SM90_TMA_LOAD;",
      "",
      "  using SmemCopyAtomScale = Copy_Atom<cute::DefaultCopy, NonVoidElementScale>;",
      "",
      "  // TMA converts f32 input to tf32 when copying from GMEM to SMEM",
      "  // For all other types, cast to size equivalent uint type to avoid any",
      "  // rounding by TMA.",
      "  static constexpr bool ConvertF32toTF32A = cute::is_same_v<float, ElementA>;",
      "  static constexpr bool ConvertF32toTF32B = cute::is_same_v<float, ElementB>;",
      "  using InternalElementA =",
      "      cute::conditional_t<ConvertF32toTF32A, tfloat32_t,",
      "                          uint_bit_t<sizeof_bits_v<ElementA>>>;",
      "  using InternalElementB =",
      "      cute::conditional_t<ConvertF32toTF32B, tfloat32_t,",
      "                          uint_bit_t<sizeof_bits_v<ElementB>>>;",
      "",
      "  using TransformA = cute::identity;",
      "  using TransformB = cute::identity;",
      "",
      "  static constexpr int IsSubbyteA = cute::sizeof_bits_v<InternalElementA> < 8;",
      "  using TmaElementA =",
      "      cute::conditional_t<IsSubbyteA, uint8_t, InternalElementA>;",
      "",
      "  using MainloopPipeline = cutlass::PipelineTmaAsync<DispatchPolicy::Stages>;",
      "  using PipelineState = cutlass::PipelineState<DispatchPolicy::Stages>;",
      "",
      "  using PipelineParams = typename MainloopPipeline::Params;",
      "",
      "  // One threads per CTA are producers (1 for operand tile)",
      "  static constexpr int NumProducerThreadEvents = 1;",
      "",
      "  using ScaleTileShape = decltype(make_shape(shape<0>(TileShape{}),",
      "                                             shape<1>(SmemLayoutAtomScale{})));",
      "",
      "  static_assert(cute::rank(SmemLayoutAtomB{}) == 2,",
      "                \"SmemLayoutAtom must be rank 2 (M/N, K)\");",
      "  static_assert((size<1>(TileShape{}) % size<0>(SmemLayoutAtomB{})) == 0,",
      "                \"SmemLayoutAtom must evenly divide tile shape.\");",
      "  static_assert((size<2>(TileShape{}) % size<1>(SmemLayoutAtomB{})) == 0,",
      "                \"SmemLayoutAtom must evenly divide tile shape.\");",
      "",
      "  static_assert(rank(SmemLayoutAtomScale{}) == 2,",
      "                \"SmemLayoutAtomScale must be rank 2\");",
      "  static_assert((size<0>(TileShape{}) % size<0>(SmemLayoutAtomScale{})) == 0,",
      "                \"SmemLayoutAtomScale must equal the tile shape.\");",
      "  static_assert((size<2>(TileShape{}) % size<1>(SmemLayoutAtomScale{})) == 0,",
      "                \"SmemLayoutAtomScale must evenly divide tile k shape.\");",
      "",
      "  // Tile along modes in a way that maximizes the TMA box size",
      "  using SmemLayoutB = decltype(tile_to_shape(",
      "      SmemLayoutAtomB{},",
      "      make_shape(shape<1>(TileShape{}), shape<2>(TileShape{}),",
      "                 Int<DispatchPolicy::Stages>{}),",
      "      conditional_t<::cutlass::gemm::detail::is_major<0, StrideB>(),",
      "                    Step<_2, _1, _3>, Step<_1, _2, _3>>{}));",
      "",
      "  // It is assumed that the scales and zero-points share the same smem layout",
      "  using SmemLayoutScale = decltype(tile_to_shape(",
      "      SmemLayoutAtomScale{},",
      "      make_shape(shape<0>(ScaleTileShape{}), shape<1>(ScaleTileShape{}),",
      "                 Int<PipelineStages>{})));",
      "",
      "  // If A mn-layout and B mn-layout, transposing B matrix since WGMMA is k-major",
      "  // only (e.g. tf32, fp32, fp8, int8).",
      "  static constexpr bool IsLayoutAmnBmn =",
      "      cute::is_same_v<gemm::detail::StrideToLayoutTagA_t<StrideA>,",
      "                      layout::ColumnMajor> &&",
      "      cute::is_same_v<gemm::detail::StrideToLayoutTagB_t<StrideB>,",
      "                      layout::RowMajor>;",
      "",
      "  static_assert(DispatchPolicy::Stages >= 2,",
      "                \"Specialization requires Stages set to value 2 or more.\");",
      "  static_assert(not cute::is_base_of<cute::GMMA::DescriptorIterator,",
      "                                     typename TiledMma::FrgTypeA>::value &&",
      "                    cute::is_base_of<cute::GMMA::DescriptorIterator,",
      "                                     typename TiledMma::FrgTypeB>::value,",
      "                \"MMA atom must source A from rmem and B operand from smem_desc \"",
      "                \"for this mainloop.\");",
      "  static_assert(cute::is_same_v<GmemTiledCopyA, SM90_TMA_LOAD> ||",
      "                    cute::is_same_v<GmemTiledCopyA, SM90_TMA_LOAD_MULTICAST>,",
      "                \"GmemTiledCopy - invalid SM90 TMA copy atom specified.\");",
      "  static_assert(cute::is_same_v<GmemTiledCopyB, SM90_TMA_LOAD> ||",
      "                    cute::is_same_v<GmemTiledCopyB, SM90_TMA_LOAD_MULTICAST>,",
      "                \"GmemTiledCopy - invalid SM90 TMA copy atom specified.\");",
      "",
      "  using GmmaSmemLayoutB = decltype(tile_to_shape(",
      "      SmemLayoutAtomB{},",
      "      make_shape(shape<1>(TileShape{}), shape<2>(TileShape{}),",
      "                 Int<DispatchPolicy::Stages>{}),",
      "      conditional_t<::cutlass::gemm::detail::is_major<0, StrideB>(),",
      "                    Step<_2, _1, _3>, Step<_1, _2, _3>>{}));",
      "",
      "  // These two restrictions are related, so we place the assertions together.",
      "  // To relax them, we need to handle loading more than 1 row of scales for",
      "  // every main loop iteration. We must also handle updating the pipeline",
      "  // transaction bytes on the fly. NOTE: Deleting this assertion without",
      "  // required changes will cause the code to hang.",
      "  static_assert(size<1>(SmemLayoutAtomScale{}) == 1,",
      "                \"size<1>(SmemLayoutAtomScale) must be 1.\");",
      "",
      " private:",
      "  static constexpr ConversionMode get_conversion_mode() {",
      "    if constexpr (cute::is_void_v<ElementScale>) {",
      "      return ConversionMode::DirectConvert;",
      "    } else if constexpr (cute::is_void_v<ElementZero>) {",
      "      return ConversionMode::ConvertAndScale;",
      "    } else {",
      "      return ConversionMode::ConvertAndScaleWithZero;",
      "    }",
      "  }",
      "",
      "  static constexpr ConversionMode KernelConversionMode = get_conversion_mode();",
      "  static constexpr bool ModeHasScales =",
      "      KernelConversionMode == ConversionMode::ConvertAndScale ||",
      "      KernelConversionMode == ConversionMode::ConvertAndScaleWithZero;",
      "",
      "  // Same as upstream, should be kept the same when possible",
      "  static constexpr auto elements_per_smem_scale() {",
      "    if constexpr (KernelConversionMode == ConversionMode::DirectConvert) {",
      "      return 0;",
      "    } else if constexpr (ModeHasScales) {",
      "      return cute::cosize_v<SmemLayoutScale>;",
      "    } else {",
      "      static_assert(cutlass::detail::dependent_false<KernelSchedule>,",
      "                    \"Type not handled in scale smem allocation.\");",
      "    }",
      "  }",
      "",
      "  // Same as upstream, should be kept the same when possible",
      "  static constexpr auto elements_per_smem_zero() {",
      "    if constexpr (KernelConversionMode == ConversionMode::DirectConvert ||",
      "                  KernelConversionMode == ConversionMode::ConvertAndScale) {",
      "      return 0;",
      "    } else if constexpr (KernelConversionMode ==",
      "                         ConversionMode::ConvertAndScaleWithZero) {",
      "      return cute::cosize_v<SmemLayoutScale>;",
      "    } else {",
      "      static_assert(cutlass::detail::dependent_false<KernelSchedule>,",
      "                    \"Type not handled in scale smem allocation.\");",
      "    }",
      "  }",
      "",
      "  // Same as upstream, should be kept the same when possible, not formatte for",
      "  // easier comparison",
      "  // clang-format off",
      "  // These methods use some the public members of the class. For that reason, we define them after the public section.",
      "  static constexpr uint32_t",
      "  compute_tma_transaction_bytes_mk() {",
      "    constexpr uint32_t baseline_bytes = cutlass::bits_to_bytes(size<0>(SmemLayoutA{}) * size<1>(SmemLayoutA{}) * static_cast<uint32_t>(cute::sizeof_bits_v<InternalElementA>));",
      "",
      "    if constexpr (KernelConversionMode == ConversionMode::DirectConvert) {",
      "      return baseline_bytes;",
      "    }",
      "    else if constexpr (ModeHasScales) {",
      "      constexpr uint32_t scale_tx_bytes = cutlass::bits_to_bytes(size<0>(SmemLayoutScale{}) * size<1>(SmemLayoutScale{}) * static_cast<uint32_t>(cute::sizeof_bits_v<ElementScale>));",
      "      static_assert(scale_tx_bytes % 128 == 0, \"Each scale stage must be 128B aligned.\"); // required by TMA",
      "      if constexpr (KernelConversionMode == ConversionMode::ConvertAndScale) {",
      "        return baseline_bytes + scale_tx_bytes;",
      "      }",
      "      else if constexpr (KernelConversionMode == ConversionMode::ConvertAndScaleWithZero) {",
      "        // Scale and zero share smem layout",
      "        constexpr uint32_t zero_tx_bytes = cutlass::bits_to_bytes(size<0>(SmemLayoutScale{}) * size<1>(SmemLayoutScale{}) * static_cast<uint32_t>(cute::sizeof_bits_v<ElementZero>));",
      "        static_assert(zero_tx_bytes % 128 == 0, \"Each zero stage must be 128B aligned.\"); // required by TMA",
      "        return baseline_bytes + scale_tx_bytes + zero_tx_bytes;",
      "      }",
      "      else {",
      "        static_assert(cutlass::detail::dependent_false<KernelSchedule>, \"Type not handled in tma transaction bytes computation.\");",
      "      }",
      "    }",
      "    else {",
      "      static_assert(cutlass::detail::dependent_false<KernelSchedule>, \"Type not handled in tma transaction bytes computation.\");",
      "    }",
      "  }",
      "",
      "  static constexpr uint32_t",
      "  compute_tma_transaction_bytes_nk() {",
      "    return cutlass::bits_to_bytes(size<0>(SmemLayoutB{}) * size<1>(SmemLayoutB{}) * static_cast<uint32_t>(cute::sizeof_bits_v<InternalElementB>));",
      "  }",
      "  // clang-format on",
      "",
      "  // ((athrid, val), (BlocksM, BlockK), L) -> (storage_idx)",
      "  using PrepackedStrideA = decltype(stride(GmemLayoutA::TVbNbKL_to_offset_copy(",
      "      make_shape(int32_t(0), int32_t(0), int32_t(0)))));",
      "",
      "  using ATensor = decltype(make_tensor(",
      "      get_logical_ptr(static_cast<InternalElementA const*>(nullptr)),",
      "      shape(GmemLayoutA::TVbNbKL_to_offset_copy(",
      "          make_shape(int32_t(0), int32_t(0), int32_t(0)))),",
      "      PrepackedStrideA{}));",
      "",
      "  using BTensor = decltype(make_tensor(",
      "      get_logical_ptr(static_cast<InternalElementB const*>(nullptr)),",
      "      repeat_like(StrideB{}, int32_t(0)), StrideB{}));",
      "  using ScaleTensor = decltype(make_tensor(",
      "      get_logical_ptr(static_cast<NonVoidElementScale const*>(nullptr)),",
      "      repeat_like(NonVoidStrideScale{}, int32_t(0)), NonVoidStrideScale{}));",
      "",
      "  using ZeroTensor = decltype(make_tensor(",
      "      get_logical_ptr(static_cast<NonVoidElementZero const*>(nullptr)),",
      "      repeat_like(NonVoidStrideScale{}, int32_t(0)), NonVoidStrideScale{}));",
      "",
      "  static constexpr auto make_tma_copy_A(ATensor tensor_a = ATensor{}) {",
      "    return make_tma_copy<TmaElementA>(",
      "        GmemTiledCopyA{}, tensor_a, SmemLayoutACopy{}(_, _, cute::Int<0>{}),",
      "        shape(SmemLayoutACopy{}(_, _, cute::Int<0>{})),",
      "        size<1>(ClusterShape{}));  // mcast along N mode for this M load, if any",
      "  }",
      "",
      "  static constexpr auto make_tma_copy_scale(",
      "      ScaleTensor tensor_scale = ScaleTensor{}) {",
      "    return make_tma_copy(GmemTiledCopyScale{}, tensor_scale,",
      "                         SmemLayoutScale{}(_, _, cute::Int<0>{}),",
      "                         ScaleTileShape{},",
      "                         _1{});  // mcast along N mode for this M load, if any",
      "  }",
      "",
      "  static constexpr auto make_tma_copy_zero(",
      "      ZeroTensor tensor_zero = ZeroTensor{}) {",
      "    return make_tma_copy(GmemTiledCopyScale{}, tensor_zero,",
      "                         SmemLayoutScale{}(_, _, cute::Int<0>{}),",
      "                         ScaleTileShape{},",
      "                         _1{});  // mcast along N mode for this M load, if any",
      "  }",
      "",
      "  static constexpr auto make_tma_copy_B(BTensor tensor_b = BTensor{}) {",
      "    return make_tma_copy(",
      "        GmemTiledCopyB{}, tensor_b, SmemLayoutB{}(_, _, cute::Int<0>{}),",
      "        make_shape(shape<1>(TileShape{}), shape<2>(TileShape{})),",
      "        size<0>(ClusterShape{}));  // mcast along M mode for this N load, if any",
      "  }",
      "",
      " public:",
      "  // Same as upstream, should be kept the same when possible, not formatted for",
      "  // easier comparison",
      "  //  with `RealInternalElementA` -> `ElementA` since we support `SwapAB` logic",
      "  // clang-format off",
      "  static constexpr size_t SmemAlignmentA = cutlass::detail::alignment_for_swizzle(SmemLayoutA{}); ",
      "",
      "  static constexpr size_t SmemAlignmentB = cutlass::detail::alignment_for_swizzle(SmemLayoutB{});",
      "",
      "  // Just pick the max alignment of A and B since it is required to be at least 128B",
      "  static constexpr size_t SmemAlignmentScale = cute::max(SmemAlignmentA, SmemAlignmentB);",
      "",
      "  static_assert(SmemAlignmentA >= 128 and SmemAlignmentB >= 128, \"Require at least 128B alignment\");",
      "",
      "  struct SharedStorage",
      "  {",
      "    static constexpr int scale_elements = elements_per_smem_scale();",
      "    static constexpr int zero_elements = elements_per_smem_zero();",
      "    struct TensorStorage : cute::aligned_struct<cute::max(SmemAlignmentA, SmemAlignmentB)> {",
      "      cute::ArrayEngine<ElementA, cute::cosize_v<SmemLayoutA>> smem_A;",
      "      cute::ArrayEngine<typename TiledMma::ValTypeB, cute::cosize_v<SmemLayoutB>> smem_B;",
      "      cute::ArrayEngine<NonVoidElementScale, scale_elements> smem_scale;",
      "      cute::ArrayEngine<NonVoidElementZero, zero_elements> smem_zero;",
      "    } tensors;",
      "",
      "    using PipelineStorage = typename MainloopPipeline::SharedStorage;",
      "    PipelineStorage pipeline;",
      "  };",
      "  using TensorStorage = typename SharedStorage::TensorStorage;",
      "  using PipelineStorage = typename SharedStorage::PipelineStorage;",
      "",
      "  // Host side kernel arguments",
      "  struct Arguments {",
      "    ElementA const* ptr_A = nullptr;",
      "    StrideA dA{};",
      "    ElementB const* ptr_B = nullptr;",
      "    StrideB dB{};",
      "    ElementScale const* ptr_S = nullptr;",
      "    NonVoidStrideScale dS{};",
      "    int group_size = 0;",
      "    ElementZero const* ptr_Z = nullptr;",
      "    uint32_t mma_promotion_interval = 4;",
      "  };",
      "  // clang-format on",
      "",
      "  //",
      "  //  section setup end",
      "  //",
      "",
      "  // Similar (but not idendtical) to upstream, should be kept the same when",
      "  // possible",
      "  //  compared to upstream we use `make_tma_copy_A`, `make_tma_copy_B` etc. to",
      "  //  define the TMA types",
      "  // Device side kernel params",
      "  struct Params {",
      "   public:",
      "    // Assumption: StrideA is congruent with Problem_MK",
      "    using TMA_A = decltype(make_tma_copy_A());",
      "    using TMA_Scale = decltype(make_tma_copy_scale());",
      "    using TMA_Zero = decltype(make_tma_copy_zero());",
      "    using TMA_B = decltype(make_tma_copy_B());",
      "",
      "    // required by outer loop: i.e.",
      "    //   cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_cooperative.hpp",
      "    TMA_A tma_load_a;",
      "    TMA_B tma_load_b;",
      "    TMA_Scale tma_load_scale;",
      "    TMA_Zero tma_load_zero;",
      "    int64_t scale_k;",
      "    int group_size;",
      "    uint32_t tma_transaction_bytes = TmaTransactionBytes;",
      "    uint32_t tma_transaction_bytes_mk = TmaTransactionBytesMK;",
      "    uint32_t tma_transaction_bytes_nk = TmaTransactionBytesNK;",
      "  };",
      "",
      "  //",
      "  // Methods",
      "  //",
      "",
      "  // Similar (but not idendtical) to upstream, should be kept the same when",
      "  // possible",
      "  //  compared to upstream we use `make_tma_copy_A` and `TVbNbKL_to_offset` here",
      "  //  to handle the prepacked layout",
      "  template <class ProblemShape>",
      "  static constexpr Params to_underlying_arguments(",
      "      ProblemShape const& problem_shape, Arguments const& args,",
      "      void* workspace) {",
      "    (void)workspace;",
      "",
      "    // Optionally append 1s until problem shape is rank-4 (MNKL), in case it is",
      "    // only rank-3 (MNK)",
      "    auto problem_shape_MNKL = append<4>(problem_shape, 1);",
      "    auto [M, N, K, L] = problem_shape_MNKL;",
      "",
      "    auto ptr_A = reinterpret_cast<InternalElementA const*>(args.ptr_A);",
      "    auto ptr_B = reinterpret_cast<InternalElementB const*>(args.ptr_B);",
      "",
      "    auto make_logical_tensor = [&](auto ptr, auto shape, auto stride) {",
      "      return make_tensor(get_logical_ptr(ptr), make_layout(shape, stride));",
      "    };",
      "",
      "    typename Params::TMA_A tma_load_a;",
      "    typename Params::TMA_B tma_load_b;",
      "    typename Params::TMA_Scale tma_load_scale;",
      "    typename Params::TMA_Zero tma_load_zero;",
      "",
      "    auto layout = GmemLayoutA::TVbNbKL_to_offset_copy(make_shape(M, K, L));",
      "    tma_load_a = make_tma_copy_A(",
      "        make_logical_tensor(ptr_A, shape(layout), stride(layout)));",
      "",
      "    tma_load_b = make_tma_copy_B(",
      "        make_logical_tensor(ptr_B, make_shape(N, K, L), args.dB));",
      "",
      "    int32_t scale_k =",
      "        (ModeHasScales) ? (K + args.group_size - 1) / args.group_size : 0;",
      "    int32_t group_size = (ModeHasScales) ? args.group_size : 0;",
      "",
      "    if constexpr (ModeHasScales) {",
      "      tma_load_scale = make_tma_copy_scale(",
      "          make_logical_tensor(args.ptr_S, make_shape(M, scale_k, L), args.dS));",
      "    }",
      "",
      "    if constexpr (KernelConversionMode ==",
      "                  ConversionMode::ConvertAndScaleWithZero) {",
      "      tma_load_zero = make_tma_copy_zero(",
      "          make_logical_tensor(args.ptr_Z, make_shape(M, scale_k, L), args.dS));",
      "    }",
      "",
      "    if constexpr (KernelConversionMode == ConversionMode::DirectConvert ||",
      "                  KernelConversionMode == ConversionMode::ConvertAndScale ||",
      "                  KernelConversionMode ==",
      "                      ConversionMode::ConvertAndScaleWithZero) {",
      "      return {tma_load_a,    tma_load_b, tma_load_scale,",
      "              tma_load_zero, scale_k,    group_size};",
      "    } else {",
      "      static_assert(cutlass::detail::dependent_false<KernelSchedule>,",
      "                    \"Conversion mode not handled in to_underlying_arguments.\");",
      "    }",
      "  }",
      "",
      "  // Same as upstream, should be kept the same when possible, not formatted for",
      "  // easier comparison",
      "  //   with `SwapAB ? N : M -> M` since we dont support SwapAB",
      "  // clang-format off",
      "  template<class ProblemShape>",
      "  static bool",
      "  can_implement(",
      "      ProblemShape const& problem_shape,",
      "      [[maybe_unused]] Arguments const& args) {",
      "    constexpr int tma_alignment_bits = 128;",
      "    auto problem_shape_MNKL = append<4>(problem_shape, 1);",
      "    auto [M,N,K,L] = problem_shape_MNKL;",
      "    ",
      "    bool implementable = true;",
      "    constexpr int min_tma_aligned_elements_A = tma_alignment_bits / cutlass::sizeof_bits<ElementA>::value;",
      "    implementable = implementable && cutlass::detail::check_alignment<min_tma_aligned_elements_A>(cute::make_shape(M,K,L), StrideA{});",
      "    constexpr int min_tma_aligned_elements_B = tma_alignment_bits / cutlass::sizeof_bits<ElementB>::value;",
      "    implementable = implementable && cutlass::detail::check_alignment<min_tma_aligned_elements_B>(cute::make_shape(N,K,L), StrideB{});",
      "",
      "    if constexpr (KernelConversionMode == ConversionMode::DirectConvert) {",
      "      implementable = implementable && (args.ptr_S == nullptr);",
      "      implementable = implementable && (args.ptr_Z == nullptr);",
      "    } ",
      "    else if constexpr (ModeHasScales) {",
      "      const int scale_mn = M;",
      "      const int scale_k = (K + args.group_size - 1) / args.group_size;",
      "      constexpr int min_tma_aligned_elements_scale = tma_alignment_bits / cutlass::sizeof_bits<ElementScale>::value;",
      "      implementable = implementable && cutlass::detail::check_alignment<min_tma_aligned_elements_scale>(cute::make_shape(scale_mn,scale_k,L), StrideScale{});",
      "      implementable = implementable && (args.group_size == K || ((args.group_size % size<2>(TileShape{})) == 0));",
      "      implementable = implementable && args.group_size != 0;",
      "      implementable = implementable && (args.ptr_S != nullptr);",
      "",
      "      if constexpr (KernelConversionMode == ConversionMode::ConvertAndScale) {",
      "        implementable = implementable && (args.ptr_Z == nullptr);",
      "      }",
      "      else if constexpr (KernelConversionMode == ConversionMode::ConvertAndScaleWithZero) {",
      "        constexpr int min_tma_aligned_elements_zero = tma_alignment_bits / cutlass::sizeof_bits<ElementZero>::value;",
      "        implementable = implementable && cutlass::detail::check_alignment<min_tma_aligned_elements_zero>(cute::make_shape(scale_mn,scale_k,L), StrideScale{});",
      "        implementable = implementable && (args.ptr_Z != nullptr);",
      "      } ",
      "      else {",
      "        static_assert(cutlass::detail::dependent_false<KernelSchedule>, \"Conversion mode not handled in can_implement.\");",
      "      }",
      "    }",
      "    else {",
      "      static_assert(cutlass::detail::dependent_false<KernelSchedule>, \"Conversion mode not handled in can_implement.\");",
      "    }",
      "",
      "    if (!implementable) {",
      "      CUTLASS_TRACE_HOST(\"  CAN IMPLEMENT: Problem Size doesn't meet the minimum alignment requirements for TMA.\\n\");",
      "    }",
      "    return implementable;",
      "  }",
      "",
      "  static constexpr int K_PIPE_MAX = DispatchPolicy::Stages;",
      "  static constexpr uint32_t TmaTransactionBytesMK = compute_tma_transaction_bytes_mk();",
      "  static constexpr uint32_t TmaTransactionBytesNK = compute_tma_transaction_bytes_nk();",
      "  static constexpr uint32_t TmaTransactionBytes = TmaTransactionBytesMK + TmaTransactionBytesNK;",
      "",
      "  /// Issue Tma Descriptor Prefetch -- ideally from a single thread for best performance",
      "  CUTLASS_DEVICE",
      "  static void prefetch_tma_descriptors(Params const& mainloop_params) {",
      "    cute::prefetch_tma_descriptor(mainloop_params.tma_load_a.get_tma_descriptor());",
      "    cute::prefetch_tma_descriptor(mainloop_params.tma_load_b.get_tma_descriptor());",
      "",
      "    if constexpr (KernelConversionMode == ConversionMode::DirectConvert) {",
      "      // Nothing extra to do",
      "    } ",
      "    else if constexpr (KernelConversionMode == ConversionMode::ConvertAndScale) {",
      "      cute::prefetch_tma_descriptor(mainloop_params.tma_load_scale.get_tma_descriptor());",
      "    }",
      "    else if constexpr (KernelConversionMode == ConversionMode::ConvertAndScaleWithZero) {",
      "      cute::prefetch_tma_descriptor(mainloop_params.tma_load_scale.get_tma_descriptor());",
      "      cute::prefetch_tma_descriptor(mainloop_params.tma_load_zero.get_tma_descriptor());",
      "    }  ",
      "    else {",
      "      static_assert(cutlass::detail::dependent_false<KernelSchedule>, \"Conversion mode not handled in TMA prefetch.\");",
      "    }",
      "    ",
      "  }",
      "  // clang-format off",
      "",
      "  // Modified from upstream, should be kept close to that when possible",
      "  //  the main difference is special handling for the prepacked A layout",
      "  //",
      "  // Set up the data needed by this collective for load and mma.",
      "  // Returns a tuple of tensors. The collective and the kernel layer have the",
      "  // contract Returned tuple must contain at least two elements, with the first",
      "  // two elements being: gA_mkl - The tma tensor, A after a local tile so it",
      "  // has shape  (TILE_V,TILE_B,m,k,l) gB_nkl - The tma tensor, B after a local",
      "  // tile so it has shape  (TILE_N,TILE_K,n,k,l) The rest of the tensors can be",
      "  // specified as needed by this collective.",
      "  // NOTE: TILE_B is the prepacked block index within a tile. TILE_V is the",
      "  // values within a prepacked block.",
      "  template <class ProblemShape_MNKL>",
      "  CUTLASS_DEVICE auto load_init(ProblemShape_MNKL const& problem_shape_MNKL,",
      "                                Params const& mainloop_params) const {",
      "    using X = Underscore;",
      "    auto M = get<0>(problem_shape_MNKL), N = get<1>(problem_shape_MNKL),",
      "         K = get<2>(problem_shape_MNKL), L = get<3>(problem_shape_MNKL);",
      "",
      "    // (TILE_V,TILE_B,m,k,l)",
      "    auto make_gA_mkl = [&]() {",
      "      // ((athrid, val), (BlocksM, BlockK), L) -> (storage_idx)",
      "      auto layout = GmemLayoutA::TVbNbKL_to_offset_copy(make_shape(M, K, L));",
      "      Tensor mA_mkl = mainloop_params.tma_load_a.get_tma_tensor(shape(layout));",
      "      return local_tile(mA_mkl,",
      "                        make_shape(size<0>(layout), PPBlocksPerTile_MK{}),",
      "                        make_coord(0, make_coord(_, _)));",
      "    };",
      "",
      "    // (TILE_N,TILE_K,n,k,l)",
      "    auto make_gB_nkl = [&]() {",
      "      Tensor mB_nkl =",
      "          mainloop_params.tma_load_b.get_tma_tensor(make_shape(N, K, L));",
      "      return local_tile(mB_nkl, TileShape{}, make_coord(_, _, _),",
      "                        Step<X, _1, _1>{});",
      "    };",
      "",
      "    // (TILE_M,TILE_Scale_K,m,scale_k,l)",
      "    auto make_gS_mkl = [&]() {",
      "      auto scale_k = mainloop_params.scale_k;",
      "      Tensor mS_mkl = mainloop_params.tma_load_scale.get_tma_tensor(",
      "          make_shape(M, scale_k, L));",
      "      return local_tile(mS_mkl, ScaleTileShape{}, make_coord(_, _));",
      "    };",
      "",
      "    // (TILE_M,TILE_Scale_K,m,scale_k,l)",
      "    auto make_gZ_mkl = [&]() {",
      "      auto scale_k = mainloop_params.scale_k;",
      "      Tensor mZ_mkl = mainloop_params.tma_load_zero.get_tma_tensor(",
      "          make_shape(M, scale_k, L));",
      "      return local_tile(mZ_mkl, ScaleTileShape{}, make_coord(_, _));",
      "    };",
      "",
      "    if constexpr (KernelConversionMode == ConversionMode::DirectConvert) {",
      "      return cute::make_tuple(make_gA_mkl(), make_gB_nkl());",
      "    } else if constexpr (KernelConversionMode ==",
      "                         ConversionMode::ConvertAndScale) {",
      "      return cute::make_tuple(make_gA_mkl(), make_gB_nkl(), make_gS_mkl());",
      "    } else if constexpr (KernelConversionMode ==",
      "                         ConversionMode::ConvertAndScaleWithZero) {",
      "      return cute::make_tuple(make_gA_mkl(), make_gB_nkl(), make_gS_mkl(),",
      "                              make_gZ_mkl());",
      "    } else {",
      "      static_assert(cutlass::detail::dependent_false<KernelSchedule>,",
      "                    \"Conversion mode not handled in load_init.\");",
      "    }",
      "  }",
      "",
      "  // Similar to upstream, should be kept close to that when possible",
      "  //  the main difference is in the layout comments",
      "  // clang-format off",
      "  /// Perform a collective-scoped matrix multiply-accumulate",
      "  /// Producer Perspective",
      "  /// This overload gets triggered when we have scales.",
      "  template <",
      "    class... Ts,",
      "    class KTileIterator, class BlockCoord",
      "  >",
      "  CUTLASS_DEVICE void",
      "  load(",
      "      Params const& mainloop_params,",
      "      MainloopPipeline pipeline, ",
      "      PipelineState smem_pipe_write,",
      "      cute::tuple<Ts...> const& load_inputs,",
      "      BlockCoord const& blk_coord,",
      "      KTileIterator k_tile_iter, int k_tile_count,",
      "      int thread_idx,",
      "      uint32_t block_rank_in_cluster,",
      "      TensorStorage& shared_tensors) {",
      "    if constexpr (KernelConversionMode == ConversionMode::DirectConvert) {",
      "      static_assert(sizeof... (Ts) == 2, \"Direct convert needs two inputs\");",
      "    } ",
      "    else if constexpr (KernelConversionMode == ConversionMode::ConvertAndScale) {",
      "      static_assert(sizeof... (Ts) == 3, \"Scaled convert needs three inputs\");",
      "    } ",
      "    else if constexpr (KernelConversionMode == ConversionMode::ConvertAndScaleWithZero) {",
      "      static_assert(sizeof... (Ts) == 4, \"Scaled and zero convert needs four inputs\");",
      "    } ",
      "    else {",
      "      static_assert(cutlass::detail::dependent_false<KernelSchedule>, \"Conversion mode not handled in TMA load.\");",
      "    }",
      "",
      "    int lane_predicate = cute::elect_one_sync();",
      "",
      "    if (lane_predicate) {",
      "      Tensor sA_ = make_tensor(make_smem_ptr(shared_tensors.smem_A.begin()), SmemLayoutA{});      // (BLK_M,BLK_K,PIPE)",
      "      Tensor sB_ = make_tensor(make_smem_ptr(shared_tensors.smem_B.begin()), SmemLayoutB{});      // (BLK_N,BLK_K,PIPE)",
      "      Tensor sA  = as_position_independent_swizzle_tensor(sA_);                                   // (BLK_M,BLK_K,PIPE)",
      "      Tensor sB  = as_position_independent_swizzle_tensor(sB_);                                   // (BLK_N,BLK_K,PIPE)",
      "",
      "      //",
      "      // Prepare the TMA loads for A, B and Scales",
      "      //",
      "      ",
      "      constexpr uint32_t cluster_shape_x = get<0>(ClusterShape());",
      "      uint2 cluster_local_block_id = {block_rank_in_cluster % cluster_shape_x, block_rank_in_cluster / cluster_shape_x};",
      "",
      "      Tensor gA_mkl = get<0>(load_inputs);",
      "      Tensor gB_nkl = get<1>(load_inputs);",
      "",
      "      auto block_tma_a = mainloop_params.tma_load_a.get_slice(cluster_local_block_id.y);",
      "      auto block_tma_b = mainloop_params.tma_load_b.get_slice(cluster_local_block_id.x);",
      "",
      "      // Partition the inputs based on the current block coordinates.",
      "      auto [m_coord, n_coord, k_coord, l_coord] = blk_coord;",
      "      Tensor gA = gA_mkl(_,_,m_coord,_,l_coord);                                                     // (TILE_V,TILE_B,k)",
      "      Tensor gB = gB_nkl(_,_,n_coord,_,l_coord);                                                     // (TILE_N,TILE_K,k)",
      "",
      "      // Applies the mapping from block_tma_a",
      "      Tensor tAgA = block_tma_a.partition_S(gA);                                                 // (TMA,TMA_M,TMA_K,k)",
      "      Tensor tAsA = block_tma_a.partition_D(sA);                                              // (TMA,TMA_M,TMA_K,PIPE)",
      "",
      "      Tensor tBgB = block_tma_b.partition_S(gB);                                                 // (TMA,TMA_N,TMA_K,k)",
      "      Tensor tBsB = block_tma_b.partition_D(sB);                                              // (TMA,TMA_N,TMA_K,PIPE)",
      "",
      "      uint16_t mcast_mask_a = 0;",
      "      uint16_t mcast_mask_b = 0;",
      "      uint16_t mcast_mask_s = 0;",
      "",
      "      // Issue TmaLoads",
      "      // Maps the tile -> block, value",
      "      if constexpr (cute::is_same_v<GmemTiledCopyA, SM90_TMA_LOAD_MULTICAST>) {",
      "        auto block_layout = Layout<typename DispatchPolicy::ClusterShape>{};                       // (m,n) -> block_id",
      "        for (int n = 0; n < size<1>(block_layout); ++n) {",
      "          mcast_mask_a |= (uint16_t(1) << block_layout(cluster_local_block_id.x,n,Int<0>{}));",
      "        }",
      "      }",
      "",
      "      if constexpr (cute::is_same_v<GmemTiledCopyB, SM90_TMA_LOAD_MULTICAST>) {",
      "        auto block_layout = Layout<typename DispatchPolicy::ClusterShape>{};                       // (m,n) -> block_id",
      "        for (int m = 0; m < size<0>(block_layout); ++m) {",
      "          mcast_mask_b |= (uint16_t(1) << block_layout(m,cluster_local_block_id.y,Int<0>{}));",
      "        }",
      "      }",
      "",
      "      auto extra_input_partitions = partition_extra_tma_inputs(mainloop_params, load_inputs, shared_tensors, cluster_local_block_id, m_coord, l_coord);",
      "",
      "      // Mainloop",
      "      CUTLASS_PRAGMA_NO_UNROLL",
      "      for ( ; k_tile_count > 0; --k_tile_count) {",
      "        // LOCK smem_pipe_write for _writing_",
      "        pipeline.producer_acquire(smem_pipe_write);",
      "",
      "        //",
      "        // Copy gmem to smem for *k_tile_iter",
      "        //",
      "",
      "        using BarrierType = typename MainloopPipeline::ProducerBarrierType;",
      "        BarrierType* tma_barrier = pipeline.producer_get_barrier(smem_pipe_write);",
      "",
      "        int write_stage = smem_pipe_write.index();",
      "        copy(mainloop_params.tma_load_a.with(*tma_barrier, mcast_mask_a), tAgA(_,_,_,*k_tile_iter), tAsA(_,_,_,write_stage));",
      "        copy(mainloop_params.tma_load_b.with(*tma_barrier, mcast_mask_b), tBgB(_,_,_,*k_tile_iter), tBsB(_,_,_,write_stage));",
      "",
      "        if constexpr (KernelConversionMode == ConversionMode::DirectConvert) {",
      "          // Nothing extra to do.",
      "        }",
      "        else if constexpr (ModeHasScales) {",
      "          auto tSgS = get<0>(extra_input_partitions);",
      "          auto tSsS = get<1>(extra_input_partitions);",
      "",
      "          // Temporary factor which will determine which k tile to reload from gmem. Needed so we don't modify tma transaction bytes",
      "          // on the fly.",
      "          // We must do a ceiling divide here to correctly handle with group_size == K. In that case, we don't require that K",
      "          // is a multiple of the threadblock tile K",
      "          const int ReloadFactor = (mainloop_params.group_size + size<2>(TileShape{}) - 1) / size<2>(TileShape{});",
      "          const int scale_load_k = *k_tile_iter / ReloadFactor; // This will always be 0 when group_size == K.",
      "          copy(mainloop_params.tma_load_scale.with(*tma_barrier, mcast_mask_s), tSgS(_,_,_,scale_load_k), tSsS(_,_,_,write_stage));",
      "",
      "          if constexpr (KernelConversionMode == ConversionMode::ConvertAndScale) {",
      "            // Nothing extra to do",
      "          } ",
      "          else if constexpr (KernelConversionMode == ConversionMode::ConvertAndScaleWithZero) {",
      "            auto tZgZ = get<2>(extra_input_partitions);",
      "            auto tZsZ = get<3>(extra_input_partitions);",
      "            copy(mainloop_params.tma_load_zero.with(*tma_barrier, mcast_mask_s), tZgZ(_,_,_,scale_load_k), tZsZ(_,_,_,write_stage));",
      "          }",
      "          else {",
      "            static_assert(cutlass::detail::dependent_false<KernelSchedule>, \"Conversion mode not handled for TMA copy op.\");",
      "          } ",
      "        } ",
      "        else {",
      "          static_assert(cutlass::detail::dependent_false<KernelSchedule>, \"Conversion mode not handled for TMA copy op.\");",
      "        }",
      "",
      "        ++k_tile_iter;",
      "",
      "        // Advance smem_pipe_write",
      "        ++smem_pipe_write;",
      "      }",
      "    }",
      "  }",
      "  // clang-format off",
      "",
      "  // Same as upstream, should be kept the same when possible, not formatted for",
      "  // easier comparison",
      "  // clang-format off",
      "  // Perform a Producer Epilogue to prevent early exit of blocks in a Cluster",
      "  CUTLASS_DEVICE void",
      "  load_tail(MainloopPipeline pipeline, PipelineState smem_pipe_write) {",
      "    int lane_predicate = cute::elect_one_sync();",
      "",
      "    // Issue the epilogue waits",
      "    if (lane_predicate) {",
      "      /* This helps avoid early exit of blocks in Cluster",
      "       * Waits for all stages to either be released (all ",
      "       * Consumer UNLOCKs), or if the stage was never used",
      "       * then would just be acquired since the phase was ",
      "       * still inverted from make_producer_start_state",
      "       */",
      "      pipeline.producer_tail(smem_pipe_write);",
      "    }",
      "  }",
      "  // clang-format on",
      "",
      "  // Modified from upstream, should be kept close to that when possible",
      "  //  the main differences are handling the prepacked A layout, and separating",
      "  //  the loading of A from upcoverting A",
      "  //",
      "  // Perform a collective-scoped matrix multiply-accumulate",
      "  // Consumer Perspective",
      "  template <class FrgTensorC>",
      "  CUTLASS_DEVICE void mma(MainloopPipeline pipeline,",
      "                          PipelineState smem_pipe_read, FrgTensorC& accum,",
      "                          int k_tile_count, int thread_idx,",
      "                          TensorStorage& shared_tensors,",
      "                          Params const& mainloop_params) {",
      "    static_assert(is_rmem<FrgTensorC>::value,",
      "                  \"C tensor must be rmem resident.\");",
      "    static_assert(cute::rank(SmemLayoutB{}) == 3,",
      "                  \"Smem layout must be rank 3.\");",
      "    static_assert(cute::rank(SmemLayoutAtomB{}) == 2,",
      "                  \"SmemLayoutAtomB must be rank 2.\");",
      "    static_assert(!cute::is_void_v<SmemCopyAtomA>,",
      "                  \"SM90 GMMA mainloops must specify a non-void copy atom for \"",
      "                  \"RF sourced instructions.\");",
      "    static_assert(cute::is_void_v<SmemCopyAtomB>,",
      "                  \"SM90 GMMA mainloops cannot have a non-void copy atom for \"",
      "                  \"smem sourced instructions.\");",
      "",
      "    // Obtain warp index",
      "    int warp_idx = canonical_warp_idx_sync();",
      "    [[maybe_unused]] int warp_group_thread_idx = thread_idx % 128;",
      "",
      "    // ((T, (FrgV,(RestM, RestK)), (BlocksM, BlocksK), pipe) -> offset",
      "    auto constexpr smem_A = SmemLayoutA{};",
      "",
      "    // convert:",
      "    //   ((T, (MMA,(MMA_M, MMA_K)), (BlocksM, BlocksK), pipe) -> offset",
      "    // to:",
      "    //   (T, MMA, ((MMA_M, BlocksM), (MMA_K, BlocksK)), pipe) -> offset",
      "    // which can be thought of as:",
      "    //   (T, MMA, (MMA_M, MMA_K), pipe) -> offset",
      "    auto constexpr smem_A_mma_ =",
      "        make_layout(get<0, 0>(smem_A), get<0, 1, 0>(smem_A),",
      "                    zip(get<0, 1, 1>(smem_A), get<1>(smem_A)), get<2>(smem_A));",
      "    // flatten to:",
      "    //   (T, MMA, MMA_M, MMA_K, pipe) -> offset",
      "    auto constexpr smem_A_mma = smem_A_mma_(_, _, make_coord(_, _), _);",
      "",
      "    Tensor sA = make_tensor(make_smem_ptr(shared_tensors.smem_A.begin()),",
      "                            smem_A_mma);  // (T, MMA, MMA_M, MMA_K, pipe)",
      "    Tensor sB = make_tensor(make_smem_ptr(shared_tensors.smem_B.begin()),",
      "                            SmemLayoutB{});  // (BLK_N,BLK_K,PIPE)",
      "",
      "    //",
      "    // Define C accumulators and A/B partitioning",
      "    //",
      "",
      "    TiledMma tiled_mma;",
      "    auto thread_mma = tiled_mma.get_thread_slice(thread_idx);",
      "",
      "    Tensor tCsA = sA(thread_idx, _, _, _, _);  // (MMA,MMA_M,MMA_K,PIPE)",
      "    Tensor tCsB = thread_mma.partition_B(sB);  // (MMA,MMA_N,MMA_K,PIPE)",
      "",
      "    // Allocate fragments and descriptors",
      "    Tensor tCrA_load = make_tensor<ElementA>(",
      "        tCsA(_, _, _, Int<0>{}).shape());  // (MMA,MMA_N,MMA_K)",
      "    Tensor tCrA_mma = make_fragment_like<ElementMma>(tCrA_load);",
      "",
      "    Tensor tCrB = thread_mma.make_fragment_B(tCsB);  // (MMA,MMA_N,MMA_K,PIPE)",
      "",
      "    static constexpr int A_CPY_VEC =",
      "        decltype(max_common_vector(tCsA, tCrA_load)){};",
      "",
      "    static constexpr int CONVERSION_WIDTH =",
      "        std::min(A_CPY_VEC, int(size<0>(tCrA_mma)));",
      "",
      "    auto load_A_to_registers = [&](int read_stage) {",
      "      copy(create_auto_vectorizing_copy<ElementA, decltype(A_CPY_VEC)>(),",
      "           tCsA(_, _, _, read_stage), tCrA_load(_, _, _));",
      "    };",
      "",
      "    // Partition of thread -> shared and thread -> RF",
      "    auto partitioned_extra_info =",
      "        partition_extra_mma_info(thread_mma, shared_tensors);",
      "    auto copy_partitions_extra_info = retile_extra_mma_info(",
      "        tiled_mma, partitioned_extra_info, warp_group_thread_idx);",
      "    CUTE_STATIC_ASSERT_V(size<1>(tCrA_mma) == size<1>(accum));  // MMA_M",
      "    CUTE_STATIC_ASSERT_V(size<1>(tCsB) == size<2>(accum));      // N",
      "    CUTE_STATIC_ASSERT_V(size<2>(tCsA) == size<2>(tCsB));       // K",
      "    CUTE_STATIC_ASSERT_V(size<3>(tCsA) == size<3>(tCsB));       // PIPE",
      "    CUTE_STATIC_ASSERT_V(Int<DispatchPolicy::Stages>{} == size<2>(sB));  // PIPE",
      "",
      "    //",
      "    // PIPELINED MAIN LOOP",
      "    //",
      "",
      "    auto convert_A = [&, a_vec = Int<CONVERSION_WIDTH>{}](int k_block,",
      "                                                          int read_stage) {",
      "      load_extra_info_to_registers(partitioned_extra_info,",
      "                                   copy_partitions_extra_info, k_block,",
      "                                   read_stage);",
      "      transform_A_kblock(tCrA_load, a_vec, tCrA_mma, partitioned_extra_info,",
      "                         k_block);",
      "    };",
      "",
      "    // We release buffers to producer warps(dma load) with some mmas in flight",
      "    PipelineState smem_pipe_release = smem_pipe_read;",
      "",
      "    tiled_mma.accumulate_ = GMMA::ScaleOut::Zero;",
      "",
      "    warpgroup_fence_operand(accum);",
      "",
      "    constexpr int K_BLOCK_MAX = size<2>(tCrA_load);",
      "",
      "    ConsumerToken barrier_token = {BarrierStatus::WaitAgain};",
      "    // first k tile",
      "    {",
      "      barrier_token = pipeline.consumer_try_wait(smem_pipe_read);",
      "      pipeline.consumer_wait(smem_pipe_read, barrier_token);",
      "",
      "      int read_stage = smem_pipe_read.index();",
      "      ++smem_pipe_read;",
      "      barrier_token = pipeline.consumer_try_wait(smem_pipe_read);",
      "",
      "      // copy smem->rmem for A operand",
      "      load_A_to_registers(read_stage);",
      "      convert_A(0, read_stage);",
      "",
      "      // Unroll the K mode manually to set scale D to 1",
      "      CUTLASS_PRAGMA_UNROLL",
      "      for (int k_block = 0; k_block < K_BLOCK_MAX; ++k_block) {",
      "        if (k_block < K_BLOCK_MAX - 1) {",
      "          convert_A(k_block + 1, smem_pipe_read.index());",
      "        }",
      "        warpgroup_arrive();",
      "        // (V,M) x (V,N) => (V,M,N)",
      "        cute::gemm(tiled_mma, tCrA_mma(_, _, k_block),",
      "                   tCrB(_, _, k_block, read_stage), accum);",
      "        tiled_mma.accumulate_ = GMMA::ScaleOut::One;",
      "        warpgroup_commit_batch();",
      "      }",
      "",
      "      --k_tile_count;",
      "      if (k_tile_count > 0) {",
      "        // Wait for K_BLOCK_MAX - 1 to be in flight to ensure that it is safe to",
      "        // overwrite the A registers for the first mma.",
      "        warpgroup_wait<K_BLOCK_MAX - 1>();",
      "        pipeline.consumer_wait(smem_pipe_read, barrier_token);",
      "        load_A_to_registers(smem_pipe_read.index());",
      "        convert_A(0, smem_pipe_read.index());",
      "      }",
      "    }",
      "",
      "    if (k_tile_count == 0) {",
      "      return;",
      "    }",
      "",
      "    warpgroup_fence_operand(accum);",
      "    // Mainloop GMMAs",
      "    CUTLASS_PRAGMA_NO_UNROLL",
      "    for (; k_tile_count > 1; --k_tile_count) {",
      "      //",
      "      // Compute on k_tile",
      "      //",
      "",
      "      int read_stage = smem_pipe_read.index();",
      "      ++smem_pipe_read;",
      "",
      "      warpgroup_fence_operand(accum);",
      "      // Unroll the K mode manually to set scale D to 1",
      "      CUTLASS_PRAGMA_UNROLL",
      "      for (int k_block = 0; k_block < K_BLOCK_MAX; ++k_block) {",
      "        warpgroup_arrive();",
      "        // (V,M) x (V,N) => (V,M,N)",
      "        cute::gemm(tiled_mma, tCrA_mma(_, _, k_block),",
      "                   tCrB(_, _, k_block, read_stage), accum);",
      "        tiled_mma.accumulate_ = GMMA::ScaleOut::One;",
      "        warpgroup_commit_batch();",
      "",
      "        warpgroup_wait<K_BLOCK_MAX - 1>();",
      "        if (k_block == K_BLOCK_MAX - 1) {",
      "          // We have K_BLOCK_MAX - 1 GMMA instructions pending for this stage,",
      "          // so we can release prior barrier",
      "          pipeline.consumer_release(",
      "              smem_pipe_release);  // UNLOCK smem_pipe_release, done _computing_",
      "                                   // on it",
      "          ++smem_pipe_release;",
      "        }",
      "",
      "        if (k_block == 0) {",
      "          barrier_token = pipeline.consumer_try_wait(smem_pipe_read);",
      "        }",
      "",
      "        if (k_block == K_BLOCK_MAX - 1) {",
      "          pipeline.consumer_wait(smem_pipe_read, barrier_token);",
      "          load_A_to_registers(smem_pipe_read.index());",
      "          convert_A(0, smem_pipe_read.index());",
      "        } else {",
      "          convert_A(k_block + 1, read_stage);",
      "        }",
      "      }",
      "      warpgroup_fence_operand(accum);",
      "    }",
      "",
      "    warpgroup_fence_operand(accum);",
      "",
      "    {",
      "      //",
      "      // Compute on k_tile",
      "      //",
      "",
      "      int read_stage = smem_pipe_read.index();",
      "",
      "      warpgroup_fence_operand(accum);",
      "",
      "      // Unroll the K mode manually to set scale D to 1",
      "      CUTLASS_PRAGMA_UNROLL",
      "      for (int k_block = 0; k_block < K_BLOCK_MAX; ++k_block) {",
      "        warpgroup_arrive();",
      "        // (V,M) x (V,N) => (V,M,N)",
      "        cute::gemm(tiled_mma, tCrA_mma(_, _, k_block),",
      "                   tCrB(_, _, k_block, read_stage), accum);",
      "        tiled_mma.accumulate_ = GMMA::ScaleOut::One;",
      "        warpgroup_commit_batch();",
      "        warpgroup_wait<K_BLOCK_MAX - 1>();",
      "        if (k_block == K_BLOCK_MAX - 1) {",
      "          // release prior barrier",
      "          pipeline.consumer_release(",
      "              smem_pipe_release);  // UNLOCK smem_pipe_release, done _computing_",
      "                                   // on it",
      "          ++smem_pipe_release;",
      "        }",
      "",
      "        if (k_block < K_BLOCK_MAX - 1) {",
      "          convert_A(k_block + 1, read_stage);",
      "        }",
      "      }",
      "    }",
      "",
      "    warpgroup_fence_operand(accum);",
      "  }",
      "",
      "  // Perform a Consumer Epilogue to release all buffers",
      "  CUTLASS_DEVICE void mma_tail(MainloopPipeline pipeline,",
      "                               PipelineState smem_pipe_release,",
      "                               int k_tile_count) {",
      "    // Prologue GMMAs",
      "    int prologue_mma_count = 1;",
      "    k_tile_count -= prologue_mma_count;",
      "",
      "    smem_pipe_release.advance(k_tile_count);",
      "",
      "    // Wait on all GMMAs to complete",
      "    warpgroup_wait<0>();",
      "",
      "    for (int count = 0; count < prologue_mma_count; ++count) {",
      "      pipeline.consumer_release(",
      "          smem_pipe_release);  // UNLOCK smem_pipe_release, done _computing_ on",
      "                               // it",
      "      ++smem_pipe_release;",
      "    }",
      "  }",
      "",
      " private:",
      "  // Same as upstream, should be kept the same when possible, not formatted for",
      "  // easier comparison",
      "  // clang-format off",
      "  /// Utilities for any additional inputs inside of the TMA load",
      "  template <class... Ts>",
      "  CUTLASS_DEVICE",
      "  auto partition_extra_tma_inputs(",
      "    Params const& mainloop_params,",
      "    cute::tuple<Ts...> const& load_inputs,",
      "    TensorStorage& shared_tensors,",
      "    uint2 const& cluster_local_block_id,",
      "    int const m_coord, ",
      "    int const l_coord) {",
      "",
      "    if constexpr (KernelConversionMode == ConversionMode::DirectConvert) {",
      "      return cute::make_tuple();",
      "    } ",
      "    else if constexpr (ModeHasScales) {",
      "      Tensor sS  = make_tensor(make_smem_ptr(shared_tensors.smem_scale.begin()), SmemLayoutScale{}); // (BLK_M,BLK_K,PIPE)",
      "      Tensor gS_mkl = get<2>(load_inputs);",
      "      auto block_tma_s = mainloop_params.tma_load_scale.get_slice(cluster_local_block_id.y);",
      "      Tensor gS = gS_mkl(_,_,m_coord,_,l_coord);                                                  // (BLK_M,BLK_K,k)",
      "",
      "      Tensor tSgS = block_tma_s.partition_S(gS);                                              // (TMA,TMA_M,TMA_K,k)",
      "      Tensor tSsS = block_tma_s.partition_D(sS);                                              // (TMA,TMA_M,TMA_K,PIPE)",
      "      if constexpr (KernelConversionMode == ConversionMode::ConvertAndScale) {",
      "        return cute::make_tuple(tSgS, tSsS);",
      "      } ",
      "      else if constexpr (KernelConversionMode == ConversionMode::ConvertAndScaleWithZero) {",
      "        Tensor sZ  = make_tensor(make_smem_ptr(shared_tensors.smem_zero.begin()), SmemLayoutScale{}); // (BLK_M,BLK_K,PIPE)",
      "        Tensor gZ_mkl = get<3>(load_inputs);",
      "        auto block_tma_z = mainloop_params.tma_load_zero.get_slice(cluster_local_block_id.y);",
      "        Tensor gZ = gZ_mkl(_,_,m_coord,_,l_coord);                                            // (BLK_M,BLK_K,k)",
      "",
      "        Tensor tZgZ = block_tma_z.partition_S(gZ);                                            // (TMA,TMA_M,TMA_K,k)",
      "        Tensor tZsZ = block_tma_z.partition_D(sZ);                                            // (TMA,TMA_M,TMA_K,PIPE)",
      "        return cute::make_tuple(tSgS, tSsS, tZgZ, tZsZ);          ",
      "      }",
      "      else {",
      "        static_assert(cutlass::detail::dependent_false<KernelSchedule>, \"Conversion mode not handled for input partitioning.\");      ",
      "      }",
      "    }",
      "    else {",
      "      static_assert(cutlass::detail::dependent_false<KernelSchedule>, \"Conversion mode not handled for input partitioning.\");      ",
      "    }",
      "  }",
      "  // clang-format off",
      "",
      "  // Same as upstream, should be kept the same when possible, not formatted for",
      "  // easier comparison",
      "  // clang-format off",
      "  /// Utilities for partitioning extra inputs for loading from smem in the mainloop.",
      "  template <class ThreadMma>",
      "  CUTLASS_DEVICE ",
      "  auto partition_extra_mma_info(",
      "    ThreadMma const& mma_thread_slice,",
      "    TensorStorage& shared_tensors) {",
      "",
      "    if constexpr (KernelConversionMode == ConversionMode::DirectConvert) {",
      "      // nothing to do",
      "      return cute::make_tuple();",
      "    }",
      "    else if constexpr (ModeHasScales) {",
      "      Tensor sS = make_tensor(make_smem_ptr(shared_tensors.smem_scale.begin()), SmemLayoutScale{});// (BLK_M,BLK_SCALE_K,PIPE)",
      "      Tensor tCsS = mma_thread_slice.partition_A(sS);",
      "      Tensor tCrS = make_tensor<ElementScale>(mma_thread_slice.partition_fragment_A(sS(_,_,Int<0>{})).shape()); ",
      "",
      "      if constexpr (KernelConversionMode == ConversionMode::ConvertAndScale) {",
      "        return cute::make_tuple(tCsS, tCrS);",
      "      }",
      "      else if constexpr (KernelConversionMode == ConversionMode::ConvertAndScaleWithZero) {",
      "        Tensor sZ = make_tensor(make_smem_ptr(shared_tensors.smem_zero.begin()), SmemLayoutScale{});// (BLK_M,BLK_SCALE_K,PIPE)",
      "        Tensor tCsZ = mma_thread_slice.partition_A(sZ);",
      "        Tensor tCrZ = make_tensor<ElementZero>(mma_thread_slice.partition_fragment_A(sZ(_,_,Int<0>{})).shape()); ",
      "        return cute::make_tuple(tCsS, tCrS, tCsZ, tCrZ);",
      "      }",
      "      else {",
      "        static_assert(cutlass::detail::dependent_false<KernelSchedule>, \"Conversion mode not handled in A -> RF path.\");",
      "      }",
      "    } ",
      "    else {",
      "      static_assert(cutlass::detail::dependent_false<KernelSchedule>, \"Conversion mode not handled in A -> RF path.\");",
      "    }",
      "  }",
      "  // clang-format on",
      "",
      "  // Same as upstream, should be kept the same when possible, not formatted for",
      "  // easier comparison",
      "  // clang-format off",
      "  /// Returns the tiled copy and copy views for the extra inputs.",
      "  template <class TiledMma, class... Ts>",
      "  CUTLASS_DEVICE",
      "  auto retile_extra_mma_info(",
      "    TiledMma const& tiled_mma,",
      "    cute::tuple<Ts...>& partitioned_extra_info,",
      "    int const warp_group_thread_idx) {",
      "",
      "    if constexpr (KernelConversionMode == ConversionMode::DirectConvert) {",
      "      // nothing to do",
      "      return cute::make_tuple();",
      "    }",
      "    else if constexpr (ModeHasScales) {",
      "      auto smem_tiled_copy_S = make_tiled_copy_A(SmemCopyAtomScale{}, tiled_mma);",
      "      auto smem_thr_copy_S   = smem_tiled_copy_S.get_thread_slice(warp_group_thread_idx);",
      "      Tensor tCrS_copy_view  = smem_thr_copy_S.retile_D(cute::get<1>(partitioned_extra_info));        // (CPY,CPY_M,CPY_K)",
      "      ",
      "      if constexpr (KernelConversionMode == ConversionMode::ConvertAndScale) {",
      "        return cute::make_tuple(smem_tiled_copy_S, tCrS_copy_view);",
      "      } ",
      "      else if constexpr (KernelConversionMode == ConversionMode::ConvertAndScaleWithZero) {",
      "        Tensor tCrZ_copy_view  = smem_thr_copy_S.retile_D(cute::get<3>(partitioned_extra_info));      // (CPY,CPY_M,CPY_K)",
      "        return cute::make_tuple(smem_tiled_copy_S, tCrS_copy_view, tCrZ_copy_view);",
      "      } ",
      "      else {",
      "        static_assert(cutlass::detail::dependent_false<KernelSchedule>, \"Conversion mode not handled in A -> RF path.\");",
      "      }",
      "    } ",
      "    else {",
      "      static_assert(cutlass::detail::dependent_false<KernelSchedule>, \"Conversion mode not handled in A -> RF path.\");",
      "    }",
      "  }",
      "  // clang-format on",
      "",
      "  // Similar to `copy_A_and_extra_info` upstream, should be kept the same when",
      "  // possible",
      "  //   the main differences this only loads the extra info into registers and",
      "  //   not A (since we now preload more of A in the main pipeline)",
      "  // Load scales and zeros into registers if required",
      "  template <class... Ts, class... Us>",
      "  CUTLASS_DEVICE void load_extra_info_to_registers(",
      "      cute::tuple<Ts...> const& partitioned_mma_extra_info,",
      "      cute::tuple<Us...> const& tiled_copy_and_views, int k_block,",
      "      int read_stage) {",
      "    if (k_block == 0) {",
      "      // We are starting a new k-tile so copy the scale",
      "      if constexpr (KernelConversionMode == ConversionMode::DirectConvert) {",
      "        // nothing to do",
      "      } else if constexpr (ModeHasScales) {",
      "        auto smem_tiled_copy_S = cute::get<0>(tiled_copy_and_views);",
      "        auto tCrS_copy_view = cute::get<1>(tiled_copy_and_views);",
      "        auto tCsS = cute::get<0>(partitioned_mma_extra_info);",
      "        copy(smem_tiled_copy_S, tCsS(_, _, k_block, read_stage),",
      "             tCrS_copy_view(_, _, k_block));",
      "        if constexpr (KernelConversionMode == ConversionMode::ConvertAndScale) {",
      "          // Nothing extra to do",
      "        } else if constexpr (KernelConversionMode ==",
      "                             ConversionMode::ConvertAndScaleWithZero) {",
      "          auto tCsZ = cute::get<2>(partitioned_mma_extra_info);",
      "          auto tCrZ_copy_view = cute::get<2>(tiled_copy_and_views);",
      "          copy(smem_tiled_copy_S, tCsZ(_, _, k_block, read_stage),",
      "               tCrZ_copy_view(_, _, k_block));",
      "        } else {",
      "          static_assert(cutlass::detail::dependent_false<KernelSchedule>,",
      "                        \"Conversion mode not handled in A -> RF path.\");",
      "        }",
      "      } else {",
      "        static_assert(cutlass::detail::dependent_false<KernelSchedule>,",
      "                      \"Conversion mode not handled in A -> RF path.\");",
      "      }",
      "    }",
      "  }",
      "",
      "  // Similar to upstream, should be kept the same when possible.",
      "  //   the main differences are that `convert_tensor` supports interleaved",
      "  //   layouts and bfloat16 has been optimized. `transform_internal_A` has also",
      "  //   been inlined for code simplicity.",
      "  // Utilities to transform A.",
      "  template <class TCrA_load, int VectorWidthA, class TCrA_mma, class... Ts>",
      "  CUTLASS_DEVICE void transform_A_kblock(",
      "      TCrA_load const& tCrA_load, cute::Int<VectorWidthA> vec_A,",
      "      TCrA_mma& tCrA_mma, cute::tuple<Ts...> const& partitioned_extra_info,",
      "      int const k_block) {",
      "    auto in = tCrA_load(_, _, k_block);",
      "    auto out = tCrA_mma(_, _, k_block);",
      "",
      "    if constexpr (KernelConversionMode == ConversionMode::DirectConvert) {",
      "      convert_tensor<IlvdBlkLayout>(in, out, vec_A);",
      "    } else if constexpr (ModeHasScales) {",
      "      auto tCrS = cute::get<1>(partitioned_extra_info);",
      "      auto converted_inputs =",
      "          make_fragment_like<ElementScale>(tCrA_mma)(_, _, k_block);",
      "      auto scales = tCrS(_, _, 0);",
      "",
      "      // First, we upcast the inputs to the scale type",
      "      convert_tensor<IlvdBlkLayout>(in, converted_inputs, vec_A);",
      "      // Apply scales and broadcast across inputs, store in converted_inputs",
      "",
      "      // We need to cast to nv_bfloat16 for the multiply since",
      "      // `cutlass::bfloat16_t` has an overloaded operator* that upconverts to",
      "      // float, which nvcc will not optimize to using vectorized fma",
      "      // instructions (i.e. hfma.bf16_v2)",
      "      if constexpr (std::is_same_v<ElementScale, cutlass::bfloat16_t>) {",
      "        cute::transform(",
      "            recast<nv_bfloat16>(converted_inputs), recast<nv_bfloat16>(scales),",
      "            recast<nv_bfloat16>(converted_inputs), cute::multiplies{});",
      "      } else {",
      "        cute::transform(converted_inputs, scales, converted_inputs,",
      "                        cute::multiplies{});",
      "      }",
      "",
      "      // Apply zeros if required",
      "      if constexpr (KernelConversionMode ==",
      "                    ConversionMode::ConvertAndScaleWithZero) {",
      "        auto tCrZ = cute::get<3>(partitioned_extra_info);",
      "        auto converted_zeros = make_fragment_like<ElementScale>(tCrZ)(_, _, 0);",
      "",
      "        convert_tensor<void>(tCrZ(_, _, 0), converted_zeros);",
      "        if constexpr (std::is_same_v<ElementScale, cutlass::bfloat16_t>) {",
      "          cute::transform(recast<nv_bfloat16>(converted_inputs),",
      "                          recast<nv_bfloat16>(converted_zeros),",
      "                          recast<nv_bfloat16>(converted_inputs), cute::plus{});",
      "        } else {",
      "          cute::transform(converted_inputs, converted_zeros, converted_inputs,",
      "                          cute::plus{});",
      "        }",
      "      }",
      "",
      "      // Finally, we convert the scaled inputs to the mma type.",
      "      convert_tensor<void>(converted_inputs, out);",
      "    } else {",
      "      static_assert(cutlass::detail::dependent_false<KernelSchedule>,",
      "                    \"No A data is loaded.\");",
      "    }",
      "  }",
      "",
      "  // Modified from upstream, should be kept the same when possible",
      "  //   the main differences is that this version supports interleaved converts",
      "  // Utilities for transforming the A operand prior to issuing tensorcore math.",
      "  template <typename IlvdBlkLayout, class EngineIn, class EngineOut,",
      "            class TensorLayout,",
      "            int ConversionVectorWidth = cosize_v<TensorLayout>>",
      "  CUTLASS_DEVICE void convert_tensor(",
      "      Tensor<EngineIn, TensorLayout> const& in,",
      "      Tensor<EngineOut, TensorLayout>& out,",
      "      cute::Int<ConversionVectorWidth> width = {}) {",
      "    // This is an element-wise conversion where we expect both tensors to have",
      "    // the same layout. As a result, we can cast as a cutlass array to use the",
      "    // fast numeric converters without worrying about indexing into the layout.",
      "    constexpr int N = cosize_v<TensorLayout>;",
      "",
      "    // The inputs must be backed by registers & be statically sized.",
      "    static_assert(is_rmem<EngineIn>::value,",
      "                  \"Input tensor for A conversion must come from registers\");",
      "    static_assert(is_rmem<EngineOut>::value,",
      "                  \"Output tensor for A conversion must come from registers\");",
      "    static_assert(is_static_v<TensorLayout>,",
      "                  \"Tensor layout for the conversion must be static\");",
      "    static_assert(cosize_v<TensorLayout> == size(TensorLayout{}),",
      "                  \"Cosize and size of the layout must be equal.\");",
      "    static_assert(",
      "        N % ConversionVectorWidth == 0,",
      "        \"Conversion vector width must divide cosize of the tensor layout.\");",
      "",
      "    using SrcType = typename EngineIn::value_type;",
      "    using DstType = typename EngineOut::value_type;",
      "",
      "    using SrcArray = cutlass::Array<SrcType, ConversionVectorWidth>;",
      "    using DstArray = cutlass::Array<DstType, ConversionVectorWidth>;",
      "",
      "    constexpr cutlass::FloatRoundStyle RoundStyle =",
      "        cutlass::FloatRoundStyle::round_to_nearest;",
      "",
      "    using Converter = cutlass::InterleavedNumericArrayConverter<",
      "        IlvdBlkLayout, DstType, SrcType, ConversionVectorWidth, RoundStyle>;",
      "",
      "    constexpr int NumIterations = N / ConversionVectorWidth;",
      "",
      "    for (int ii = 0; ii < NumIterations; ++ii) {",
      "      SrcArray const* src_array_ptr =",
      "          reinterpret_cast<SrcArray const*>(raw_pointer_cast(in.data())) + ii;",
      "      DstArray* dst_array_ptr =",
      "          reinterpret_cast<DstArray*>(raw_pointer_cast(out.data())) + ii;",
      "      *dst_array_ptr = Converter::convert(*src_array_ptr);",
      "    }",
      "  }",
      "};",
      "",
      "}  // namespace machete"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/machete/machete_pytorch.cu",
    "source": [
      "#include \"machete_mm_launcher.cuh\"",
      "#include \"machete_prepack_launcher.cuh\"",
      "#include \"core/scalar_type.hpp\"",
      "",
      "#include \"core/registration.h\"",
      "",
      "namespace machete {",
      "",
      "using namespace vllm;",
      "",
      "std::vector<std::string> supported_schedules(",
      "    at::ScalarType a_type, int64_t b_type_id,",
      "    std::optional<at::ScalarType> maybe_group_scales_type,",
      "    std::optional<at::ScalarType> maybe_group_zeros_type,",
      "    std::optional<at::ScalarType> maybe_channel_scales_type,",
      "    std::optional<at::ScalarType> maybe_token_scales_type,",
      "    std::optional<at::ScalarType> maybe_out_type) {",
      "  ScalarType const b_type = ScalarType::from_id(b_type_id);",
      "  return supported_schedules_dispatch({",
      "      .a_type = a_type,",
      "      .b_type = b_type,",
      "      .maybe_group_scales_type = maybe_group_scales_type,",
      "      .maybe_group_zeros_type = maybe_group_zeros_type,",
      "      .maybe_channel_scales_type = maybe_channel_scales_type,",
      "      .maybe_token_scales_type = maybe_token_scales_type,",
      "      .maybe_out_type = maybe_out_type,",
      "  });",
      "}",
      "",
      "torch::Tensor mm(torch::Tensor const& A, torch::Tensor const& B,",
      "                 int64_t b_type_id,",
      "                 std::optional<at::ScalarType> const& maybe_out_type,",
      "                 std::optional<torch::Tensor> const& maybe_group_scales,",
      "                 std::optional<torch::Tensor> const& maybe_group_zeros,",
      "                 std::optional<int64_t> maybe_group_size,",
      "                 std::optional<torch::Tensor> const& maybe_channel_scales,",
      "                 std::optional<torch::Tensor> const& maybe_token_scales,",
      "                 std::optional<std::string> maybe_schedule) {",
      "  ScalarType const b_type = ScalarType::from_id(b_type_id);",
      "  return mm_dispatch({.A = A,",
      "                      .B = B,",
      "                      .b_type = b_type,",
      "                      .maybe_out_type = maybe_out_type,",
      "                      .maybe_group_scales = maybe_group_scales,",
      "                      .maybe_group_zeros = maybe_group_zeros,",
      "                      .maybe_group_size = maybe_group_size,",
      "                      .maybe_channel_scales = maybe_channel_scales,",
      "                      .maybe_token_scales = maybe_token_scales,",
      "                      .maybe_schedule = maybe_schedule});",
      "}",
      "",
      "torch::Tensor prepack_B(",
      "    torch::Tensor const& B, at::ScalarType const& a_type, int64_t b_type_id,",
      "    std::optional<at::ScalarType> const& maybe_group_scales_type) {",
      "  ScalarType const b_type = ScalarType::from_id(b_type_id);",
      "  return prepack_B_dispatch(",
      "      {.B = B,",
      "       .a_type = a_type,",
      "       .b_type = b_type,",
      "       .maybe_group_scales_type = maybe_group_scales_type});",
      "}",
      "",
      "TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, CUDA, m) {",
      "  m.impl(\"machete_prepack_B\", &prepack_B);",
      "  m.impl(\"machete_mm\", &mm);",
      "}",
      "",
      "// use CatchAll since supported_schedules has no tensor arguments",
      "TORCH_LIBRARY_IMPL(TORCH_EXTENSION_NAME, CatchAll, m) {",
      "  m.impl(\"machete_supported_schedules\", &supported_schedules);",
      "}",
      "",
      "};  // namespace machete"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w4a8/w4a8_mm_entry.cu",
    "source": [
      "//",
      "// Based off of:",
      "//   https://github.com/NVIDIA/cutlass/blob/main/examples/55_hopper_mixed_dtype_gemm/55_hopper_int4_fp8_gemm.cu",
      "//",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <torch/all.h>",
      "#include \"cutlass_extensions/torch_utils.hpp\"",
      "",
      "#include \"core/registration.h\"",
      "",
      "#include \"cutlass/cutlass.h\"",
      "#include <limits>",
      "",
      "#include \"cute/tensor.hpp\"",
      "#include \"cutlass/gemm/collective/collective_builder.hpp\"",
      "#include \"cutlass/epilogue/collective/collective_builder.hpp\"",
      "#include \"cutlass/gemm/device/gemm_universal_adapter.h\"",
      "#include \"cutlass/gemm/kernel/gemm_universal.hpp\"",
      "",
      "#include \"cutlass/util/packed_stride.hpp\"",
      "#include \"cutlass/util/mixed_dtype_utils.hpp\"",
      "",
      "#include \"cutlass_extensions/common.hpp\"",
      "#include \"cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp\"",
      "",
      "namespace vllm::cutlass_w4a8 {",
      "",
      "using namespace cute;",
      "",
      "// -------------------------------------------------------------------------------------",
      "// Static configuration shared across all instantiations",
      "// -------------------------------------------------------------------------------------",
      "using MmaType = cutlass::float_e4m3_t;  // A/scale element type",
      "using QuantType = cutlass::int4b_t;     // B element type (packed int4)",
      "",
      "static int constexpr TileShapeK = 128 * 8 / sizeof_bits<MmaType>::value;",
      "static int constexpr ScalePackSize = 8;  // pack 8 scale elements together",
      "static int constexpr PackFactor = 8;     // 8 4-bit packed into int32",
      "",
      "// A matrix configuration",
      "using ElementA = MmaType;                   // Element type for A matrix operand",
      "using LayoutA = cutlass::layout::RowMajor;  // Layout type for A matrix operand",
      "using LayoutA_Transpose =",
      "    typename cutlass::layout::LayoutTranspose<LayoutA>::type;",
      "constexpr int AlignmentA =",
      "    128 / cutlass::sizeof_bits<",
      "              ElementA>::value;  // Memory access granularity/alignment of A",
      "                                 // matrix in units of elements (up to 16 bytes)",
      "using StrideA = cutlass::detail::TagToStrideA_t<LayoutA>;",
      "",
      "// B matrix configuration",
      "using ElementB = QuantType;  // Element type for B matrix operand",
      "using LayoutB =",
      "    cutlass::layout::ColumnMajor;  // Layout type for B matrix operand",
      "using LayoutB_Transpose =",
      "    typename cutlass::layout::LayoutTranspose<LayoutB>::type;",
      "constexpr int AlignmentB =",
      "    128 / cutlass::sizeof_bits<",
      "              ElementB>::value;  // Memory access granularity/alignment of B",
      "                                 // matrix in units of elements (up to 16 bytes)",
      "using StrideB = cutlass::detail::TagToStrideB_t<LayoutB>;",
      "",
      "// Define the CuTe layout for reordered quantized tensor B",
      "// LayoutAtomQuant places values that will be read by the same thread in",
      "// contiguous locations in global memory. It specifies the reordering within a",
      "// single warp's fragment",
      "using LayoutAtomQuant =",
      "    decltype(cutlass::compute_memory_reordering_atom<MmaType>());",
      "using LayoutB_Reordered = decltype(cute::tile_to_shape(",
      "    LayoutAtomQuant{}, Layout<Shape<int, int, int>, StrideB>{}));",
      "",
      "// Group-wise scales",
      "using ElementScale = MmaType;",
      "using LayoutScale = cutlass::layout::RowMajor;",
      "",
      "// Per-tok, per-chan scales",
      "using ElementSChannel = float;",
      "",
      "// C/D matrix configuration",
      "using ElementC =",
      "    cutlass::bfloat16_t;  // Element type for C and D matrix operands",
      "using LayoutC =",
      "    cutlass::layout::RowMajor;  // Layout type for C and D matrix operands",
      "constexpr int AlignmentC =",
      "    128 / cutlass::sizeof_bits<",
      "              ElementC>::value;  // Memory access granularity/alignment of C",
      "                                 // matrix in units of elements (up to 16 bytes)",
      "",
      "using ElementD = ElementC;",
      "using LayoutD = LayoutC;",
      "constexpr int AlignmentD = 128 / cutlass::sizeof_bits<ElementD>::value;",
      "",
      "// Core kernel configurations",
      "using ElementAccumulator = float;     // Element type for internal accumulation",
      "using ElementCompute = float;         // Element type for epilogue computation",
      "using ArchTag = cutlass::arch::Sm90;  // Tag indicating the minimum SM that",
      "                                      // supports the intended feature",
      "using OperatorClass = cutlass::arch::OpClassTensorOp;  // Operator class tag",
      "using KernelSchedule =",
      "    cutlass::gemm::KernelTmaWarpSpecializedCooperative;  // Kernel to launch",
      "                                                         // based on the default",
      "                                                         // setting in the",
      "                                                         // Collective Builder",
      "using EpilogueSchedule = cutlass::epilogue::TmaWarpSpecializedCooperative;",
      "using EpilogueTileType = cutlass::epilogue::collective::EpilogueTileAuto;",
      "",
      "// ----------------------------------------------------------------------------",
      "// Kernel template \u2014 Tile/Cluster shapes",
      "// ----------------------------------------------------------------------------",
      "template <class TileShape_MN, class ClusterShape_MNK>",
      "struct W4A8GemmKernel {",
      "  using TileShape =",
      "      decltype(cute::append(TileShape_MN{}, cute::Int<TileShapeK>{}));",
      "  using ClusterShape = ClusterShape_MNK;",
      "",
      "  // Epilogue per-tok, per-chan scales",
      "  using ChTokScalesEpilogue =",
      "      typename vllm::c3x::ScaledEpilogue<ElementAccumulator, ElementD,",
      "                                         TileShape>;",
      "  using EVTCompute = typename ChTokScalesEpilogue::EVTCompute;",
      "  using CollectiveEpilogue =",
      "      typename cutlass::epilogue::collective::CollectiveBuilder<",
      "          ArchTag, OperatorClass, TileShape, ClusterShape, EpilogueTileType,",
      "          ElementAccumulator, ElementSChannel,",
      "          // Transpose layout of D here since we use explicit swap + transpose",
      "          // the void type for C tells the builder to allocate 0 smem for the C",
      "          // matrix. We can enable this if beta == 0 by changing ElementC to",
      "          // void below.",
      "          ElementC, typename cutlass::layout::LayoutTranspose<LayoutC>::type,",
      "          AlignmentC, ElementD,",
      "          typename cutlass::layout::LayoutTranspose<LayoutD>::type, AlignmentD,",
      "          EpilogueSchedule,  // This is the only epi supporting the required",
      "                             // swap + transpose.",
      "          EVTCompute>::CollectiveOp;",
      "",
      "  // The Scale information must get paired with the operand that will be scaled.",
      "  // In this example, B is scaled so we make a tuple of B's information and the",
      "  // scale information.",
      "  using CollectiveMainloopShuffled =",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          ArchTag, OperatorClass,",
      "          cute::tuple<ElementB, cutlass::Array<ElementScale, ScalePackSize>>,",
      "          LayoutB_Reordered, AlignmentB, ElementA, LayoutA_Transpose,",
      "          AlignmentA, ElementAccumulator, TileShape, ClusterShape,",
      "          cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(",
      "              sizeof(typename CollectiveEpilogue::SharedStorage))>,",
      "          KernelSchedule>::CollectiveOp;",
      "",
      "  using GemmKernelShuffled = cutlass::gemm::kernel::GemmUniversal<",
      "      Shape<int, int, int, int>,  // Indicates ProblemShape",
      "      CollectiveMainloopShuffled, CollectiveEpilogue>;",
      "  using GemmShuffled =",
      "      cutlass::gemm::device::GemmUniversalAdapter<GemmKernelShuffled>;",
      "",
      "  using StrideC = typename GemmKernelShuffled::StrideC;",
      "  using StrideD = typename GemmKernelShuffled::StrideD;",
      "  using StrideS = typename CollectiveMainloopShuffled::StrideScale;",
      "",
      "  static torch::Tensor mm(torch::Tensor const& A,",
      "                          torch::Tensor const& B,             // already packed",
      "                          torch::Tensor const& group_scales,  // already packed",
      "                          int64_t group_size,",
      "                          torch::Tensor const& channel_scales,",
      "                          torch::Tensor const& token_scales,",
      "                          std::optional<at::ScalarType> const& maybe_out_type) {",
      "    // TODO: param validation",
      "    int m = A.size(0);",
      "    int k = A.size(1);",
      "    int n = B.size(1);",
      "",
      "    // safely cast group_size to int",
      "    TORCH_CHECK(group_size > 0 && group_size <= std::numeric_limits<int>::max(),",
      "                \"group_size out of supported range for int: \", group_size);",
      "    int const group_size_int = static_cast<int>(group_size);",
      "",
      "    // Allocate output",
      "    const at::cuda::OptionalCUDAGuard device_guard(device_of(A));",
      "    auto device = A.device();",
      "    auto stream = at::cuda::getCurrentCUDAStream(device.index());",
      "    torch::Tensor D =",
      "        torch::empty({m, n}, torch::TensorOptions()",
      "                                 .dtype(equivalent_scalar_type_v<ElementD>)",
      "                                 .device(device));",
      "    // prepare arg pointers",
      "    auto A_ptr = static_cast<MmaType const*>(A.const_data_ptr());",
      "    auto B_ptr = static_cast<QuantType const*>(B.const_data_ptr());",
      "    auto D_ptr = static_cast<ElementD*>(D.data_ptr());",
      "    // can we avoid hardcode the 8 here",
      "    auto S_ptr =",
      "        static_cast<cutlass::Array<ElementScale, ScalePackSize> const*>(",
      "            group_scales.const_data_ptr());",
      "",
      "    // runtime layout for B",
      "    auto shape_B = cute::make_shape(n, k, 1);",
      "    LayoutB_Reordered layout_B_reordered =",
      "        cute::tile_to_shape(LayoutAtomQuant{}, shape_B);",
      "",
      "    // strides",
      "    int const scale_k = cutlass::ceil_div(k, group_size_int);",
      "    StrideA stride_A =",
      "        cutlass::make_cute_packed_stride(StrideA{}, cute::make_shape(m, k, 1));",
      "    // Reverse stride here due to swap and transpose",
      "    StrideD stride_D =",
      "        cutlass::make_cute_packed_stride(StrideD{}, cute::make_shape(n, m, 1));",
      "    StrideS stride_S = cutlass::make_cute_packed_stride(",
      "        StrideS{}, cute::make_shape(n, scale_k, 1));",
      "",
      "    // Create a structure of gemm kernel arguments suitable for invoking an",
      "    // instance of Gemm auto arguments =",
      "    // args_from_options<GemmShuffled>(options);",
      "    /// Populates a Gemm::Arguments structure from the given arguments",
      "    /// Swap the A and B tensors, as well as problem shapes here.",
      "    using Args = typename GemmShuffled::Arguments;",
      "    using MainloopArguments = typename GemmKernelShuffled::MainloopArguments;",
      "    using EpilogueArguments = typename GemmKernelShuffled::EpilogueArguments;",
      "",
      "    MainloopArguments mainloop_arguments{",
      "        B_ptr, layout_B_reordered, A_ptr,         stride_A,",
      "        S_ptr, stride_S,           group_size_int};",
      "",
      "    EpilogueArguments epilogue_arguments{",
      "        ChTokScalesEpilogue::prepare_args(channel_scales, token_scales),",
      "        nullptr,",
      "        {},  // no C",
      "        D_ptr,",
      "        stride_D};",
      "",
      "    Args arguments{cutlass::gemm::GemmUniversalMode::kGemm,",
      "                   {n, m, k, 1},  // shape",
      "                   mainloop_arguments,",
      "                   epilogue_arguments};",
      "",
      "    // Workspace",
      "    size_t workspace_size = GemmShuffled::get_workspace_size(arguments);",
      "    torch::Tensor workspace =",
      "        torch::empty(workspace_size,",
      "                     torch::TensorOptions().dtype(torch::kU8).device(device));",
      "",
      "    // Run GEMM",
      "    GemmShuffled gemm;",
      "    CUTLASS_CHECK(gemm.can_implement(arguments));",
      "    CUTLASS_CHECK(gemm.initialize(arguments, workspace.data_ptr(), stream));",
      "    CUTLASS_CHECK(gemm.run(stream));",
      "",
      "    return D;",
      "  }",
      "};",
      "",
      "// ----------------------------------------------------------------------------",
      "// Kernel instantiations and dispatch logic",
      "// ----------------------------------------------------------------------------",
      "using Kernel_256x128_1x1x1 =",
      "    W4A8GemmKernel<Shape<_256, _128>, Shape<_1, _1, _1>>;",
      "using Kernel_256x64_1x1x1 = W4A8GemmKernel<Shape<_256, _64>, Shape<_1, _1, _1>>;",
      "using Kernel_256x32_1x1x1 = W4A8GemmKernel<Shape<_256, _32>, Shape<_1, _1, _1>>;",
      "using Kernel_256x16_1x1x1 = W4A8GemmKernel<Shape<_256, _16>, Shape<_1, _1, _1>>;",
      "using Kernel_128x256_2x1x1 =",
      "    W4A8GemmKernel<Shape<_128, _256>, Shape<_2, _1, _1>>;",
      "using Kernel_128x256_1x1x1 =",
      "    W4A8GemmKernel<Shape<_128, _256>, Shape<_1, _1, _1>>;",
      "using Kernel_128x128_1x1x1 =",
      "    W4A8GemmKernel<Shape<_128, _128>, Shape<_1, _1, _1>>;",
      "using Kernel_128x64_1x1x1 = W4A8GemmKernel<Shape<_128, _64>, Shape<_1, _1, _1>>;",
      "using Kernel_128x32_1x1x1 = W4A8GemmKernel<Shape<_128, _32>, Shape<_1, _1, _1>>;",
      "using Kernel_128x16_1x1x1 = W4A8GemmKernel<Shape<_128, _16>, Shape<_1, _1, _1>>;",
      "",
      "torch::Tensor mm_dispatch(torch::Tensor const& A,",
      "                          torch::Tensor const& B,             // already packed",
      "                          torch::Tensor const& group_scales,  // already packed",
      "                          int64_t group_size,",
      "                          torch::Tensor const& channel_scales,",
      "                          torch::Tensor const& token_scales,",
      "                          std::optional<at::ScalarType> const& maybe_out_type,",
      "                          const std::string& schedule) {",
      "  if (schedule == \"256x128_1x1x1\") {",
      "    return Kernel_256x128_1x1x1::mm(A, B, group_scales, group_size,",
      "                                    channel_scales, token_scales,",
      "                                    maybe_out_type);",
      "  } else if (schedule == \"256x64_1x1x1\") {",
      "    return Kernel_256x64_1x1x1::mm(A, B, group_scales, group_size,",
      "                                   channel_scales, token_scales,",
      "                                   maybe_out_type);",
      "  } else if (schedule == \"256x32_1x1x1\") {",
      "    return Kernel_256x32_1x1x1::mm(A, B, group_scales, group_size,",
      "                                   channel_scales, token_scales,",
      "                                   maybe_out_type);",
      "  } else if (schedule == \"256x16_1x1x1\") {",
      "    return Kernel_256x16_1x1x1::mm(A, B, group_scales, group_size,",
      "                                   channel_scales, token_scales,",
      "                                   maybe_out_type);",
      "  } else if (schedule == \"128x256_2x1x1\") {",
      "    return Kernel_128x256_2x1x1::mm(A, B, group_scales, group_size,",
      "                                    channel_scales, token_scales,",
      "                                    maybe_out_type);",
      "  } else if (schedule == \"128x256_1x1x1\") {",
      "    return Kernel_128x256_1x1x1::mm(A, B, group_scales, group_size,",
      "                                    channel_scales, token_scales,",
      "                                    maybe_out_type);",
      "  } else if (schedule == \"128x128_1x1x1\") {",
      "    return Kernel_128x128_1x1x1::mm(A, B, group_scales, group_size,",
      "                                    channel_scales, token_scales,",
      "                                    maybe_out_type);",
      "  } else if (schedule == \"128x64_1x1x1\") {",
      "    return Kernel_128x64_1x1x1::mm(A, B, group_scales, group_size,",
      "                                   channel_scales, token_scales,",
      "                                   maybe_out_type);",
      "  } else if (schedule == \"128x32_1x1x1\") {",
      "    return Kernel_128x32_1x1x1::mm(A, B, group_scales, group_size,",
      "                                   channel_scales, token_scales,",
      "                                   maybe_out_type);",
      "  } else if (schedule == \"128x16_1x1x1\") {",
      "    return Kernel_128x16_1x1x1::mm(A, B, group_scales, group_size,",
      "                                   channel_scales, token_scales,",
      "                                   maybe_out_type);",
      "  }",
      "  TORCH_CHECK(false, \"Unknown W4A8 schedule: \", schedule);",
      "  return {};",
      "}",
      "",
      "torch::Tensor mm(torch::Tensor const& A,",
      "                 torch::Tensor const& B,             // already packed",
      "                 torch::Tensor const& group_scales,  // already packed",
      "                 int64_t group_size, torch::Tensor const& channel_scales,",
      "                 torch::Tensor const& token_scales,",
      "                 std::optional<at::ScalarType> const& maybe_out_type,",
      "                 std::optional<std::string> maybe_schedule) {",
      "  // requested a specific schedule",
      "  if (maybe_schedule) {",
      "    return mm_dispatch(A, B, group_scales, group_size, channel_scales,",
      "                       token_scales, maybe_out_type, *maybe_schedule);",
      "  }",
      "  std::string schedule;",
      "  int M = A.size(0);",
      "  int K = A.size(1);",
      "  int N = B.size(1);",
      "  // heuristic",
      "  if (M <= 16) {",
      "    schedule = (K == 16384 && N == 18432) ? \"256x16_1x1x1\" : \"128x16_1x1x1\";",
      "  } else if (M <= 32) {",
      "    schedule = (K == 16384 && N == 18432) ? \"256x32_1x1x1\" : \"128x32_1x1x1\";",
      "  } else if (M <= 64) {",
      "    if (K == 16384 && N == 18432)",
      "      schedule = \"256x64_1x1x1\";",
      "    else if (N <= 8192 && K <= 8192)",
      "      schedule = \"128x32_1x1x1\";",
      "    else",
      "      schedule = \"128x64_1x1x1\";",
      "  } else if (M <= 128) {",
      "    if (K == 16384 && N == 18432)",
      "      schedule = \"256x128_1x1x1\";",
      "    else if (N <= 8192)",
      "      schedule = \"128x64_1x1x1\";",
      "    else",
      "      schedule = \"128x128_1x1x1\";",
      "  } else if (M <= 256) {",
      "    if (N <= 4096)",
      "      schedule = \"128x64_1x1x1\";",
      "    else if (N <= 8192)",
      "      schedule = \"128x128_1x1x1\";",
      "    else",
      "      schedule = \"128x256_1x1x1\";",
      "  } else if (M <= 512 && N <= 4096) {",
      "    schedule = \"128x128_1x1x1\";",
      "  } else if (M <= 1024) {",
      "    schedule = \"128x256_1x1x1\";",
      "  } else {",
      "    schedule = \"128x256_2x1x1\";",
      "  }",
      "  return mm_dispatch(A, B, group_scales, group_size, channel_scales,",
      "                     token_scales, maybe_out_type, schedule);",
      "}",
      "",
      "// ----------------------------------------------------------------------------",
      "// Pre-processing utils",
      "// ----------------------------------------------------------------------------",
      "torch::Tensor pack_scale_fp8(torch::Tensor const& scales) {",
      "  TORCH_CHECK(scales.dtype() == torch::kFloat8_e4m3fn);",
      "  TORCH_CHECK(scales.is_contiguous());",
      "  TORCH_CHECK(scales.is_cuda());",
      "",
      "  auto packed_scales = torch::empty(",
      "      {scales.numel() * ScalePackSize},",
      "      torch::TensorOptions().dtype(scales.dtype()).device(scales.device()));",
      "  auto scales_ptr = static_cast<MmaType const*>(scales.const_data_ptr());",
      "  auto packed_scales_ptr =",
      "      static_cast<cutlass::Array<ElementScale, ScalePackSize>*>(",
      "          packed_scales.data_ptr());",
      "",
      "  cutlass::pack_scale_fp8(scales_ptr, packed_scales_ptr, scales.numel());",
      "",
      "  return packed_scales;",
      "}",
      "",
      "torch::Tensor encode_and_reorder_int4b(torch::Tensor const& B) {",
      "  TORCH_CHECK(B.dtype() == torch::kInt32);",
      "  TORCH_CHECK(B.dim() == 2);",
      "",
      "  torch::Tensor B_packed = torch::empty_like(B);",
      "",
      "  int k = B.size(0) * PackFactor;  // logical k",
      "  int n = B.size(1);",
      "",
      "  auto B_ptr = static_cast<QuantType const*>(B.const_data_ptr());",
      "  auto B_packed_ptr = static_cast<QuantType*>(B_packed.data_ptr());",
      "  auto shape_B = cute::make_shape(n, k, 1);",
      "  auto layout_B = make_layout(shape_B, LayoutRight{});  // row major",
      "  LayoutB_Reordered layout_B_reordered =",
      "      cute::tile_to_shape(LayoutAtomQuant{}, shape_B);",
      "",
      "  cutlass::unified_encode_int4b(B_ptr, B_packed_ptr, n * k);",
      "  cutlass::reorder_tensor(B_packed_ptr, layout_B, layout_B_reordered);",
      "",
      "  return B_packed;",
      "}",
      "",
      "TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, CUDA, m) {",
      "  m.impl(\"cutlass_w4a8_mm\", &mm);",
      "  m.impl(\"cutlass_pack_scale_fp8\", &pack_scale_fp8);",
      "  m.impl(\"cutlass_encode_and_reorder_int4b\", &encode_and_reorder_int4b);",
      "}",
      "",
      "}  // namespace vllm::cutlass_w4a8"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu",
    "source": [
      "/*",
      " * Notice: This file was modified by Neuralmagic inc to include 8-bit support",
      " *",
      " * Copyright (C) 2024 Roberto Lopez Castro (roberto.lopez.castro@udc.es). All",
      " * Rights Reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *       http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "#include <torch/all.h>",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <cuda.h>",
      "#include <cuda_fp16.h>",
      "#include <cuda_runtime.h>",
      "",
      "#include <iostream>",
      "",
      "#include \"common/base.h\"",
      "#include \"core/scalar_type.hpp\"",
      "#include \"core/registration.h\"",
      "",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "",
      "#else",
      "",
      "  #include \"common/mem.h\"",
      "  #include \"common/mma.h\"",
      "",
      "#endif",
      "",
      "template <typename T>",
      "inline std::string str(T x) {",
      "  return std::to_string(x);",
      "}",
      "",
      "namespace marlin_24 {",
      "",
      "// 8 warps are a good choice since every SM has 4 schedulers and having more",
      "// than 1 warp per schedule allows some more latency hiding. At the same time,",
      "// we want relatively few warps to have many registers per warp and small tiles.",
      "static constexpr int THREADS = 256;",
      "static constexpr int STAGES = 4;",
      "",
      "static constexpr int min_thread_n = 128;",
      "",
      "static constexpr int tile_size = 16;",
      "static constexpr int max_par = 64;",
      "",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "",
      "template <const int num_bits,         // weight bits",
      "          const int threads,          // number of threads in a threadblock",
      "          const int thread_m_blocks,  // number of 16x16 blocks in the m",
      "                                      // dimension (batchsize) of the",
      "                                      // threadblock",
      "          const int thread_n_blocks,  // same for n dimension (output)",
      "          const int thread_k_blocks,  // same for k dimension (reduction)",
      "          const int stages,  // number of stages for the async global->shared",
      "                             // fetch pipeline",
      "          const int group_blocks = -1  // number of consecutive 16x16 blocks",
      "                                       // with a separate quantization scale",
      "          >",
      "__global__ void Marlin_24(",
      "    const int4* __restrict__ A,     // fp16 input matrix of shape mxk",
      "    const int4* __restrict__ B,     // 4bit quantized weight matrix of shape kxn",
      "    const int4* __restrict__ meta,  // 2bit metadata information about 2:4",
      "                                    // format on B",
      "    int4* __restrict__ C,           // fp16 output buffer of shape mxn",
      "    const int4* __restrict__ s,     // fp16 quantization scales of shape",
      "                                    // (k/groupsize)xn",
      "    int prob_m,                     // batch dimension m",
      "    int prob_n,                     // output dimension n",
      "    int prob_k,                     // reduction dimension k",
      "    int* locks  // extra global storage for barrier synchronization",
      ") {}",
      "",
      "torch::Tensor gptq_marlin_24_gemm(torch::Tensor& a, torch::Tensor& b_q_weight,",
      "                                  torch::Tensor& b_meta,",
      "                                  torch::Tensor& b_scales,",
      "                                  torch::Tensor& workspace,",
      "                                  vllm::ScalarTypeId const b_q_type_id,",
      "                                  int64_t size_m, int64_t size_n,",
      "                                  int64_t size_k) {",
      "  TORCH_CHECK_NOT_IMPLEMENTED(",
      "      false, \"gptq_marlin_24_gemm(..) requires CUDA_ARCH >= 8.0\");",
      "  return torch::empty({1, 1});",
      "}",
      "",
      "#else",
      "",
      "template <const int num_bits,         // weight bits",
      "          const int threads,          // number of threads in a threadblock",
      "          const int thread_m_blocks,  // number of 16x16 blocks in the m",
      "                                      // dimension (batchsize) of the",
      "                                      // threadblock",
      "          const int thread_n_blocks,  // same for n dimension (output)",
      "          const int thread_k_blocks,  // same for k dimension (reduction)",
      "          const int stages,  // number of stages for the async global->shared",
      "                             // fetch pipeline",
      "          const int group_blocks = -1  // number of consecutive 16x16 blocks",
      "                                       // with a separate quantization scale",
      "          >",
      "__global__ void Marlin_24(",
      "    const int4* __restrict__ A,     // fp16 input matrix of shape mxk",
      "    const int4* __restrict__ B,     // 4bit quantized weight matrix of shape kxn",
      "    const int4* __restrict__ meta,  // 2bit metadata information about 2:4",
      "                                    // format on B",
      "    int4* __restrict__ C,           // fp16 output buffer of shape mxn",
      "    const int4* __restrict__ s,     // fp16 quantization scales of shape",
      "                                    // (k/groupsize)xn",
      "    int prob_m,                     // batch dimension m",
      "    int prob_n,                     // output dimension n",
      "    int prob_k,                     // reduction dimension k",
      "    int* locks  // extra global storage for barrier synchronization",
      ") {",
      "  // Each threadblock processes one \"stripe\" of the B matrix with (roughly) the",
      "  // same size, which might involve multiple column \"slices\" (of width 16 *",
      "  // `thread_n_blocks`). Stripes are defined as shown in the 3x3 matrix 5 SM",
      "  // example:",
      "  //   0 1 3",
      "  //   0 2 3",
      "  //   1 2 4",
      "  // While this kind of partitioning makes things somewhat more complicated, it",
      "  // ensures good utilization of all SMs for many kinds of shape and GPU",
      "  // configurations, while requiring as few slow global cross-threadblock",
      "  // reductions as possible.",
      "",
      "  // For larger GEMMs we run multiple batchsize 64 versions in parallel for a",
      "  // better partitioning with less reductions",
      "  int parallel = 1;",
      "  if (prob_m > 16 * thread_m_blocks) {",
      "    parallel = prob_m / (16 * thread_m_blocks);",
      "    prob_m = 16 * thread_m_blocks;",
      "  }",
      "",
      "  // number of thread_k_blocks in k-dim",
      "  int k_tiles = prob_k / 32 / thread_k_blocks;",
      "  // number of thread_n_blocks in n-dim",
      "  int n_tiles = prob_n / 16 / thread_n_blocks;",
      "  // iters needed to cover all slices",
      "  int iters = ceildiv(k_tiles * n_tiles * parallel, gridDim.x);",
      "",
      "  // Ensure that the number of tiles in each stripe is a multiple of the",
      "  // groupsize; this avoids an annoying special case where a stripe starts in",
      "  // the middle of group.",
      "  if (group_blocks != -1)",
      "    iters = (group_blocks / thread_k_blocks) *",
      "            ceildiv(iters, (group_blocks / thread_k_blocks));",
      "",
      "  int slice_row = (iters * blockIdx.x) % k_tiles;",
      "  int slice_col_par = (iters * blockIdx.x) / k_tiles;",
      "  int slice_col = slice_col_par;",
      "  // number of threadblock tiles in the current slice",
      "  int slice_iters;",
      "  // total number of active threadblocks in the current slice",
      "  int slice_count = 0;",
      "  // index of threadblock in current slice; numbered bottom to top",
      "  int slice_idx;",
      "",
      "  // We can easily implement parallel problem execution by just remapping",
      "  // indices and advancing global pointers",
      "  if (slice_col_par >= n_tiles) {",
      "    A += (slice_col_par / n_tiles) * 16 * thread_m_blocks * prob_k / 8;",
      "    C += (slice_col_par / n_tiles) * 16 * thread_m_blocks * prob_n / 8;",
      "    locks += (slice_col_par / n_tiles) * n_tiles;",
      "    slice_col = slice_col_par % n_tiles;",
      "  }",
      "",
      "  // Compute all information about the current slice which is required for",
      "  // synchronization.",
      "  auto init_slice = [&]() {",
      "    slice_iters =",
      "        iters * (blockIdx.x + 1) - (k_tiles * slice_col_par + slice_row);",
      "    if (slice_iters < 0 || slice_col_par >= n_tiles * parallel) slice_iters = 0;",
      "    if (slice_iters == 0) return;",
      "    if (slice_row + slice_iters > k_tiles) slice_iters = k_tiles - slice_row;",
      "    slice_count = 1;",
      "    slice_idx = 0;",
      "    int col_first = iters * ceildiv(k_tiles * slice_col_par, iters);",
      "    if (col_first <= k_tiles * (slice_col_par + 1)) {",
      "      int col_off = col_first - k_tiles * slice_col_par;",
      "      slice_count = ceildiv(k_tiles - col_off, iters);",
      "      if (col_off > 0) slice_count++;",
      "      int delta_first = iters * blockIdx.x - col_first;",
      "      if (delta_first < 0 || (col_off == 0 && delta_first == 0))",
      "        slice_idx = slice_count - 1;",
      "      else {",
      "        slice_idx = slice_count - 1 - delta_first / iters;",
      "        if (col_off > 0) slice_idx--;",
      "      }",
      "    }",
      "    if (slice_col == n_tiles) {",
      "      A += 16 * thread_m_blocks * prob_k / 8;",
      "      C += 16 * thread_m_blocks * prob_n / 8;",
      "      locks += n_tiles;",
      "      slice_col = 0;",
      "    }",
      "  };",
      "  init_slice();",
      "",
      "  // RLC: 8 is vec_size -> 128-bit instructions, 8 fp16 elements",
      "  int a_gl_stride = prob_k / 8;  // stride of the A matrix in global memory",
      "",
      "  // stride of an A matrix tile in shared memory",
      "  constexpr int a_sh_stride = 32 * thread_k_blocks / 8;",
      "  // delta between subsequent A tiles in global memory",
      "  constexpr int a_gl_rd_delta_o = 32 * thread_k_blocks / 8;",
      "  // between subsequent accesses within a tile",
      "  int a_gl_rd_delta_i = a_gl_stride * (threads / a_gl_rd_delta_o);",
      "  // between shared memory writes",
      "  constexpr int a_sh_wr_delta = a_sh_stride * (threads / a_gl_rd_delta_o);",
      "  // between shared memory tile reads //RLC: 2 * #warps k-dim",
      "  constexpr int a_sh_rd_delta_o = 4 * ((threads / 32) / (thread_n_blocks / 4));",
      "  // within a shared memory tile",
      "  constexpr int a_sh_rd_delta_i = a_sh_stride * 16;",
      "  // overall size of a tile",
      "  constexpr int a_sh_stage = a_sh_stride * (16 * thread_m_blocks);",
      "  // number of shared write iterations for a tile",
      "  constexpr int a_sh_wr_iters = ceildiv(a_sh_stage, a_sh_wr_delta);",
      "",
      "  constexpr int pack_factor = 32 / num_bits;",
      "",
      "  int b_gl_stride = 16 * prob_n / (pack_factor * 4);",
      "  constexpr int b_sh_stride = ((thread_n_blocks * 16) * 16 / pack_factor) / 4;",
      "  constexpr int b_thread_vecs = num_bits == 4 ? 1 : 2;",
      "  constexpr int b_sh_stride_threads = b_sh_stride / b_thread_vecs;",
      "  int b_gl_rd_delta_o = b_gl_stride * thread_k_blocks;",
      "  int b_gl_rd_delta_i = b_gl_stride * (threads / b_sh_stride_threads);",
      "  constexpr int b_sh_wr_delta = threads * b_thread_vecs;",
      "  constexpr int b_sh_rd_delta = threads * b_thread_vecs;",
      "  constexpr int b_sh_stage = b_sh_stride * thread_k_blocks;",
      "  constexpr int b_sh_wr_iters = b_sh_stage / b_sh_wr_delta;",
      "",
      "  int m_gl_stride = 2 * prob_n / 8;  // (16*2*4 / 8) = 16",
      "  constexpr int m_sh_stride =",
      "      (16 * thread_n_blocks) / 4;  // #warps n-dim * threads/warp",
      "  int m_gl_rd_delta_o = m_gl_stride * thread_k_blocks;",
      "  int m_gl_rd_delta_i = m_gl_stride * (threads / m_sh_stride);",
      "  constexpr int m_sh_wr_delta = threads / 2;",
      "  constexpr int m_sh_rd_delta = threads / 2;",
      "  constexpr int m_sh_stage = m_sh_stride * thread_k_blocks;",
      "  constexpr int m_sh_iters = ceildiv(m_sh_stage, m_sh_wr_delta);",
      "",
      "  int s_gl_stride = prob_n / 8;",
      "  constexpr int s_sh_stride = 16 * thread_n_blocks / 8;",
      "  constexpr int s_sh_stage = s_sh_stride;",
      "  int s_gl_rd_delta = s_gl_stride;",
      "",
      "  // Global A read index of current thread.",
      "  int a_gl_rd = a_gl_stride * (threadIdx.x / a_gl_rd_delta_o) +",
      "                (threadIdx.x % a_gl_rd_delta_o);",
      "  a_gl_rd += a_gl_rd_delta_o * slice_row;",
      "  // Shared write index of current thread.",
      "  int a_sh_wr = a_sh_stride * (threadIdx.x / a_gl_rd_delta_o) +",
      "                (threadIdx.x % a_gl_rd_delta_o);",
      "  // Shared read index.",
      "  int a_sh_rd =",
      "      a_sh_stride * ((threadIdx.x % 32) % 16) + (threadIdx.x % 32) / 16;",
      "  a_sh_rd += 4 * ((threadIdx.x / 32) / (thread_n_blocks / 4));",
      "",
      "  int b_gl_rd = b_gl_stride * (threadIdx.x / b_sh_stride_threads) +",
      "                (threadIdx.x % b_sh_stride_threads) * b_thread_vecs;",
      "  b_gl_rd += b_sh_stride * slice_col;",
      "  b_gl_rd += b_gl_rd_delta_o * slice_row;",
      "  auto b_sh_wr = threadIdx.x * b_thread_vecs;",
      "  auto b_sh_rd = threadIdx.x * b_thread_vecs;",
      "",
      "  int m_gl_rd = m_gl_stride * (threadIdx.x / (m_sh_stride)) +",
      "                (threadIdx.x % (m_sh_stride));",
      "  m_gl_rd += (m_sh_stride)*slice_col;",
      "  m_gl_rd += m_gl_rd_delta_o * slice_row;",
      "  auto m_sh_wr = threadIdx.x;",
      "  auto m_sh_rd = threadIdx.x % 16 + (threadIdx.x / 32) * 16;",
      "",
      "  int s_gl_rd;",
      "  if constexpr (group_blocks == -1) {",
      "    s_gl_rd = s_sh_stride * slice_col + threadIdx.x;",
      "  } else {",
      "    s_gl_rd = s_gl_stride * ((thread_k_blocks * slice_row) / group_blocks) +",
      "              s_sh_stride * slice_col + threadIdx.x;",
      "  }",
      "",
      "  auto s_sh_wr = threadIdx.x;",
      "  int s_sh_rd;",
      "  // We use a different scale layout for grouped and column-wise quantization as",
      "  // we scale a `half2` tile in column-major layout in the former and in",
      "  // row-major in the latter case.",
      "  s_sh_rd = 8 * ((threadIdx.x / 32) % (thread_n_blocks / 4)) +",
      "            (threadIdx.x % 32) / 4;  // Note that in the original Marlin kernel",
      "                                     // this is (threadIdx.x % 32) / 4",
      "",
      "  // Precompute which thread should not read memory in which iterations; this is",
      "  // needed if there are more threads than required for a certain tilesize or",
      "  // when the batchsize is not a multiple of 16.",
      "  bool a_sh_wr_pred[a_sh_wr_iters];",
      "  #pragma unroll",
      "  for (int i = 0; i < a_sh_wr_iters; i++) {",
      "    a_sh_wr_pred[i] = a_sh_wr_delta * i + a_sh_wr < a_sh_stride * prob_m;",
      "  }",
      "  bool s_sh_wr_pred = threadIdx.x < s_sh_stride;",
      "",
      "  // To ensure that writing and reading A tiles to/from shared memory, the",
      "  // latter in fragment format, is fully bank conflict free, we need to use a",
      "  // rather fancy XOR-based layout. The key here is that neither reads nor",
      "  // writes of the 16-byte `int4` blocks of 8 consecutive threads involve the",
      "  // same shared memory banks. Further, it seems (based on NSight-Compute) that",
      "  // each warp must also write a consecutive memory segment?",
      "  auto transform_a = [&](int i) {",
      "    int row = i / a_gl_rd_delta_o;",
      "    return a_gl_rd_delta_o * row + (i % a_gl_rd_delta_o) ^ row;",
      "  };",
      "  // Since the computation of this remapping is non-trivial and, due to our main",
      "  // loop unrolls, all shared memory accesses are static, we simply precompute",
      "  // both transformed reads and writes.",
      "  int a_sh_wr_trans[a_sh_wr_iters];",
      "  #pragma unroll",
      "  for (int i = 0; i < a_sh_wr_iters; i++)",
      "    a_sh_wr_trans[i] = transform_a(a_sh_wr_delta * i + a_sh_wr);",
      "  int a_sh_rd_trans[2][b_sh_wr_iters][thread_m_blocks];",
      "  #pragma unroll",
      "  for (int i = 0; i < b_sh_wr_iters; i++) {",
      "  #pragma unroll",
      "    for (int j = 0; j < thread_m_blocks; j++) {",
      "      a_sh_rd_trans[0][i][j] =",
      "          transform_a(a_sh_rd_delta_o * i + a_sh_rd_delta_i * j + a_sh_rd);",
      "      a_sh_rd_trans[1][i][j] =",
      "          transform_a(a_sh_rd_delta_o * i + a_sh_rd_delta_i * j + a_sh_rd + 2);",
      "    }",
      "  }",
      "",
      "  // Since B-accesses have non-constant stride they have to be computed at",
      "  // runtime; we break dependencies between subsequent accesses with a tile by",
      "  // maintining multiple pointers (we have enough registers), a tiny",
      "  // optimization.",
      "  const int4* B_ptr[b_sh_wr_iters];",
      "  #pragma unroll",
      "  for (int i = 0; i < b_sh_wr_iters; i++)",
      "    B_ptr[i] = B + b_gl_rd_delta_i * i + b_gl_rd;",
      "",
      "  bool m_sh_wr_pred = threadIdx.x < m_sh_wr_delta;",
      "  const int4* meta_ptr[m_sh_iters];",
      "  #pragma unroll",
      "  for (int i = 0; i < m_sh_iters; i++)",
      "    meta_ptr[i] = meta + m_gl_rd_delta_i * i + m_gl_rd;",
      "",
      "  extern __shared__ int4 sh[];",
      "  // Shared memory storage for global fetch pipelines.",
      "  int4* sh_a = sh;",
      "  int4* sh_b = sh_a + (stages * a_sh_stage);",
      "  int4* sh_s = sh_b + (stages * b_sh_stage);",
      "  int4* sh_m = sh_s + (stages * s_sh_stage);",
      "  // Register storage for double buffer of shared memory reads.",
      "  FragA frag_a[2][thread_m_blocks][2];",
      "  I4 frag_b_quant[2][b_thread_vecs];",
      "  FragM frag_m[2][2];",
      "  FragC frag_c[thread_m_blocks][4][2];",
      "  FragS frag_s[2][4];",
      "",
      "  // Zero accumulators.",
      "  auto zero_accums = [&]() {",
      "  #pragma unroll",
      "    for (int i = 0; i < thread_m_blocks * 4 * 2 * 4; i++)",
      "      reinterpret_cast<float*>(frag_c)[i] = 0;",
      "  };",
      "",
      "  // Asynchronously fetch the next A, B and s tile from global to the next",
      "  // shared memory pipeline location.",
      "  auto fetch_to_shared = [&](int pipe, int a_off, bool pred = true) {",
      "    if (pred) {",
      "      int4* sh_a_stage = sh_a + a_sh_stage * pipe;",
      "  #pragma unroll",
      "      for (int i = 0; i < a_sh_wr_iters; i++) {",
      "        cp_async4_pred(",
      "            &sh_a_stage[a_sh_wr_trans[i]],",
      "            &A[a_gl_rd_delta_i * i + a_gl_rd + a_gl_rd_delta_o * a_off],",
      "            a_sh_wr_pred[i]);",
      "      }",
      "      int4* sh_b_stage = sh_b + b_sh_stage * pipe;",
      "  #pragma unroll",
      "      for (int i = 0; i < b_sh_wr_iters; i++) {",
      "  #pragma unroll",
      "        for (int j = 0; j < b_thread_vecs; j++) {",
      "          cp_async4(&sh_b_stage[b_sh_wr_delta * i + b_sh_wr + j], B_ptr[i] + j);",
      "        }",
      "        B_ptr[i] += b_gl_rd_delta_o;",
      "      }",
      "      int4* sh_meta_stage = sh_m + m_sh_stage * pipe;",
      "  #pragma unroll",
      "      for (int i = 0; i < m_sh_iters; i++) {",
      "        if (m_sh_wr_pred)",
      "          cp_async4(&sh_meta_stage[m_sh_wr_delta * i + m_sh_wr], meta_ptr[i]);",
      "        meta_ptr[i] += m_gl_rd_delta_o;",
      "      }",
      "      // Only fetch scales if this tile starts a new group",
      "      if constexpr (group_blocks != -1) {",
      "        // This assumes group_blocks >= thread_k_blocks",
      "        // and would need to be modified to support smaller groups.",
      "        static_assert(group_blocks >= thread_k_blocks);",
      "        if (pipe % (group_blocks / thread_k_blocks) == 0) {",
      "          int4* sh_s_stage = sh_s + s_sh_stage * pipe;",
      "          if (s_sh_wr_pred) cp_async4(&sh_s_stage[s_sh_wr], &s[s_gl_rd]);",
      "          s_gl_rd += s_gl_rd_delta;",
      "        }",
      "      }",
      "    }",
      "    // Insert a fence even when we are winding down the pipeline to ensure that",
      "    // waiting is also correct at this point.",
      "    cp_async_fence();",
      "  };",
      "",
      "  // Wait until the next thread tile has been loaded to shared memory.",
      "  auto wait_for_stage = [&]() {",
      "    // We only have `stages - 2` active fetches since we are double buffering",
      "    // and can only issue the next fetch when it is guaranteed that the previous",
      "    // shared memory load is fully complete (as it may otherwise be",
      "    // overwritten).",
      "    cp_async_wait<stages - 2>();",
      "    __syncthreads();",
      "  };",
      "",
      "  // Load the next sub-tile from the current location in the shared memory pipe",
      "  // into the current register buffer.",
      "  auto fetch_to_registers = [&](int k, int pipe) {",
      "    // It may seem inefficient that we reload the groups for every sub-tile;",
      "    // however, this does not seem to be a significant bottleneck, while some",
      "    // theoretically better attempts have lead to bad instruction ordering by",
      "    // the compiler and correspondingly a noticeable drop in performance.",
      "    if constexpr (group_blocks != -1) {",
      "      // This assumes group_blocks >= thread_k_blocks",
      "      // and would need to be modified to support smaller groups.",
      "      static_assert(group_blocks >= thread_k_blocks);",
      "      int4* sh_s_stage =",
      "          sh_s + s_sh_stage * ((group_blocks / thread_k_blocks) *",
      "                               (pipe / (group_blocks / thread_k_blocks)));",
      "      reinterpret_cast<int4*>(&frag_s[k % 2])[0] = sh_s_stage[s_sh_rd];",
      "    }",
      "    int4* sh_a_stage = sh_a + a_sh_stage * pipe;",
      "  #pragma unroll",
      "    for (int i = 0; i < thread_m_blocks; i++) {",
      "      ldsm4(frag_a[k % 2][i][0],",
      "            &sh_a_stage[a_sh_rd_trans[0][k % b_sh_wr_iters][i]]);",
      "      ldsm4(frag_a[k % 2][i][1],",
      "            &sh_a_stage[a_sh_rd_trans[1][k % b_sh_wr_iters][i]]);",
      "    }",
      "",
      "    int4* sh_b_stage = sh_b + b_sh_stage * pipe;",
      "  #pragma unroll",
      "    for (int i = 0; i < b_thread_vecs; i++) {",
      "      frag_b_quant[k % 2][i] = *reinterpret_cast<I4*>(",
      "          &sh_b_stage[b_sh_rd_delta * (k % b_sh_wr_iters) + b_sh_rd + i]);",
      "    }",
      "",
      "    // Load meta with ldsm4",
      "    int4* sh_m_stage = sh_m + m_sh_stage * pipe;",
      "    ldsm4_m(frag_m[k % 2][0],",
      "            &sh_m_stage[m_sh_rd_delta * (k % m_sh_iters) + m_sh_rd]);",
      "  };",
      "",
      "  // Execute the actual tensor core matmul of a sub-tile.",
      "  auto matmul = [&](int k) {",
      "  // We have the m dimension as the inner loop in order to encourage overlapping",
      "  // dequantization and matmul operations.",
      "  #pragma unroll",
      "    for (int j = 0; j < 4; j++) {",
      "      FragB frag_b0;",
      "      FragB frag_b1;",
      "",
      "      if constexpr (num_bits == 4) {",
      "        int b_quant = frag_b_quant[k % 2][0][j];",
      "        int b_quant_shift = b_quant >> 8;",
      "",
      "        frag_b0 = dequant_4bit(b_quant);",
      "        frag_b1 = dequant_4bit(b_quant_shift);",
      "",
      "      } else {",
      "        int* frag_b_quant_ptr = reinterpret_cast<int*>(frag_b_quant[k % 2]);",
      "        int b_quant_0 = frag_b_quant_ptr[j * 2 + 0];",
      "        int b_quant_1 = frag_b_quant_ptr[j * 2 + 1];",
      "",
      "        frag_b0 = dequant_8bit(b_quant_0);",
      "        frag_b1 = dequant_8bit(b_quant_1);",
      "      }",
      "",
      "      // If there are no groups, we can just scale the final output once and can",
      "      // avoid doing so for each weight.",
      "      if constexpr (group_blocks != -1) {",
      "        scale(frag_b0, frag_s[k % 2][j], 0);",
      "      }",
      "      if constexpr (group_blocks != -1) {",
      "        scale(frag_b1, frag_s[k % 2][j], 1);",
      "      }",
      "",
      "  #pragma unroll",
      "      for (int i = 0; i < thread_m_blocks; i++) {",
      "        mma_sp(frag_b0, frag_b1, frag_a[k % 2][i][0], frag_c[i][j][0],",
      "               frag_m[k % 2][j / 2], j % 2);",
      "      }",
      "    }",
      "  };",
      "",
      "  // Since we slice across the k dimension of a tile in order to increase the",
      "  // number of warps while keeping the n dimension of a tile reasonable, we have",
      "  // multiple warps that accumulate their partial sums of the same output",
      "  // location; which we have to reduce over in the end. We do in shared memory.",
      "  auto thread_block_reduce = [&]() {",
      "    constexpr int red_off = threads / b_sh_stride_threads / 2;",
      "    if (red_off >= 1) {",
      "      auto red_idx = threadIdx.x / b_sh_stride_threads;",
      "      constexpr int red_sh_stride = b_sh_stride_threads * 4 * 2;",
      "      constexpr int red_sh_delta = b_sh_stride_threads;",
      "      int red_sh_rd = red_sh_stride * (threadIdx.x / b_sh_stride_threads) +",
      "                      (threadIdx.x % b_sh_stride_threads);",
      "",
      "  // Parallel logarithmic shared memory reduction. We make sure to avoid any",
      "  // unnecessary read or write iterations, e.g., for two warps we write only",
      "  // once by warp 1 and read only once by warp 0.",
      "  #pragma unroll",
      "      for (int m_block = 0; m_block < thread_m_blocks; m_block++) {",
      "  #pragma unroll",
      "        for (int i = red_off; i > 0; i /= 2) {",
      "          if (i <= red_idx && red_idx < 2 * i) {",
      "  #pragma unroll",
      "            for (int j = 0; j < 4 * 2; j++) {",
      "              int red_sh_wr =",
      "                  red_sh_delta * j + (red_sh_rd - red_sh_stride * i);",
      "              if (i < red_off) {",
      "                float* c_rd =",
      "                    reinterpret_cast<float*>(&sh[red_sh_delta * j + red_sh_rd]);",
      "                float* c_wr = reinterpret_cast<float*>(&sh[red_sh_wr]);",
      "  #pragma unroll",
      "                for (int k = 0; k < 4; k++)",
      "                  reinterpret_cast<FragC*>(frag_c)[4 * 2 * m_block + j][k] +=",
      "                      c_rd[k] + c_wr[k];",
      "              }",
      "              sh[red_sh_wr] =",
      "                  reinterpret_cast<int4*>(&frag_c)[4 * 2 * m_block + j];",
      "            }",
      "          }",
      "          __syncthreads();",
      "        }",
      "        if (red_idx == 0) {",
      "  #pragma unroll",
      "          for (int i = 0; i < 4 * 2; i++) {",
      "            float* c_rd =",
      "                reinterpret_cast<float*>(&sh[red_sh_delta * i + red_sh_rd]);",
      "  #pragma unroll",
      "            for (int j = 0; j < 4; j++)",
      "              reinterpret_cast<FragC*>(frag_c)[4 * 2 * m_block + i][j] +=",
      "                  c_rd[j];",
      "          }",
      "        }",
      "        __syncthreads();",
      "      }",
      "    }",
      "  };",
      "",
      "  // Since multiple threadblocks may process parts of the same column slice, we",
      "  // finally have to globally reduce over the results. As the striped",
      "  // partitioning minimizes the number of such reductions and our outputs are",
      "  // usually rather small, we perform this reduction serially in L2 cache.",
      "  auto global_reduce = [&](bool first = false, bool last = false) {",
      "    // We are very careful here to reduce directly in the output buffer to",
      "    // maximize L2 cache utilization in this step. To do this, we write out",
      "    // results in FP16 (but still reduce with FP32 compute).",
      "    constexpr int active_threads = 32 * thread_n_blocks / 4;",
      "    if (threadIdx.x < active_threads) {",
      "      int c_gl_stride = prob_n / 8;",
      "      int c_gl_wr_delta_o = 2 * 4 * c_gl_stride;",
      "      int c_gl_wr_delta_i =",
      "          c_gl_stride;  // 8 threads (e.g., 0,4,8,12,16,20,24,28)",
      "      int c_gl_wr = 2 * c_gl_stride * (threadIdx.x % 4) +",
      "                    8 * (threadIdx.x / 32) + (threadIdx.x % 32) / 4;",
      "      c_gl_wr += (2 * thread_n_blocks) * slice_col;",
      "      constexpr int c_sh_wr_delta = active_threads;",
      "      auto c_sh_wr = threadIdx.x;",
      "",
      "      int col = 2 * ((threadIdx.x % 32) % 4);",
      "",
      "      if (!first) {",
      "  // Interestingly, doing direct global accesses here really seems to mess up",
      "  // the compiler and lead to slowdowns, hence we also use async-copies even",
      "  // though these fetches are not actually asynchronous.",
      "  #pragma unroll",
      "        for (int i = 0; i < thread_m_blocks * 4; i++) {",
      "          cp_async4_pred(&sh[c_sh_wr + c_sh_wr_delta * i],",
      "                         &C[c_gl_wr + c_gl_wr_delta_o * (i / 2) +",
      "                            c_gl_wr_delta_i * (i % 2)],",
      "                         i < (thread_m_blocks - 1) * 4 ||",
      "                             8 * (i / 2) + col + (i % 2) < prob_m);",
      "        }",
      "        cp_async_fence();",
      "        cp_async_wait<0>();",
      "      }",
      "",
      "  #pragma unroll",
      "      for (int i = 0; i < thread_m_blocks * 4; i++) {",
      "        if (i < (thread_m_blocks - 1) * 4 ||",
      "            8 * (i / 2) + col + (i % 2) < prob_m) {",
      "          if (!first) {",
      "            int4 c_red = sh[c_sh_wr + i * c_sh_wr_delta];",
      "  #pragma unroll",
      "            for (int j2 = 0; j2 < 2; j2++) {",
      "  #pragma unroll",
      "              for (int j1 = 0; j1 < 4; j1++) {",
      "                reinterpret_cast<float*>(",
      "                    &frag_c)[4 * 2 * 4 * (i / 4) + 8 * j1 + 2 * j2 +",
      "                             4 * ((i % 4) / 2) + i % 2] +=",
      "                    __half2float(",
      "                        reinterpret_cast<__half*>(&c_red)[(j2 * 4 + j1)]);",
      "              }",
      "            }",
      "          }",
      "          if (!last) {",
      "            int4 c;",
      "  #pragma unroll",
      "            for (int j2 = 0; j2 < 2; j2++) {",
      "  #pragma unroll",
      "              for (int j1 = 0; j1 < 4; j1++) {",
      "                reinterpret_cast<__half*>(&c)[(j2 * 4 + j1)] =",
      "                    __float2half(reinterpret_cast<float*>(",
      "                        &frag_c)[4 * 2 * 4 * (i / 4) + 8 * j1 + 2 * j2 +",
      "                                 4 * ((i % 4) / 2) + i % 2]);",
      "              }",
      "            }",
      "            C[c_gl_wr + c_gl_wr_delta_o * (i / 2) + c_gl_wr_delta_i * (i % 2)] =",
      "                c;",
      "          }",
      "        }",
      "      }",
      "    }",
      "  };",
      "",
      "  // Write out the reduce final result in the correct layout. We only actually",
      "  // reshuffle matrix fragments in this step, the reduction above is performed",
      "  // in fragment layout.",
      "  auto write_result = [&]() {",
      "    int c_gl_stride = prob_n / 8;",
      "",
      "    constexpr int c_sh_stride = 2 * thread_n_blocks;              // RLC:",
      "    constexpr int c_sh_stride_2 = 2 * c_sh_stride + 2;            // RLC:",
      "    constexpr int c_sh_stride_3 = 2 * (2 * thread_n_blocks) + 2;  // RLC:",
      "",
      "    int c_gl_wr_delta = c_gl_stride * (threads / (2 * thread_n_blocks));",
      "",
      "    int c_gl_wr = c_gl_stride * (threadIdx.x / (2 * thread_n_blocks)) +",
      "                  (threadIdx.x % (2 * thread_n_blocks));",
      "    c_gl_wr += (2 * thread_n_blocks) * slice_col;",
      "",
      "    int c_sh_wr = c_sh_stride_2 * ((threadIdx.x % 32) % 4) +",
      "                  ((threadIdx.x % 32) / 4);  // RLC:",
      "    c_sh_wr += 8 * (threadIdx.x / 32);       // 128/4(half4)",
      "",
      "    constexpr int c_sh_rd_delta =",
      "        c_sh_stride_3 * (threads / (2 * 2 * thread_n_blocks));  // RLC:",
      "    int c_sh_rd = c_sh_stride_3 * (threadIdx.x / (2 * 2 * thread_n_blocks)) +",
      "                  (threadIdx.x % (2 * 2 * thread_n_blocks));",
      "",
      "    int c_gl_wr_end = c_gl_stride * prob_m;",
      "",
      "    auto write = [&](int idx, float c0, float c1, float c2, float c3, FragS& s0,",
      "                     float c4, float c5, float c6, float c7, FragS& s1) {",
      "      uint2 res[2];",
      "      res[0] = to_half4(c0, c1, c2, c3);",
      "      res[1] = to_half4(c4, c5, c6, c7);",
      "      half2* tmp = (half2*)&res;",
      "      // for per-column quantization we finally apply the scale here",
      "      if constexpr (group_blocks == -1 && num_bits == 4) {",
      "        tmp[0] = __hmul2(tmp[0], s0[0]);",
      "        tmp[1] = __hmul2(tmp[1], s0[1]);",
      "        tmp[2] = __hmul2(tmp[2], s1[0]);",
      "        tmp[3] = __hmul2(tmp[3], s1[1]);",
      "      }",
      "      ((int4*)sh)[idx] = *((int4*)&res[0]);",
      "    };",
      "",
      "    // RLC:  only warp 0 and 1 baseline example",
      "    if (threadIdx.x / 32 < thread_n_blocks / 4) {",
      "  #pragma unroll",
      "      for (int i = 0; i < thread_m_blocks; i++) {",
      "        int wr = c_sh_wr;",
      "        write(wr, frag_c[i][0][0][0], frag_c[i][1][0][0], frag_c[i][2][0][0],",
      "              frag_c[i][3][0][0], frag_s[0][0], frag_c[i][0][0][2],",
      "              frag_c[i][1][0][2], frag_c[i][2][0][2], frag_c[i][3][0][2],",
      "              frag_s[0][2]);",
      "        write(wr + c_sh_stride, frag_c[i][0][0][1], frag_c[i][1][0][1],",
      "              frag_c[i][2][0][1], frag_c[i][3][0][1], frag_s[0][0],",
      "              frag_c[i][0][0][3], frag_c[i][1][0][3], frag_c[i][2][0][3],",
      "              frag_c[i][3][0][3], frag_s[0][2]);",
      "        write(wr + 4 * c_sh_stride_2, frag_c[i][0][1][0], frag_c[i][1][1][0],",
      "              frag_c[i][2][1][0], frag_c[i][3][1][0], frag_s[0][0],",
      "              frag_c[i][0][1][2], frag_c[i][1][1][2], frag_c[i][2][1][2],",
      "              frag_c[i][3][1][2], frag_s[0][2]);",
      "        write(wr + 4 * c_sh_stride_2 + c_sh_stride, frag_c[i][0][1][1],",
      "              frag_c[i][1][1][1], frag_c[i][2][1][1], frag_c[i][3][1][1],",
      "              frag_s[0][0], frag_c[i][0][1][3], frag_c[i][1][1][3],",
      "              frag_c[i][2][1][3], frag_c[i][3][1][3], frag_s[0][2]);",
      "",
      "        c_sh_wr += 8 * c_sh_stride_2;",
      "      }",
      "    }",
      "    __syncthreads();",
      "",
      "  #pragma unroll",
      "    for (int i = 0;",
      "         i < ceildiv(16 * thread_m_blocks, threads / (2 * thread_n_blocks));",
      "         i++) {",
      "      if (c_gl_wr < c_gl_wr_end) {",
      "        C[c_gl_wr] = sh[c_sh_rd];",
      "        c_gl_wr += c_gl_wr_delta;",
      "        c_sh_rd += c_sh_rd_delta;",
      "      }",
      "    }",
      "  };",
      "",
      "  // Start global fetch and register load pipelines.",
      "  auto start_pipes = [&]() {",
      "  #pragma unroll",
      "    for (int i = 0; i < stages - 1; i++) fetch_to_shared(i, i, i < slice_iters);",
      "    zero_accums();",
      "    wait_for_stage();",
      "    fetch_to_registers(0, 0);",
      "    a_gl_rd += a_gl_rd_delta_o * (stages - 1);",
      "  };",
      "  start_pipes();",
      "",
      "  // Main loop.",
      "  while (slice_iters) {",
      "  // We unroll over both the global fetch and the register load pipeline to",
      "  // ensure all shared memory accesses are static. Note that both pipelines have",
      "  // even length meaning that the next iteration will always start at index 0.",
      "  #pragma unroll",
      "    for (int pipe = 0; pipe < stages;) {",
      "      fetch_to_shared((pipe + stages - 1) % stages, pipe,",
      "                      slice_iters >= stages);",
      "      matmul(pipe);",
      "      wait_for_stage();",
      "",
      "      fetch_to_registers(pipe + 1, (pipe + 1) % stages);",
      "",
      "      pipe++;",
      "      slice_iters--;",
      "      if (slice_iters == 0) break;",
      "    }",
      "    a_gl_rd += a_gl_rd_delta_o * stages;",
      "",
      "    // Process results and, if necessary, proceed to the next column slice.",
      "    // While this pattern may not be the most readable, other ways of writing",
      "    // the loop seemed to noticeably worse performance after compilation.",
      "    if (slice_iters == 0) {",
      "      cp_async_wait<0>();",
      "      bool last = slice_idx == slice_count - 1;",
      "      // For per-column scales, we only fetch them here in the final step before",
      "      // write-out",
      "      if constexpr (group_blocks == -1) {",
      "        if constexpr (num_bits == 8) {",
      "          if (s_sh_wr_pred) cp_async4(&sh_s[s_sh_wr], &s[s_gl_rd]);",
      "          cp_async_fence();",
      "        } else {",
      "          if (last) {",
      "            if (s_sh_wr_pred) cp_async4(&sh_s[s_sh_wr], &s[s_gl_rd]);",
      "            cp_async_fence();",
      "          }",
      "        }",
      "      }",
      "      thread_block_reduce();",
      "",
      "      if constexpr (group_blocks == -1) {",
      "        if constexpr (num_bits == 8) {",
      "          cp_async_wait<0>();",
      "          __syncthreads();",
      "          if (threadIdx.x / 32 < thread_n_blocks / 4) {",
      "            *(float4*)(frag_s) = *(float4*)(&sh_s[s_sh_rd]);",
      "          }",
      "        } else {",
      "          if (last) {",
      "            cp_async_wait<0>();",
      "            __syncthreads();",
      "            if (threadIdx.x / 32 < thread_n_blocks / 4) {",
      "              *(float4*)(frag_s) = *(float4*)(&sh_s[s_sh_rd]);",
      "            }",
      "          }",
      "        }",
      "      }",
      "",
      "      // For 8-bit channelwise, we apply the scale before the global reduction",
      "      // that converts the fp32 results to fp16 (so that we avoid possible",
      "      // overflow in fp16)",
      "      if constexpr (group_blocks == -1 && num_bits == 8) {",
      "        if (threadIdx.x / 32 < thread_n_blocks / 4) {",
      "  #pragma unroll",
      "          for (int i = 0; i < thread_m_blocks; i++) {",
      "            scale_floats(&frag_c[i][0][0][0], &frag_c[i][1][0][0],",
      "                         &frag_c[i][2][0][0], &frag_c[i][3][0][0], frag_s[0][0],",
      "                         &frag_c[i][0][0][2], &frag_c[i][1][0][2],",
      "                         &frag_c[i][2][0][2], &frag_c[i][3][0][2],",
      "                         frag_s[0][2]);",
      "",
      "            scale_floats(&frag_c[i][0][0][1], &frag_c[i][1][0][1],",
      "                         &frag_c[i][2][0][1], &frag_c[i][3][0][1], frag_s[0][0],",
      "                         &frag_c[i][0][0][3], &frag_c[i][1][0][3],",
      "                         &frag_c[i][2][0][3], &frag_c[i][3][0][3],",
      "                         frag_s[0][2]);",
      "",
      "            scale_floats(&frag_c[i][0][1][0], &frag_c[i][1][1][0],",
      "                         &frag_c[i][2][1][0], &frag_c[i][3][1][0], frag_s[0][0],",
      "                         &frag_c[i][0][1][2], &frag_c[i][1][1][2],",
      "                         &frag_c[i][2][1][2], &frag_c[i][3][1][2],",
      "                         frag_s[0][2]);",
      "",
      "            scale_floats(&frag_c[i][0][1][1], &frag_c[i][1][1][1],",
      "                         &frag_c[i][2][1][1], &frag_c[i][3][1][1], frag_s[0][0],",
      "                         &frag_c[i][0][1][3], &frag_c[i][1][1][3],",
      "                         &frag_c[i][2][1][3], &frag_c[i][3][1][3],",
      "                         frag_s[0][2]);",
      "          }",
      "        }",
      "      }",
      "",
      "      if (slice_count > 1) {  // only globally reduce if there is more than one",
      "                              // block in a slice",
      "        barrier_acquire(&locks[slice_col], slice_idx);",
      "        global_reduce(slice_idx == 0, last);",
      "        barrier_release(&locks[slice_col], last);",
      "      }",
      "      if (last)  // only the last block in a slice actually writes the result",
      "        write_result();",
      "",
      "      slice_row = 0;",
      "      slice_col_par++;",
      "      slice_col++;",
      "      init_slice();",
      "      if (slice_iters) {",
      "        a_gl_rd = a_gl_stride * (threadIdx.x / a_gl_rd_delta_o) +",
      "                  (threadIdx.x % a_gl_rd_delta_o);",
      "  #pragma unroll",
      "        for (int i = 0; i < b_sh_wr_iters; i++)",
      "          B_ptr[i] += b_sh_stride - b_gl_rd_delta_o * k_tiles;",
      "  #pragma unroll",
      "        for (int i = 0; i < m_sh_iters; i++)",
      "          meta_ptr[i] += (m_sh_stride)-m_gl_rd_delta_o * k_tiles;",
      "        if (slice_col == 0) {",
      "  #pragma unroll",
      "          for (int i = 0; i < b_sh_wr_iters; i++) B_ptr[i] -= b_gl_stride;",
      "  #pragma unroll",
      "          for (int i = 0; i < m_sh_iters; i++) meta_ptr[i] -= m_gl_stride;",
      "        }",
      "        s_gl_rd = s_sh_stride * slice_col + threadIdx.x;",
      "        start_pipes();",
      "      }",
      "    }",
      "  }",
      "}",
      "",
      "#endif",
      "",
      "#define CALL_IF_2_4(NUM_BITS, THREAD_M_BLOCKS, THREAD_N_BLOCKS,               \\",
      "                    THREAD_K_BLOCKS, GROUP_BLOCKS)                            \\",
      "  else if (num_bits == NUM_BITS && thread_m_blocks == THREAD_M_BLOCKS &&      \\",
      "           thread_n_blocks == THREAD_N_BLOCKS &&                              \\",
      "           thread_k_blocks == THREAD_K_BLOCKS &&                              \\",
      "           group_blocks == GROUP_BLOCKS) {                                    \\",
      "    cudaFuncSetAttribute(                                                     \\",
      "        Marlin_24<NUM_BITS, THREADS, THREAD_N_BLOCKS, THREAD_M_BLOCKS,        \\",
      "                  THREAD_K_BLOCKS, STAGES, GROUP_BLOCKS>,                     \\",
      "        cudaFuncAttributeMaxDynamicSharedMemorySize, max_shared_mem);         \\",
      "    Marlin_24<NUM_BITS, THREADS, THREAD_N_BLOCKS, THREAD_M_BLOCKS,            \\",
      "              THREAD_K_BLOCKS, STAGES, GROUP_BLOCKS>                          \\",
      "        <<<blocks, THREADS, max_shared_mem, stream>>>(A_ptr, B_ptr, meta_ptr, \\",
      "                                                      C_ptr, s_ptr, prob_n,   \\",
      "                                                      prob_m, prob_k, locks); \\",
      "  }",
      "",
      "void marlin_cuda_2_4(const void* A, const void* B, const void* meta, void* C,",
      "                     void* s, int prob_m, int prob_n, int prob_k,",
      "                     void* workspace, int num_bits, int groupsize = -1,",
      "                     int dev = 0, cudaStream_t stream = 0, int thread_k = -1,",
      "                     int thread_m = -1, int sms = -1, int max_par = 16) {",
      "  int tot_n = prob_n;",
      "  int tot_n_blocks = ceildiv(tot_n, 16);",
      "  int pad = 16 * tot_n_blocks - tot_n;",
      "",
      "  if (sms == -1) {",
      "    cudaDeviceGetAttribute(&sms, cudaDevAttrMultiProcessorCount, dev);",
      "  }",
      "  TORCH_CHECK(sms > 0);",
      "",
      "  int max_shared_mem = 0;",
      "  cudaDeviceGetAttribute(&max_shared_mem,",
      "                         cudaDevAttrMaxSharedMemoryPerBlockOptin, dev);",
      "  TORCH_CHECK(max_shared_mem > 0);",
      "",
      "  if (thread_k == -1 || thread_m == -1) {",
      "    if (prob_n <= 16) {",
      "      // For small batchizes, better partitioningif is slightly more important",
      "      // than better compute utilization",
      "      thread_k = 128;",
      "      thread_m = 128;",
      "    } else {",
      "      thread_k = 64;",
      "      thread_m = 256;",
      "    }",
      "    // Also had",
      "    // if prob_n > 256",
      "    //   thread_k = 32;",
      "    //   thread_m = 512;",
      "    // but this is broken,",
      "    // TODO(Lucas, Alex M): figure out why",
      "  }",
      "",
      "  int thread_k_blocks = thread_k / 32;  // 2:4 version with m16n8k32 instruction",
      "  int thread_m_blocks = thread_m / 16;",
      "  int group_blocks = (groupsize == -1) ? -1 : groupsize / 16;",
      "  int blocks = sms;",
      "",
      "  TORCH_CHECK(prob_m % thread_m == 0, \"prob_m = \", prob_m,",
      "              \" is not divisible by thread_m = \", thread_m);",
      "  TORCH_CHECK(prob_k % thread_k == 0, \"prob_k = \", prob_k,",
      "              \" is not divisible by thread_k = \", thread_k);",
      "  if (group_blocks != -1) {",
      "    TORCH_CHECK((prob_k / 2) % group_blocks == 0, \"prob_k/2 = \", prob_k / 2,",
      "                \" is not divisible by group_blocks = \", group_blocks);",
      "  }",
      "",
      "  TORCH_CHECK(prob_m > 0 && prob_n > 0 && prob_k > 0, \"Invalid MNK = [\", prob_m,",
      "              \", \", prob_n, \", \", prob_k, \"]\");",
      "",
      "  const int4* A_ptr = (const int4*)A;",
      "  const int4* B_ptr = (const int4*)B;",
      "  const int4* meta_ptr = (const int4*)meta;",
      "  int4* C_ptr = (int4*)C;",
      "  const int4* s_ptr = (const int4*)s;",
      "",
      "  constexpr int max_m_blocks = 4;",
      "",
      "  int* locks = (int*)workspace;",
      "  for (int i = 0; i < tot_n_blocks; i += max_m_blocks) {",
      "    int thread_n_blocks = tot_n_blocks - i;",
      "    prob_n = tot_n - 16 * i;",
      "    int par = 1;",
      "    if (thread_n_blocks > max_m_blocks) {",
      "      // Note that parallel > 1 currently only works for inputs without any",
      "      // padding",
      "      par = (16 * thread_n_blocks - pad) / (max_m_blocks * 16);",
      "      if (par > max_par) par = max_par;",
      "      prob_n = (max_m_blocks * 16) * par;",
      "      i += max_m_blocks * (par - 1);",
      "      thread_n_blocks = max_m_blocks;",
      "    }",
      "",
      "    // For compilation speed, we only define the kernel configurations that have",
      "    // seemed useful (in terms of performance) in our testing, however many more",
      "    // are, in principle, possible.",
      "",
      "    // the false is start of the CALL_IF macros",
      "    if (false) {",
      "    }  //         BMxBNxBK,   group",
      "    // 4-bit",
      "    CALL_IF_2_4(4, 8, 1, 4, -1)  // e.g., 16x128x128",
      "    CALL_IF_2_4(4, 8, 1, 4, 4)   // e.g., 16x128x128, 64",
      "",
      "    CALL_IF_2_4(4, 16, 1, 2, -1)  // e.g., 16x256x64",
      "    CALL_IF_2_4(4, 16, 1, 2, 4)   // e.g., 16x256x64,  64",
      "    CALL_IF_2_4(4, 16, 2, 2, -1)  // e.g.. 32x256x64",
      "    CALL_IF_2_4(4, 16, 2, 2, 4)",
      "    CALL_IF_2_4(4, 16, 3, 2, -1)",
      "    CALL_IF_2_4(4, 16, 3, 2, 4)",
      "    CALL_IF_2_4(4, 16, 4, 2, -1)",
      "    CALL_IF_2_4(4, 16, 4, 2, 4)",
      "",
      "    CALL_IF_2_4(4, 32, 1, 1, -1)  // e.g., 16x256x64",
      "    CALL_IF_2_4(4, 32, 1, 1, 4)   // e.g., 16x256x64,  64",
      "    CALL_IF_2_4(4, 32, 2, 1, -1)  // e.g.. 32x256x64",
      "    CALL_IF_2_4(4, 32, 2, 1, 4)",
      "    CALL_IF_2_4(4, 32, 3, 1, -1)",
      "    CALL_IF_2_4(4, 32, 3, 1, 4)",
      "    CALL_IF_2_4(4, 32, 4, 1, -1)",
      "    CALL_IF_2_4(4, 32, 4, 1, 4)",
      "",
      "    // 8-bit",
      "    CALL_IF_2_4(8, 8, 1, 4, -1)  // e.g., 16x128x128",
      "    CALL_IF_2_4(8, 8, 1, 4, 4)   // e.g., 16x128x128, 64",
      "",
      "    CALL_IF_2_4(8, 16, 1, 2, -1)  // e.g., 16x256x64",
      "    CALL_IF_2_4(8, 16, 1, 2, 4)   // e.g., 16x256x64,  64",
      "    CALL_IF_2_4(8, 16, 2, 2, -1)  // e.g.. 32x256x64",
      "    CALL_IF_2_4(8, 16, 2, 2, 4)",
      "    CALL_IF_2_4(8, 16, 3, 2, -1)",
      "    CALL_IF_2_4(8, 16, 3, 2, 4)",
      "    CALL_IF_2_4(8, 16, 4, 2, -1)",
      "    CALL_IF_2_4(8, 16, 4, 2, 4)",
      "",
      "    CALL_IF_2_4(8, 32, 1, 1, -1)  // e.g., 16x256x64",
      "    CALL_IF_2_4(8, 32, 1, 1, 4)   // e.g., 16x256x64,  64",
      "    CALL_IF_2_4(8, 32, 2, 1, -1)  // e.g.. 32x256x64",
      "    CALL_IF_2_4(8, 32, 2, 1, 4)",
      "    CALL_IF_2_4(8, 32, 3, 1, -1)",
      "    CALL_IF_2_4(8, 32, 3, 1, 4)",
      "    CALL_IF_2_4(8, 32, 4, 1, -1)",
      "    CALL_IF_2_4(8, 32, 4, 1, 4)",
      "    else {",
      "      throw std::runtime_error(\"Unsupported shapes: MKN = [\" + str(prob_m) +",
      "                               \", \" + str(prob_k) + \", \" + str(prob_n) + \"]\" +",
      "                               \", groupsize = \" + str(groupsize) +",
      "                               \", thread_m_blocks = \" + str(thread_m_blocks) +",
      "                               \", thread_n_blocks = \" + str(thread_n_blocks) +",
      "                               \", thread_k_blocks = \" + str(thread_k_blocks));",
      "    }",
      "",
      "    A_ptr += 16 * thread_n_blocks * (prob_k / 8) * par;",
      "    C_ptr += 16 * thread_n_blocks * (prob_m / 8) * par;",
      "  }",
      "}",
      "",
      "}  // namespace marlin_24",
      "",
      "torch::Tensor gptq_marlin_24_gemm(torch::Tensor& a, torch::Tensor& b_q_weight,",
      "                                  torch::Tensor& b_meta,",
      "                                  torch::Tensor& b_scales,",
      "                                  torch::Tensor& workspace,",
      "                                  vllm::ScalarTypeId const b_q_type_id,",
      "                                  int64_t size_m, int64_t size_n,",
      "                                  int64_t size_k) {",
      "  vllm::ScalarType const b_q_type = vllm::ScalarType::from_id(b_q_type_id);",
      "  // Verify num_bits",
      "  TORCH_CHECK(b_q_type == vllm::kU4B8 || b_q_type == vllm::kU8B128,",
      "              \"num_bits must be uint4b8 or uint8b128. Got = \", b_q_type.str());",
      "  int pack_factor = 32 / b_q_type.size_bits();",
      "",
      "  // Verify M",
      "  TORCH_CHECK(size_m == a.size(0),",
      "              \"Shape mismatch: a.size(0) = \" + str(a.size(0)) +",
      "                  \", size_m = \" + str(size_m));",
      "",
      "  // Verify K",
      "  TORCH_CHECK(size_k == a.size(1),",
      "              \"Shape mismatch: a.size(1) = \" + str(a.size(1)) +",
      "                  \", size_k = \" + str(size_k));",
      "  TORCH_CHECK(size_k % marlin_24::tile_size == 0,",
      "              \"size_k = \" + str(size_k) + \" is not divisible by tile_size = \" +",
      "                  str(marlin_24::tile_size));",
      "  TORCH_CHECK((size_k / marlin_24::tile_size / 2) == b_q_weight.size(0),",
      "              \"Shape mismatch: b_q_weight.size(0) = \" +",
      "                  str(b_q_weight.size(0)) + \", size_k = \" + str(size_k) +",
      "                  \", tile_size = \" + str(marlin_24::tile_size));",
      "",
      "  // Verify N",
      "  TORCH_CHECK(b_scales.size(1) == size_n,",
      "              \"b_scales.size(1) = \" + str(b_scales.size(1)) +",
      "                  \", size_n = \" + str(size_n));",
      "  TORCH_CHECK(",
      "      b_q_weight.size(1) % marlin_24::tile_size == 0,",
      "      \"b_q_weight.size(1) = \" + str(b_q_weight.size(1)) +",
      "          \" is not divisible by tile_size = \" + str(marlin_24::tile_size));",
      "",
      "  int actual_size_n = (b_q_weight.size(1) / marlin_24::tile_size) * pack_factor;",
      "  TORCH_CHECK(",
      "      size_n == actual_size_n,",
      "      \"size_n = \" + str(size_n) + \", actual_size_n = \" + str(actual_size_n));",
      "",
      "  // Verify meta",
      "  TORCH_CHECK(b_meta.size(0) == size_k / 8 / 2 / 2,",
      "              \"b_meta.size(0) = \", b_meta.size(0),",
      "              \" is not size_k / 8 / 2 / 2 = \", size_k / 8 / 2 / 2);",
      "  TORCH_CHECK(b_meta.size(1) == size_n * 2, \"b_meta.size(1) = \", b_meta.size(1),",
      "              \" is not size_n * 2 = \", size_n * 2);",
      "",
      "  // Verify A device and strides",
      "  TORCH_CHECK(a.device().is_cuda(), \"A is not on GPU\");",
      "  TORCH_CHECK(a.is_contiguous(), \"A is not contiguous\");",
      "  TORCH_CHECK(a.dtype() == torch::kFloat16,",
      "              \"A is not float16, currently only float16 is supported\");",
      "",
      "  // Verify B device and strides",
      "  TORCH_CHECK(b_q_weight.device().is_cuda(), \"b_q_weight is not on GPU\");",
      "  TORCH_CHECK(b_q_weight.is_contiguous(), \"b_q_weight is not contiguous\");",
      "",
      "  // Verify b_meta device and strides",
      "  TORCH_CHECK(b_meta.device().is_cuda(), \"b_meta is not on GPU\");",
      "  TORCH_CHECK(b_meta.is_contiguous(), \"b_meta is not contiguous\");",
      "",
      "  // Verify scales device and strides",
      "  TORCH_CHECK(b_scales.device().is_cuda(), \"b_scales is not on GPU\");",
      "  TORCH_CHECK(b_scales.is_contiguous(), \"b_scales is not contiguous\");",
      "  TORCH_CHECK(b_scales.dtype() == torch::kFloat16,",
      "              \"A is not float16, currently only float16 is supported\");",
      "",
      "  // Alloc C matrix",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(a));",
      "  auto options = torch::TensorOptions().dtype(a.dtype()).device(a.device());",
      "  torch::Tensor c = torch::empty({size_m, size_n}, options);",
      "",
      "  int thread_k = -1;",
      "  int thread_m = -1;",
      "  int sms = -1;",
      "  int max_par = marlin_24::max_par;",
      "",
      "  int groupsize = -1;",
      "  if (b_scales.size(0) > 1) {",
      "    TORCH_CHECK(size_k % b_scales.size(0) == 0,",
      "                \"size_k = \" + str(size_k) +",
      "                    \", is not divisible by b_scales.size(0) = \" +",
      "                    str(b_scales.size(0)));",
      "    groupsize = size_k / b_scales.size(0);",
      "    groupsize /= 2;  // Because of 24",
      "  }",
      "",
      "  // Verify groupsize",
      "  TORCH_CHECK(groupsize == -1 || groupsize == 64,",
      "              \"Unexpected groupsize = \" + str(groupsize));",
      "",
      "  // Verify workspace size",
      "  TORCH_CHECK(size_n % marlin_24::min_thread_n == 0,",
      "              \"size_n = \" + str(size_n) +",
      "                  \", is not divisible by min_thread_n = \" +",
      "                  str(marlin_24::min_thread_n));",
      "  int min_workspace_size =",
      "      (size_n / marlin_24::min_thread_n) * marlin_24::max_par;",
      "  TORCH_CHECK(workspace.numel() >= min_workspace_size,",
      "              \"workspace.numel = \" + str(workspace.numel()) +",
      "                  \" is below min_workspace_size = \" + str(min_workspace_size));",
      "",
      "  int dev = a.get_device();",
      "  marlin_24::marlin_cuda_2_4(",
      "      a.data_ptr(), b_q_weight.data_ptr(), b_meta.data_ptr(), c.data_ptr(),",
      "      b_scales.data_ptr(), size_n, size_m, size_k, workspace.data_ptr(),",
      "      b_q_type.size_bits(), groupsize, dev, at::cuda::getCurrentCUDAStream(dev),",
      "      thread_k, thread_m, sms, max_par);",
      "",
      "  return c;",
      "}",
      "",
      "TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, CUDA, m) {",
      "  m.impl(\"gptq_marlin_24_gemm\", &gptq_marlin_24_gemm);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fp8/common.cu",
    "source": [
      "#include \"common.cuh\"",
      "#include \"dispatch_utils.h\"",
      "#include \"../vectorization_utils.cuh\"",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <ATen/cuda/Exceptions.h>",
      "",
      "#ifndef USE_ROCM",
      "  #include <cub/cub.cuh>",
      "#else",
      "  #include <hipcub/hipcub.hpp>",
      "#endif",
      "",
      "namespace vllm {",
      "",
      "template <typename scalar_t, typename fp8_type>",
      "__global__ void scaled_fp8_quant_kernel_strided(",
      "    fp8_type* __restrict__ out, const scalar_t* __restrict__ input,",
      "    const float* __restrict__ scale, int hidden_size, int64_t in_row_stride,",
      "    int64_t out_row_stride) {",
      "  const int64_t token_idx = blockIdx.x;  // one token per block",
      "  const int tid = threadIdx.x;",
      "",
      "  const scalar_t* token_in = input + token_idx * in_row_stride;",
      "  fp8_type* token_out = out + token_idx * out_row_stride;",
      "",
      "  const float inv_scale = 1.0f / (*scale);",
      "",
      "  vectorize_with_alignment<16>(",
      "      token_in, token_out, hidden_size, tid, blockDim.x,",
      "      [=] __device__(fp8_type & dst, const scalar_t& src) {",
      "        dst = scaled_fp8_conversion<true, fp8_type>(static_cast<float>(src),",
      "                                                    inv_scale);",
      "      });",
      "}",
      "",
      "template <typename scalar_t, typename fp8_type>",
      "__global__ void segmented_max_reduction_strided(",
      "    float* __restrict__ scale, const scalar_t* __restrict__ input,",
      "    int hidden_size, int64_t in_row_stride, int64_t num_tokens) {",
      "  __shared__ float cache[256];",
      "  const int tid = threadIdx.x;",
      "  int64_t token_idx = blockIdx.x;",
      "",
      "  // one block per token. Guard in case gridDim.x > num_tokens.",
      "  if (token_idx >= num_tokens) {",
      "    return;",
      "  }",
      "",
      "  const scalar_t* row_ptr = input + token_idx * in_row_stride;",
      "",
      "  // each thread scans elements of the row in a strided fashion.",
      "  float thread_max = 0.0f;",
      "  for (int e = tid; e < hidden_size; e += blockDim.x) {",
      "    float v = fabsf(static_cast<float>(row_ptr[e]));",
      "    thread_max = fmaxf(thread_max, v);",
      "  }",
      "",
      "  cache[tid] = thread_max;",
      "  __syncthreads();",
      "",
      "  // parallel reduction to find row max.",
      "  for (int offset = blockDim.x / 2; offset > 0; offset >>= 1) {",
      "    if (tid < offset) {",
      "      cache[tid] = fmaxf(cache[tid], cache[tid + offset]);",
      "    }",
      "    __syncthreads();",
      "  }",
      "",
      "  // thread 0 updates global scale (per-tensor) atomically.",
      "  if (tid == 0) {",
      "    atomicMaxFloat(scale, cache[0] / quant_type_max_v<fp8_type>);",
      "  }",
      "}",
      "",
      "template <typename scalar_t, typename fp8_type>",
      "__global__ void scaled_fp8_quant_kernel_strided_dynamic(",
      "    fp8_type* __restrict__ out, const scalar_t* __restrict__ input,",
      "    const float* __restrict__ scale, int hidden_size, int64_t in_row_stride,",
      "    int64_t out_row_stride) {",
      "  const int64_t token_idx = blockIdx.x;",
      "  const int tid = threadIdx.x;",
      "",
      "  const scalar_t* token_in = input + token_idx * in_row_stride;",
      "  fp8_type* token_out = out + token_idx * out_row_stride;",
      "",
      "  const float reciprocal_scale = 1.0f / (*scale);",
      "  vectorize_with_alignment<16>(",
      "      token_in, token_out, hidden_size, tid, blockDim.x,",
      "      [=] __device__(fp8_type & dst, const scalar_t& src) {",
      "        dst = scaled_fp8_conversion<true, fp8_type>(static_cast<float>(src),",
      "                                                    reciprocal_scale);",
      "      });",
      "}",
      "",
      "template <typename scalar_t, typename fp8_type>",
      "__global__ void dynamic_per_token_scaled_fp8_quant_kernel_strided(",
      "    fp8_type* __restrict__ out, float* __restrict__ scale,",
      "    const scalar_t* __restrict__ input, const float* __restrict__ scale_ub,",
      "    int hidden_size, int64_t in_row_stride, int64_t out_row_stride) {",
      "  const int64_t token_idx = blockIdx.x;",
      "  const int tid = threadIdx.x;",
      "",
      "  // Use int64 to avoid overflowing an int32 when calculating this offset",
      "  int64_t in_offset = static_cast<int64_t>(token_idx) * in_row_stride;",
      "  int64_t out_offset = static_cast<int64_t>(token_idx) * out_row_stride;",
      "  const scalar_t* token_in = input + in_offset;",
      "  fp8_type* token_out = out + out_offset;",
      "",
      "  // 1) per-token absmax",
      "  float absmax_val = 0.f;",
      "  vectorize_read_with_alignment<16>(",
      "      token_in, hidden_size, tid, blockDim.x, [&] __device__(scalar_t v) {",
      "        absmax_val = fmaxf(absmax_val, fabsf(static_cast<float>(v)));",
      "      });",
      "",
      "  using BlockReduce = cub::BlockReduce<float, 256>;",
      "  __shared__ typename BlockReduce::TempStorage tmp;",
      "  const float block_max =",
      "      BlockReduce(tmp).Reduce(absmax_val, cub::Max{}, blockDim.x);",
      "",
      "  __shared__ float token_scale;",
      "  if (tid == 0) {",
      "    token_scale = scale_ub ? fminf(block_max, *scale_ub) : block_max;",
      "    token_scale = fmaxf(token_scale / quant_type_max_v<fp8_type>,",
      "                        min_scaling_factor<fp8_type>::val());",
      "    scale[token_idx] = token_scale;",
      "  }",
      "  __syncthreads();",
      "",
      "  // 2) quantize",
      "  vectorize_with_alignment<16>(",
      "      token_in, token_out, hidden_size, tid, blockDim.x,",
      "      [=] __device__(fp8_type & dst, const scalar_t& src) {",
      "        dst = scaled_fp8_conversion<false, fp8_type>(static_cast<float>(src),",
      "                                                     token_scale);",
      "      });",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "void static_scaled_fp8_quant(torch::Tensor& out,          // [..., d]",
      "                             torch::Tensor const& input,  // [..., d]",
      "                             torch::Tensor const& scale)  // [1]",
      "{",
      "  TORCH_CHECK(input.stride(-1) == 1,",
      "              \"last dimension of input must be contiguous\");",
      "  TORCH_CHECK(out.stride(-1) == 1,",
      "              \"last dimension of output must be contiguous\");",
      "",
      "  const int hidden_size = input.size(-1);",
      "  const int num_tokens = input.numel() / hidden_size;",
      "  const int block_size = 256;",
      "  dim3 grid(num_tokens);",
      "  dim3 block(block_size);",
      "",
      "  const int64_t in_row_stride = input.stride(-2);",
      "  const int64_t out_row_stride = out.stride(-2);",
      "",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  VLLM_DISPATCH_FLOATING_TYPES(",
      "      input.scalar_type(), \"scaled_fp8_quant_kernel_scalar_type\", [&] {",
      "        VLLM_DISPATCH_FP8_TYPES(",
      "            out.scalar_type(), \"scaled_fp8_quant_kernel_fp8_type\", [&] {",
      "              vllm::scaled_fp8_quant_kernel_strided<scalar_t, fp8_t>",
      "                  <<<grid, block, 0, stream>>>(",
      "                      out.data_ptr<fp8_t>(), input.data_ptr<scalar_t>(),",
      "                      scale.data_ptr<float>(), hidden_size, in_row_stride,",
      "                      out_row_stride);",
      "            });",
      "      });",
      "}",
      "",
      "void dynamic_scaled_fp8_quant(torch::Tensor& out,          // [..., d]",
      "                              torch::Tensor const& input,  // [..., d]",
      "                              torch::Tensor& scale)        // [1]",
      "{",
      "  TORCH_CHECK(input.stride(-1) == 1,",
      "              \"last dimension of input must be contiguous\");",
      "  TORCH_CHECK(out.stride(-1) == 1,",
      "              \"last dimension of output must be contiguous\");",
      "",
      "  const int hidden_size = input.size(-1);",
      "  const int num_tokens = input.numel() / hidden_size;",
      "  const int block_size = 256;",
      "  dim3 grid(num_tokens);",
      "  dim3 block(block_size);",
      "",
      "  const int64_t in_row_stride = input.stride(-2);",
      "  const int64_t out_row_stride = out.stride(-2);",
      "",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "",
      "  // scale tensor should be initialised to <=0 before reduction",
      "  AT_CUDA_CHECK(",
      "      cudaMemsetAsync(scale.data_ptr<float>(), 0, sizeof(float), stream));",
      "",
      "  VLLM_DISPATCH_FLOATING_TYPES(",
      "      input.scalar_type(), \"scaled_fp8_quant_kernel_scalar_type\", [&] {",
      "        VLLM_DISPATCH_FP8_TYPES(",
      "            out.scalar_type(), \"scaled_fp8_quant_kernel_fp8_type\", [&] {",
      "              vllm::segmented_max_reduction_strided<scalar_t, fp8_t>",
      "                  <<<grid, block, 0, stream>>>(",
      "                      scale.data_ptr<float>(), input.data_ptr<scalar_t>(),",
      "                      hidden_size, in_row_stride,",
      "                      static_cast<int64_t>(num_tokens));",
      "",
      "              vllm::scaled_fp8_quant_kernel_strided_dynamic<scalar_t, fp8_t>",
      "                  <<<grid, block, 0, stream>>>(",
      "                      out.data_ptr<fp8_t>(), input.data_ptr<scalar_t>(),",
      "                      scale.data_ptr<float>(), hidden_size, in_row_stride,",
      "                      out_row_stride);",
      "            });",
      "      });",
      "}",
      "",
      "void dynamic_per_token_scaled_fp8_quant(",
      "    torch::Tensor& out,          // [..., d]",
      "    torch::Tensor const& input,  // [..., d]",
      "    torch::Tensor& scales, std::optional<at::Tensor> const& scale_ub) {",
      "  TORCH_CHECK(input.stride(-1) == 1,",
      "              \"last dimension of input must be contiguous\");",
      "  TORCH_CHECK(out.stride(-1) == 1,",
      "              \"last dimension of output must be contiguous\");",
      "",
      "  const int hidden_size = input.size(-1);",
      "  const int num_tokens = input.numel() / hidden_size;",
      "  const int block_size = 256;",
      "  dim3 grid(num_tokens);",
      "  dim3 block(std::min(hidden_size, block_size));",
      "",
      "  const int64_t in_row_stride = input.stride(-2);",
      "  const int64_t out_row_stride = out.stride(-2);",
      "",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  VLLM_DISPATCH_FLOATING_TYPES(",
      "      input.scalar_type(),",
      "      \"dynamic_per_token_scaled_fp8_quant_kernel_scalar_type\", [&] {",
      "        VLLM_DISPATCH_FP8_TYPES(",
      "            out.scalar_type(),",
      "            \"dynamic_per_token_scaled_fp8_quant_kernel_fp8_type\", [&] {",
      "              vllm::dynamic_per_token_scaled_fp8_quant_kernel_strided<",
      "                  scalar_t, fp8_t><<<grid, block, 0, stream>>>(",
      "                  out.data_ptr<fp8_t>(), scales.data_ptr<float>(),",
      "                  input.data_ptr<scalar_t>(),",
      "                  scale_ub.has_value() ? scale_ub->data_ptr<float>() : nullptr,",
      "                  hidden_size, in_row_stride, out_row_stride);",
      "            });",
      "      });",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fp8/common.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"quantization/vectorization.cuh\"",
      "#include \"quantization/utils.cuh\"",
      "",
      "#include <cmath>",
      "",
      "#ifdef USE_ROCM",
      "  #include \"amd/quant_utils.cuh\"",
      "#endif",
      "",
      "// Determines the preferred FP8 type for the current platform.",
      "// Note that for CUDA this just returns true,",
      "// but on ROCm it will check device props.",
      "static bool is_fp8_ocp() {",
      "#ifndef USE_ROCM",
      "  return true;",
      "#else",
      "  auto dprops = at::cuda::getCurrentDeviceProperties();",
      "  std::string device_arch = dprops->gcnArchName;",
      "  size_t substring = device_arch.find(\"gfx94\");",
      "  return substring == std::string::npos;",
      "#endif",
      "}",
      "",
      "namespace vllm {",
      "",
      "__device__ __forceinline__ float atomicMaxFloat(float* addr, float value) {",
      "  float old;",
      "  old = (value >= 0)",
      "            ? __int_as_float(atomicMax((int*)addr, __float_as_int(value)))",
      "            : __uint_as_float(",
      "                  atomicMin((unsigned int*)addr, __float_as_uint(value)));",
      "",
      "  return old;",
      "}",
      "",
      "template <bool is_scale_inverted, typename fp8_type>",
      "__device__ __forceinline__ fp8_type scaled_fp8_conversion(float const val,",
      "                                                          float const scale) {",
      "  float x = 0.0f;",
      "  if constexpr (is_scale_inverted) {",
      "    x = val * scale;",
      "  } else {",
      "    x = val / scale;",
      "  }",
      "",
      "  float r =",
      "      fmaxf(-quant_type_max_v<fp8_type>, fminf(x, quant_type_max_v<fp8_type>));",
      "#ifndef USE_ROCM",
      "  return static_cast<fp8_type>(r);",
      "#else",
      "  // Use hardware cvt instruction for fp8 on rocm",
      "  return fp8::cvt_c10<fp8_type>(r);",
      "#endif",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fp8/per_token_group_quant.cu",
    "source": [
      "#include <ATen/cuda/CUDAContext.h>",
      "",
      "#include \"../per_token_group_quant_8bit.h\"",
      "",
      "#include <cmath>",
      "",
      "#include <cuda_fp8.h>",
      "",
      "#include <torch/all.h>",
      "",
      "#include \"../vectorization.cuh\"",
      "#include \"../vectorization_utils.cuh\"",
      "#include \"../../dispatch_utils.h\"",
      "",
      "__device__ __forceinline__ float GroupReduceMax(float val, const int tid) {",
      "  unsigned mask = 0xffff;",
      "",
      "  val = fmaxf(val, __shfl_xor_sync(mask, val, 8));",
      "  val = fmaxf(val, __shfl_xor_sync(mask, val, 4));",
      "  val = fmaxf(val, __shfl_xor_sync(mask, val, 2));",
      "  val = fmaxf(val, __shfl_xor_sync(mask, val, 1));",
      "  return val;",
      "}",
      "",
      "template <typename T, typename DST_DTYPE, bool IS_COLUMN_MAJOR = false,",
      "          bool SCALE_UE8M0 = false, typename scale_packed_t = float>",
      "__global__ void per_token_group_quant_8bit_kernel(",
      "    const T* __restrict__ input, void* __restrict__ output_q,",
      "    scale_packed_t* __restrict__ output_s, const int group_size,",
      "    const int num_groups, const int groups_per_block, const float eps,",
      "    const float min_8bit, const float max_8bit, const int scale_num_rows = 0,",
      "    const int scale_stride = 0) {",
      "  const int threads_per_group = 16;",
      "  const int64_t local_group_id = threadIdx.x / threads_per_group;",
      "  const int lane_id = threadIdx.x % threads_per_group;",
      "",
      "  const int64_t block_group_id = blockIdx.x * groups_per_block;",
      "  const int64_t global_group_id = block_group_id + local_group_id;",
      "  const int64_t block_group_offset = global_group_id * group_size;",
      "",
      "  float local_absmax = eps;",
      "",
      "  using scale_element_t = float;",
      "  static_assert(sizeof(scale_packed_t) % sizeof(scale_element_t) == 0);",
      "",
      "  const T* group_input = input + block_group_offset;",
      "  DST_DTYPE* group_output =",
      "      static_cast<DST_DTYPE*>(output_q) + block_group_offset;",
      "  scale_element_t* scale_output;",
      "",
      "  if constexpr (IS_COLUMN_MAJOR) {",
      "    const int num_elems_per_pack =",
      "        static_cast<int>(sizeof(scale_packed_t) / sizeof(scale_element_t));",
      "    const int scale_num_rows_element = scale_num_rows * num_elems_per_pack;",
      "    const int row_idx = global_group_id / scale_num_rows_element;",
      "    const int col_idx_raw = global_group_id % scale_num_rows_element;",
      "    const int col_idx = col_idx_raw / num_elems_per_pack;",
      "    const int pack_idx = col_idx_raw % num_elems_per_pack;",
      "    scale_output = reinterpret_cast<scale_element_t*>(output_s) +",
      "                   (col_idx * scale_stride * num_elems_per_pack +",
      "                    row_idx * num_elems_per_pack + pack_idx);",
      "  } else {",
      "    scale_output = output_s + global_group_id;",
      "  }",
      "",
      "  // shared memory to cache each group's data to avoid double DRAM reads.",
      "  extern __shared__ __align__(16) char smem_raw[];",
      "  T* smem = reinterpret_cast<T*>(smem_raw);",
      "  T* smem_group = smem + local_group_id * group_size;",
      "",
      "  constexpr int vec_size = 16 / sizeof(T);",
      "  using vec_t = vllm::vec_n_t<T, vec_size>;",
      "",
      "  // copy global -> shared & compute absmax",
      "  auto scalar_op_cache = [&] __device__(T & dst, const T& src) {",
      "    float abs_v = fabsf(static_cast<float>(src));",
      "    local_absmax = fmaxf(local_absmax, abs_v);",
      "    dst = src;",
      "  };",
      "",
      "  vllm::vectorize_with_alignment<vec_size>(",
      "      group_input,        // in",
      "      smem_group,         // out (shared)",
      "      group_size,         // elements per group",
      "      lane_id,            // thread id",
      "      threads_per_group,  // stride in group",
      "      scalar_op_cache);   // scalar handler",
      "",
      "  local_absmax = GroupReduceMax(local_absmax, lane_id);",
      "",
      "  float y_s = local_absmax / max_8bit;",
      "  if constexpr (SCALE_UE8M0) {",
      "    y_s = exp2f(ceilf(log2f(fmaxf(fabsf(y_s), 1e-10f))));",
      "  }",
      "",
      "  scale_element_t y_s_quant = y_s;",
      "",
      "  if (lane_id == 0) {",
      "    *scale_output = y_s_quant;",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  // quantize shared -> global 8-bit",
      "  auto scalar_op_quant = [&] __device__(DST_DTYPE & dst, const T& src) {",
      "    float q = fminf(fmaxf(static_cast<float>(src) / y_s, min_8bit), max_8bit);",
      "    dst = DST_DTYPE(q);",
      "  };",
      "",
      "  vllm::vectorize_with_alignment<vec_size>(",
      "      smem_group,         // in (shared)",
      "      group_output,       // out (global quant tensor)",
      "      group_size,         // elements",
      "      lane_id,            // tid",
      "      threads_per_group,  // stride",
      "      scalar_op_quant);   // scalar handler",
      "}",
      "",
      "void per_token_group_quant_8bit(const torch::Tensor& input,",
      "                                torch::Tensor& output_q,",
      "                                torch::Tensor& output_s, int64_t group_size,",
      "                                double eps, double min_8bit, double max_8bit,",
      "                                bool scale_ue8m0) {",
      "  TORCH_CHECK(input.is_contiguous());",
      "  TORCH_CHECK(output_q.is_contiguous());",
      "",
      "  const int num_groups = input.numel() / group_size;",
      "",
      "  TORCH_CHECK(input.numel() % group_size == 0);",
      "  TORCH_CHECK(output_s.dim() == 2);",
      "",
      "  cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "",
      "  constexpr int THREADS_PER_GROUP = 16;",
      "",
      "  int groups_per_block = 1;",
      "",
      "  if (num_groups % 16 == 0) {",
      "    groups_per_block = 16;",
      "  } else if (num_groups % 8 == 0) {",
      "    groups_per_block = 8;",
      "  } else if (num_groups % 4 == 0) {",
      "    groups_per_block = 4;",
      "  } else if (num_groups % 2 == 0) {",
      "    groups_per_block = 2;",
      "  }",
      "",
      "  auto dst_type = output_q.scalar_type();",
      "  const int num_blocks = num_groups / groups_per_block;",
      "  const int num_threads = groups_per_block * THREADS_PER_GROUP;",
      "",
      "  const bool is_column_major = output_s.stride(0) < output_s.stride(1);",
      "  const int scale_num_rows = output_s.size(1);",
      "  const int scale_stride = output_s.stride(1);",
      "",
      "#define LAUNCH_KERNEL(T, DST_DTYPE)                                        \\",
      "  do {                                                                     \\",
      "    dim3 grid(num_blocks);                                                 \\",
      "    dim3 block(num_threads);                                               \\",
      "    size_t smem_bytes =                                                    \\",
      "        static_cast<size_t>(groups_per_block) * group_size * sizeof(T);    \\",
      "    if (is_column_major) {                                                 \\",
      "      if (scale_ue8m0) {                                                   \\",
      "        per_token_group_quant_8bit_kernel<T, DST_DTYPE, true, true>        \\",
      "            <<<grid, block, smem_bytes, stream>>>(                         \\",
      "                static_cast<T*>(input.data_ptr()), output_q.data_ptr(),    \\",
      "                static_cast<float*>(output_s.data_ptr()), group_size,      \\",
      "                num_groups, groups_per_block, (float)eps, (float)min_8bit, \\",
      "                (float)max_8bit, scale_num_rows, scale_stride);            \\",
      "      } else {                                                             \\",
      "        per_token_group_quant_8bit_kernel<T, DST_DTYPE, true, false>       \\",
      "            <<<grid, block, smem_bytes, stream>>>(                         \\",
      "                static_cast<T*>(input.data_ptr()), output_q.data_ptr(),    \\",
      "                static_cast<float*>(output_s.data_ptr()), group_size,      \\",
      "                num_groups, groups_per_block, (float)eps, (float)min_8bit, \\",
      "                (float)max_8bit, scale_num_rows, scale_stride);            \\",
      "      }                                                                    \\",
      "    } else {                                                               \\",
      "      if (scale_ue8m0) {                                                   \\",
      "        per_token_group_quant_8bit_kernel<T, DST_DTYPE, false, true>       \\",
      "            <<<grid, block, smem_bytes, stream>>>(                         \\",
      "                static_cast<T*>(input.data_ptr()), output_q.data_ptr(),    \\",
      "                static_cast<float*>(output_s.data_ptr()), group_size,      \\",
      "                num_groups, groups_per_block, (float)eps, (float)min_8bit, \\",
      "                (float)max_8bit);                                          \\",
      "      } else {                                                             \\",
      "        per_token_group_quant_8bit_kernel<T, DST_DTYPE, false, false>      \\",
      "            <<<grid, block, smem_bytes, stream>>>(                         \\",
      "                static_cast<T*>(input.data_ptr()), output_q.data_ptr(),    \\",
      "                static_cast<float*>(output_s.data_ptr()), group_size,      \\",
      "                num_groups, groups_per_block, (float)eps, (float)min_8bit, \\",
      "                (float)max_8bit);                                          \\",
      "      }                                                                    \\",
      "    }                                                                      \\",
      "  } while (0)",
      "",
      "  VLLM_DISPATCH_FLOATING_TYPES(",
      "      input.scalar_type(), \"per_token_group_quant_8bit\", ([&] {",
      "        if (dst_type == at::ScalarType::Float8_e4m3fn) {",
      "          LAUNCH_KERNEL(scalar_t, __nv_fp8_e4m3);",
      "        } else if (dst_type == at::ScalarType::Char) {",
      "          LAUNCH_KERNEL(scalar_t, int8_t);",
      "        }",
      "      }));",
      "",
      "#undef LAUNCH_KERNEL",
      "}",
      "",
      "void per_token_group_quant_fp8(const torch::Tensor& input,",
      "                               torch::Tensor& output_q, torch::Tensor& output_s,",
      "                               int64_t group_size, double eps, double fp8_min,",
      "                               double fp8_max, bool scale_ue8m0) {",
      "  per_token_group_quant_8bit(input, output_q, output_s, group_size, eps,",
      "                             fp8_min, fp8_max, scale_ue8m0);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fp8/amd/quant_utils.cuh",
    "source": [
      "#pragma once",
      "#include <hip/hip_fp8.h>",
      "",
      "#include <hip/hip_fp16.h>",
      "#include <hip/hip_bf16.h>",
      "#include <hip/hip_bfloat16.h>",
      "",
      "#include \"../../../attention/attention_dtypes.h\"",
      "",
      "namespace vllm {",
      "#ifdef USE_ROCM",
      "",
      "namespace fp8 {",
      "  #ifdef ENABLE_FP8",
      "",
      "// Use hardware cvt instruction for fp8 on rocm",
      "template <typename fp8_type>",
      "__device__ __forceinline__ fp8_type cvt_c10(float const r) {",
      "  return {};",
      "}",
      "",
      "// __hip_fp8_e4m3 only exists starting in ROCm 6.3. The macro",
      "// HIP_FP8_TYPE_OCP comes from the hip_fp8.h header and also makes",
      "// its first appearance in ROCm 6.3. Since VLLM_DISPATCH_FP8_TYPES",
      "// on ROCm instantiates both OCP and FNUZ kernels, we need to replace",
      "// the new HW cvt with something reasonable that doesn't rely on the",
      "// ROCm 6.3 feature. This allows compiling on ROCm 6.2 or newer.",
      "template <>",
      "__device__ __forceinline__ c10::Float8_e4m3fn cvt_c10(float const r) {",
      "    #if HIP_FP8_TYPE_OCP",
      "  return c10::Float8_e4m3fn(",
      "      __hip_cvt_float_to_fp8(r, __hip_fp8_e4m3::__default_saturation,",
      "                             __hip_fp8_e4m3::__default_interpret),",
      "      c10::Float8_e4m3fn::from_bits());",
      "    #else",
      "  // Cast implemented by pytorch. Uses bit manipulation instead of HW cvt.",
      "  // HW cvt above is faster when it is available (ROCm 6.3 or newer).",
      "  return static_cast<c10::Float8_e4m3fn>(r);",
      "    #endif",
      "}",
      "",
      "template <>",
      "__device__ __forceinline__ c10::Float8_e4m3fnuz cvt_c10(float const r) {",
      "  return c10::Float8_e4m3fnuz(",
      "      __hip_cvt_float_to_fp8(r, __hip_fp8_e4m3_fnuz::__default_saturation,",
      "                             __hip_fp8_e4m3_fnuz::__default_interpret),",
      "      c10::Float8_e4m3fnuz::from_bits());",
      "}",
      "",
      "template <typename Tout, typename Tin>",
      "__inline__ __device__ Tout vec_conversion(const Tin& x) {",
      "  return x;",
      "}",
      "",
      "template <typename Tout, typename Tin>",
      "__inline__ __device__ Tout scaled_vec_conversion(const Tin& x,",
      "                                                 const float scale) {",
      "  return x;",
      "}",
      "",
      "    #if HIP_FP8_TYPE_OCP",
      "using fp8_type = __hip_fp8_e4m3;",
      "using fp8x2_type = __hip_fp8x2_e4m3;",
      "    #else",
      "using fp8_type = __hip_fp8_e4m3_fnuz;",
      "using fp8x2_type = __hip_fp8x2_e4m3_fnuz;",
      "    #endif",
      "",
      "// fp8 -> half",
      "template <>",
      "__inline__ __device__ uint16_t",
      "vec_conversion<uint16_t, uint8_t>(const uint8_t& a) {",
      "  return __hip_cvt_fp8_to_halfraw(a, fp8_type::__default_interpret).x;",
      "}",
      "",
      "// fp8x2 -> half2",
      "template <>",
      "__inline__ __device__ uint32_t",
      "vec_conversion<uint32_t, uint16_t>(const uint16_t& a) {",
      "  union {",
      "    __half2_raw h2r;",
      "    uint32_t ui32;",
      "  } tmp;",
      "  tmp.h2r = __hip_cvt_fp8x2_to_halfraw2(a, fp8_type::__default_interpret);",
      "  return tmp.ui32;",
      "}",
      "",
      "// fp8x4 -> half2x2",
      "template <>",
      "__inline__ __device__ uint2 vec_conversion<uint2, uint32_t>(const uint32_t& a) {",
      "  union {",
      "    uint2 u32x2;",
      "    uint32_t u32[2];",
      "  } tmp;",
      "  tmp.u32[0] = vec_conversion<uint32_t, uint16_t>((uint16_t)a);",
      "  tmp.u32[1] = vec_conversion<uint32_t, uint16_t>((uint16_t)(a >> 16U));",
      "  return tmp.u32x2;",
      "}",
      "",
      "// fp8x8 -> half2x4",
      "template <>",
      "__inline__ __device__ uint4 vec_conversion<uint4, uint2>(const uint2& a) {",
      "  union {",
      "    uint4 u64x2;",
      "    uint2 u64[2];",
      "  } tmp;",
      "  tmp.u64[0] = vec_conversion<uint2, uint32_t>(a.x);",
      "  tmp.u64[1] = vec_conversion<uint2, uint32_t>(a.y);",
      "  return tmp.u64x2;",
      "}",
      "",
      "using __nv_bfloat16 = __hip_bfloat16;",
      "",
      "// fp8 -> __nv_bfloat16",
      "template <>",
      "__inline__ __device__ __nv_bfloat16",
      "vec_conversion<__nv_bfloat16, uint8_t>(const uint8_t& a) {",
      "  fp8_type f8;",
      "  f8.__x = a;",
      "  return __float2bfloat16(static_cast<float>(f8));",
      "}",
      "",
      "using __nv_bfloat162 = __hip_bfloat162;",
      "",
      "// fp8x2 -> __nv_bfloat162",
      "template <>",
      "__inline__ __device__ __nv_bfloat162",
      "vec_conversion<__nv_bfloat162, uint16_t>(const uint16_t& a) {",
      "  __nv_bfloat162 res;",
      "  res.x = vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)a);",
      "  res.y = vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)(a >> 8U));",
      "  return res;",
      "}",
      "",
      "// fp8x4 -> bf16_4_t",
      "template <>",
      "__inline__ __device__ bf16_4_t",
      "vec_conversion<bf16_4_t, uint32_t>(const uint32_t& a) {",
      "  bf16_4_t res;",
      "  res.x = vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)a);",
      "  res.y = vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)(a >> 16U));",
      "  return res;",
      "}",
      "",
      "// fp8x8 -> bf16_8_t",
      "template <>",
      "__inline__ __device__ bf16_8_t vec_conversion<bf16_8_t, uint2>(const uint2& a) {",
      "  bf16_4_t tmp1, tmp2;",
      "  tmp1 = vec_conversion<bf16_4_t, uint32_t>(a.x);",
      "  tmp2 = vec_conversion<bf16_4_t, uint32_t>(a.y);",
      "  bf16_8_t res;",
      "  res.x = tmp1.x;",
      "  res.y = tmp1.y;",
      "  res.z = tmp2.x;",
      "  res.w = tmp2.y;",
      "  return res;",
      "}",
      "",
      "// fp8 -> float",
      "template <>",
      "__inline__ __device__ float vec_conversion<float, uint8_t>(const uint8_t& a) {",
      "  fp8_type f8;",
      "  f8.__x = a;",
      "  return static_cast<float>(f8);",
      "}",
      "",
      "// fp8x2 -> float2",
      "template <>",
      "__inline__ __device__ float2",
      "vec_conversion<float2, uint16_t>(const uint16_t& a) {",
      "  fp8x2_type f8x2;",
      "  f8x2.__x = a;",
      "  return static_cast<float2>(f8x2);",
      "}",
      "",
      "// fp8x4 -> float4",
      "template <>",
      "__inline__ __device__ Float4_",
      "vec_conversion<Float4_, uint32_t>(const uint32_t& a) {",
      "  Float4_ res;",
      "  res.x = vec_conversion<float2, uint16_t>((uint16_t)a);",
      "  res.y = vec_conversion<float2, uint16_t>((uint16_t)(a >> 16U));",
      "  return res;",
      "}",
      "",
      "// fp8x4 -> float4",
      "template <>",
      "__inline__ __device__ float4",
      "vec_conversion<float4, uint32_t>(const uint32_t& a) {",
      "  Float4_ tmp = vec_conversion<Float4_, uint32_t>(a);",
      "  float4 res = make_float4(tmp.x.x, tmp.x.y, tmp.y.x, tmp.y.y);",
      "  return res;",
      "}",
      "",
      "// fp8x8 -> float8",
      "template <>",
      "__inline__ __device__ Float8_ vec_conversion<Float8_, uint2>(const uint2& a) {",
      "  Float4_ tmp1, tmp2;",
      "  tmp1 = vec_conversion<Float4_, uint32_t>(a.x);",
      "  tmp2 = vec_conversion<Float4_, uint32_t>(a.y);",
      "  Float8_ res;",
      "  res.x = tmp1.x;",
      "  res.y = tmp1.y;",
      "  res.z = tmp2.x;",
      "  res.w = tmp2.y;",
      "  return res;",
      "}",
      "",
      "// half -> fp8",
      "template <>",
      "__inline__ __device__ uint8_t",
      "vec_conversion<uint8_t, uint16_t>(const uint16_t& a) {",
      "  __half_raw tmp;",
      "  tmp.x = a;",
      "  return __hip_cvt_halfraw_to_fp8(tmp, fp8_type::__default_saturation,",
      "                                  fp8_type::__default_interpret);",
      "}",
      "",
      "template <>",
      "__inline__ __device__ uint16_t",
      "vec_conversion<uint16_t, uint32_t>(const uint32_t& a) {",
      "  union {",
      "    uint32_t ui32;",
      "    __half2_raw h2r;",
      "  } tmp;",
      "  tmp.ui32 = a;",
      "  return __hip_cvt_halfraw2_to_fp8x2(tmp.h2r, fp8_type::__default_saturation,",
      "                                     fp8_type::__default_interpret);",
      "}",
      "",
      "// bf16 -> fp8",
      "template <>",
      "__inline__ __device__ uint8_t",
      "vec_conversion<uint8_t, __nv_bfloat16>(const __nv_bfloat16& a) {",
      "  return __hip_cvt_float_to_fp8(__bfloat162float(a),",
      "                                fp8_type::__default_saturation,",
      "                                fp8_type::__default_interpret);",
      "}",
      "",
      "// float -> fp8",
      "template <>",
      "__inline__ __device__ uint8_t vec_conversion<uint8_t, float>(const float& a) {",
      "  return __hip_cvt_float_to_fp8(a, fp8_type::__default_saturation,",
      "                                fp8_type::__default_interpret);",
      "}",
      "",
      "// float2 -> half2",
      "template <>",
      "__inline__ __device__ uint32_t",
      "vec_conversion<uint32_t, float2>(const float2& a) {",
      "  union {",
      "    half2 float16;",
      "    uint32_t uint32;",
      "  };",
      "",
      "  float16 = __float22half2_rn(a);",
      "  return uint32;",
      "}",
      "",
      "// Float4 -> half2x2",
      "template <>",
      "__inline__ __device__ uint2 vec_conversion<uint2, Float4_>(const Float4_& a) {",
      "  uint2 b;",
      "  float2 val;",
      "  val.x = a.x.x;",
      "  val.y = a.x.y;",
      "  b.x = vec_conversion<uint32_t, float2>(val);",
      "",
      "  val.x = a.y.x;",
      "  val.y = a.y.y;",
      "  b.y = vec_conversion<uint32_t, float2>(val);",
      "  return b;",
      "}",
      "",
      "// Float4 -> float4",
      "template <>",
      "__inline__ __device__ float4 vec_conversion<float4, Float4_>(const Float4_& a) {",
      "  float4 b;",
      "  b.x = a.x.x;",
      "  b.y = a.x.y;",
      "  b.z = a.y.x;",
      "  b.w = a.y.y;",
      "  return b;",
      "}",
      "",
      "// Float8 -> half2x4",
      "template <>",
      "__inline__ __device__ uint4 vec_conversion<uint4, Float8_>(const Float8_& a) {",
      "  uint4 b;",
      "  b.x = vec_conversion<uint32_t, float2>(a.x);",
      "  b.y = vec_conversion<uint32_t, float2>(a.y);",
      "  b.z = vec_conversion<uint32_t, float2>(a.z);",
      "  b.w = vec_conversion<uint32_t, float2>(a.w);",
      "  return b;",
      "}",
      "",
      "// float2 -> bfloat162",
      "template <>",
      "__inline__ __device__ __nv_bfloat162",
      "vec_conversion<__nv_bfloat162, float2>(const float2& a) {",
      "  __nv_bfloat162 b = __float22bfloat162_rn(a);",
      "  return b;",
      "}",
      "",
      "// Float4 -> bfloat162x2",
      "template <>",
      "__inline__ __device__ bf16_4_t",
      "vec_conversion<bf16_4_t, Float4_>(const Float4_& a) {",
      "  bf16_4_t b;",
      "  b.x = __float22bfloat162_rn(a.x);",
      "  b.y = __float22bfloat162_rn(a.y);",
      "  return b;",
      "}",
      "",
      "// Float8 -> bfloat162x4",
      "template <>",
      "__inline__ __device__ bf16_8_t",
      "vec_conversion<bf16_8_t, Float8_>(const Float8_& a) {",
      "  bf16_8_t b;",
      "  b.x = __float22bfloat162_rn(a.x);",
      "  b.y = __float22bfloat162_rn(a.y);",
      "  b.z = __float22bfloat162_rn(a.z);",
      "  b.w = __float22bfloat162_rn(a.w);",
      "  return b;",
      "}",
      "",
      "/* Scaled and vectorized conversions, for data exchange between high and low",
      "   precision domains",
      "",
      "   Convention of the scale in API, e.g: FP8_data = Quantization(",
      "   High_Precision_data / scale ) s.t. Quantize(HP / scale) => FP8 Dequant(FP8) *",
      "   scale =>  HP",
      "",
      " */",
      "",
      "using __nv_bfloat16 = __hip_bfloat16;",
      "",
      "// fp8 -> __nv_bfloat16",
      "template <>",
      "__inline__ __device__ __nv_bfloat16",
      "scaled_vec_conversion<__nv_bfloat16, uint8_t>(const uint8_t& a, float scale) {",
      "  fp8_type f8;",
      "  f8.__x = a;",
      "  return __float2bfloat16(static_cast<float>(f8) * scale);",
      "}",
      "",
      "// fp8x2 -> __nv_bfloat162",
      "template <>",
      "__inline__ __device__ __nv_bfloat162",
      "scaled_vec_conversion<__nv_bfloat162, uint16_t>(const uint16_t& a,",
      "                                                float scale) {",
      "  __nv_bfloat162 res;",
      "  res.x = scaled_vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)a, scale);",
      "  res.y =",
      "      scaled_vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)(a >> 8U), scale);",
      "  return res;",
      "}",
      "",
      "// fp8x4 -> bf16_4_t",
      "template <>",
      "__inline__ __device__ bf16_4_t",
      "scaled_vec_conversion<bf16_4_t, uint32_t>(const uint32_t& a, float scale) {",
      "  bf16_4_t res;",
      "  res.x = scaled_vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)a, scale);",
      "  res.y = scaled_vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)(a >> 16U),",
      "                                                          scale);",
      "  return res;",
      "}",
      "",
      "// fp8x8 -> bf16_8_t",
      "template <>",
      "__inline__ __device__ bf16_8_t",
      "scaled_vec_conversion<bf16_8_t, uint2>(const uint2& a, float scale) {",
      "  bf16_4_t tmp1, tmp2;",
      "  tmp1 = scaled_vec_conversion<bf16_4_t, uint32_t>(a.x, scale);",
      "  tmp2 = scaled_vec_conversion<bf16_4_t, uint32_t>(a.y, scale);",
      "  bf16_8_t res;",
      "  res.x = tmp1.x;",
      "  res.y = tmp1.y;",
      "  res.z = tmp2.x;",
      "  res.w = tmp2.y;",
      "  return res;",
      "}",
      "",
      "// fp8 -> float",
      "template <>",
      "__inline__ __device__ float scaled_vec_conversion<float, uint8_t>(",
      "    const uint8_t& a, float scale) {",
      "  fp8_type f8;",
      "  f8.__x = a;",
      "  return static_cast<float>(f8) * scale;",
      "}",
      "",
      "// fp8x2 -> float2",
      "template <>",
      "__inline__ __device__ float2",
      "scaled_vec_conversion<float2, uint16_t>(const uint16_t& a, float scale) {",
      "  fp8x2_type f8x2;",
      "  f8x2.__x = a;",
      "  return static_cast<float2>(f8x2) * scale;",
      "}",
      "",
      "// fp8x4 -> float4",
      "template <>",
      "__inline__ __device__ Float4_",
      "scaled_vec_conversion<Float4_, uint32_t>(const uint32_t& a, const float scale) {",
      "  Float4_ res;",
      "  res.x = scaled_vec_conversion<float2, uint16_t>((uint16_t)a, scale);",
      "  res.y = scaled_vec_conversion<float2, uint16_t>((uint16_t)(a >> 16U), scale);",
      "  return res;",
      "}",
      "",
      "// fp8x4 -> float4",
      "template <>",
      "__inline__ __device__ float4",
      "scaled_vec_conversion<float4, uint32_t>(const uint32_t& a, float scale) {",
      "  Float4_ res = scaled_vec_conversion<Float4_, uint32_t>(a, scale);",
      "  return {res.x.x, res.x.y, res.y.x, res.y.y};",
      "}",
      "",
      "// fp8x8 -> float8",
      "template <>",
      "__inline__ __device__ Float8_",
      "scaled_vec_conversion<Float8_, uint2>(const uint2& a, float scale) {",
      "  Float4_ tmp1, tmp2;",
      "  tmp1 = scaled_vec_conversion<Float4_, uint32_t>(a.x, scale);",
      "  tmp2 = scaled_vec_conversion<Float4_, uint32_t>(a.y, scale);",
      "  Float8_ res;",
      "  res.x = tmp1.x;",
      "  res.y = tmp1.y;",
      "  res.z = tmp2.x;",
      "  res.w = tmp2.y;",
      "  return res;",
      "}",
      "",
      "// fp8 -> half",
      "template <>",
      "__inline__ __device__ uint16_t",
      "scaled_vec_conversion<uint16_t, uint8_t>(const uint8_t& a, float scale) {",
      "  __half_raw res;",
      "  res.data = scaled_vec_conversion<float, uint8_t>(a, scale);",
      "  return res.x;",
      "}",
      "",
      "// fp8x2 -> half2",
      "template <>",
      "__inline__ __device__ uint32_t",
      "scaled_vec_conversion<uint32_t, uint16_t>(const uint16_t& a, float scale) {",
      "  union {",
      "    __half2_raw h2r;",
      "    uint32_t ui32;",
      "  } tmp;",
      "  tmp.h2r = __hip_cvt_fp8x2_to_halfraw2(a, fp8_type::__default_interpret);",
      "  tmp.h2r.x.data *= scale;",
      "  tmp.h2r.y.data *= scale;",
      "  return tmp.ui32;",
      "}",
      "",
      "// fp8x4 -> half2x2",
      "template <>",
      "__inline__ __device__ uint2",
      "scaled_vec_conversion<uint2, uint32_t>(const uint32_t& a, float scale) {",
      "  union {",
      "    uint2 u32x2;",
      "    uint32_t u32[2];",
      "  } tmp;",
      "  tmp.u32[0] = scaled_vec_conversion<uint32_t, uint16_t>((uint16_t)a, scale);",
      "  tmp.u32[1] =",
      "      scaled_vec_conversion<uint32_t, uint16_t>((uint16_t)(a >> 16U), scale);",
      "  return tmp.u32x2;",
      "}",
      "",
      "// fp8x8 -> half2x4",
      "template <>",
      "__inline__ __device__ uint4 scaled_vec_conversion<uint4, uint2>(const uint2& a,",
      "                                                                float scale) {",
      "  union {",
      "    uint4 u64x2;",
      "    uint2 u64[2];",
      "  } tmp;",
      "  tmp.u64[0] = scaled_vec_conversion<uint2, uint32_t>(a.x, scale);",
      "  tmp.u64[1] = scaled_vec_conversion<uint2, uint32_t>(a.y, scale);",
      "  return tmp.u64x2;",
      "}",
      "",
      "// half -> fp8",
      "template <>",
      "__inline__ __device__ uint8_t",
      "scaled_vec_conversion<uint8_t, uint16_t>(const uint16_t& a, float scale) {",
      "  __half_raw tmp;",
      "  tmp.x = a;",
      "  tmp.data /= scale;",
      "  return __hip_cvt_halfraw_to_fp8(tmp, fp8_type::__default_saturation,",
      "                                  fp8_type::__default_interpret);",
      "}",
      "",
      "// halfx2 -> fp8x2",
      "template <>",
      "__inline__ __device__ uint16_t",
      "scaled_vec_conversion<uint16_t, uint32_t>(const uint32_t& a, float scale) {",
      "  union {",
      "    uint32_t ui32;",
      "    __half2_raw h2r;",
      "  } tmp;",
      "  tmp.ui32 = a;",
      "  tmp.h2r.x.data /= scale;",
      "  tmp.h2r.y.data /= scale;",
      "  return __hip_cvt_halfraw2_to_fp8x2(tmp.h2r, fp8_type::__default_saturation,",
      "                                     fp8_type::__default_interpret);",
      "}",
      "",
      "// half2x2 -> fp8x4",
      "template <>",
      "__inline__ __device__ uint32_t",
      "scaled_vec_conversion<uint32_t, uint2>(const uint2& a, float scale) {",
      "  union {",
      "    uint16_t ui16[2];",
      "    uint32_t ui32;",
      "  } tmp;",
      "  tmp.ui16[0] = scaled_vec_conversion<uint16_t, uint32_t>(a.x, scale);",
      "  tmp.ui16[1] = scaled_vec_conversion<uint16_t, uint32_t>(a.y, scale);",
      "  return tmp.ui32;",
      "}",
      "",
      "// half2x4 -> fp8x8",
      "template <>",
      "__inline__ __device__ uint2 scaled_vec_conversion<uint2, uint4>(const uint4& a,",
      "                                                                float scale) {",
      "  union {",
      "    uint2 ui2[2];",
      "    uint4 ui4;",
      "  } tmp;",
      "  tmp.ui4 = a;",
      "  uint2 res;",
      "  res.x = scaled_vec_conversion<uint32_t, uint2>(tmp.ui2[0], scale);",
      "  res.y = scaled_vec_conversion<uint32_t, uint2>(tmp.ui2[1], scale);",
      "  return res;",
      "}",
      "",
      "// bf16 -> fp8",
      "template <>",
      "__inline__ __device__ uint8_t scaled_vec_conversion<uint8_t, __nv_bfloat16>(",
      "    const __nv_bfloat16& a, float scale) {",
      "  return __hip_cvt_float_to_fp8(__bfloat162float(a) / scale,",
      "                                fp8_type::__default_saturation,",
      "                                fp8_type::__default_interpret);",
      "}",
      "",
      "// bf16x2 -> fp8x2",
      "template <>",
      "__inline__ __device__ uint16_t scaled_vec_conversion<uint16_t, __nv_bfloat162>(",
      "    const __nv_bfloat162& a, float scale) {",
      "  union {",
      "    uint8_t ui8[2];",
      "    uint16_t ui16;",
      "  } tmp;",
      "  tmp.ui8[0] = scaled_vec_conversion<uint8_t, __nv_bfloat16>(a.x, scale);",
      "  tmp.ui8[1] = scaled_vec_conversion<uint8_t, __nv_bfloat16>(a.y, scale);",
      "  return tmp.ui16;",
      "}",
      "",
      "// bf16x4 -> fp8x4",
      "template <>",
      "__inline__ __device__ uint32_t",
      "scaled_vec_conversion<uint32_t, bf16_4_t>(const bf16_4_t& a, float scale) {",
      "  union {",
      "    uint16_t ui16[2];",
      "    uint32_t ui32;",
      "  } tmp;",
      "  tmp.ui16[0] = scaled_vec_conversion<uint16_t, __nv_bfloat162>(a.x, scale);",
      "  tmp.ui16[1] = scaled_vec_conversion<uint16_t, __nv_bfloat162>(a.y, scale);",
      "  return tmp.ui32;",
      "}",
      "",
      "// bf16x8 -> fp8x8",
      "template <>",
      "__inline__ __device__ uint2",
      "scaled_vec_conversion<uint2, bf16_8_t>(const bf16_8_t& a, float scale) {",
      "  uint2 res;",
      "  res.x = scaled_vec_conversion<uint32_t, bf16_4_t>({a.x, a.y}, scale);",
      "  res.y = scaled_vec_conversion<uint32_t, bf16_4_t>({a.z, a.w}, scale);",
      "  return res;",
      "}",
      "",
      "// float -> fp8",
      "template <>",
      "__inline__ __device__ uint8_t",
      "scaled_vec_conversion<uint8_t, float>(const float& a, float scale) {",
      "  return __hip_cvt_float_to_fp8(a / scale, fp8_type::__default_saturation,",
      "                                fp8_type::__default_interpret);",
      "}",
      "",
      "// floatx2 -> fp8x2",
      "template <>",
      "__inline__ __device__ uint16_t",
      "scaled_vec_conversion<uint16_t, float2>(const float2& a, float scale) {",
      "  return __hip_cvt_float2_to_fp8x2(a / scale, fp8_type::__default_saturation,",
      "                                   fp8_type::__default_interpret);",
      "}",
      "",
      "// floatx4 -> fp8x4",
      "template <>",
      "__inline__ __device__ uint32_t",
      "scaled_vec_conversion<uint32_t, float4>(const float4& a, float scale) {",
      "  union {",
      "    uint16_t ui16[2];",
      "    uint32_t ui32;",
      "  } tmp;",
      "  tmp.ui16[0] = scaled_vec_conversion<uint16_t, float2>({a.x, a.y}, scale);",
      "  tmp.ui16[1] = scaled_vec_conversion<uint16_t, float2>({a.z, a.w}, scale);",
      "  return tmp.ui32;",
      "}",
      "  #endif  // ENABLE_FP8",
      "",
      "template <typename Tout, typename Tin, Fp8KVCacheDataType kv_dt>",
      "__inline__ __device__ Tout convert(const Tin& x) {",
      "  #ifdef ENABLE_FP8",
      "  if constexpr (kv_dt == Fp8KVCacheDataType::kFp8E4M3) {",
      "    return vec_conversion<Tout, Tin>(x);",
      "  }",
      "  #endif",
      "  assert(false);",
      "  return {};  // Squash missing return statement warning",
      "}",
      "",
      "template <typename Tout, typename Tin, Fp8KVCacheDataType kv_dt>",
      "__inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {",
      "  #ifdef ENABLE_FP8",
      "  if constexpr (kv_dt == Fp8KVCacheDataType::kFp8E4M3) {",
      "    return scaled_vec_conversion<Tout, Tin>(x, scale);",
      "  }",
      "  #endif",
      "  assert(false);",
      "  return {};  // Squash missing return statement warning",
      "}",
      "",
      "  // The following macro is used to dispatch the conversion function based on",
      "  // the data type of the key and value cache. The FN is a macro that calls a",
      "  // function with template<typename scalar_t, typename cache_t,",
      "  // Fp8KVCacheDataType kv_dt>.",
      "  #define DISPATCH_BY_KV_CACHE_DTYPE(SRC_DTYPE, KV_DTYPE, FN)                  \\",
      "    if (KV_DTYPE == \"auto\") {                                                  \\",
      "      if (SRC_DTYPE == at::ScalarType::Float) {                                \\",
      "        FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\",
      "      } else if (SRC_DTYPE == at::ScalarType::Half) {                          \\",
      "        FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\",
      "      } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                      \\",
      "        FN(__nv_bfloat16, __nv_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\",
      "      } else {                                                                 \\",
      "        TORCH_CHECK(false, \"Unsupported input type of kv cache: \", SRC_DTYPE); \\",
      "      }                                                                        \\",
      "    } else {                                                                   \\",
      "      if (KV_DTYPE == \"fp8\" || KV_DTYPE == \"fp8_e4m3\") {                       \\",
      "        if (SRC_DTYPE == at::ScalarType::Float) {                              \\",
      "          FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\",
      "        } else if (SRC_DTYPE == at::ScalarType::Half) {                        \\",
      "          FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\",
      "        } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                    \\",
      "          FN(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\",
      "        } else {                                                               \\",
      "          TORCH_CHECK(false,                                                   \\",
      "                      \"Unsupported input type of kv cache: \", SRC_DTYPE);      \\",
      "        }                                                                      \\",
      "      } else {                                                                 \\",
      "        TORCH_CHECK(false, \"Unsupported data type of kv cache: \", KV_DTYPE);   \\",
      "      }                                                                        \\",
      "    }",
      "",
      "}  // namespace fp8",
      "#endif  // USE_ROCM",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fp8/nvidia/quant_utils.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"../../../attention/attention_dtypes.h\"",
      "#include <assert.h>",
      "#include <float.h>",
      "#include <stdint.h>",
      "#include <type_traits>",
      "",
      "namespace vllm {",
      "#ifndef USE_ROCM",
      "",
      "namespace fp8 {",
      "  #ifdef ENABLE_FP8",
      "",
      "    #if 0  // Disable the following code to reduce the binary size.",
      "template <typename Tout, typename Tin>",
      "__inline__ __device__ Tout",
      "vec_conversion(const Tin &x, const __nv_fp8_interpretation_t fp8_type) {",
      "  return x;",
      "}",
      "",
      "// fp8 -> half",
      "template <>",
      "__inline__ __device__ uint16_t vec_conversion<uint16_t, uint8_t>(",
      "    const uint8_t &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  __half_raw res = __nv_cvt_fp8_to_halfraw(a, fp8_type);",
      "  return res.x;",
      "}",
      "",
      "// fp8x2 -> half2",
      "template <>",
      "__inline__ __device__ uint32_t vec_conversion<uint32_t, uint16_t>(",
      "    const uint16_t &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  union {",
      "    uint16_t u16[2];",
      "    uint32_t u32;",
      "  } tmp;",
      "  __half2_raw res = __nv_cvt_fp8x2_to_halfraw2(a, fp8_type);",
      "  tmp.u16[0] = res.x;",
      "  tmp.u16[1] = res.y;",
      "  return tmp.u32;",
      "}",
      "",
      "// fp8x4 -> half2x2",
      "template <>",
      "__inline__ __device__ uint2 vec_conversion<uint2, uint32_t>(",
      "    const uint32_t &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  union {",
      "    uint2 u32x2;",
      "    uint32_t u32[2];",
      "  } tmp;",
      "  tmp.u32[0] = vec_conversion<uint32_t, uint16_t>((uint16_t)a, fp8_type);",
      "  tmp.u32[1] =",
      "      vec_conversion<uint32_t, uint16_t>((uint16_t)(a >> 16U), fp8_type);",
      "  return tmp.u32x2;",
      "}",
      "",
      "// fp8x8 -> half2x4",
      "template <>",
      "__inline__ __device__ uint4 vec_conversion<uint4, uint2>(",
      "    const uint2 &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  union {",
      "    uint4 u64x2;",
      "    uint2 u64[2];",
      "  } tmp;",
      "  tmp.u64[0] = vec_conversion<uint2, uint32_t>(a.x, fp8_type);",
      "  tmp.u64[1] = vec_conversion<uint2, uint32_t>(a.y, fp8_type);",
      "  return tmp.u64x2;",
      "}",
      "",
      "// fp8 -> __nv_bfloat16",
      "template <>",
      "__inline__ __device__ __nv_bfloat16 vec_conversion<__nv_bfloat16, uint8_t>(",
      "    const uint8_t &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  // Note there is no direct convert function from fp8 to bf16.",
      "  // fp8 -> half",
      "  __half_raw res = __nv_cvt_fp8_to_halfraw(a, fp8_type);",
      "  // half -> float -> bf16",
      "  float tmp = half_to_float(res.x);",
      "  return __float2bfloat16(tmp);",
      "}",
      "",
      "// fp8x2 -> __nv_bfloat162",
      "template <>",
      "__inline__ __device__ __nv_bfloat162 vec_conversion<__nv_bfloat162, uint16_t>(",
      "    const uint16_t &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  __nv_bfloat162 res;",
      "  res.x = vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)a, fp8_type);",
      "  res.y = vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)(a >> 8U), fp8_type);",
      "  return res;",
      "}",
      "",
      "// fp8x4 -> bf16_4_t",
      "template <>",
      "__inline__ __device__ bf16_4_t vec_conversion<bf16_4_t, uint32_t>(",
      "    const uint32_t &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  bf16_4_t res;",
      "  res.x = vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)a, fp8_type);",
      "  res.y =",
      "      vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)(a >> 16U), fp8_type);",
      "  return res;",
      "}",
      "",
      "// fp8x8 -> bf16_8_t",
      "template <>",
      "__inline__ __device__ bf16_8_t vec_conversion<bf16_8_t, uint2>(",
      "    const uint2 &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  bf16_4_t tmp1, tmp2;",
      "  tmp1 = vec_conversion<bf16_4_t, uint32_t>(a.x, fp8_type);",
      "  tmp2 = vec_conversion<bf16_4_t, uint32_t>(a.y, fp8_type);",
      "  bf16_8_t res;",
      "  res.x = tmp1.x;",
      "  res.y = tmp1.y;",
      "  res.z = tmp2.x;",
      "  res.w = tmp2.y;",
      "  return res;",
      "}",
      "",
      "// fp8 -> float",
      "template <>",
      "__inline__ __device__ float",
      "vec_conversion<float, uint8_t>(const uint8_t &a,",
      "                               const __nv_fp8_interpretation_t fp8_type) {",
      "  // fp8 -> half",
      "  uint16_t tmp = vec_conversion<uint16_t, uint8_t>(a, fp8_type);",
      "  // half -> float",
      "  return half_to_float(tmp);",
      "}",
      "",
      "// fp8x2 -> float2",
      "template <>",
      "__inline__ __device__ float2 vec_conversion<float2, uint16_t>(",
      "    const uint16_t &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  // fp8x2 -> half2",
      "  uint32_t tmp = vec_conversion<uint32_t, uint16_t>(a, fp8_type);",
      "  // half2 -> float2",
      "  return half2_to_float2(tmp);",
      "}",
      "",
      "// fp8x4 -> float4",
      "template <>",
      "__inline__ __device__ Float4_ vec_conversion<Float4_, uint32_t>(",
      "    const uint32_t &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  Float4_ res;",
      "  res.x = vec_conversion<float2, uint16_t>((uint16_t)a, fp8_type);",
      "  res.y = vec_conversion<float2, uint16_t>((uint16_t)(a >> 16U), fp8_type);",
      "  return res;",
      "}",
      "",
      "// fp8x8 -> float8",
      "template <>",
      "__inline__ __device__ Float8_ vec_conversion<Float8_, uint2>(",
      "    const uint2 &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  Float4_ tmp1, tmp2;",
      "  tmp1 = vec_conversion<Float4_, uint32_t>(a.x, fp8_type);",
      "  tmp2 = vec_conversion<Float4_, uint32_t>(a.y, fp8_type);",
      "  Float8_ res;",
      "  res.x = tmp1.x;",
      "  res.y = tmp1.y;",
      "  res.z = tmp2.x;",
      "  res.w = tmp2.y;",
      "  return res;",
      "}",
      "",
      "// half -> fp8",
      "template <>",
      "__inline__ __device__ uint8_t vec_conversion<uint8_t, uint16_t>(",
      "    const uint16_t &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  __half_raw tmp;",
      "  tmp.x = a;",
      "  __nv_fp8_storage_t res =",
      "      __nv_cvt_halfraw_to_fp8(tmp, __NV_SATFINITE, fp8_type);",
      "  return (uint8_t)res;",
      "}",
      "",
      "// bf16 -> fp8",
      "template <>",
      "__inline__ __device__ uint8_t vec_conversion<uint8_t, __nv_bfloat16>(",
      "    const __nv_bfloat16 &a, const __nv_fp8_interpretation_t fp8_type) {",
      "      #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "  assert(false);",
      "      #else",
      "  __nv_fp8_storage_t res = __nv_cvt_bfloat16raw_to_fp8(",
      "      __nv_bfloat16_raw(a), __NV_SATFINITE, fp8_type);",
      "  return (uint8_t)res;",
      "      #endif",
      "}",
      "",
      "// float -> fp8",
      "template <>",
      "__inline__ __device__ uint8_t vec_conversion<uint8_t, float>(",
      "    const float &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  __nv_fp8_storage_t res = __nv_cvt_float_to_fp8(a, __NV_SATFINITE, fp8_type);",
      "  return (uint8_t)res;",
      "}",
      "",
      "// fp8x4 -> float4",
      "template <>",
      "__inline__ __device__ float4 vec_conversion<float4, uint32_t>(",
      "    const uint32_t &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  Float4_ tmp = vec_conversion<Float4_, uint32_t>(a, fp8_type);",
      "  float4 res = make_float4(tmp.x.x, tmp.x.y, tmp.y.x, tmp.y.y);",
      "  return res;",
      "}",
      "",
      "template <>",
      "__inline__ __device__ uint32_t vec_conversion<uint32_t, float2>(",
      "    const float2 &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  union {",
      "    half2 float16;",
      "    uint32_t uint32;",
      "  };",
      "",
      "  float16 = __float22half2_rn(a);",
      "  return uint32;",
      "}",
      "",
      "template <>",
      "__inline__ __device__ uint2 vec_conversion<uint2, Float4_>(",
      "    const Float4_ &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  uint2 b;",
      "  float2 val;",
      "  val.x = a.x.x;",
      "  val.y = a.x.y;",
      "  b.x = vec_conversion<uint32_t, float2>(val, fp8_type);",
      "",
      "  val.x = a.y.x;",
      "  val.y = a.y.y;",
      "  b.y = vec_conversion<uint32_t, float2>(val, fp8_type);",
      "",
      "  return b;",
      "}",
      "",
      "template <>",
      "__inline__ __device__ float4 vec_conversion<float4, Float4_>(",
      "    const Float4_ &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  float4 b;",
      "  b.x = a.x.x;",
      "  b.y = a.x.y;",
      "  b.z = a.y.x;",
      "  b.w = a.y.y;",
      "  return b;",
      "}",
      "",
      "template <>",
      "__inline__ __device__ uint4 vec_conversion<uint4, Float8_>(",
      "    const Float8_ &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  uint4 b;",
      "  b.x = vec_conversion<uint32_t, float2>(a.x, fp8_type);",
      "  b.y = vec_conversion<uint32_t, float2>(a.y, fp8_type);",
      "  b.z = vec_conversion<uint32_t, float2>(a.z, fp8_type);",
      "  b.w = vec_conversion<uint32_t, float2>(a.w, fp8_type);",
      "  return b;",
      "}",
      "",
      "template <>",
      "__inline__ __device__ __nv_bfloat162 vec_conversion<__nv_bfloat162, float2>(",
      "    const float2 &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  __nv_bfloat162 b;",
      "  from_float(b, a);",
      "  return b;",
      "}",
      "",
      "template <>",
      "__inline__ __device__ bf16_4_t vec_conversion<bf16_4_t, Float4_>(",
      "    const Float4_ &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  bf16_4_t b;",
      "  from_float(b, a);",
      "  return b;",
      "}",
      "",
      "template <>",
      "__inline__ __device__ bf16_8_t vec_conversion<bf16_8_t, Float8_>(",
      "    const Float8_ &a, const __nv_fp8_interpretation_t fp8_type) {",
      "  bf16_8_t b;",
      "  from_float(b, a);",
      "  return b;",
      "}",
      "    #endif",
      "",
      "/* Scaled and vectorized conversions, for data exchange between high and low",
      "   precision domains Convention of the scale in API, e.g: FP8_data =",
      "   Quantization( High_Precision_data / scale ) s.t. Quantize(HP / scale) => FP8",
      "     Dequant(FP8) * scale =>  HP",
      " */",
      "",
      "template <typename Tout, typename Tin>",
      "__inline__ __device__ Tout scaled_vec_conversion(",
      "    const Tin& x, const float scale, const __nv_fp8_interpretation_t fp8_type) {",
      "  return x;",
      "}",
      "",
      "// fp8 -> half",
      "template <>",
      "__inline__ __device__ uint16_t scaled_vec_conversion<uint16_t, uint8_t>(",
      "    const uint8_t& a, const float scale,",
      "    const __nv_fp8_interpretation_t fp8_type) {",
      "  __half_raw tmp = __nv_cvt_fp8_to_halfraw(a, fp8_type);",
      "  return float_to_half(half_to_float(tmp.x) * scale);",
      "}",
      "",
      "// fp8x2 -> half2",
      "template <>",
      "__inline__ __device__ uint32_t scaled_vec_conversion<uint32_t, uint16_t>(",
      "    const uint16_t& a, const float scale,",
      "    const __nv_fp8_interpretation_t fp8_type) {",
      "  union {",
      "    uint16_t u16[2];",
      "    uint32_t u32;",
      "  } tmp;",
      "  __half2_raw res = __nv_cvt_fp8x2_to_halfraw2(a, fp8_type);",
      "  tmp.u16[0] = float_to_half(half_to_float(res.x) * scale);",
      "  tmp.u16[1] = float_to_half(half_to_float(res.y) * scale);",
      "  return tmp.u32;",
      "}",
      "",
      "// fp8x4 -> half2x2",
      "template <>",
      "__inline__ __device__ uint2 scaled_vec_conversion<uint2, uint32_t>(",
      "    const uint32_t& a, const float scale,",
      "    const __nv_fp8_interpretation_t fp8_type) {",
      "  union {",
      "    uint2 u32x2;",
      "    uint32_t u32[2];",
      "  } tmp;",
      "  tmp.u32[0] =",
      "      scaled_vec_conversion<uint32_t, uint16_t>((uint16_t)a, scale, fp8_type);",
      "  tmp.u32[1] = scaled_vec_conversion<uint32_t, uint16_t>((uint16_t)(a >> 16U),",
      "                                                         scale, fp8_type);",
      "  return tmp.u32x2;",
      "}",
      "",
      "// fp8x8 -> half2x4",
      "template <>",
      "__inline__ __device__ uint4",
      "scaled_vec_conversion<uint4, uint2>(const uint2& a, const float scale,",
      "                                    const __nv_fp8_interpretation_t fp8_type) {",
      "  union {",
      "    uint4 u64x2;",
      "    uint2 u64[2];",
      "  } tmp;",
      "  tmp.u64[0] = scaled_vec_conversion<uint2, uint32_t>(a.x, scale, fp8_type);",
      "  tmp.u64[1] = scaled_vec_conversion<uint2, uint32_t>(a.y, scale, fp8_type);",
      "  return tmp.u64x2;",
      "}",
      "",
      "// fp8 -> __nv_bfloat16",
      "template <>",
      "__inline__ __device__ __nv_bfloat16",
      "scaled_vec_conversion<__nv_bfloat16, uint8_t>(",
      "    const uint8_t& a, const float scale,",
      "    const __nv_fp8_interpretation_t fp8_type) {",
      "  // Note there is no direct convert function from fp8 to bf16.",
      "  // fp8 -> half",
      "  __half_raw res = __nv_cvt_fp8_to_halfraw(a, fp8_type);",
      "  // half -> float -> bf16",
      "  float tmp = half_to_float(res.x);",
      "  return __float2bfloat16(tmp * scale);",
      "}",
      "",
      "// fp8x2 -> __nv_bfloat162",
      "template <>",
      "__inline__ __device__ __nv_bfloat162",
      "scaled_vec_conversion<__nv_bfloat162, uint16_t>(",
      "    const uint16_t& a, const float scale,",
      "    const __nv_fp8_interpretation_t fp8_type) {",
      "  __nv_bfloat162 res;",
      "  res.x = scaled_vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)a, scale,",
      "                                                        fp8_type);",
      "  res.y = scaled_vec_conversion<__nv_bfloat16, uint8_t>((uint8_t)(a >> 8U),",
      "                                                        scale, fp8_type);",
      "  return res;",
      "}",
      "",
      "// fp8x4 -> bf16_4_t",
      "template <>",
      "__inline__ __device__ bf16_4_t scaled_vec_conversion<bf16_4_t, uint32_t>(",
      "    const uint32_t& a, const float scale,",
      "    const __nv_fp8_interpretation_t fp8_type) {",
      "  bf16_4_t res;",
      "  res.x = scaled_vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)a, scale,",
      "                                                          fp8_type);",
      "  res.y = scaled_vec_conversion<__nv_bfloat162, uint16_t>((uint16_t)(a >> 16U),",
      "                                                          scale, fp8_type);",
      "  return res;",
      "}",
      "",
      "// fp8x8 -> bf16_8_t",
      "template <>",
      "__inline__ __device__ bf16_8_t scaled_vec_conversion<bf16_8_t, uint2>(",
      "    const uint2& a, const float scale,",
      "    const __nv_fp8_interpretation_t fp8_type) {",
      "  bf16_4_t tmp1, tmp2;",
      "  tmp1 = scaled_vec_conversion<bf16_4_t, uint32_t>(a.x, scale, fp8_type);",
      "  tmp2 = scaled_vec_conversion<bf16_4_t, uint32_t>(a.y, scale, fp8_type);",
      "  bf16_8_t res;",
      "  res.x = tmp1.x;",
      "  res.y = tmp1.y;",
      "  res.z = tmp2.x;",
      "  res.w = tmp2.y;",
      "  return res;",
      "}",
      "",
      "// fp8 -> float",
      "template <>",
      "__inline__ __device__ float scaled_vec_conversion<float, uint8_t>(",
      "    const uint8_t& a, const float scale,",
      "    const __nv_fp8_interpretation_t fp8_type) {",
      "  // fp8 -> half",
      "  __half_raw res = __nv_cvt_fp8_to_halfraw(a, fp8_type);",
      "  uint16_t tmp = res.x;",
      "",
      "  // half -> float",
      "  return half_to_float(tmp) * scale;",
      "}",
      "",
      "// fp8x2 -> float2",
      "template <>",
      "__inline__ __device__ float2 scaled_vec_conversion<float2, uint16_t>(",
      "    const uint16_t& a, const float scale,",
      "    const __nv_fp8_interpretation_t fp8_type) {",
      "  // fp8x2 -> half2",
      "  uint32_t tmp = scaled_vec_conversion<uint32_t, uint16_t>(a, scale, fp8_type);",
      "  // half2 -> float2",
      "  return half2_to_float2(tmp);",
      "}",
      "",
      "// fp8x4 -> float4",
      "template <>",
      "__inline__ __device__ Float4_ scaled_vec_conversion<Float4_, uint32_t>(",
      "    const uint32_t& a, const float scale,",
      "    const __nv_fp8_interpretation_t fp8_type) {",
      "  Float4_ res;",
      "  res.x = scaled_vec_conversion<float2, uint16_t>((uint16_t)a, scale, fp8_type);",
      "  res.y = scaled_vec_conversion<float2, uint16_t>((uint16_t)(a >> 16U), scale,",
      "                                                  fp8_type);",
      "  return res;",
      "}",
      "",
      "// fp8x8 -> float8",
      "template <>",
      "__inline__ __device__ Float8_ scaled_vec_conversion<Float8_, uint2>(",
      "    const uint2& a, const float scale,",
      "    const __nv_fp8_interpretation_t fp8_type) {",
      "  Float4_ tmp1, tmp2;",
      "  tmp1 = scaled_vec_conversion<Float4_, uint32_t>(a.x, scale, fp8_type);",
      "  tmp2 = scaled_vec_conversion<Float4_, uint32_t>(a.y, scale, fp8_type);",
      "  Float8_ res;",
      "  res.x = tmp1.x;",
      "  res.y = tmp1.y;",
      "  res.z = tmp2.x;",
      "  res.w = tmp2.y;",
      "  return res;",
      "}",
      "",
      "// half -> fp8",
      "template <>",
      "__inline__ __device__ uint8_t scaled_vec_conversion<uint8_t, uint16_t>(",
      "    const uint16_t& a, const float scale,",
      "    const __nv_fp8_interpretation_t fp8_type) {",
      "  __nv_fp8_storage_t res =",
      "      __nv_cvt_float_to_fp8(half_to_float(a) / scale, __NV_SATFINITE, fp8_type);",
      "  return (uint8_t)res;",
      "}",
      "",
      "// bf16 -> fp8",
      "template <>",
      "__inline__ __device__ uint8_t scaled_vec_conversion<uint8_t, __nv_bfloat16>(",
      "    const __nv_bfloat16& a, const float scale,",
      "    const __nv_fp8_interpretation_t fp8_type) {",
      "    #if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "  assert(false);",
      "    #else",
      "  __nv_fp8_storage_t res = __nv_cvt_float_to_fp8(__bfloat162float(a) / scale,",
      "                                                 __NV_SATFINITE, fp8_type);",
      "  return (uint8_t)res;",
      "    #endif",
      "  __builtin_unreachable();  // Suppress missing return statement warning",
      "}",
      "",
      "// float -> fp8",
      "template <>",
      "__inline__ __device__ uint8_t scaled_vec_conversion<uint8_t, float>(",
      "    const float& a, const float scale,",
      "    const __nv_fp8_interpretation_t fp8_type) {",
      "  __nv_fp8_storage_t res =",
      "      __nv_cvt_float_to_fp8(a / scale, __NV_SATFINITE, fp8_type);",
      "  return (uint8_t)res;",
      "}",
      "",
      "// fp8x4 -> float4",
      "template <>",
      "__inline__ __device__ float4 scaled_vec_conversion<float4, uint32_t>(",
      "    const uint32_t& a, const float scale,",
      "    const __nv_fp8_interpretation_t fp8_type) {",
      "  Float4_ tmp = scaled_vec_conversion<Float4_, uint32_t>(a, scale, fp8_type);",
      "  float4 res = make_float4(tmp.x.x, tmp.x.y, tmp.y.x, tmp.y.y);",
      "  return res;",
      "}",
      "  #endif  // ENABLE_FP8",
      "",
      "template <typename Tout, typename Tin, Fp8KVCacheDataType kv_dt>",
      "__inline__ __device__ Tout convert(const Tin& x) {",
      "  #if 0  // Disable the following code to reduce the binary size.",
      "  if constexpr (kv_dt == Fp8KVCacheDataType::kFp8E4M3) {",
      "    return vec_conversion<Tout, Tin>(x, __NV_E4M3);",
      "  } else if constexpr (kv_dt == Fp8KVCacheDataType::kFp8E5M2) {",
      "    return vec_conversion<Tout, Tin>(x, __NV_E5M2);",
      "  }",
      "  #endif",
      "  assert(false);",
      "  __builtin_unreachable();  // Suppress missing return statement warning",
      "}",
      "",
      "template <typename Tout, typename Tin, Fp8KVCacheDataType kv_dt>",
      "__inline__ __device__ Tout scaled_convert(const Tin& x, const float scale) {",
      "  #ifdef ENABLE_FP8",
      "  if constexpr (kv_dt == Fp8KVCacheDataType::kFp8E4M3) {",
      "    return scaled_vec_conversion<Tout, Tin>(x, scale, __NV_E4M3);",
      "  } else if constexpr (kv_dt == Fp8KVCacheDataType::kFp8E5M2) {",
      "    return scaled_vec_conversion<Tout, Tin>(x, scale, __NV_E5M2);",
      "  }",
      "  #endif",
      "  assert(false);",
      "  __builtin_unreachable();  // Suppress missing return statement warning",
      "}",
      "",
      "  // The following macro is used to dispatch the conversion function based on",
      "  // the data type of the key and value cache. The FN is a macro that calls a",
      "  // function with template<typename scalar_t, typename cache_t,",
      "  // Fp8KVCacheDataType kv_dt>.",
      "  #define DISPATCH_BY_KV_CACHE_DTYPE(SRC_DTYPE, KV_DTYPE, FN)                  \\",
      "    if (KV_DTYPE == \"auto\") {                                                  \\",
      "      if (SRC_DTYPE == at::ScalarType::Float) {                                \\",
      "        FN(float, float, vllm::Fp8KVCacheDataType::kAuto);                     \\",
      "      } else if (SRC_DTYPE == at::ScalarType::Half) {                          \\",
      "        FN(uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto);               \\",
      "      } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                      \\",
      "        FN(__nv_bfloat16, __nv_bfloat16, vllm::Fp8KVCacheDataType::kAuto);     \\",
      "      } else {                                                                 \\",
      "        TORCH_CHECK(false, \"Unsupported input type of kv cache: \", SRC_DTYPE); \\",
      "      }                                                                        \\",
      "    } else {                                                                   \\",
      "      if (KV_DTYPE == \"fp8\" || KV_DTYPE == \"fp8_e4m3\") {                       \\",
      "        if (SRC_DTYPE == at::ScalarType::Float) {                              \\",
      "          FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);              \\",
      "        } else if (SRC_DTYPE == at::ScalarType::Half) {                        \\",
      "          FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);           \\",
      "        } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                    \\",
      "          FN(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E4M3);      \\",
      "        } else {                                                               \\",
      "          TORCH_CHECK(false,                                                   \\",
      "                      \"Unsupported input type of kv cache: \", SRC_DTYPE);      \\",
      "        }                                                                      \\",
      "      } else if (KV_DTYPE == \"fp8_e5m2\") {                                     \\",
      "        if (SRC_DTYPE == at::ScalarType::Float) {                              \\",
      "          FN(float, uint8_t, vllm::Fp8KVCacheDataType::kFp8E5M2);              \\",
      "        } else if (SRC_DTYPE == at::ScalarType::Half) {                        \\",
      "          FN(uint16_t, uint8_t, vllm::Fp8KVCacheDataType::kFp8E5M2);           \\",
      "        } else if (SRC_DTYPE == at::ScalarType::BFloat16) {                    \\",
      "          FN(__nv_bfloat16, uint8_t, vllm::Fp8KVCacheDataType::kFp8E5M2);      \\",
      "        } else {                                                               \\",
      "          TORCH_CHECK(false,                                                   \\",
      "                      \"Unsupported input type of kv cache: \", SRC_DTYPE);      \\",
      "        }                                                                      \\",
      "      } else {                                                                 \\",
      "        TORCH_CHECK(false, \"Unsupported data type of kv cache: \", KV_DTYPE);   \\",
      "      }                                                                        \\",
      "    }",
      "",
      "}  // namespace fp8",
      "#endif  // not USE_ROCM",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq_allspark/allspark_qgemm_w8a16.cu",
    "source": [
      "#include \"allspark_utils.cuh\"",
      "#include <torch/all.h>",
      "#include \"core/registration.h\"",
      "#include <cublas_v2.h>",
      "",
      "at::Tensor as_g_workspace;",
      "",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "",
      "torch::Tensor allspark_w8a16_gemm(",
      "    torch::Tensor const& a, torch::Tensor const& b_qweight,",
      "    torch::Tensor const& b_scales, std::optional<torch::Tensor> const& b_qzeros,",
      "    int64_t n, int64_t group_size, int64_t sm_count, int64_t sm_version,",
      "    int64_t CUBLAS_M_THRESHOLD, bool has_zp, bool n32k16_reorder) {",
      "  TORCH_CHECK_NOT_IMPLEMENTED(",
      "      false, \"allspark_w8a16_gemm(..) requires CUDA_ARCH >= 8.0\");",
      "  return torch::empty({1, 1});",
      "}",
      "",
      "#else",
      "namespace allspark {",
      "/*",
      " * GemmTile manage data movement from Global Memory to Shared Memory",
      " * requiring N % 8 == 0\uff0c K % 16 == 0 by loading uint",
      " * BN is obtained by padding the original N to a multiple of 32",
      " * weight B is rearranged as N32K16 order,",
      " * i.e. a initial data block of size 32(n)x16(k) is reordered as n8k4n4k4\uff0c",
      " * in order to put data loaded by the same thread of 32x16 data block together",
      " * continuously (see",
      " * https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#matrix-fragments-for-mma-m16n8k16-with-floating-point-type)",
      " */",
      "template <typename FType, typename QType, int Mtile, int Ntile, int NStage,",
      "          int BLOCK>",
      "struct GmemTile_W8A16_PerC_MtilexNtilex32_multistage_SM8x_SplitK {",
      "  // element num loaded by a LDG inst.",
      "  static constexpr int LDG_ELEMENT_CNT_A = 8;",
      "  static constexpr int LDG_ELEMENT_CNT_B = 16;",
      "  static constexpr int WARP_SIZE = 32;",
      "  static constexpr int M_SIZE_ONE_LOAD = (BLOCK * LDG_ELEMENT_CNT_A) / 32;",
      "  static constexpr int N_SIZE_ONE_LOAD = (BLOCK * LDG_ELEMENT_CNT_B) / 32;",
      "",
      "  __device__ GmemTile_W8A16_PerC_MtilexNtilex32_multistage_SM8x_SplitK(",
      "      const SM8x_GEMM_W8A16_Splitk_Params<FType, QType>& k_params,",
      "      const uint32_t& A_smem_addr, const uint32_t& BQ_smem_addr,",
      "      const uint32_t& A_stage_stride, const uint32_t& BQ_stage_stride)",
      "      : params(k_params),",
      "        A_smem_base_addr(A_smem_addr),",
      "        BQ_smem_base_addr(BQ_smem_addr),",
      "        A_smem_stage_stride(A_stage_stride),",
      "        BQ_smem_stage_stride(BQ_stage_stride) {",
      "    this_block_A_base_ptr = params.A_ptr + blockIdx.x * Mtile * params.K +",
      "                            blockIdx.z * params.SplitK;",
      "    // here B is rearranged as N32K16 order, i.e. 4 continuous N-direction",
      "    // 8(N)x16(K) size data blocks are packed together",
      "    this_block_B_base_ptr = params.B_ptr + blockIdx.y * Ntile * params.K +",
      "                            blockIdx.z * params.SplitK * 4;",
      "",
      "    const auto lane_id = threadIdx.x % WARP_SIZE;",
      "",
      "    // For matrix A, a block load/store Mtile(row) x 32(col) elements in",
      "    // multiple iters, 8x4 warp load/store 8(row) x 32(col) elements per iter",
      "    const auto Aldg_row_base_idx = threadIdx.x / 4;",
      "    Aldg_col_idx = (threadIdx.x % 4) * LDG_ELEMENT_CNT_A;",
      "    const int Aldg_base_offset = Aldg_row_base_idx * params.K + Aldg_col_idx;",
      "",
      "    // For matrix B, a block load/store elements of (Ntile / 4) row x 128 col",
      "    // elements of N32K16 packing in multiple iters, 4x8 warp load/store 4(row)",
      "    // * 128(col) per iter",
      "    Bldg_col_idx = (threadIdx.x % 8) * LDG_ELEMENT_CNT_B;",
      "    const auto Bldg_row_base_idx = threadIdx.x / 8;",
      "    const int Bldg_base_offset =",
      "        Bldg_row_base_idx * params.K * 4 + Bldg_col_idx;",
      "",
      "    this_block_A_base_ptr += Aldg_base_offset;",
      "    this_block_B_base_ptr += Bldg_base_offset;",
      "",
      "    const int sts_a_base_offset =",
      "        (threadIdx.x / 4) * 32 +",
      "        ((lane_id % 4) ^ ((lane_id / 4) % 4) ^ ((lane_id / 4) / 4)) *",
      "            LDG_ELEMENT_CNT_A;",
      "    const int sts_bq_base_offset =",
      "        Bldg_row_base_idx * 32 * 4 +",
      "        ((threadIdx.x % 8) ^ (((threadIdx.x / 8) % 2) * 4)) * LDG_ELEMENT_CNT_B;",
      "",
      "    A_smem_base_addr += sts_a_base_offset * sizeof(FType);",
      "    BQ_smem_base_addr += sts_bq_base_offset * sizeof(uint8_t);",
      "",
      "    A_ldg_guard = 0;",
      "    B_ldg_guard = 0;",
      "  #pragma unroll",
      "    for (int i = 0; i < (Mtile + M_SIZE_ONE_LOAD - 1) / M_SIZE_ONE_LOAD; ++i) {",
      "      auto m_idx = blockIdx.x * Mtile + Aldg_row_base_idx + i * M_SIZE_ONE_LOAD;",
      "      if (m_idx < params.M) {",
      "        A_ldg_guard |= (1u << i);",
      "      }",
      "    }",
      "",
      "    const int N_padded = (params.N + 31) / 32 * 32;",
      "  #pragma unroll",
      "    for (int i = 0; i < (Ntile + N_SIZE_ONE_LOAD - 1) / N_SIZE_ONE_LOAD; ++i) {",
      "      auto n_idx = blockIdx.y * Ntile + (Bldg_row_base_idx / 8) * 32 +",
      "                   i * N_SIZE_ONE_LOAD;",
      "      if (n_idx < N_padded) {",
      "        B_ldg_guard |= (1u << i);",
      "      }",
      "    }",
      "  }",
      "",
      "  __device__ void ldgsts_first_ktiles(const int& first_k_tile,",
      "                                      const int& k_tiles) {",
      "    // load first k_tile",
      "    // load A",
      "    const int A_src_size = Aldg_col_idx < first_k_tile ? 16 : 0;",
      "  #pragma unroll",
      "    for (int i = 0; i < (Mtile + M_SIZE_ONE_LOAD - 1) / M_SIZE_ONE_LOAD; ++i) {",
      "      cp_async<16>(",
      "          A_smem_base_addr + (i * M_SIZE_ONE_LOAD * 32) * sizeof(FType),",
      "          this_block_A_base_ptr + i * M_SIZE_ONE_LOAD * params.K, A_src_size,",
      "          (A_ldg_guard & (1u << i)) != 0);",
      "    }",
      "",
      "    // load B",
      "    const int B_src_size = (Bldg_col_idx / 4) < first_k_tile ? 16 : 0;",
      "  #pragma unroll",
      "    for (int i = 0; i < (Ntile + N_SIZE_ONE_LOAD - 1) / N_SIZE_ONE_LOAD; ++i) {",
      "      cp_async<16>(",
      "          BQ_smem_base_addr + (i * N_SIZE_ONE_LOAD * 32) * sizeof(uint8_t),",
      "          this_block_B_base_ptr + i * N_SIZE_ONE_LOAD * params.K, B_src_size,",
      "          (B_ldg_guard & (1u << i)) != 0);",
      "    }",
      "",
      "    cp_async_commit_group();",
      "    this_block_A_base_ptr += first_k_tile;",
      "    this_block_B_base_ptr += (first_k_tile * 4);",
      "",
      "    // load second to (N-stage - 1) k_tiles",
      "    for (int stage_idx = 1; stage_idx < NStage - 1; ++stage_idx) {",
      "      if (stage_idx < k_tiles) {",
      "  #pragma unroll",
      "        for (int i = 0; i < (Mtile + M_SIZE_ONE_LOAD - 1) / M_SIZE_ONE_LOAD;",
      "             ++i) {",
      "          cp_async<16>(A_smem_base_addr + stage_idx * A_smem_stage_stride +",
      "                           (i * M_SIZE_ONE_LOAD * 32) * sizeof(FType),",
      "                       this_block_A_base_ptr + i * M_SIZE_ONE_LOAD * params.K,",
      "                       16, (A_ldg_guard & (1u << i)) != 0);",
      "        }",
      "",
      "  #pragma unroll",
      "        for (int i = 0; i < (Ntile + N_SIZE_ONE_LOAD - 1) / N_SIZE_ONE_LOAD;",
      "             ++i) {",
      "          cp_async<16>(BQ_smem_base_addr + stage_idx * BQ_smem_stage_stride +",
      "                           (i * N_SIZE_ONE_LOAD * 32) * sizeof(uint8_t),",
      "                       this_block_B_base_ptr + i * N_SIZE_ONE_LOAD * params.K,",
      "                       16, (B_ldg_guard & (1u << i)) != 0);",
      "        }",
      "",
      "        this_block_A_base_ptr += 32;",
      "        this_block_B_base_ptr += (32 * 4);",
      "      }",
      "      cp_async_commit_group();",
      "    }",
      "  }",
      "",
      "  __device__ void ldgsts(const int& sts_stage_idx) {",
      "    const int a_stage_offset = sts_stage_idx * A_smem_stage_stride;",
      "    const int bq_stage_offset = sts_stage_idx * BQ_smem_stage_stride;",
      "  #pragma unroll",
      "    for (int i = 0; i < (Mtile + M_SIZE_ONE_LOAD - 1) / M_SIZE_ONE_LOAD; ++i) {",
      "      cp_async<16>(A_smem_base_addr + a_stage_offset +",
      "                       (i * M_SIZE_ONE_LOAD * 32) * sizeof(FType),",
      "                   this_block_A_base_ptr + i * M_SIZE_ONE_LOAD * params.K, 16,",
      "                   (A_ldg_guard & (1u << i)) != 0);",
      "    }",
      "",
      "  #pragma unroll",
      "    for (int i = 0; i < (Ntile + N_SIZE_ONE_LOAD - 1) / N_SIZE_ONE_LOAD; ++i) {",
      "      cp_async<16>(BQ_smem_base_addr + bq_stage_offset +",
      "                       (i * N_SIZE_ONE_LOAD * 32) * sizeof(uint8_t),",
      "                   this_block_B_base_ptr + i * N_SIZE_ONE_LOAD * params.K, 16,",
      "                   (B_ldg_guard & (1u << i)) != 0);",
      "    }",
      "",
      "    cp_async_commit_group();",
      "    this_block_A_base_ptr += 32;",
      "    this_block_B_base_ptr += (32 * 4);",
      "  }",
      "",
      "  const FType* this_block_A_base_ptr = nullptr;",
      "  const QType* this_block_B_base_ptr = nullptr;",
      "",
      "  int Aldg_col_idx;",
      "  int Bldg_col_idx;",
      "",
      "  uint32_t A_ldg_guard;",
      "  uint32_t B_ldg_guard;",
      "",
      "  uint32_t A_smem_base_addr, BQ_smem_base_addr;",
      "  const uint32_t A_smem_stage_stride, BQ_smem_stage_stride;",
      "",
      "  const SM8x_GEMM_W8A16_Splitk_Params<FType, QType>& params;",
      "};",
      "",
      "/*",
      " * requiring N % 8 == 0",
      " */",
      "template <typename FType, typename QType, int Mtile, int Ntile, int BLOCK,",
      "          bool EnableFuse, bool has_zp>",
      "struct ComputeTile_W8A16_PerC_MtilexNtilex32_multistage_SM8x_SplitK {",
      "  static constexpr int WARP_SIZE = 32;",
      "  static constexpr int WARP_CNT = BLOCK / WARP_SIZE;",
      "  static constexpr int WARP_NTILE = Ntile / WARP_CNT;",
      "  static constexpr int WARP_NITER = WARP_NTILE / 8;  // hmma16816",
      "  static_assert(WARP_NTILE == 32 or WARP_NTILE == 64,",
      "                \"now only support WARP_NTILE = 32 or 64!\");",
      "",
      "  __device__ ComputeTile_W8A16_PerC_MtilexNtilex32_multistage_SM8x_SplitK(",
      "      const SM8x_GEMM_W8A16_Splitk_Params<FType, QType>& k_params,",
      "      const uint32_t& A_smem_addr, const uint32_t& BQ_smem_addr,",
      "      const uint32_t& A_stage_stride, const uint32_t& BQ_stage_stride)",
      "      : params(k_params),",
      "        A_smem_base_addr(A_smem_addr),",
      "        BQ_smem_base_addr(BQ_smem_addr),",
      "        A_smem_stage_stride(A_stage_stride),",
      "        BQ_smem_stage_stride(BQ_stage_stride) {",
      "    warp_id = threadIdx.x / WARP_SIZE;",
      "    lane_id = threadIdx.x % WARP_SIZE;",
      "",
      "    load_a_base_offset[0] =",
      "        (lane_id % 16) * 32 +",
      "        ((lane_id / 16) ^ (lane_id % 4) ^ ((lane_id / 4) % 2)) * 8;",
      "    load_a_base_offset[1] =",
      "        (lane_id % 16) * 32 +",
      "        ((lane_id / 16 + 2) ^ (lane_id % 4) ^ ((lane_id / 4) % 2)) * 8;",
      "",
      "    load_b_base_offset[0] =",
      "        (lane_id / 4 + warp_id * (WARP_NTILE / 4)) * 32 * 4 +",
      "        (lane_id % 4) * 16 + ((lane_id / 4) % 2) * 16 * 4;",
      "    load_b_base_offset[1] =",
      "        (lane_id / 4 + warp_id * (WARP_NTILE / 4)) * 32 * 4 +",
      "        (lane_id % 4) * 16 + (((lane_id / 4) % 2) ^ 1) * 16 * 4;",
      "",
      "    sts_c_base_offset = warp_id * Mtile * WARP_NTILE +",
      "                        (lane_id / 4) * WARP_NTILE + (lane_id % 4) * 2;",
      "",
      "    if (EnableFuse) {",
      "      this_block_C_base_ptr =",
      "          params.C_ptr + blockIdx.x * Mtile * params.N + blockIdx.y * Ntile;",
      "    } else {",
      "      this_block_C_base_ptr =",
      "          params.C_split_ptr + blockIdx.z * params.M * params.N +",
      "          blockIdx.x * Mtile * params.N + blockIdx.y * Ntile;",
      "    }",
      "    int store_thds_in_row = WARP_NTILE / 8;",
      "    store_c_row_base_idx = lane_id / store_thds_in_row;",
      "    store_c_col_idx = warp_id * WARP_NTILE + (lane_id % store_thds_in_row) * 8;",
      "    store_c_base_offset = store_c_row_base_idx * params.N + store_c_col_idx;",
      "",
      "  #pragma unroll",
      "    for (int i = 0; i < Mtile / 16; ++i) {",
      "  #pragma unroll",
      "      for (int j = 0; j < WARP_NITER; ++j) {",
      "  #pragma unroll",
      "        for (int k = 0; k < 4; ++k) {",
      "          C_frag[i][j][k] = 0.f;",
      "        }",
      "      }",
      "    }",
      "    params_n_idx =",
      "        blockIdx.y * Ntile + warp_id * WARP_NTILE + (lane_id / 4) * 4;",
      "  }",
      "",
      "  __device__ void lds(const int& smem_stage_idx, const int& reg_buf_idx,",
      "                      const int& k_phase_idx) {",
      "    uint32_t A_smem_addr =",
      "        A_smem_base_addr + A_smem_stage_stride * smem_stage_idx;",
      "    uint32_t B_smem_addr =",
      "        BQ_smem_base_addr + BQ_smem_stage_stride * smem_stage_idx;",
      "",
      "  #pragma unroll",
      "    for (int i = 0; i < Mtile / 16; ++i) {",
      "      ldsm_4(A_frag[reg_buf_idx][i][0], A_frag[reg_buf_idx][i][1],",
      "             A_frag[reg_buf_idx][i][2], A_frag[reg_buf_idx][i][3],",
      "             A_smem_addr + (load_a_base_offset[k_phase_idx] + i * 16 * 32) *",
      "                               sizeof(FType));",
      "    }",
      "  #pragma unroll",
      "    for (int i = 0; i < WARP_NTILE / 32; ++i) {",
      "      lds128(BQ_frag[reg_buf_idx][4 * i + 0], BQ_frag[reg_buf_idx][4 * i + 1],",
      "             BQ_frag[reg_buf_idx][4 * i + 2], BQ_frag[reg_buf_idx][4 * i + 3],",
      "             B_smem_addr + (load_b_base_offset[k_phase_idx] + i * 32 * 32) *",
      "                               sizeof(uint8_t));",
      "    }",
      "",
      "  // dequant B",
      "  #pragma unroll",
      "    for (int i = 0; i < WARP_NITER / 2; ++i) {",
      "      cvt_8bx4_to_16bx4_bias128(BQ_frag[reg_buf_idx][2 * i],",
      "                                BF_frag[reg_buf_idx][2 * i]);",
      "      if (has_zp) {",
      "        BF_frag[reg_buf_idx][2 * i][0] =",
      "            __hsub2(BF_frag[reg_buf_idx][2 * i][0], num2num2(B_zero[i].x));",
      "        BF_frag[reg_buf_idx][2 * i][1] =",
      "            __hsub2(BF_frag[reg_buf_idx][2 * i][1], num2num2(B_zero[i].x));",
      "      }",
      "",
      "      BF_frag[reg_buf_idx][2 * i][0] =",
      "          __hmul2(BF_frag[reg_buf_idx][2 * i][0], num2num2(B_scale[i].x));",
      "      BF_frag[reg_buf_idx][2 * i][1] =",
      "          __hmul2(BF_frag[reg_buf_idx][2 * i][1], num2num2(B_scale[i].x));",
      "",
      "      cvt_8bx4_to_16bx4_bias128(BQ_frag[reg_buf_idx][2 * i + 1],",
      "                                BF_frag[reg_buf_idx][2 * i + 1]);",
      "      if (has_zp) {",
      "        BF_frag[reg_buf_idx][2 * i + 1][0] =",
      "            __hsub2(BF_frag[reg_buf_idx][2 * i + 1][0], num2num2(B_zero[i].y));",
      "        BF_frag[reg_buf_idx][2 * i + 1][1] =",
      "            __hsub2(BF_frag[reg_buf_idx][2 * i + 1][1], num2num2(B_zero[i].y));",
      "      }",
      "",
      "      BF_frag[reg_buf_idx][2 * i + 1][0] =",
      "          __hmul2(BF_frag[reg_buf_idx][2 * i + 1][0], num2num2(B_scale[i].y));",
      "      BF_frag[reg_buf_idx][2 * i + 1][1] =",
      "          __hmul2(BF_frag[reg_buf_idx][2 * i + 1][1], num2num2(B_scale[i].y));",
      "    }",
      "  }",
      "",
      "  __device__ void ldg_params() {",
      "    const int N_padded = (params.N + 31) / 32 * 32;",
      "    // load B scale and zero_point",
      "  #pragma unroll",
      "    for (int i = 0; i < WARP_NTILE / 32; ++i) {",
      "      ldg64_ca(B_scale[2 * i + 0], B_scale[2 * i + 1],",
      "               params.B_scale_ptr + params_n_idx + i * 32,",
      "               (params_n_idx + i * 32) < N_padded);",
      "      if (has_zp) {",
      "        ldg64_ca(B_zero[2 * i + 0], B_zero[2 * i + 1],",
      "                 params.B_zero_ptr + params_n_idx + i * 32,",
      "                 (params_n_idx + i * 32) < N_padded);",
      "      }",
      "    }",
      "  }",
      "",
      "  __device__ void mma(const int& reg_buf_idx) {",
      "  #pragma unroll",
      "    for (int m_idx = 0; m_idx < Mtile / 16; ++m_idx) {",
      "  #pragma unroll",
      "      for (int n_idx = 0; n_idx < WARP_NITER; ++n_idx) {",
      "        hmma16816_f32<FType>(",
      "            C_frag[m_idx][n_idx], A_frag[reg_buf_idx][m_idx],",
      "            reinterpret_cast<uint32_t (&)[2]>(BF_frag[reg_buf_idx][n_idx]));",
      "      }",
      "    }",
      "  }",
      "",
      "  __device__ void fused_splitk_reduce() {",
      "    // need splitk-reduce if enable splitk",
      "    if (gridDim.z > 1) {",
      "      auto blk_red_idx = blockIdx.x * gridDim.y + blockIdx.y;",
      "      // Wait for all previous blocks in the splitk direction to accumulate the",
      "      // results into C_tmp",
      "      if (threadIdx.x == 0) {",
      "        uint32_t* red_count_ptr = params.red_count_ptr + blk_red_idx;",
      "        uint32_t count;",
      "        do {",
      "          // make sure the ld.cg inside the do-wile loop",
      "          __threadfence_block();",
      "          asm volatile(\"ld.global.cg.b32 %0, [%1];\"",
      "                       : \"=r\"(count)",
      "                       : \"l\"(red_count_ptr));",
      "        } while (count != blockIdx.z);",
      "      }",
      "      __syncthreads();",
      "",
      "      auto C_tmp_base_offset = blk_red_idx * Mtile * Ntile + threadIdx.x * 4;",
      "      if (blockIdx.z != 0) {",
      "        // expecting that temporary register here reuses the previous A&B frag",
      "        // register",
      "        float temp_frag[Mtile / 16][WARP_NITER][4];",
      "  #pragma unroll",
      "        for (int m_idx = 0; m_idx < Mtile / 16; ++m_idx) {",
      "  #pragma unroll",
      "          for (int n_idx = 0; n_idx < WARP_NITER; ++n_idx) {",
      "            int offset =",
      "                C_tmp_base_offset + (m_idx * WARP_NITER + n_idx) * BLOCK * 4;",
      "            *reinterpret_cast<int4*>(temp_frag[m_idx][n_idx]) =",
      "                *reinterpret_cast<int4*>(params.C_tmp_ptr + offset);",
      "          }",
      "        }",
      "  #pragma unroll",
      "        for (int m_idx = 0; m_idx < Mtile / 16; ++m_idx) {",
      "  #pragma unroll",
      "          for (int n_idx = 0; n_idx < WARP_NITER; ++n_idx) {",
      "  #pragma unroll",
      "            for (int idx = 0; idx < 4; ++idx) {",
      "              C_frag[m_idx][n_idx][idx] += temp_frag[m_idx][n_idx][idx];",
      "            }",
      "          }",
      "        }",
      "      }",
      "",
      "      // first splitk - 1 blocks need to write partial results into C_tmp",
      "      if (blockIdx.z != gridDim.z - 1) {",
      "  #pragma unroll",
      "        for (int m_idx = 0; m_idx < Mtile / 16; ++m_idx) {",
      "  #pragma unroll",
      "          for (int n_idx = 0; n_idx < WARP_NITER; ++n_idx) {",
      "            int offset =",
      "                C_tmp_base_offset + (m_idx * WARP_NITER + n_idx) * BLOCK * 4;",
      "            asm volatile(",
      "                \"{st.global.cg.v4.b32 [%0], {%1, %2, %3, %4};}\\n\"",
      "                :",
      "                : \"l\"(params.C_tmp_ptr + offset), \"f\"(C_frag[m_idx][n_idx][0]),",
      "                  \"f\"(C_frag[m_idx][n_idx][1]), \"f\"(C_frag[m_idx][n_idx][2]),",
      "                  \"f\"(C_frag[m_idx][n_idx][3]));",
      "          }",
      "        }",
      "        __threadfence();",
      "        __syncthreads();",
      "        if (threadIdx.x == 0) {",
      "          uint32_t* red_count_ptr = params.red_count_ptr + blk_red_idx;",
      "          atomicInc(red_count_ptr, gridDim.z);",
      "        }",
      "      }",
      "    }",
      "  }",
      "",
      "  __device__ void stg(char* smem) {",
      "    if (EnableFuse) {",
      "      if (blockIdx.z != gridDim.z - 1) return;",
      "    }",
      "    uint32_t* C_sts_ptr =",
      "        reinterpret_cast<uint32_t*>(smem + sts_c_base_offset * sizeof(FType));",
      "    // C_tile sts",
      "  #pragma unroll",
      "    for (int m_idx = 0; m_idx < Mtile / 16; ++m_idx) {",
      "  #pragma unroll",
      "      for (int n_idx = 0; n_idx < WARP_NITER; ++n_idx) {",
      "  #pragma unroll",
      "        for (int k_idx = 0; k_idx < 2; ++k_idx) {",
      "          FType low16 =",
      "              ScalarType<FType>::float2num(C_frag[m_idx][n_idx][k_idx * 2]);",
      "          FType high16 =",
      "              ScalarType<FType>::float2num(C_frag[m_idx][n_idx][k_idx * 2 + 1]);",
      "          uint32_t tmp = (reinterpret_cast<uint32_t&>(low16) & 0xffff) |",
      "                         (reinterpret_cast<uint32_t&>(high16) << 16);",
      "          int sts_offset =",
      "              m_idx * 16 * (WARP_NTILE / 2) +",
      "              (((lane_id / (32 / WARP_NITER)) + n_idx) % WARP_NITER) * (8 / 2) +",
      "              k_idx * 8 * (WARP_NTILE / 2);",
      "          C_sts_ptr[sts_offset] = tmp;",
      "        }",
      "      }",
      "    }",
      "",
      "    __syncthreads();",
      "",
      "    FType* C_base_ptr = this_block_C_base_ptr + store_c_base_offset;",
      "    // C_tile lds and stg",
      "    auto m_base_idx = store_c_row_base_idx + blockIdx.x * Mtile;",
      "    bool n_guard = (store_c_col_idx + blockIdx.y * Ntile) < params.N;",
      "    if (WARP_NTILE == 32) {",
      "      int lds_c_base_offset = warp_id * Mtile * WARP_NTILE +",
      "                              (lane_id / 4) * WARP_NTILE +",
      "                              ((lane_id % 4 + lane_id / 8) % 4) * 8;",
      "      uint4* C_lds_ptr =",
      "          reinterpret_cast<uint4*>(smem + lds_c_base_offset * sizeof(FType));",
      "  #pragma unroll",
      "      for (int i = 0; i < (Mtile / 16) * (WARP_NITER / 2); ++i) {",
      "        uint4 stg_reg = C_lds_ptr[i * 8 * 4];",
      "        stg128(stg_reg.x, stg_reg.y, stg_reg.z, stg_reg.w,",
      "               C_base_ptr + i * 8 * params.N,",
      "               (m_base_idx + i * 8) < params.M && n_guard);",
      "      }",
      "    } else if (WARP_NTILE == 64) {",
      "      int lds_c_base_offset =",
      "          warp_id * Mtile * WARP_NTILE + (lane_id / 8) * WARP_NTILE;",
      "  #pragma unroll",
      "      for (int i = 0; i < (Mtile / 16) * (WARP_NITER / 2); ++i) {",
      "        int lds_c_offset = lds_c_base_offset + i * 4 * WARP_NTILE +",
      "                           ((lane_id % 8 + lane_id / 8 + (i % 2) * 4) % 8) * 8;",
      "        uint4 stg_reg =",
      "            *reinterpret_cast<uint4*>(smem + lds_c_offset * sizeof(FType));",
      "        stg128(stg_reg.x, stg_reg.y, stg_reg.z, stg_reg.w,",
      "               C_base_ptr + i * 4 * params.N,",
      "               (m_base_idx + i * 4) < params.M && n_guard);",
      "      }",
      "    }",
      "  }",
      "",
      "  const SM8x_GEMM_W8A16_Splitk_Params<FType, QType>& params;",
      "",
      "  int load_a_base_offset[2];",
      "  int load_b_base_offset[2];",
      "  int sts_c_base_offset;",
      "",
      "  int store_c_base_offset;",
      "",
      "  int store_c_row_base_idx, store_c_col_idx;",
      "  FType* this_block_C_base_ptr = nullptr;",
      "",
      "  int params_n_idx;",
      "  const uint32_t A_smem_base_addr, BQ_smem_base_addr;",
      "  const uint32_t A_smem_stage_stride, BQ_smem_stage_stride;",
      "",
      "  int lane_id;",
      "  int warp_id;",
      "  // first 2 denotes double buffer, second dim denotes M direction",
      "  uint32_t A_frag[2][Mtile / 16][4];",
      "",
      "  typename HalfType<FType>::T2 B_scale[WARP_NITER / 2];",
      "  typename HalfType<FType>::T2 B_zero[WARP_NITER / 2];",
      "  uint32_t BQ_frag[2][WARP_NITER];",
      "  // first 2 denotes double buffer, second dim denotes N direction, last 2",
      "  // denotes K direction",
      "  typename HalfType<FType>::T2 BF_frag[2][WARP_NITER][2];",
      "  // first dim denotes M direction, second dim denotes N direction",
      "  float C_frag[Mtile / 16][WARP_NITER][4];",
      "};",
      "",
      "/*",
      " *  @brief W8A16 Perchannel Quantization GEMM,",
      " *         requires N % 8 == 0, K % 16 == 0",
      " *         accumulator precision: FP32",
      " *  @tparam FType: DataType for A, B_scale, B_zero, and C, supports half or",
      " * nv_bfloat16",
      " *  @tparam QType: DataType for B, support uint8(bias128)",
      " *  @tparam Mtile: M-dimensional size of the gemm block tile, supports 16, 32,",
      " * 48 or 64",
      " *  @tparam Ntile: N-dimensional size of the gemm block tile, supports 128 or",
      " * 256",
      " *  @tparam NStage: Num of stages for async copy",
      " *  @tparam BLOCK: BLOCK size",
      " *  @tparam EnableFuse: If true, use fused splitk-reduce, otherwise use",
      " * non-fused splitk-reduce",
      " *  @tparam has_zp: whether to use zero_point",
      " *",
      " *  @fparam params struct consists of following parameters:",
      " *      @param A_ptr: Matrix A value ptr, A = (M, K)",
      " *      @param B_ptr: Matrix B value ptr, B = (N32_align, K) (N32K16 special",
      " * format), N32_align = (N + 32 - 1) / 32 * 32",
      " *      @param B_scale_ptr: B_scale value ptr, B_scale = (N32_align,) (N32K16",
      " * special format)",
      " *      @param B_zero_ptr: B_zero value ptr, B_zero = (N32_align,) (N32K16",
      " * special format)",
      " *      @param C_ptr: Matrix C value ptr, C = (M, N)",
      " *      @param M: dimnesion m",
      " *      @param N: dimnesion n",
      " *      @param K: dimnesion k",
      " *      @param SplitK: split size along K-dimension",
      " *      @param C_split_ptr: Matrix C_split value ptr, used only in non-fused",
      " * splitk-reduce",
      " *      @param C_tmp_ptr: Matrix C_tmp value ptr, used only in fused",
      " * splitk-reduce",
      " *      @param red_count_ptr: 1-D red_count value ptr, used only in fused",
      " * splitk-reduce",
      " */",
      "template <typename FType, typename QType, int Mtile, int Ntile, int NStage,",
      "          int BLOCK, bool EnableFuse, bool has_zp>",
      "__global__ void __launch_bounds__(BLOCK)",
      "    ampere_hgemm_W8A16_perc_f16_f16_MtilexNtilex32_hmma16816_multistage_AN_BTN32K16_CN_splitk_kernel(",
      "        const SM8x_GEMM_W8A16_Splitk_Params<FType, QType> params) {",
      "  // A smem size = 64 * 32 * 2B/elem * 4(stage) = 16KB",
      "  // B smem size = 128 * 32 * 1B/elem * 4(stage) = 16KB",
      "  constexpr int smem_size_one_stage = Mtile * 32 * 2 + Ntile * 32;",
      "  __shared__ char smem[NStage * smem_size_one_stage];",
      "  char* A_smem = smem;",
      "  char* BQ_smem = smem + Mtile * 32 * 2 * NStage;",
      "",
      "  uint32_t A_smem_addr = smem_u32addr(A_smem);",
      "  uint32_t BQ_smem_addr = smem_u32addr(BQ_smem);",
      "  uint32_t A_smem_stage_stride = Mtile * 32 * 2;",
      "  uint32_t BQ_smem_stage_stride = Ntile * 32;",
      "",
      "  // initialize the data move process from GM to SMEM for this block",
      "  GmemTile_W8A16_PerC_MtilexNtilex32_multistage_SM8x_SplitK<",
      "      FType, QType, Mtile, Ntile, NStage, BLOCK>",
      "      gmem_tile(params, A_smem_addr, BQ_smem_addr, A_smem_stage_stride,",
      "                BQ_smem_stage_stride);",
      "",
      "  int sts_stage_idx = 0;",
      "  int lds_stage_idx = 0;",
      "",
      "  auto tb_k_slice = blockIdx.z * params.SplitK + params.SplitK <= params.K",
      "                        ? params.SplitK",
      "                        : params.K - blockIdx.z * params.SplitK;",
      "  int k_tiles = (tb_k_slice + 31) / 32;",
      "  int first_k_tile = tb_k_slice - (k_tiles - 1) * 32;",
      "",
      "  // load first three tiles to shared memory",
      "  gmem_tile.ldgsts_first_ktiles(first_k_tile, k_tiles);",
      "  sts_stage_idx += (NStage - 2);",
      "  ComputeTile_W8A16_PerC_MtilexNtilex32_multistage_SM8x_SplitK<",
      "      FType, QType, Mtile, Ntile, BLOCK, EnableFuse, has_zp>",
      "      compute_tile(params, A_smem_addr, BQ_smem_addr, A_smem_stage_stride,",
      "                   BQ_smem_stage_stride);",
      "  compute_tile.ldg_params();",
      "  cp_asyc_wait_group<NStage - 2>();",
      "  __syncthreads();",
      "",
      "  compute_tile.lds(lds_stage_idx, 0, 0);",
      "  int reg_buf_idx = 1;",
      "",
      "  // main loop",
      "  for (; k_tiles > NStage - 1; --k_tiles) {",
      "    // load next A&B tile",
      "    sts_stage_idx = sts_stage_idx < NStage - 1 ? sts_stage_idx + 1 : 0;",
      "    gmem_tile.ldgsts(sts_stage_idx);",
      "",
      "  #pragma unroll",
      "    for (int k_phase_idx = 0; k_phase_idx < 2; k_phase_idx++) {",
      "      // dequantize next B tile",
      "      if (k_phase_idx == 1) {",
      "        cp_asyc_wait_group<NStage - 2>();",
      "        __syncthreads();",
      "        lds_stage_idx = lds_stage_idx < NStage - 1 ? lds_stage_idx + 1 : 0;",
      "      }",
      "",
      "      compute_tile.lds(lds_stage_idx, reg_buf_idx, (k_phase_idx + 1) % 2);",
      "",
      "      compute_tile.mma(reg_buf_idx ^ 1);",
      "      reg_buf_idx ^= 1;",
      "    }",
      "  }",
      "",
      "  // last NStage-1 tiles",
      "  for (; k_tiles > 0; --k_tiles) {",
      "    cp_async_commit_group();",
      "  #pragma unroll",
      "    for (int k_phase_idx = 0; k_phase_idx < 2; k_phase_idx++) {",
      "      // dequantize next B tile",
      "      if (k_phase_idx == 1) {",
      "        cp_asyc_wait_group<NStage - 2>();",
      "        __syncthreads();",
      "        lds_stage_idx = lds_stage_idx < NStage - 1 ? lds_stage_idx + 1 : 0;",
      "      }",
      "",
      "      compute_tile.lds(lds_stage_idx, reg_buf_idx, (k_phase_idx + 1) % 2);",
      "",
      "      compute_tile.mma(reg_buf_idx ^ 1);",
      "      reg_buf_idx ^= 1;",
      "    }",
      "  }",
      "",
      "  if (EnableFuse) {",
      "    compute_tile.fused_splitk_reduce();",
      "  }",
      "  compute_tile.stg(smem);",
      "}",
      "",
      "  #define __CALL_IF(MTILE, NTILE, NUM_THREADS, ENABLE_FUSE, HAS_ZP)                                     \\",
      "    else if (Mtile == MTILE && Ntile == NTILE && BLOCK == NUM_THREADS &&                                \\",
      "             enable_fuse == ENABLE_FUSE && has_zp == HAS_ZP) {                                          \\",
      "      ampere_hgemm_W8A16_perc_f16_f16_MtilexNtilex32_hmma16816_multistage_AN_BTN32K16_CN_splitk_kernel< \\",
      "          FType, QType, MTILE, NTILE, 4, NUM_THREADS, ENABLE_FUSE, HAS_ZP>                              \\",
      "          <<<grid, block, 0, stream>>>(params);                                                         \\",
      "    }",
      "",
      "template <typename FType, typename QType>",
      "void ampere_hgemm_W8A16_perc_f16_f16_MtilexNtilex32_mma16816_multistage_AN_BTN32K16_CN_splitk(",
      "    const FType* A, const QType* B, const FType* B_scale, const FType* B_zero,",
      "    FType* C, const int M, const int N, const int K, void* workspace,",
      "    const int sm_version, const BlockTileSplitkParams& fused_gemm_params,",
      "    cudaStream_t stream) {",
      "  int Mtile = fused_gemm_params.Mtile;",
      "  int grid_x = (M + Mtile - 1) / Mtile;",
      "  int Ntile = fused_gemm_params.Ntile;",
      "  int grid_y = (N + Ntile - 1) / Ntile;",
      "  int SplitK = fused_gemm_params.SplitK;",
      "  int grid_z = (K + SplitK - 1) / SplitK;",
      "",
      "  int BLOCK = (Ntile == 256) ? 256 : 128;",
      "",
      "  dim3 grid(grid_x, grid_y, grid_z);",
      "  dim3 block(BLOCK);",
      "",
      "  bool enable_fuse = fused_gemm_params.EnableFuse;",
      "  bool has_zp = B_zero != nullptr;",
      "  if (enable_fuse) {",
      "    float* C_tmp = reinterpret_cast<float*>(workspace);",
      "    uint32_t* red_count = reinterpret_cast<uint32_t*>(",
      "        (char*)workspace + grid_x * Mtile * grid_y * Ntile * sizeof(float));",
      "    CHECK_CUDA(cudaMemsetAsync(red_count, 0, grid_x * grid_y * sizeof(uint32_t),",
      "                               stream));",
      "    SM8x_GEMM_W8A16_Splitk_Params<FType, QType> params{",
      "        A, B,      B_scale, B_zero, C,       M,     N,",
      "        K, SplitK, 0,       -1,     nullptr, C_tmp, red_count};",
      "",
      "    if (false) {",
      "    }",
      "    // Select the template parameters for kernel launch",
      "    // according to the above settings. Tuning is not supported.",
      "    __CALL_IF(16, 256, 256, true, false)",
      "    __CALL_IF(32, 256, 256, true, false)",
      "    __CALL_IF(48, 256, 256, true, false)",
      "    __CALL_IF(64, 128, 128, true, false)",
      "    __CALL_IF(64, 256, 256, true, false)",
      "    __CALL_IF(16, 256, 256, true, true)",
      "    __CALL_IF(32, 256, 256, true, true)",
      "    __CALL_IF(48, 256, 256, true, true)",
      "    __CALL_IF(64, 128, 128, true, true)",
      "    __CALL_IF(64, 256, 256, true, true)",
      "  } else {",
      "    FType* C_split = reinterpret_cast<FType*>(workspace);",
      "    SM8x_GEMM_W8A16_Splitk_Params<FType, QType> params{",
      "        A, B,      B_scale, B_zero, C,       M,       N,",
      "        K, SplitK, 0,       -1,     C_split, nullptr, nullptr};",
      "",
      "    if (false) {",
      "    }",
      "    // Select the template parameters for kernel launch",
      "    // according to the above settings. Tuning is not supported.",
      "    __CALL_IF(16, 256, 256, false, false)",
      "    __CALL_IF(32, 256, 256, false, false)",
      "    __CALL_IF(48, 256, 256, false, false)",
      "    __CALL_IF(64, 128, 128, false, false)",
      "    __CALL_IF(64, 256, 256, false, false)",
      "    __CALL_IF(16, 256, 256, false, true)",
      "    __CALL_IF(32, 256, 256, false, true)",
      "    __CALL_IF(48, 256, 256, false, true)",
      "    __CALL_IF(64, 128, 128, false, true)",
      "    __CALL_IF(64, 256, 256, false, true)",
      "",
      "    // SplitK reduce",
      "    f16_gemm_splitk_reduce(C_split, C, M, N, grid_z, stream);",
      "  }",
      "}",
      "",
      "size_t allspark_qgemm_w8a16_perc_n32k16_ampere_workspace_size(",
      "    int m, int n, int k, int sm_count,",
      "    BlockTileSplitkParams& fused_gemm_params) {",
      "  // Determine the block tile and splitk strategy",
      "  int m16_times = (m + 16 - 1) / 16;",
      "  int Mtile = m16_times <= 4 ? m16_times * 16 : 64;",
      "  int grid_x = (m + Mtile - 1) / Mtile;",
      "  int Ntile =",
      "      (float(grid_x * ((n + 127) / 128)) / sm_count > 10) || (Mtile < 64) ? 256",
      "                                                                          : 128;",
      "  int grid_y = (n + Ntile - 1) / Ntile;",
      "  int grid_z;",
      "",
      "  // split-k",
      "  const float SPLIT_THRESHOLD = 0.8;",
      "  int n_slice;",
      "  for (n_slice = 1; n_slice < k / 256; ++n_slice) {",
      "    int n_block = grid_x * grid_y * n_slice;",
      "    if (n_block >= sm_count * SPLIT_THRESHOLD &&",
      "        (n_block % sm_count == 0 || n_block % sm_count >= sm_count * 0.5)) {",
      "      break;",
      "    }",
      "  }",
      "",
      "  int k_slice =",
      "      (k / n_slice) % 32 == 0 ? k / n_slice : k / n_slice / 32 * 32 + 32;",
      "  grid_z = (k + k_slice - 1) / k_slice;",
      "  bool enable_fuse = float(grid_x * grid_y) / sm_count >= 0.5 ? 1 : 0;",
      "",
      "  size_t ws_size;",
      "  if (enable_fuse) {",
      "    ws_size = grid_x * Mtile * grid_y * Ntile * sizeof(float)  // For C_tmp",
      "              + grid_x * grid_y * sizeof(uint32_t);            // For red_count",
      "  } else {",
      "    ws_size = grid_z * m * n * sizeof(__half);",
      "  }",
      "",
      "  fused_gemm_params.Mtile = Mtile;",
      "  fused_gemm_params.Ntile = Ntile;",
      "  fused_gemm_params.SplitK = k_slice;",
      "  fused_gemm_params.EnableFuse = enable_fuse;",
      "  return ws_size;",
      "}",
      "",
      "// restore from N32K16 order to original N-major order",
      "// K % 16 == 0, N % 8 == 0",
      "// each block process 64(k) * 32(n) result elements",
      "template <typename FT, typename QT>",
      "__global__ void restore_N32_K16_dequantize_rhs_w8a16_perc_kernel(",
      "    const QT* qdata, const FT* scales, const FT* zeros, FT* fdata,",
      "    const int N_32align, const int N, const int K) {",
      "  __shared__ FT smem[64 * 32];",
      "  auto warp_id = threadIdx.x / 32;",
      "  auto lane_id = threadIdx.x % 32;",
      "  const auto src_row_idx = blockIdx.x * 8 + lane_id / 4;",
      "  const int src_col_idx =",
      "      blockIdx.y * 64 * 4 + warp_id * 16 * 4 + (lane_id % 4) * 16;",
      "  const int src_offset = src_row_idx * K * 4 + src_col_idx;",
      "  auto params_nidx = blockIdx.x * 32 + (lane_id / 4) * 4;",
      "",
      "  QT qval_reg[16];",
      "  const QT* pdata = qdata + src_offset;",
      "  if (src_col_idx < (K * 4)) {",
      "    *(reinterpret_cast<uint4*>(qval_reg)) =",
      "        *(reinterpret_cast<const uint4*>(qdata + src_offset));",
      "  }",
      "  FT scale_reg[4];",
      "  *(reinterpret_cast<uint2*>(scale_reg)) =",
      "      *(reinterpret_cast<const uint2*>(scales + params_nidx));",
      "  FT zero_reg[4];",
      "  if (zeros != nullptr) {",
      "    *(reinterpret_cast<uint2*>(zero_reg)) =",
      "        *(reinterpret_cast<const uint2*>(zeros + params_nidx));",
      "  }",
      "  FT fval_reg[16];",
      "",
      "  const int sts_base_offset =",
      "      (warp_id * 16 + (lane_id % 4) * 2) * 32 + lane_id / 4;",
      "  #pragma unroll",
      "  for (int ni = 0; ni < 4; ++ni) {",
      "    cvt_8bx4_to_16bx4_bias128(",
      "        *reinterpret_cast<uint32_t*>(&qval_reg[ni * 4]),",
      "        reinterpret_cast<typename HalfType<FT>::T2*>(&(fval_reg[ni * 4])));",
      "  #pragma unroll",
      "    for (int ki = 0; ki < 4; ++ki) {",
      "      if (zeros != nullptr) {",
      "        fval_reg[ni * 4 + ki] = __hsub(fval_reg[ni * 4 + ki], zero_reg[ni]);",
      "      }",
      "      fval_reg[ni * 4 + ki] = __hmul(fval_reg[ni * 4 + ki], scale_reg[ni]);",
      "      int sts_offset = sts_base_offset + ((ki / 2) * 8 + (ki % 2)) * 32 +",
      "                       ((ni + lane_id % 4) % 4) * 8;",
      "      smem[sts_offset] = fval_reg[ni * 4 + ki];",
      "    }",
      "  }",
      "  __syncthreads();",
      "",
      "  const int lds_base_offset =",
      "      (threadIdx.x / 4) * 32 + ((threadIdx.x % 4 + threadIdx.x / 8) % 4) * 8;",
      "  #pragma unroll",
      "  for (int i = 0; i < 2; ++i) {",
      "    *reinterpret_cast<uint4*>(fval_reg + i * 8) =",
      "        *reinterpret_cast<uint4*>(smem + lds_base_offset + i * 32 * 32);",
      "  }",
      "",
      "  const auto dst_row_base_kidx = blockIdx.y * 64 + threadIdx.x / 4;",
      "  const auto dst_col_nidx = blockIdx.x * 32 + (threadIdx.x % 4) * 8;",
      "  #pragma unroll",
      "  for (int i = 0; i < 2; ++i) {",
      "    int dst_row_kidx = dst_row_base_kidx + i * 32;",
      "    int dst_offset = dst_row_kidx * N + dst_col_nidx;",
      "    if (dst_row_kidx < K && dst_col_nidx < N) {",
      "      *reinterpret_cast<uint4*>(fdata + dst_offset) =",
      "          *reinterpret_cast<uint4*>(fval_reg + i * 8);",
      "    }",
      "  }",
      "}",
      "",
      "template <typename FT, typename QT>",
      "void restore_N32_K16_dequantize_rhs_w8a16(const QT* qdata, const FT* scales,",
      "                                          const FT* zeros, FT* fdata,",
      "                                          const int N_32align, const int N,",
      "                                          const int K, const int GroupSize,",
      "                                          cudaStream_t stream) {",
      "  TORCH_CHECK(N % 8 == 0 && K % 16 == 0 && N_32align % 32 == 0,",
      "              \"Unsupported shape\");",
      "  if (GroupSize == -1) {",
      "    const int BLOCK = 128;",
      "    dim3 grid(N_32align / 32, ((K / 16) + 3) / 4);",
      "    restore_N32_K16_dequantize_rhs_w8a16_perc_kernel<FT, QT>",
      "        <<<grid, BLOCK, 0, stream>>>(qdata, scales, zeros, fdata, N_32align, N,",
      "                                     K);",
      "  }",
      "  // TODO: Support SubChannel",
      "  else {",
      "    TORCH_CHECK(false, \"Now only support PerChannel\");",
      "  }",
      "}",
      "",
      "template <typename FT, typename QT>",
      "void w8a16_gemm_dq_cublas(const FT* in, const QT* rhs_qdata_ptr,",
      "                          const FT* rhs_scales_ptr, const FT* rhs_zeros_ptr,",
      "                          FT* out, void* workspace, const int M,",
      "                          const int N_32align, const int N, const int K,",
      "                          const int group_size, cudaStream_t stream,",
      "                          cublasHandle_t handle) {",
      "  static_assert(",
      "      std::is_same<FT, half>::value || std::is_same<FT, nv_bfloat16>::value,",
      "      \"only float16 and bfloat16 is supported\");",
      "  // Dequant",
      "  FT* rhs_fdata_ptr = static_cast<FT*>(workspace);",
      "  restore_N32_K16_dequantize_rhs_w8a16(rhs_qdata_ptr, rhs_scales_ptr,",
      "                                       rhs_zeros_ptr, rhs_fdata_ptr, N_32align,",
      "                                       N, K, group_size, stream);",
      "  // cuBLAS GEMM",
      "  int lda = K;",
      "  int ldb = N;",
      "  int ldc = N;",
      "  const float alpha = 1.0f;",
      "  const float beta = 0.0f;",
      "  cudaDataType_t cuda_type;",
      "  if (std::is_same<FT, __half>::value) {",
      "    cuda_type = CUDA_R_16F;",
      "  } else {",
      "    cuda_type = CUDA_R_16BF;",
      "  }",
      "  CHECK_CUBLAS(cublasGemmEx(handle, CUBLAS_OP_N, CUBLAS_OP_N, N, M, K, &alpha,",
      "                            rhs_fdata_ptr, cuda_type, ldb, in, cuda_type, lda,",
      "                            &beta, out, cuda_type, ldc, CUDA_R_32F,",
      "                            CUBLAS_GEMM_DEFAULT_TENSOR_OP));",
      "}",
      "",
      "template <typename FType, typename QType>",
      "void allspark_qgemm_w8a16_perc_ampere(",
      "    const FType* A, const QType* B, const FType* B_scale, const FType* B_zero,",
      "    FType* C, const int M, const int N_32align, const int N, const int K,",
      "    void* workspace, const BlockTileSplitkParams& fused_gemm_params,",
      "    const int group_size, int CUBLAS_M_THRESHOLD, const int sm_version,",
      "    cudaStream_t stream, cublasHandle_t handle) {",
      "  if (M > CUBLAS_M_THRESHOLD) {",
      "    w8a16_gemm_dq_cublas<FType, QType>(A, B, B_scale, B_zero, C, workspace, M,",
      "                                       N_32align, N, K, group_size, stream,",
      "                                       handle);",
      "  } else {",
      "    ampere_hgemm_W8A16_perc_f16_f16_MtilexNtilex32_mma16816_multistage_AN_BTN32K16_CN_splitk<",
      "        FType, QType>(A, B, B_scale, B_zero, C, M, N, K, workspace, sm_version,",
      "                      fused_gemm_params, stream);",
      "  }",
      "}",
      "",
      "}  // namespace allspark",
      "",
      "torch::Tensor allspark_w8a16_gemm(",
      "    torch::Tensor const& a, torch::Tensor const& b_qweight,",
      "    torch::Tensor const& b_scales, std::optional<torch::Tensor> const& b_qzeros,",
      "    int64_t n, int64_t group_size, int64_t sm_count, int64_t sm_version,",
      "    int64_t CUBLAS_M_THRESHOLD, bool has_zp, bool n32k16_reorder) {",
      "  // Verify device and strides",
      "  TORCH_CHECK(a.device().is_cuda(), \"A is not on GPU\");",
      "  TORCH_CHECK(a.is_contiguous(), \"A is not contiguous\");",
      "",
      "  TORCH_CHECK(b_qweight.device().is_cuda(), \"b_qweight is not on GPU\");",
      "  TORCH_CHECK(b_qweight.is_contiguous(), \"b_qweight is not contiguous\");",
      "",
      "  TORCH_CHECK(b_scales.device().is_cuda(), \"b_scales is not on GPU\");",
      "  TORCH_CHECK(b_scales.is_contiguous(), \"b_scales is not contiguous\");",
      "",
      "  if (has_zp) {",
      "    TORCH_CHECK(b_qzeros.value().device().is_cuda(), \"b_qzeros is not on GPU\");",
      "    TORCH_CHECK(b_qzeros.value().is_contiguous(), \"b_qzeros is not contiguous\");",
      "  }",
      "",
      "  int m = a.size(0);",
      "  int n_32align = (n + 32 - 1) / 32 * 32;",
      "  int k = a.size(1);",
      "",
      "  // Verify shape",
      "  TORCH_CHECK(b_qweight.size(0) == n_32align,",
      "              \"Shape mismatch: b_qweight.size(0) = \", b_qweight.size(0),",
      "              \", n_32align = \", n_32align);",
      "  TORCH_CHECK(b_qweight.size(1) == k,",
      "              \"Shape mismatch: b_qweight.size(1) = \", b_qweight.size(1),",
      "              \", k = \", k);",
      "",
      "  TORCH_CHECK(group_size == -1, \"Currently only supports group_size = -1\");",
      "",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(a));",
      "  const void* a_ptr = reinterpret_cast<const void*>(a.data_ptr());",
      "  const uint8_t* b_ptr = reinterpret_cast<const uint8_t*>(b_qweight.data_ptr());",
      "  const void* b_scale_ptr = reinterpret_cast<const void*>(b_scales.data_ptr());",
      "  const void* b_zero_ptr = nullptr;",
      "  if (b_qzeros.has_value()) {",
      "    b_zero_ptr = reinterpret_cast<const void*>(b_qzeros.value().data_ptr());",
      "  }",
      "",
      "  auto c_options = torch::TensorOptions().dtype(a.dtype()).device(a.device());",
      "  torch::Tensor c = torch::empty({m, n}, c_options);",
      "  void* c_ptr = reinterpret_cast<void*>(c.data_ptr());",
      "",
      "  cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();",
      "",
      "  allspark::BlockTileSplitkParams fused_gemm_params;",
      "",
      "  size_t ws_size = 0;",
      "  if (m > CUBLAS_M_THRESHOLD) {",
      "    ws_size = k * n * 2;  // sizeof(f16)==2",
      "  } else {",
      "    ws_size = allspark::allspark_qgemm_w8a16_perc_n32k16_ampere_workspace_size(",
      "        m, n, k, sm_count, fused_gemm_params);",
      "  }",
      "",
      "  auto ws_options = torch::TensorOptions().dtype(at::kChar).device(a.device());",
      "  if (as_g_workspace.numel() <",
      "      ws_size) {  // ws_options: kChar, so numel() is bytes",
      "    as_g_workspace = torch::empty({long(ws_size)}, ws_options);",
      "  }",
      "  void* ws = reinterpret_cast<void*>(as_g_workspace.data_ptr());",
      "",
      "  if (a.dtype() == at::ScalarType::Half) {",
      "    allspark::allspark_qgemm_w8a16_perc_ampere<__half, uint8_t>(",
      "        reinterpret_cast<const __half*>(a_ptr), b_ptr,",
      "        reinterpret_cast<const __half*>(b_scale_ptr),",
      "        reinterpret_cast<const __half*>(b_zero_ptr),",
      "        reinterpret_cast<__half*>(c_ptr), m, n_32align, n, k, ws,",
      "        fused_gemm_params, group_size, CUBLAS_M_THRESHOLD, sm_version, stream,",
      "        handle);",
      "  } else if (a.dtype() == at::ScalarType::BFloat16) {",
      "    allspark::allspark_qgemm_w8a16_perc_ampere<__nv_bfloat16, uint8_t>(",
      "        reinterpret_cast<const __nv_bfloat16*>(a_ptr), b_ptr,",
      "        reinterpret_cast<const __nv_bfloat16*>(b_scale_ptr),",
      "        reinterpret_cast<const __nv_bfloat16*>(b_zero_ptr),",
      "        reinterpret_cast<__nv_bfloat16*>(c_ptr), m, n_32align, n, k, ws,",
      "        fused_gemm_params, group_size, CUBLAS_M_THRESHOLD, sm_version, stream,",
      "        handle);",
      "  }",
      "",
      "  return c;",
      "}",
      "",
      "#endif",
      "",
      "TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, CUDA, m) {",
      "  m.impl(\"allspark_w8a16_gemm\", &allspark_w8a16_gemm);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq_allspark/allspark_repack.cu",
    "source": [
      "#include \"allspark_utils.cuh\"",
      "#include <torch/all.h>",
      "#include \"core/registration.h\"",
      "",
      "namespace allspark {",
      "",
      "// Rearrange B to facilitate Ampere Tensor Core load data",
      "// reorder B from (K, N) to (N_32align / 4, K * 4)",
      "// K % 16 == 0, N % 16 == 0, N_32align % 32 == 0",
      "template <typename FType>",
      "__global__ void __launch_bounds__(128)",
      "    rearrange_kn_weight_as_n32k16_order_ldg16_kernel(",
      "        const uint8_t* B, const FType* B_scale, const FType* B_zero,",
      "        uint8_t* B_result, FType* B_scale_result, FType* B_zero_result,",
      "        const int K, const int N, const int N_32align) {",
      "  const auto lane_id = threadIdx.x % 32;",
      "  const auto warp_id = threadIdx.x / 32;",
      "",
      "  if (blockIdx.x != gridDim.x - 1) {",
      "    // Load B",
      "    // per block process 64(k) * 128(n) B elements",
      "    // per warp process 16(k) * 128 B elements",
      "    const int src_row_base_idx =",
      "        blockIdx.x * 64 + warp_id * 16 + ((lane_id % 8) / 2) * 2;",
      "    const int src_col_idx =",
      "        blockIdx.y * 128 + (lane_id / 8) * 32 + (lane_id % 2) * 16;",
      "    uint8_t B_frag[4][16];",
      "#pragma unroll",
      "    for (int i = 0; i < 4; ++i) {",
      "      int src_row_idx = src_row_base_idx + (i / 2) * 8 + (i % 2);",
      "      int src_offset = src_row_idx * N + src_col_idx;",
      "      bool guard = src_row_idx < K && src_col_idx < N;",
      "      ldg128_cg_0(*reinterpret_cast<uint32_t*>(B_frag[i]),",
      "                  *(reinterpret_cast<uint32_t*>(B_frag[i]) + 1),",
      "                  *(reinterpret_cast<uint32_t*>(B_frag[i]) + 2),",
      "                  *(reinterpret_cast<uint32_t*>(B_frag[i]) + 3), B + src_offset,",
      "                  guard);",
      "    }",
      "",
      "    // reorder B",
      "    uint8_t B_reorder_frag[8][8];",
      "#pragma unroll",
      "    for (int i = 0; i < 4; ++i) {",
      "#pragma unroll",
      "      for (int j = 0; j < 16; ++j) {",
      "        int dst_i = j % 8;",
      "        int dst_j = i + (j / 8) * 4;",
      "        B_reorder_frag[dst_i][dst_j] = B_frag[i][j];",
      "      }",
      "    }",
      "",
      "    // Store B",
      "    const auto dst_row_base_idx = blockIdx.y * (128 / 4) + (lane_id / 8) * 8;",
      "    const int dst_col_idx =",
      "        blockIdx.x * (64 * 4) + warp_id * 64 + (lane_id % 8) * 8;",
      "    for (int i = 0; i < 8; ++i) {",
      "      int dst_row_idx = dst_row_base_idx + i;",
      "      int dst_offset = dst_row_idx * K * 4 + dst_col_idx;",
      "      bool guard = (dst_row_base_idx < N_32align / 4) && (dst_col_idx < K * 4);",
      "      if (guard) {",
      "        *reinterpret_cast<int2*>(B_result + dst_offset) =",
      "            *reinterpret_cast<int2*>(B_reorder_frag[i]);",
      "      }",
      "    }",
      "  } else {",
      "    // Load B_scale and B_zero",
      "    FType b_scale_reg, b_zero_reg;",
      "    auto src_offset = blockIdx.y * 128 + threadIdx.x;",
      "    ldg16_cg_0(b_scale_reg, B_scale + src_offset, src_offset < N);",
      "    if (B_zero != nullptr)",
      "      ldg16_cg_0(b_zero_reg, B_zero + src_offset, src_offset < N);",
      "    int dst_offset =",
      "        blockIdx.y * 128 + warp_id * 32 + (lane_id % 8) * 4 + lane_id / 8;",
      "    if (dst_offset < N_32align) {",
      "      B_scale_result[dst_offset] = b_scale_reg;",
      "      if (B_zero != nullptr) B_zero_result[dst_offset] = b_zero_reg;",
      "    }",
      "  }",
      "}",
      "",
      "template <typename FType>",
      "void rearrange_kn_weight_as_n32k16_order_ldg16(",
      "    const uint8_t* B, const FType* B_scale, const FType* B_zero,",
      "    uint8_t* B_result, FType* B_scale_result, FType* B_zero_result,",
      "    const int64_t K, const int64_t N, const int64_t N_32align,",
      "    cudaStream_t stream) {",
      "  if (N % 16 != 0 || K % 16 != 0) {",
      "    std::cerr << \"Now only support N and K is multiples of 16\" << std::endl;",
      "  }",
      "  const int BLOCK = 128;",
      "  int grid_x = (K + 64 - 1) / 64 + 1;",
      "  int grid_y = (N + 128 - 1) / 128;",
      "  dim3 grid(grid_x, grid_y);",
      "",
      "  rearrange_kn_weight_as_n32k16_order_ldg16_kernel<FType>",
      "      <<<grid, BLOCK, 0, stream>>>(B, B_scale, B_zero, B_result, B_scale_result,",
      "                                   B_zero_result, K, N, N_32align);",
      "}",
      "}  // namespace allspark",
      "",
      "void rearrange_kn_weight_as_n32k16_order(",
      "    torch::Tensor const& b_qweight, torch::Tensor const& b_scales,",
      "    std::optional<torch::Tensor> const& b_zeros, bool has_zp,",
      "    torch::Tensor& b_qweight_reorder, torch::Tensor& b_scales_reorder,",
      "    std::optional<torch::Tensor> const& b_zeros_reorder, const int64_t K,",
      "    const int64_t N, const int64_t N_32align) {",
      "  // Verify device and strides",
      "  TORCH_CHECK(b_qweight.device().is_cuda(), \"b_qweight is not on GPU\");",
      "  TORCH_CHECK(b_qweight.is_contiguous(), \"b_qweight is not contiguous\");",
      "",
      "  TORCH_CHECK(b_scales.device().is_cuda(), \"b_scales is not on GPU\");",
      "  TORCH_CHECK(b_scales.is_contiguous(), \"b_scales is not contiguous\");",
      "",
      "  TORCH_CHECK(b_qweight_reorder.device().is_cuda(),",
      "              \"b_qweight_reorder is not on GPU\");",
      "  TORCH_CHECK(b_qweight_reorder.is_contiguous(),",
      "              \"b_qweight_reorder is not contiguous\");",
      "",
      "  TORCH_CHECK(b_scales_reorder.device().is_cuda(),",
      "              \"b_scales_reorder is not on GPU\");",
      "  TORCH_CHECK(b_scales_reorder.is_contiguous(),",
      "              \"b_scales_reorder is not contiguous\");",
      "",
      "  if (has_zp) {",
      "    TORCH_CHECK(b_zeros.value().device().is_cuda(), \"b_zeros is not on GPU\");",
      "    TORCH_CHECK(b_zeros.value().is_contiguous(), \"b_zeros is not contiguous\");",
      "",
      "    TORCH_CHECK(b_zeros_reorder.value().device().is_cuda(),",
      "                \"b_zeros_reorder is not on GPU\");",
      "    TORCH_CHECK(b_zeros_reorder.value().is_contiguous(),",
      "                \"b_zeros_reorder is not contiguous\");",
      "  }",
      "",
      "  const uint8_t* matB = reinterpret_cast<const uint8_t*>(b_qweight.data_ptr());",
      "  const void* b_scale = b_scales.data_ptr();",
      "  const void* b_zero = has_zp ? b_zeros.value().data_ptr() : nullptr;",
      "",
      "  uint8_t* matB_reorder =",
      "      reinterpret_cast<uint8_t*>(b_qweight_reorder.data_ptr());",
      "  void* b_scale_reorder = b_scales_reorder.data_ptr();",
      "  void* b_zero_reorder = has_zp ? b_zeros_reorder.value().data_ptr() : nullptr;",
      "",
      "  cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  if (b_scales.dtype() == at::ScalarType::Half) {",
      "    allspark::rearrange_kn_weight_as_n32k16_order_ldg16<__half>(",
      "        matB, reinterpret_cast<const __half*>(b_scale),",
      "        reinterpret_cast<const __half*>(b_zero), matB_reorder,",
      "        reinterpret_cast<__half*>(b_scale_reorder),",
      "        reinterpret_cast<__half*>(b_zero_reorder), K, N, N_32align, stream);",
      "  } else if (b_scales.dtype() == at::ScalarType::BFloat16) {",
      "    allspark::rearrange_kn_weight_as_n32k16_order_ldg16<__nv_bfloat16>(",
      "        matB, reinterpret_cast<const __nv_bfloat16*>(b_scale),",
      "        reinterpret_cast<const __nv_bfloat16*>(b_zero), matB_reorder,",
      "        reinterpret_cast<__nv_bfloat16*>(b_scale_reorder),",
      "        reinterpret_cast<__nv_bfloat16*>(b_zero_reorder), K, N, N_32align,",
      "        stream);",
      "  }",
      "}",
      "",
      "TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, CUDA, m) {",
      "  m.impl(\"rearrange_kn_weight_as_n32k16_order\",",
      "         &rearrange_kn_weight_as_n32k16_order);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq_allspark/allspark_utils.cuh",
    "source": [
      "#pragma once",
      "",
      "#include <torch/all.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <cuda_runtime.h>",
      "#include <cuda_fp16.h>",
      "#include <cuda_bf16.h>",
      "#include <iostream>",
      "#include \"../gptq_marlin/marlin_dtypes.cuh\"",
      "using marlin::ScalarType;",
      "",
      "namespace allspark {",
      "",
      "#define CHECK_CUDA(cmd)                                             \\",
      "  do {                                                              \\",
      "    cudaError_t cuda_status = cmd;                                  \\",
      "    if (cuda_status != cudaSuccess) {                               \\",
      "      std::string err_str = cudaGetErrorString(cuda_status);        \\",
      "      std::cerr << \"Failed: \" << __FILE__ << \":\" << __LINE__ << \" \" \\",
      "                << err_str;                                         \\",
      "      exit(-1);                                                     \\",
      "    }                                                               \\",
      "  } while (0)",
      "",
      "#define CHECK_CUBLAS(cmd)                                            \\",
      "  do {                                                               \\",
      "    cublasStatus_t cublas_status = cmd;                              \\",
      "    if (cublas_status != CUBLAS_STATUS_SUCCESS) {                    \\",
      "      std::cerr << \"Failed:  \" << __FILE__ << \":\" << __LINE__ << \" \" \\",
      "                << cublas_status << std::endl;                       \\",
      "      exit(-1);                                                      \\",
      "    }                                                                \\",
      "  } while (0)",
      "",
      "template <typename FType, typename QType>",
      "struct SM8x_GEMM_W8A16_Splitk_Params {",
      "  const FType* A_ptr;",
      "  const QType* B_ptr;",
      "  const FType* B_scale_ptr;",
      "  const FType* B_zero_ptr;",
      "  FType* C_ptr;",
      "  int M;",
      "  int N;",
      "  int K;",
      "  int SplitK;",
      "  int GroupCnt;",
      "  int GroupSize;",
      "  FType* C_split_ptr;       // for non-fused splitk reduce",
      "  float* C_tmp_ptr;         // for fused splitk reduce",
      "  uint32_t* red_count_ptr;  // for fused splitk reduce",
      "};",
      "",
      "struct alignas(16) BlockTileSplitkParams {",
      "  int Mtile;",
      "  int Ntile;",
      "  int SplitK;",
      "  bool EnableFuse;",
      "};",
      "",
      "template <typename FType, int BLOCK, int N_MATRIX>",
      "__global__ void f16_gemm_splitk_reduce_kernel(const FType* C_split, FType* C,",
      "                                              uint32_t n, uint32_t n_matrix,",
      "                                              uint32_t matrix_size) {",
      "  auto idx = blockIdx.x * BLOCK + threadIdx.x;",
      "",
      "  if (idx >= matrix_size) {",
      "    return;",
      "  }",
      "",
      "  float sum = 0.f;",
      "",
      "  int n_mat = N_MATRIX > 0 ? N_MATRIX : (int)n_matrix;",
      "  for (int i = 0; i < n_mat; ++i) {",
      "    sum += ScalarType<FType>::num2float(C_split[idx + i * matrix_size]);",
      "  }",
      "",
      "  C[idx] = ScalarType<FType>::float2num(sum);",
      "}",
      "",
      "template <typename FType>",
      "void f16_gemm_splitk_reduce(const FType* C_split, FType* C, const uint32_t m,",
      "                            const uint32_t n, const uint32_t n_matrix,",
      "                            cudaStream_t stream) {",
      "  const int BLOCK = 128;",
      "  uint32_t matrix_size = m * n;",
      "  int grid = (matrix_size + BLOCK - 1) / BLOCK;",
      "",
      "  void (*kernel)(const FType*, FType*, uint32_t, uint32_t, uint32_t) = nullptr;",
      "",
      "  switch (n_matrix) {",
      "    case 4:",
      "      kernel = f16_gemm_splitk_reduce_kernel<FType, BLOCK, 4>;",
      "      break;",
      "    case 5:",
      "      kernel = f16_gemm_splitk_reduce_kernel<FType, BLOCK, 5>;",
      "      break;",
      "    case 6:",
      "      kernel = f16_gemm_splitk_reduce_kernel<FType, BLOCK, 6>;",
      "      break;",
      "    case 7:",
      "      kernel = f16_gemm_splitk_reduce_kernel<FType, BLOCK, 7>;",
      "      break;",
      "    case 8:",
      "      kernel = f16_gemm_splitk_reduce_kernel<FType, BLOCK, 8>;",
      "      break;",
      "    case 9:",
      "      kernel = f16_gemm_splitk_reduce_kernel<FType, BLOCK, 9>;",
      "      break;",
      "    case 10:",
      "      kernel = f16_gemm_splitk_reduce_kernel<FType, BLOCK, 10>;",
      "      break;",
      "    case 11:",
      "      kernel = f16_gemm_splitk_reduce_kernel<FType, BLOCK, 11>;",
      "      break;",
      "    case 12:",
      "      kernel = f16_gemm_splitk_reduce_kernel<FType, BLOCK, 12>;",
      "      break;",
      "    default:",
      "      kernel = f16_gemm_splitk_reduce_kernel<FType, BLOCK, -1>;",
      "      break;",
      "  }",
      "",
      "  kernel<<<grid, BLOCK, 0, stream>>>(C_split, C, n, n_matrix, matrix_size);",
      "}",
      "",
      "template <typename T>",
      "struct HalfType;",
      "template <>",
      "struct HalfType<half> {",
      "  using T1 = __half;",
      "  using T2 = __half2;",
      "};",
      "template <>",
      "struct HalfType<__nv_bfloat16> {",
      "  using T1 = __nv_bfloat16;",
      "  using T2 = __nv_bfloat162;",
      "};",
      "",
      "// convert 64-bit pointer to 32-bit smem addr",
      "__device__ __forceinline__ uint32_t smem_u32addr(const void* smem_ptr) {",
      "  uint32_t addr;",
      "  asm(\"{.reg .u64 u64addr;\\n\"",
      "      \" cvta.to.shared.u64 u64addr, %1;\\n\"",
      "      \" cvt.u32.u64 %0, u64addr;}\\n\"",
      "      : \"=r\"(addr)",
      "      : \"l\"(smem_ptr));",
      "",
      "  return addr;",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ void ldg16_cg_0(T& r0, const void* ptr, bool guard) {",
      "  static_assert(sizeof(T) == 2, \"ldg16_cg_0: invalid T\");",
      "",
      "  asm volatile(",
      "      \"{.reg .pred p;\\n\"",
      "      \" setp.ne.b32 p, %2, 0;\\n\"",
      "      \" @!p mov.b16 %0, 0;\\n\"",
      "#if __CUDACC_VER_MAJOR__ >= 11 && __CUDACC_VER_MINOR__ >= 4 && \\",
      "    __CUDA_ARCH__ >= 750",
      "      \" @p ld.global.cg.L2::128B.b16 {%0}, [%1];}\\n\"",
      "#else",
      "      \" @p ld.global.ca.b16 {%0}, [%1];}\\n\"",
      "#endif",
      "      : \"=h\"(reinterpret_cast<uint16_t&>(r0))",
      "      : \"l\"(ptr), \"r\"((int)guard));",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ void ldg64_ca(T& r0, T& r1, const void* ptr,",
      "                                         bool guard) {",
      "  static_assert(sizeof(T) == 4, \"ldg64_ca: invalid T\");",
      "",
      "  asm volatile(",
      "      \"{.reg .pred p;\\n\"",
      "      \" setp.ne.b32 p, %3, 0;\\n\"",
      "#if __CUDACC_VER_MAJOR__ >= 11 && __CUDACC_VER_MINOR__ >= 4 && \\",
      "    __CUDA_ARCH__ >= 750",
      "      \" @p ld.global.ca.L2::128B.v2.b32 {%0, %1}, [%2];}\\n\"",
      "#else",
      "      \" @p ld.global.ca.v2.b32 {%0, %1}, [%2];}\\n\"",
      "#endif",
      "      : \"=r\"(reinterpret_cast<uint32_t&>(r0)),",
      "        \"=r\"(reinterpret_cast<uint32_t&>(r1))",
      "      : \"l\"(ptr), \"r\"((int)guard));",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ void ldg128_cg_0(T& r0, T& r1, T& r2, T& r3,",
      "                                            const void* ptr, bool guard) {",
      "  static_assert(sizeof(T) == 4, \"ldg128_cg_0: invalid T\");",
      "",
      "  asm volatile(",
      "      \"{.reg .pred p;\\n\"",
      "      \" setp.ne.b32 p, %5, 0;\\n\"",
      "      \" @!p mov.b32 %0, 0;\\n\"",
      "      \" @!p mov.b32 %1, 0;\\n\"",
      "      \" @!p mov.b32 %2, 0;\\n\"",
      "      \" @!p mov.b32 %3, 0;\\n\"",
      "#if __CUDACC_VER_MAJOR__ >= 11 && __CUDACC_VER_MINOR__ >= 4 && \\",
      "    __CUDA_ARCH__ >= 750",
      "      \" @p ld.global.cg.L2::128B.v4.b32 {%0, %1, %2, %3}, [%4];}\\n\"",
      "#else",
      "      \" @p ld.global.cg.v4.b32 {%0, %1, %2, %3}, [%4];}\\n\"",
      "#endif",
      "      : \"=r\"(reinterpret_cast<uint32_t&>(r0)),",
      "        \"=r\"(reinterpret_cast<uint32_t&>(r1)),",
      "        \"=r\"(reinterpret_cast<uint32_t&>(r2)),",
      "        \"=r\"(reinterpret_cast<uint32_t&>(r3))",
      "      : \"l\"(ptr), \"r\"((int)guard));",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ void lds128(T& reg0, T& reg1, T& reg2, T& reg3,",
      "                                       const uint32_t addr) {",
      "  static_assert(sizeof(T) == 4, \"lds128: invalid T\");",
      "",
      "  asm volatile(\"ld.shared.v4.b32 {%0, %1, %2, %3}, [%4];\\n\"",
      "               : \"=r\"(reinterpret_cast<uint32_t&>(reg0)),",
      "                 \"=r\"(reinterpret_cast<uint32_t&>(reg1)),",
      "                 \"=r\"(reinterpret_cast<uint32_t&>(reg2)),",
      "                 \"=r\"(reinterpret_cast<uint32_t&>(reg3))",
      "               : \"r\"(addr));",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ void stg128(const T& r0, const T& r1, const T& r2,",
      "                                       const T& r3, const void* ptr,",
      "                                       bool guard) {",
      "  static_assert(sizeof(T) == 4, \"stg128: invalid T\");",
      "",
      "  asm volatile(",
      "      \"{.reg .pred p;\\n\"",
      "      \" setp.ne.b32 p, %1, 0;\\n\"",
      "      \" @p st.global.v4.b32 [%0], {%2, %3, %4, %5};}\\n\"",
      "      :",
      "      : \"l\"(ptr), \"r\"((int)guard), \"r\"(reinterpret_cast<const uint32_t&>(r0)),",
      "        \"r\"(reinterpret_cast<const uint32_t&>(r1)),",
      "        \"r\"(reinterpret_cast<const uint32_t&>(r2)),",
      "        \"r\"(reinterpret_cast<const uint32_t&>(r3)));",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ void ldsm_4(T& r0, T& r1, T& r2, T& r3,",
      "                                       const uint32_t& addr) {",
      "  static_assert(sizeof(T) == 4, \"ldsm_4: invalid T\");",
      "#if (__CUDA_ARCH__ >= 750) && (__CUDACC_VER_MAJOR__ >= 11)",
      "  asm volatile(",
      "      \"ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%0, %1, %2, %3}, [%4];\\n\"",
      "      : \"=r\"(reinterpret_cast<uint32_t&>(r0)),",
      "        \"=r\"(reinterpret_cast<uint32_t&>(r1)),",
      "        \"=r\"(reinterpret_cast<uint32_t&>(r2)),",
      "        \"=r\"(reinterpret_cast<uint32_t&>(r3))",
      "      : \"r\"(addr));",
      "#endif",
      "}",
      "",
      "template <typename FType>",
      "__device__ __forceinline__ void hmma16816_f32(float (&d)[4],",
      "                                              const uint32_t (&a)[4],",
      "                                              const uint32_t (&b)[2]);",
      "",
      "template <>",
      "__device__ __forceinline__ void hmma16816_f32<__half>(float (&d)[4],",
      "                                                      const uint32_t (&a)[4],",
      "                                                      const uint32_t (&b)[2]) {",
      "#if (__CUDA_ARCH__ >= 800) && (__CUDACC_VER_MAJOR__ >= 11)",
      "  asm volatile(",
      "      \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%0, %1, %2, %3}, \"",
      "      \"{%4, %5, %6, %7}, {%8, %9}, {%0, %1, %2, %3};\\n\"",
      "      : \"+f\"(d[0]), \"+f\"(d[1]), \"+f\"(d[2]), \"+f\"(d[3])",
      "      : \"r\"(a[0]), \"r\"(a[1]), \"r\"(a[2]), \"r\"(a[3]), \"r\"(b[0]), \"r\"(b[1]));",
      "#endif",
      "}",
      "",
      "template <>",
      "__device__ __forceinline__ void hmma16816_f32<__nv_bfloat16>(",
      "    float (&d)[4], const uint32_t (&a)[4], const uint32_t (&b)[2]) {",
      "#if (__CUDA_ARCH__ >= 800) && (__CUDACC_VER_MAJOR__ >= 11)",
      "  asm volatile(",
      "      \"mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%0, %1, %2, %3}, \"",
      "      \"{%4, %5, %6, %7}, {%8, %9}, {%0, %1, %2, %3};\\n\"",
      "      : \"+f\"(d[0]), \"+f\"(d[1]), \"+f\"(d[2]), \"+f\"(d[3])",
      "      : \"r\"(a[0]), \"r\"(a[1]), \"r\"(a[2]), \"r\"(a[3]), \"r\"(b[0]), \"r\"(b[1]));",
      "#endif",
      "}",
      "",
      "template <int SIZE_IN_BYTES>",
      "__device__ __forceinline__ void cp_async(const uint32_t smem_addr,",
      "                                         const void* gmem_ptr,",
      "                                         const int src_in_bytes, bool guard) {",
      "  static_assert(",
      "      (SIZE_IN_BYTES == 4 || SIZE_IN_BYTES == 8 || SIZE_IN_BYTES == 16),",
      "      \"Size is not supported\");",
      "#if __CUDACC_VER_MAJOR__ >= 11 && __CUDA_ARCH__ >= 800",
      "  asm volatile(",
      "      \"{.reg.pred p;\\n\"",
      "      \" setp.ne.b32 p, %4, 0;\\n\"",
      "  #if __CUDACC_VER_MINOR__ >= 4",
      "      \" @p cp.async.cg.shared.global.L2::256B [%0], [%1], %2, %3;}\\n\"",
      "  #else",
      "      \" @p cp.async.cg.shared.global [%0], [%1], %2, %3;}\\n\"",
      "  #endif",
      "      ::\"r\"(smem_addr),",
      "      \"l\"(gmem_ptr), \"n\"(SIZE_IN_BYTES), \"r\"(src_in_bytes), \"r\"((int)guard));",
      "#endif",
      "}",
      "",
      "template <int SIZE_IN_BYTES>",
      "__device__ __forceinline__ void cp_async_ca(const uint32_t smem_addr,",
      "                                            const void* gmem_ptr,",
      "                                            const int src_in_bytes,",
      "                                            bool guard) {",
      "  static_assert(",
      "      (SIZE_IN_BYTES == 4 || SIZE_IN_BYTES == 8 || SIZE_IN_BYTES == 16),",
      "      \"Size is not supported\");",
      "#if __CUDACC_VER_MAJOR__ >= 11 && __CUDA_ARCH__ >= 800",
      "  asm volatile(",
      "      \"{.reg.pred p;\\n\"",
      "      \" setp.ne.b32 p, %4, 0;\\n\"",
      "  #if __CUDACC_VER_MINOR__ >= 4",
      "      \" @p cp.async.ca.shared.global.L2::256B [%0], [%1], %2, %3;}\\n\"",
      "  #else",
      "      \" @p cp.async.ca.shared.global [%0], [%1], %2, %3;}\\n\"",
      "  #endif",
      "      ::\"r\"(smem_addr),",
      "      \"l\"(gmem_ptr), \"n\"(SIZE_IN_BYTES), \"r\"(src_in_bytes), \"r\"((int)guard));",
      "#endif",
      "}",
      "",
      "__device__ __forceinline__ void cp_async_commit_group() {",
      "#if __CUDACC_VER_MAJOR__ >= 11 && __CUDA_ARCH__ >= 800",
      "  asm volatile(\"cp.async.commit_group;\\n\");",
      "#endif",
      "}",
      "",
      "template <int N>",
      "__device__ __forceinline__ void cp_asyc_wait_group() {",
      "#if __CUDACC_VER_MAJOR__ >= 11 && __CUDA_ARCH__ >= 800",
      "  asm volatile(\"cp.async.wait_group %0;\\n\" : : \"n\"(N));",
      "#endif",
      "}",
      "",
      "template <typename T>",
      "__device__ __forceinline__ void cvt_8bx4_to_16bx4_bias128(const uint32_t& idata,",
      "                                                          T* fdata);",
      "",
      "template <>",
      "// fast conversion: 4xuint8 to 4xhalf, subtracting bias = 128",
      "__device__ __forceinline__ void cvt_8bx4_to_16bx4_bias128<__half2>(",
      "    const uint32_t& idata, __half2* fdata) {",
      "  uint32_t i10, i32;",
      "  asm volatile(",
      "      \"prmt.b32 %0, %2, 0x64, 0x4140;\"",
      "      \"prmt.b32 %1, %2, 0x64, 0x4342;\"",
      "      : \"=r\"(i10), \"=r\"(i32)",
      "      : \"r\"(idata));",
      "",
      "  static constexpr uint32_t MAGIC_NUM = 0x64806480;",
      "  fdata[0] = __hsub2(reinterpret_cast<const __half2&>(i10),",
      "                     reinterpret_cast<const __half2&>(MAGIC_NUM));",
      "  fdata[1] = __hsub2(reinterpret_cast<const __half2&>(i32),",
      "                     reinterpret_cast<const __half2&>(MAGIC_NUM));",
      "}",
      "",
      "template <>",
      "// fast conversion: 4xuint8 to 4xbfloat16, subtracting bias = 128",
      "// reference from marlin fast implementation",
      "__device__ __forceinline__ void cvt_8bx4_to_16bx4_bias128<__nv_bfloat162>(",
      "    const uint32_t& idata, __nv_bfloat162* fdata) {",
      "  float fp32_imd[4];",
      "  uint32_t* fp32_imd_casted = reinterpret_cast<uint32_t*>(fp32_imd);",
      "  asm volatile(",
      "      \"prmt.b32 %0, %4, 0x4B000000, 0x7650;\"",
      "      \"prmt.b32 %1, %4, 0x4B000000, 0x7651;\"",
      "      \"prmt.b32 %2, %4, 0x4B000000, 0x7652;\"",
      "      \"prmt.b32 %3, %4, 0x4B000000, 0x7653;\"",
      "      : \"=r\"(fp32_imd_casted[0]), \"=r\"(fp32_imd_casted[1]),",
      "        \"=r\"(fp32_imd_casted[2]), \"=r\"(fp32_imd_casted[3])",
      "      : \"r\"(idata));",
      "",
      "  fp32_imd[0] -= 8388736.f;",
      "  fp32_imd[1] -= 8388736.f;",
      "  fp32_imd[2] -= 8388736.f;",
      "  fp32_imd[3] -= 8388736.f;",
      "",
      "  uint32_t* bf16_res = reinterpret_cast<uint32_t*>(fdata);",
      "  asm volatile(",
      "      \"prmt.b32 %0, %2, %3, 0x7632;\"",
      "      \"prmt.b32 %1, %4, %5, 0x7632;\"",
      "      : \"=r\"(bf16_res[0]), \"=r\"(bf16_res[1])",
      "      : \"r\"(fp32_imd_casted[0]), \"r\"(fp32_imd_casted[1]),",
      "        \"r\"(fp32_imd_casted[2]), \"r\"(fp32_imd_casted[3]));",
      "}",
      "",
      "static __device__ nv_bfloat162 inline num2num2(const nv_bfloat16 x) {",
      "#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800",
      "  assert(false);",
      "#else",
      "  return __bfloat162bfloat162(x);",
      "#endif",
      "  __builtin_unreachable();  // Suppress missing return statement warning",
      "}",
      "",
      "static __device__ half2 inline num2num2(const half x) {",
      "  return __half2half2(x);",
      "}",
      "",
      "}  // namespace allspark"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gguf/moe_vec.cuh",
    "source": [
      "// copied and adapted from",
      "// https://github.com/ggerganov/llama.cpp/blob/b2899/ggml-cuda/mmvq.cu",
      "template <typename scalar_t, int qk, int qi, typename block_q_t, int vdr,",
      "          vec_dot_q_cuda_t vec_dot_q_cuda>",
      "static __global__ void moe_vec_q(const void* __restrict__ vx,",
      "                                 const void* __restrict__ vy,",
      "                                 scalar_t* __restrict__ dst,",
      "                                 const int* topk_ids, const int topk,",
      "                                 const int ncols, const int nrows,",
      "                                 const int token_stride) {",
      "  const auto row = blockIdx.x * blockDim.y + threadIdx.y;",
      "",
      "  const auto token = blockIdx.z / topk;",
      "  const auto expert = (topk_ids)[blockIdx.z];",
      "",
      "  if (row >= nrows) {",
      "    return;",
      "  }",
      "",
      "  const int blocks_per_row = ncols / qk;",
      "  const int blocks_per_warp = vdr * WARP_SIZE / qi;",
      "",
      "  // partial sum for each thread",
      "  float tmp = 0.0f;",
      "",
      "  const block_q_t* x = ((const block_q_t*)vx) + expert * nrows * blocks_per_row;",
      "  const block_q8_1* y =",
      "      (const block_q8_1*)(((const int*)vy) + token * token_stride);",
      "",
      "  for (auto i = threadIdx.x / (qi / vdr); i < blocks_per_row;",
      "       i += blocks_per_warp) {",
      "    const int ibx = row * blocks_per_row + i;  // x block index",
      "",
      "    const int iby = i * (qk / QK8_1);  // y block index that aligns with ibx",
      "",
      "    const int iqs =",
      "        vdr *",
      "        (threadIdx.x %",
      "         (qi / vdr));  // x block quant index when casting the quants to int",
      "",
      "    tmp += vec_dot_q_cuda(&x[ibx], &y[iby], iqs);",
      "  }",
      "",
      "  // sum up partial sums and write back result",
      "#pragma unroll",
      "  for (int mask = WARP_SIZE / 2; mask > 0; mask >>= 1) {",
      "    tmp += VLLM_SHFL_XOR_SYNC(tmp, mask);",
      "  }",
      "",
      "  if (threadIdx.x == 0) {",
      "    dst[blockIdx.z * nrows + row] = tmp;",
      "  }",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_q4_0_q8_1_cuda(const void* vx, const void* vy,",
      "                                   scalar_t* dst, const int* topk_ids,",
      "                                   const int top_k, const int tokens,",
      "                                   const int ncols, const int nrows,",
      "                                   const int token_stride,",
      "                                   cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK4_0, QI4_0, block_q4_0, VDR_Q4_0_Q8_1_MMVQ,",
      "            vec_dot_q4_0_q8_1><<<block_nums, block_dims, 0, stream>>>(",
      "      vx, vy, dst, topk_ids, top_k, ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_q4_1_q8_1_cuda(const void* vx, const void* vy,",
      "                                   scalar_t* dst, const int* topk_ids,",
      "                                   const int top_k, const int tokens,",
      "                                   const int ncols, const int nrows,",
      "                                   const int token_stride,",
      "                                   cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK4_0, QI4_1, block_q4_1, VDR_Q4_1_Q8_1_MMVQ,",
      "            vec_dot_q4_1_q8_1><<<block_nums, block_dims, 0, stream>>>(",
      "      vx, vy, dst, topk_ids, top_k, ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_q5_0_q8_1_cuda(const void* vx, const void* vy,",
      "                                   scalar_t* dst, const int* topk_ids,",
      "                                   const int top_k, const int tokens,",
      "                                   const int ncols, const int nrows,",
      "                                   const int token_stride,",
      "                                   cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK5_0, QI5_0, block_q5_0, VDR_Q5_0_Q8_1_MMVQ,",
      "            vec_dot_q5_0_q8_1><<<block_nums, block_dims, 0, stream>>>(",
      "      vx, vy, dst, topk_ids, top_k, ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_q5_1_q8_1_cuda(const void* vx, const void* vy,",
      "                                   scalar_t* dst, const int* topk_ids,",
      "                                   const int top_k, const int tokens,",
      "                                   const int ncols, const int nrows,",
      "                                   const int token_stride,",
      "                                   cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK5_1, QI5_1, block_q5_1, VDR_Q5_1_Q8_1_MMVQ,",
      "            vec_dot_q5_1_q8_1><<<block_nums, block_dims, 0, stream>>>(",
      "      vx, vy, dst, topk_ids, top_k, ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_q8_0_q8_1_cuda(const void* vx, const void* vy,",
      "                                   scalar_t* dst, const int* topk_ids,",
      "                                   const int top_k, const int tokens,",
      "                                   const int ncols, const int nrows,",
      "                                   const int token_stride,",
      "                                   cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK8_0, QI8_0, block_q8_0, VDR_Q8_0_Q8_1_MMVQ,",
      "            vec_dot_q8_0_q8_1><<<block_nums, block_dims, 0, stream>>>(",
      "      vx, vy, dst, topk_ids, top_k, ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_q2_K_q8_1_cuda(const void* vx, const void* vy,",
      "                                   scalar_t* dst, const int* topk_ids,",
      "                                   const int top_k, const int tokens,",
      "                                   const int ncols, const int nrows,",
      "                                   const int token_stride,",
      "                                   cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK_K, QI2_K, block_q2_K, VDR_Q2_K_Q8_1_MMVQ,",
      "            vec_dot_q2_K_q8_1><<<block_nums, block_dims, 0, stream>>>(",
      "      vx, vy, dst, topk_ids, top_k, ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_q3_K_q8_1_cuda(const void* vx, const void* vy,",
      "                                   scalar_t* dst, const int* topk_ids,",
      "                                   const int top_k, const int tokens,",
      "                                   const int ncols, const int nrows,",
      "                                   const int token_stride,",
      "                                   cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK_K, QI3_K, block_q3_K, VDR_Q3_K_Q8_1_MMVQ,",
      "            vec_dot_q3_K_q8_1><<<block_nums, block_dims, 0, stream>>>(",
      "      vx, vy, dst, topk_ids, top_k, ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_q4_K_q8_1_cuda(const void* vx, const void* vy,",
      "                                   scalar_t* dst, const int* topk_ids,",
      "                                   const int top_k, const int tokens,",
      "                                   const int ncols, const int nrows,",
      "                                   const int token_stride,",
      "                                   cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK_K, QI4_K, block_q4_K, VDR_Q4_K_Q8_1_MMVQ,",
      "            vec_dot_q4_K_q8_1><<<block_nums, block_dims, 0, stream>>>(",
      "      vx, vy, dst, topk_ids, top_k, ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_q5_K_q8_1_cuda(const void* vx, const void* vy,",
      "                                   scalar_t* dst, const int* topk_ids,",
      "                                   const int top_k, const int tokens,",
      "                                   const int ncols, const int nrows,",
      "                                   const int token_stride,",
      "                                   cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK_K, QI5_K, block_q5_K, VDR_Q5_K_Q8_1_MMVQ,",
      "            vec_dot_q5_K_q8_1><<<block_nums, block_dims, 0, stream>>>(",
      "      vx, vy, dst, topk_ids, top_k, ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_q6_K_q8_1_cuda(const void* vx, const void* vy,",
      "                                   scalar_t* dst, const int* topk_ids,",
      "                                   const int top_k, const int tokens,",
      "                                   const int ncols, const int nrows,",
      "                                   const int token_stride,",
      "                                   cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK_K, QI6_K, block_q6_K, VDR_Q6_K_Q8_1_MMVQ,",
      "            vec_dot_q6_K_q8_1><<<block_nums, block_dims, 0, stream>>>(",
      "      vx, vy, dst, topk_ids, top_k, ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_iq2_xxs_q8_1_cuda(const void* vx, const void* vy,",
      "                                      scalar_t* dst, const int* topk_ids,",
      "                                      const int top_k, const int tokens,",
      "                                      const int ncols, const int nrows,",
      "                                      const int token_stride,",
      "                                      cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK_K, QI2_XXS, block_iq2_xxs, 1, vec_dot_iq2_xxs_q8_1>",
      "      <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, topk_ids, top_k,",
      "                                              ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_iq2_xs_q8_1_cuda(const void* vx, const void* vy,",
      "                                     scalar_t* dst, const int* topk_ids,",
      "                                     const int top_k, const int tokens,",
      "                                     const int ncols, const int nrows,",
      "                                     const int token_stride,",
      "                                     cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK_K, QI2_XS, block_iq2_xs, 1, vec_dot_iq2_xs_q8_1>",
      "      <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, topk_ids, top_k,",
      "                                              ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_iq2_s_q8_1_cuda(const void* vx, const void* vy,",
      "                                    scalar_t* dst, const int* topk_ids,",
      "                                    const int top_k, const int tokens,",
      "                                    const int ncols, const int nrows,",
      "                                    const int token_stride,",
      "                                    cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK_K, QI2_S, block_iq2_s, 1, vec_dot_iq2_s_q8_1>",
      "      <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, topk_ids, top_k,",
      "                                              ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_iq3_xxs_q8_1_cuda(const void* vx, const void* vy,",
      "                                      scalar_t* dst, const int* topk_ids,",
      "                                      const int top_k, const int tokens,",
      "                                      const int ncols, const int nrows,",
      "                                      const int token_stride,",
      "                                      cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK_K, QI3_XXS, block_iq3_xxs, 1, vec_dot_iq3_xxs_q8_1>",
      "      <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, topk_ids, top_k,",
      "                                              ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_iq1_s_q8_1_cuda(const void* vx, const void* vy,",
      "                                    scalar_t* dst, const int* topk_ids,",
      "                                    const int top_k, const int tokens,",
      "                                    const int ncols, const int nrows,",
      "                                    const int token_stride,",
      "                                    cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK_K, QI1_S, block_iq1_s, 1, vec_dot_iq1_s_q8_1>",
      "      <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, topk_ids, top_k,",
      "                                              ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_iq1_m_q8_1_cuda(const void* vx, const void* vy,",
      "                                    scalar_t* dst, const int* topk_ids,",
      "                                    const int top_k, const int tokens,",
      "                                    const int ncols, const int nrows,",
      "                                    const int token_stride,",
      "                                    cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK_K, QI1_M, block_iq1_m, 1, vec_dot_iq1_m_q8_1>",
      "      <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, topk_ids, top_k,",
      "                                              ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_iq4_nl_q8_1_cuda(const void* vx, const void* vy,",
      "                                     scalar_t* dst, const int* topk_ids,",
      "                                     const int top_k, const int tokens,",
      "                                     const int ncols, const int nrows,",
      "                                     const int token_stride,",
      "                                     cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK4_NL, QI4_NL, block_iq4_nl, VDR_Q4_0_Q8_1_MMVQ,",
      "            vec_dot_iq4_nl_q8_1><<<block_nums, block_dims, 0, stream>>>(",
      "      vx, vy, dst, topk_ids, top_k, ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_iq4_xs_q8_1_cuda(const void* vx, const void* vy,",
      "                                     scalar_t* dst, const int* topk_ids,",
      "                                     const int top_k, const int tokens,",
      "                                     const int ncols, const int nrows,",
      "                                     const int token_stride,",
      "                                     cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK_K, QI4_XS, block_iq4_xs, 1, vec_dot_iq4_xs_q8_1>",
      "      <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, topk_ids, top_k,",
      "                                              ncols, nrows, token_stride);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void moe_vec_iq3_s_q8_1_cuda(const void* vx, const void* vy,",
      "                                    scalar_t* dst, const int* topk_ids,",
      "                                    const int top_k, const int tokens,",
      "                                    const int ncols, const int nrows,",
      "                                    const int token_stride,",
      "                                    cudaStream_t stream) {",
      "  const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "  const dim3 block_nums(block_num_y, 1, tokens * top_k);",
      "  const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "  moe_vec_q<scalar_t, QK_K, QI3_XS, block_iq3_s, 1, vec_dot_iq3_s_q8_1>",
      "      <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, topk_ids, top_k,",
      "                                              ncols, nrows, token_stride);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gguf/vecdotq.cuh",
    "source": [
      "// copied and adapted from https://github.com/ggerganov/llama.cpp/blob/b2899/ggml-cuda/vecdotq.cuh",
      "// and https://github.com/ggerganov/llama.cpp/blob/b2899/ggml-cuda/mmq.cu",
      "static __device__ __forceinline__ int get_int_b2(const void * x, const int & i32) {",
      "    const uint16_t * x16 = (const uint16_t *) x; // assume at least 2 byte alignment",
      "",
      "    int x32  = x16[2*i32 + 0] <<  0;",
      "    x32     |= x16[2*i32 + 1] << 16;",
      "",
      "    return x32;",
      "}",
      "",
      "static __device__ __forceinline__ int get_int_b4(const void * x, const int & i32) {",
      "    return ((const int *) x)[i32]; // assume at least 4 byte alignment",
      "}",
      "",
      "static __device__ __forceinline__ int get_int_from_int8(const int8_t * x8, const int & i32) {",
      "    const uint16_t * x16 = (const uint16_t *) (x8 + sizeof(int) * i32); // assume at least 2 byte alignment",
      "    int x32 = 0;",
      "    x32 |= x16[0] <<  0;",
      "    x32 |= x16[1] << 16;",
      "    return x32;",
      "}",
      "",
      "static __device__ __forceinline__ int get_int_from_uint8(const uint8_t * x8, const int & i32) {",
      "    const uint16_t * x16 = (const uint16_t *) (x8 + sizeof(int) * i32); // assume at least 2 byte alignment",
      "    int x32 = 0;",
      "    x32 |= x16[0] <<  0;",
      "    x32 |= x16[1] << 16;",
      "    return x32;",
      "}",
      "",
      "static __device__ __forceinline__ int get_int_from_int8_aligned(const int8_t * x8, const int & i32) {",
      "    return *((const int *) (x8 + sizeof(int) * i32)); // assume at least 4 byte alignment",
      "}",
      "",
      "static __device__ __forceinline__ int get_int_from_uint8_aligned(const uint8_t * x8, const int & i32) {",
      "    return *((const int *) (x8 + sizeof(int) * i32)); // assume at least 4 byte alignment",
      "}",
      "",
      "// VDR = vec dot ratio, how many contiguous integers each thread processes when the vec dot kernel is called",
      "// MMVQ = mul_mat_vec_q, MMQ = mul_mat_q",
      "",
      "#define VDR_Q4_0_Q8_1_MMVQ 2",
      "#define VDR_Q4_0_Q8_1_MMQ  4",
      "",
      "template <int vdr> static __device__ __forceinline__ float vec_dot_q4_0_q8_1_impl(",
      "    const int * v, const int * u, const float & d4, const half2 & ds8) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    int sumi = 0;",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < vdr; ++i) {",
      "        const int vi0 = (v[i] >> 0) & 0x0F0F0F0F;",
      "        const int vi1 = (v[i] >> 4) & 0x0F0F0F0F;",
      "",
      "        // SIMD dot product of quantized values",
      "        sumi = __dp4a(vi0, u[2*i+0], sumi);",
      "        sumi = __dp4a(vi1, u[2*i+1], sumi);",
      "    }",
      "",
      "    const float2 ds8f = __half22float2(ds8);",
      "",
      "    // second part effectively subtracts 8 from each quant value",
      "    return d4 * (sumi * ds8f.x - (8*vdr/QI4_0) * ds8f.y);",
      "#endif",
      "}",
      "",
      "#define VDR_Q4_1_Q8_1_MMVQ 2",
      "#define VDR_Q4_1_Q8_1_MMQ  4",
      "",
      "template <int vdr> static __device__ __forceinline__ float vec_dot_q4_1_q8_1_impl(",
      "    const int * v, const int * u, const half2 & dm4, const half2 & ds8) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    int sumi = 0;",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < vdr; ++i) {",
      "        const int vi0 = (v[i] >> 0) & 0x0F0F0F0F;",
      "        const int vi1 = (v[i] >> 4) & 0x0F0F0F0F;",
      "",
      "        // SIMD dot product of quantized values",
      "        sumi = __dp4a(vi0, u[2*i+0], sumi);",
      "        sumi = __dp4a(vi1, u[2*i+1], sumi);",
      "    }",
      "",
      "    const float2 tmp = __half22float2(__hmul2(dm4, ds8));",
      "    const float d4d8 = tmp.x;",
      "    const float m4s8 = tmp.y;",
      "",
      "    // scale second part of sum by QI8_1/(vdr * QR4_1) to compensate for multiple threads adding it",
      "    return sumi * d4d8 + m4s8 / (QI8_1 / (vdr * QR4_1));",
      "#endif",
      "}",
      "",
      "#define VDR_Q5_0_Q8_1_MMVQ 2",
      "#define VDR_Q5_0_Q8_1_MMQ  4",
      "",
      "template <int vdr> static __device__ __forceinline__ float vec_dot_q5_0_q8_1_impl(",
      "    const int * vl, const int * vh, const int * u, const float & d5, const half2 & ds8) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    int sumi = 0;",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < vdr; ++i) {",
      "        int vi0 = (vl[i] >>  0) & 0x0F0F0F0F; // lower 4 qs bits, still need qh as 5th bits",
      "        vi0    |= (vh[i] <<  4) & 0x00000010; // 0 ->  4",
      "        vi0    |= (vh[i] << 11) & 0x00001000; // 1 -> 12",
      "        vi0    |= (vh[i] << 18) & 0x00100000; // 2 -> 20",
      "        vi0    |= (vh[i] << 25) & 0x10000000; // 3 -> 28",
      "        sumi = __dp4a(vi0, u[2*i+0], sumi); // SIMD dot product of quantized values",
      "",
      "        int vi1 = (vl[i] >>  4) & 0x0F0F0F0F; // upper 4 qs bits, still need qh as 5th bits",
      "        vi1    |= (vh[i] >> 12) & 0x00000010; // 16 ->  4",
      "        vi1    |= (vh[i] >>  5) & 0x00001000; // 17 -> 12",
      "        vi1    |= (vh[i] <<  2) & 0x00100000; // 18 -> 20",
      "        vi1    |= (vh[i] <<  9) & 0x10000000; // 19 -> 28",
      "        sumi = __dp4a(vi1, u[2*i+1], sumi); // SIMD dot product of quantized values",
      "    }",
      "",
      "    const float2 ds8f = __half22float2(ds8);",
      "",
      "    // second part effectively subtracts 16 from each quant value",
      "    return d5 * (sumi * ds8f.x - (16*vdr/QI5_0) * ds8f.y);",
      "#endif",
      "}",
      "",
      "",
      "#define VDR_Q5_1_Q8_1_MMVQ 2",
      "#define VDR_Q5_1_Q8_1_MMQ  4",
      "",
      "template <int vdr> static __device__ __forceinline__ float vec_dot_q5_1_q8_1_impl(",
      "    const int * vl, const int * vh, const int * u, const half2 & dm5, const half2 & ds8) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    int sumi = 0;",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < vdr; ++i) {",
      "        int vi0 = (vl[i] >>  0) & 0x0F0F0F0F; // lower 4 qs bits, still need qh as 5th bits",
      "        vi0    |= (vh[i] <<  4) & 0x00000010; // 0 ->  4",
      "        vi0    |= (vh[i] << 11) & 0x00001000; // 1 -> 12",
      "        vi0    |= (vh[i] << 18) & 0x00100000; // 2 -> 20",
      "        vi0    |= (vh[i] << 25) & 0x10000000; // 3 -> 28",
      "        sumi = __dp4a(vi0, u[2*i+0], sumi); // SIMD dot product of quantized values",
      "",
      "        int vi1 = (vl[i] >>  4) & 0x0F0F0F0F; // upper 4 qs bits, still need qh as 5th bits",
      "        vi1    |= (vh[i] >> 12) & 0x00000010; // 16 ->  4",
      "        vi1    |= (vh[i] >>  5) & 0x00001000; // 17 -> 12",
      "        vi1    |= (vh[i] <<  2) & 0x00100000; // 18 -> 20",
      "        vi1    |= (vh[i] <<  9) & 0x10000000; // 19 -> 28",
      "        sumi = __dp4a(vi1, u[2*i+1], sumi); // SIMD dot product of quantized values",
      "    }",
      "",
      "    const float2 tmp = __half22float2(__hmul2(dm5, ds8));",
      "    const float d5d8 = tmp.x;",
      "    const float m5s8 = tmp.y;",
      "",
      "    // scale second part of sum by QI5_1 / vdr to compensate for multiple threads adding it",
      "    return sumi*d5d8 + m5s8 / (QI5_1 / vdr);",
      "#endif",
      "}",
      "",
      "#define VDR_Q8_0_Q8_1_MMVQ 2",
      "#define VDR_Q8_0_Q8_1_MMQ 8",
      "",
      "template <int vdr> static __device__ __forceinline__ float vec_dot_q8_0_q8_1_impl(",
      "    const int * v, const int * u, const float & d8_0, const float & d8_1) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    int sumi = 0;",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < vdr; ++i) {",
      "        // SIMD dot product of quantized values",
      "        sumi = __dp4a(v[i], u[i], sumi);",
      "    }",
      "    return d8_0*d8_1 * sumi;",
      "#endif",
      "}",
      "",
      "template <int vdr> static __device__ __forceinline__ float vec_dot_q8_1_q8_1_impl(",
      "    const int * v, const int * u, const half2 & dm8, const half2 & ds8) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "",
      "    int sumi = 0;",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < vdr; ++i) {",
      "        // SIMD dot product of quantized values",
      "        sumi = __dp4a(v[i], u[i], sumi);",
      "    }",
      "",
      "    const float2 tmp = __half22float2(__hmul2(dm8, ds8));",
      "    const float d8d8 = tmp.x;",
      "    const float m8s8 = tmp.y;",
      "",
      "    // scale second part of sum by QI8_1/ vdr to compensate for multiple threads adding it",
      "    return sumi*d8d8 + m8s8 / (QI8_1 / vdr);",
      "#endif",
      "}",
      "",
      "#define VDR_Q2_K_Q8_1_MMVQ 1",
      "#define VDR_Q2_K_Q8_1_MMQ  2",
      "",
      "// contiguous v/x values",
      "static __device__ __forceinline__ float vec_dot_q2_K_q8_1_impl_mmvq(",
      "    const int & v, const int * __restrict__ u, const uint8_t * __restrict__ scales,",
      "    const half2 & dm2, const float * __restrict__ d8) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    float sumf_d = 0.0f;",
      "    float sumf_m = 0.0f;",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < QR2_K; ++i) {",
      "        const int sc = scales[2*i];",
      "",
      "        const int vi = (v >> (2*i)) & 0x03030303;",
      "",
      "        sumf_d += d8[i] * (__dp4a(vi, u[i], 0) * (sc & 0xF)); // SIMD dot product",
      "",
      "        // fill int with 4x m",
      "        int m = sc >> 4;",
      "        m |= m <<  8;",
      "        m |= m << 16;",
      "        sumf_m += d8[i] * __dp4a(m, u[i], 0); // multiply constant q2_K part with sum of q8_1 values",
      "    }",
      "",
      "    const float2 dm2f = __half22float2(dm2);",
      "",
      "    return dm2f.x*sumf_d - dm2f.y*sumf_m;",
      "#endif",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q2_K_q8_1_impl_mmq(",
      "    const int * __restrict__ v, const int * __restrict__ u, const uint8_t * __restrict__ scales,",
      "    const half2 & dm2, const float & d8) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    int sumi_d = 0;",
      "    int sumi_m = 0;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < QI8_1; i0 += QI8_1/2) {",
      "        int sumi_d_sc = 0;",
      "",
      "        const int sc = scales[i0 / (QI8_1/2)];",
      "",
      "        // fill int with 4x m",
      "        int m = sc >> 4;",
      "        m |= m <<  8;",
      "        m |= m << 16;",
      "",
      "#pragma unroll",
      "        for (int i = i0; i < i0 + QI8_1/2; ++i) {",
      "            sumi_d_sc = __dp4a(v[i], u[i], sumi_d_sc); // SIMD dot product",
      "            sumi_m    = __dp4a(m,    u[i], sumi_m); // multiply sum of q8_1 values with m",
      "        }",
      "",
      "        sumi_d += sumi_d_sc * (sc & 0xF);",
      "    }",
      "",
      "    const float2 dm2f = __half22float2(dm2);",
      "",
      "    return d8 * (dm2f.x*sumi_d - dm2f.y*sumi_m);",
      "#endif",
      "}",
      "",
      "#define VDR_Q3_K_Q8_1_MMVQ 1",
      "#define VDR_Q3_K_Q8_1_MMQ  2",
      "",
      "// contiguous v/x values",
      "static __device__ __forceinline__ float vec_dot_q3_K_q8_1_impl_mmvq(",
      "    const int & vl, const int & vh, const int * __restrict__ u, const uint8_t * __restrict__ scales,",
      "    const int & scale_offset, const float & d3, const float * __restrict__ d8) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "",
      "    float sumf = 0.0f;",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < QR3_K; ++i) {",
      "        const int isc = scale_offset + 2*i;",
      "",
      "        const int isc_low = isc % (QK_K/32);",
      "        const int sc_shift_low = 4 * (isc / (QK_K/32));",
      "        const int sc_low  = (scales[isc_low] >> sc_shift_low) & 0xF;",
      "",
      "        const int isc_high = isc % (QK_K/64);",
      "        const int sc_shift_high = 2 * (isc / (QK_K/64));",
      "        const int sc_high = ((scales[(QK_K/32) + isc_high] >> sc_shift_high) & 3) << 4;",
      "",
      "        const int sc = (sc_low | sc_high) - 32;",
      "",
      "        const int vil = (vl >> (2*i)) & 0x03030303;",
      "",
      "        const int vih = ((vh >> i) << 2) & 0x04040404;",
      "",
      "        const int vi = __vsubss4(vil, vih);",
      "",
      "        sumf += d8[i] * (__dp4a(vi, u[i], 0) * sc); // SIMD dot product",
      "    }",
      "",
      "    return d3 * sumf;",
      "#endif",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q3_K_q8_1_impl_mmq(",
      "    const int * __restrict__ v, const int * __restrict__ u, const int8_t * __restrict__ scales,",
      "    const float & d3, const float & d8) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    int sumi = 0;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < QR3_K*VDR_Q3_K_Q8_1_MMQ; i0 += QI8_1/2) {",
      "        int sumi_sc = 0;",
      "",
      "        for (int i = i0; i < i0 + QI8_1/2; ++i) {",
      "            sumi_sc = __dp4a(v[i], u[i], sumi_sc); // SIMD dot product",
      "        }",
      "",
      "        sumi += sumi_sc * scales[i0 / (QI8_1/2)];",
      "    }",
      "",
      "    return d3*d8 * sumi;",
      "#endif",
      "}",
      "",
      "#define VDR_Q4_K_Q8_1_MMVQ 2",
      "#define VDR_Q4_K_Q8_1_MMQ  8",
      "",
      "// contiguous v/x values",
      "static __device__ __forceinline__ float vec_dot_q4_K_q8_1_impl_vmmq(",
      "    const int * __restrict__ v, const int * __restrict__ u, const uint8_t * __restrict__ sc,",
      "    const uint8_t * __restrict__ m, const half2 & dm4, const float * __restrict__ d8) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "",
      "    float sumf_d = 0.0f;",
      "    float sumf_m = 0.0f;",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < QR4_K; ++i) {",
      "        const int v0i = (v[0] >> (4*i)) & 0x0F0F0F0F;",
      "        const int v1i = (v[1] >> (4*i)) & 0x0F0F0F0F;",
      "",
      "        const int dot1 = __dp4a(v1i, u[2*i+1], __dp4a(v0i, u[2*i+0], 0)); // SIMD dot product",
      "        const int dot2 = __dp4a(0x01010101, u[2*i+1], __dp4a(0x01010101, u[2*i+0], 0)); // sum of u",
      "",
      "        sumf_d += d8[i] * (dot1 * sc[i]);",
      "        sumf_m += d8[i] * (dot2 * m[i]);  // multiply constant part of q4_K with sum of q8_1 values",
      "    }",
      "",
      "    const float2 dm4f = __half22float2(dm4);",
      "    return dm4f.x*sumf_d - dm4f.y*sumf_m;",
      "#endif",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q4_K_q8_1_impl_mmq(",
      "    const int * __restrict__ v, const int * __restrict__ u, const uint8_t * __restrict__ sc,",
      "    const uint8_t * __restrict__ m, const half2 & dm4, const half2 * __restrict__ ds8) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    float sumf_d = 0.0f;",
      "    float sumf_m = 0.0f;",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < QR4_K*VDR_Q4_K_Q8_1_MMQ/QI8_1; ++i) {",
      "        int sumi_d = 0;",
      "",
      "#pragma unroll",
      "        for (int j = 0; j < QI8_1; ++j) {",
      "            sumi_d = __dp4a((v[j] >> (4*i)) & 0x0F0F0F0F, u[i*QI8_1 + j], sumi_d); // SIMD dot product",
      "        }",
      "",
      "        const float2 ds8f = __half22float2(ds8[i]);",
      "",
      "        sumf_d += ds8f.x * (sc[i] * sumi_d);",
      "        sumf_m += ds8f.y *   m[i]; // sum of q8_1 block * q4_K min val",
      "    }",
      "",
      "    const float2 dm4f = __half22float2(dm4);",
      "",
      "    return dm4f.x*sumf_d - dm4f.y*sumf_m;",
      "#endif",
      "}",
      "",
      "#define VDR_Q5_K_Q8_1_MMVQ 2",
      "#define VDR_Q5_K_Q8_1_MMQ  8",
      "",
      "static __device__ __forceinline__ float vec_dot_q5_K_q8_1_impl_vmmq(",
      "    const int * __restrict__ vl, const int * __restrict__ vh, const int * __restrict__ u, const uint8_t * __restrict__ sc,",
      "    const uint8_t * __restrict__ m, const half2 & dm5, const float * __restrict__ d8) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "",
      "    float sumf_d = 0.0f;",
      "    float sumf_m = 0.0f;",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < QR5_K; ++i) {",
      "        const int vl0i = (vl[0] >> (4*i)) & 0x0F0F0F0F;",
      "        const int vl1i = (vl[1] >> (4*i)) & 0x0F0F0F0F;",
      "",
      "        const int vh0i = ((vh[0] >> i) << 4) & 0x10101010;",
      "        const int vh1i = ((vh[1] >> i) << 4) & 0x10101010;",
      "",
      "        const int v0i = vl0i | vh0i;",
      "        const int v1i = vl1i | vh1i;",
      "",
      "        const int dot1 = __dp4a(v0i, u[2*i+0], __dp4a(v1i, u[2*i+1], 0)); // SIMD dot product",
      "        const int dot2 = __dp4a(0x01010101, u[2*i+0], __dp4a(0x01010101, u[2*i+1], 0)); // sum of u",
      "",
      "        sumf_d += d8[i] * (dot1 * sc[i]);",
      "        sumf_m += d8[i] * (dot2 * m[i]);",
      "    }",
      "",
      "    const float2 dm5f = __half22float2(dm5);",
      "    return dm5f.x*sumf_d - dm5f.y*sumf_m;",
      "#endif",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q5_K_q8_1_impl_mmq(",
      "    const int * __restrict__ v, const int * __restrict__ u, const uint8_t * __restrict__ sc,",
      "    const uint8_t * __restrict__ m, const half2 & dm4, const half2 * __restrict__ ds8) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    float sumf_d = 0.0f;",
      "    float sumf_m = 0.0f;",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < QR5_K*VDR_Q5_K_Q8_1_MMQ/QI8_1; ++i) {",
      "        int sumi_d = 0;",
      "",
      "#pragma unroll",
      "        for (int j = 0; j < QI8_1; ++j) {",
      "            sumi_d = __dp4a(v[i*QI8_1 + j], u[i*QI8_1 + j], sumi_d); // SIMD dot product",
      "        }",
      "",
      "        const float2 ds8f = __half22float2(ds8[i]);",
      "",
      "        sumf_d += ds8f.x * (sc[i] * sumi_d);",
      "        sumf_m += ds8f.y *   m[i]; // sum of q8_1 block * q4_K min val",
      "    }",
      "",
      "    const float2 dm4f = __half22float2(dm4);",
      "",
      "    return dm4f.x*sumf_d - dm4f.y*sumf_m;",
      "#endif",
      "}",
      "",
      "#define VDR_Q6_K_Q8_1_MMVQ 1",
      "#define VDR_Q6_K_Q8_1_MMQ  8",
      "",
      "// contiguous v/x values",
      "static __device__ __forceinline__ float vec_dot_q6_K_q8_1_impl_mmvq(",
      "    const int & vl, const int & vh, const int * __restrict__ u, const int8_t * __restrict__ scales,",
      "    const float & d, const float * __restrict__ d8) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    float sumf = 0.0f;",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < QR6_K; ++i) {",
      "        const int sc = scales[4*i];",
      "        const int vil = (vl >> (4*i)) & 0x0F0F0F0F;",
      "        const int vih = ((vh >> (4*i)) << 4) & 0x30303030;",
      "        const int vi = __vsubss4((vil | vih), 0x20202020); // vi = (vil | vih) - 32",
      "",
      "        sumf += d8[i] * (__dp4a(vi, u[i], 0) * sc); // SIMD dot product",
      "    }",
      "",
      "    return d*sumf;",
      "#endif",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q6_K_q8_1_impl_mmq(",
      "    const int * __restrict__ v, const int * __restrict__ u, const int8_t * __restrict__ sc,",
      "    const float & d6, const float * __restrict__ d8) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    float sumf_d = 0.0f;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < VDR_Q6_K_Q8_1_MMQ; i0 += 4) {",
      "        int2 sumi_d = {0, 0}; // 2 q6_K scales per q8_1 scale",
      "",
      "#pragma unroll",
      "        for (int i = i0; i < i0 + 2; ++i) {",
      "            sumi_d.x = __dp4a(v[2*i+0], u[2*i+0], sumi_d.x); // SIMD dot product",
      "            sumi_d.x = __dp4a(v[2*i+1], u[2*i+1], sumi_d.x); // SIMD dot product",
      "",
      "            sumi_d.y = __dp4a(v[2*i+4], u[2*i+4], sumi_d.y); // SIMD dot product",
      "            sumi_d.y = __dp4a(v[2*i+5], u[2*i+5], sumi_d.y); // SIMD dot product",
      "        }",
      "",
      "        sumf_d += d8[i0/4] * (sc[i0/2+0]*sumi_d.x + sc[i0/2+1]*sumi_d.y);",
      "    }",
      "",
      "    return d6 * sumf_d;",
      "#endif",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q4_0_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "",
      "    const block_q4_0 * bq4_0 = (const block_q4_0 *) vbq;",
      "",
      "    int v[VDR_Q4_0_Q8_1_MMVQ];",
      "    int u[2*VDR_Q4_0_Q8_1_MMVQ];",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < VDR_Q4_0_Q8_1_MMVQ; ++i) {",
      "        v[i]     = get_int_from_uint8(bq4_0->qs, iqs + i);",
      "        u[2*i+0] = get_int_from_int8_aligned(bq8_1->qs, iqs + i);",
      "        u[2*i+1] = get_int_from_int8_aligned(bq8_1->qs, iqs + i + QI4_0);",
      "    }",
      "",
      "    return vec_dot_q4_0_q8_1_impl<VDR_Q4_0_Q8_1_MMVQ>(v, u, __half2float(bq4_0->d), bq8_1->ds);",
      "}",
      "",
      "template <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q4_0(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {",
      "    __shared__ int  tile_x_qs[mmq_y * (WARP_SIZE_GGUF)       + mmq_y];",
      "    __shared__ float tile_x_d[mmq_y * (WARP_SIZE_GGUF/QI4_0) + mmq_y/QI4_0];",
      "    *x_ql = tile_x_qs;",
      "    *x_dm = (half2 *) tile_x_d;",
      "}",
      "",
      "template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(",
      "    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,",
      "    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {",
      "    const int kbx  = k / QI4_0;",
      "    const int kqsx = k % QI4_0;",
      "",
      "    const block_q4_0 * bx0 = (const block_q4_0 *) vx;",
      "    float * x_dmf = (float *) x_dm;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {",
      "        int i = i0 + i_offset;",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q4_0 * bxi = bx0 + i*blocks_per_row + kbx;",
      "        x_ql[i * (WARP_SIZE_GGUF + 1) + k] = get_int_from_uint8(bxi->qs, kqsx);",
      "        // x_dmf[i * (WARP_SIZE_GGUF/QI4_0) + i / QI4_0 + kbx] = bxi->d;",
      "    }",
      "",
      "    const int blocks_per_tile_x_row = WARP_SIZE_GGUF / QI4_0;",
      "    const int kbxd = k % blocks_per_tile_x_row;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI4_0) {",
      "        int i = i0 + i_offset * QI4_0 + k / blocks_per_tile_x_row;",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q4_0 * bxi = bx0 + i*blocks_per_row + kbxd;",
      "        x_dmf[i * (WARP_SIZE_GGUF/QI4_0) + i / QI4_0 + kbxd] = __half2float(bxi->d);",
      "    }",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q4_0_q8_1_mul_mat(",
      "    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,",
      "    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {",
      "    (void)x_qh; (void)x_sc;",
      "",
      "    const int kyqs = k % (QI8_1/2) + QI8_1 * (k / (QI8_1/2));",
      "    const float * x_dmf = (const float *) x_dm;",
      "",
      "    int u[2*VDR_Q4_0_Q8_1_MMQ];",
      "",
      "#pragma unroll",
      "    for (int l = 0; l < VDR_Q4_0_Q8_1_MMQ; ++l) {",
      "        u[2*l+0] = y_qs[j * WARP_SIZE_GGUF + (kyqs + l)         % WARP_SIZE_GGUF];",
      "        u[2*l+1] = y_qs[j * WARP_SIZE_GGUF + (kyqs + l + QI4_0) % WARP_SIZE_GGUF];",
      "    }",
      "",
      "    return vec_dot_q4_0_q8_1_impl<VDR_Q4_0_Q8_1_MMQ>",
      "        (&x_ql[i * (WARP_SIZE_GGUF + 1) + k], u, x_dmf[i * (WARP_SIZE_GGUF/QI4_0) + i/QI4_0 + k/QI4_0],",
      "         y_ds[j * (WARP_SIZE_GGUF/QI8_1) + (2*k/QI8_1) % (WARP_SIZE_GGUF/QI8_1)]);",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q4_1_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "",
      "    const block_q4_1 * bq4_1 = (const block_q4_1 *) vbq;",
      "",
      "    int v[VDR_Q4_1_Q8_1_MMVQ];",
      "    int u[2*VDR_Q4_1_Q8_1_MMVQ];",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < VDR_Q4_1_Q8_1_MMVQ; ++i) {",
      "        v[i]    = get_int_from_uint8_aligned(bq4_1->qs, iqs + i);",
      "        u[2*i+0] = get_int_from_int8_aligned(bq8_1->qs, iqs + i);",
      "        u[2*i+1] = get_int_from_int8_aligned(bq8_1->qs, iqs + i + QI4_1);",
      "    }",
      "",
      "    return vec_dot_q4_1_q8_1_impl<VDR_Q4_1_Q8_1_MMVQ>(v, u, bq4_1->dm, bq8_1->ds);",
      "}",
      "",
      "template <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q4_1(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {",
      "    __shared__ int   tile_x_qs[mmq_y * (WARP_SIZE_GGUF) +     + mmq_y];",
      "    __shared__ half2 tile_x_dm[mmq_y * (WARP_SIZE_GGUF/QI4_1) + mmq_y/QI4_1];",
      "    *x_ql = tile_x_qs;",
      "    *x_dm = tile_x_dm;",
      "}",
      "",
      "template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q4_1(",
      "    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,",
      "    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {",
      "    const int kbx  = k / QI4_1;",
      "    const int kqsx = k % QI4_1;",
      "",
      "    const block_q4_1 * bx0 = (const block_q4_1 *) vx;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {",
      "        int i = i0 + i_offset;",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q4_1 * bxi = bx0 + i*blocks_per_row + kbx;",
      "        x_ql[i * (WARP_SIZE_GGUF + 1) + k] = get_int_from_uint8_aligned(bxi->qs, kqsx);",
      "    }",
      "",
      "    const int blocks_per_tile_x_row = WARP_SIZE_GGUF / QI4_1;",
      "    const int kbxd = k % blocks_per_tile_x_row;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI4_1) {",
      "        int i = i0 + i_offset * QI4_1 + k / blocks_per_tile_x_row;",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q4_1 * bxi = bx0 + i*blocks_per_row + kbxd;",
      "        x_dm[i * (WARP_SIZE_GGUF/QI4_1) + i / QI4_1 + kbxd] = bxi->dm;",
      "    }",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q4_1_q8_1_mul_mat(",
      "    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,",
      "    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {",
      "    const int kyqs = k % (QI8_1/2) + QI8_1 * (k / (QI8_1/2));",
      "",
      "    int u[2*VDR_Q4_1_Q8_1_MMQ];",
      "",
      "#pragma unroll",
      "    for (int l = 0; l < VDR_Q4_1_Q8_1_MMQ; ++l) {",
      "        u[2*l+0] = y_qs[j * WARP_SIZE_GGUF + (kyqs + l)         % WARP_SIZE_GGUF];",
      "        u[2*l+1] = y_qs[j * WARP_SIZE_GGUF + (kyqs + l + QI4_1) % WARP_SIZE_GGUF];",
      "    }",
      "",
      "    return vec_dot_q4_1_q8_1_impl<VDR_Q4_1_Q8_1_MMQ>",
      "        (&x_ql[i * (WARP_SIZE_GGUF + 1) + k], u, x_dm[i * (WARP_SIZE_GGUF/QI4_1) + i/QI4_1 + k/QI4_1],",
      "         y_ds[j * (WARP_SIZE_GGUF/QI8_1) + (2*k/QI8_1) % (WARP_SIZE_GGUF/QI8_1)]);",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q5_0_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "",
      "    const block_q5_0 * bq5_0 = (const block_q5_0 *) vbq;",
      "",
      "    int vl[VDR_Q5_0_Q8_1_MMVQ];",
      "    int vh[VDR_Q5_0_Q8_1_MMVQ];",
      "    int  u[2*VDR_Q5_0_Q8_1_MMVQ];",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < VDR_Q5_0_Q8_1_MMVQ; ++i) {",
      "        vl[i]    = get_int_from_uint8(bq5_0->qs, iqs + i);",
      "        vh[i]    = get_int_from_uint8(bq5_0->qh, 0) >> (4 * (iqs + i));",
      "        u[2*i+0] = get_int_from_int8_aligned(bq8_1->qs, iqs + i);",
      "        u[2*i+1] = get_int_from_int8_aligned(bq8_1->qs, iqs + i + QI5_0);",
      "    }",
      "",
      "    return vec_dot_q5_0_q8_1_impl<VDR_Q5_0_Q8_1_MMVQ>(vl, vh, u, __half2float(bq5_0->d), bq8_1->ds);",
      "}",
      "",
      "template <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q5_0(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {",
      "    __shared__ int  tile_x_ql[mmq_y * (2*WARP_SIZE_GGUF)     + mmq_y];",
      "    __shared__ float tile_x_d[mmq_y * (WARP_SIZE_GGUF/QI5_0) + mmq_y/QI5_0];",
      "",
      "    *x_ql = tile_x_ql;",
      "    *x_dm = (half2 *) tile_x_d;",
      "}",
      "",
      "template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q5_0(",
      "    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,",
      "    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {",
      "    const int kbx  = k / QI5_0;",
      "    const int kqsx = k % QI5_0;",
      "",
      "    const block_q5_0 * bx0 = (const block_q5_0 *) vx;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {",
      "        int i = i0 + i_offset;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q5_0 * bxi = bx0 + i*blocks_per_row + kbx;",
      "        const int ql = get_int_from_uint8(bxi->qs, kqsx);",
      "        const int qh = get_int_from_uint8(bxi->qh, 0) >> (4 * (k % QI5_0));",
      "",
      "        int qs0 = (ql >>  0)   & 0x0F0F0F0F;",
      "        qs0    |= (qh <<  4)   & 0x00000010;  // 0 ->  4",
      "        qs0    |= (qh << 11)   & 0x00001000;  // 1 -> 12",
      "        qs0    |= (qh << 18)   & 0x00100000;  // 2 -> 20",
      "        qs0    |= (qh << 25)   & 0x10000000;  // 3 -> 28",
      "        qs0     = __vsubss4(qs0, 0x10101010); // subtract 16",
      "",
      "        x_ql[i * (2*WARP_SIZE_GGUF + 1) + 2*k+0] = qs0;",
      "",
      "        int qs1 = (ql >>  4)   & 0x0F0F0F0F;",
      "        qs1    |= (qh >> 12)   & 0x00000010;  // 16 ->  4",
      "        qs1    |= (qh >>  5)   & 0x00001000;  // 17 -> 12",
      "        qs1    |= (qh <<  2)   & 0x00100000;  // 18 -> 20",
      "        qs1    |= (qh <<  9)   & 0x10000000;  // 19 -> 28",
      "        qs1     = __vsubss4(qs1, 0x10101010); // subtract 16",
      "",
      "        x_ql[i * (2*WARP_SIZE_GGUF + 1) + 2*k+1] = qs1;",
      "    }",
      "",
      "    const int blocks_per_tile_x_row = WARP_SIZE_GGUF / QI5_0;",
      "    const int kbxd = k % blocks_per_tile_x_row;",
      "    float * x_dmf = (float *) x_dm;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI5_0) {",
      "        int i = i0 + i_offset * QI5_0 + k / blocks_per_tile_x_row;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "",
      "        const block_q5_0 * bxi = bx0 + i*blocks_per_row + kbxd;",
      "        x_dmf[i * (WARP_SIZE_GGUF/QI5_0) + i / QI5_0 + kbxd] = __half2float(bxi->d);",
      "    }",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q5_0_q8_1_mul_mat(",
      "    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,",
      "    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {",
      "    const int kyqs = k % (QI8_1/2) + QI8_1 * (k / (QI8_1/2));",
      "    const int index_bx = i * (WARP_SIZE_GGUF/QI5_0) + i/QI5_0 + k/QI5_0;",
      "    const float * x_dmf = (const float *) x_dm;",
      "    const float * y_df  = (const float *) y_ds;",
      "",
      "    int u[2*VDR_Q5_0_Q8_1_MMQ];",
      "",
      "#pragma unroll",
      "    for (int l = 0; l < VDR_Q5_0_Q8_1_MMQ; ++l) {",
      "        u[2*l+0] = y_qs[j * WARP_SIZE_GGUF + (kyqs + l)         % WARP_SIZE_GGUF];",
      "        u[2*l+1] = y_qs[j * WARP_SIZE_GGUF + (kyqs + l + QI5_0) % WARP_SIZE_GGUF];",
      "    }",
      "",
      "    return vec_dot_q8_0_q8_1_impl<QR5_0*VDR_Q5_0_Q8_1_MMQ>",
      "        (&x_ql[i * (2*WARP_SIZE_GGUF + 1) + 2 * k], u, x_dmf[index_bx], y_df[j * (WARP_SIZE_GGUF/QI8_1) + (2*k/QI8_1) % (WARP_SIZE_GGUF/QI8_1)]);",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q5_1_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "",
      "    const block_q5_1 * bq5_1 = (const block_q5_1 *) vbq;",
      "",
      "    int vl[VDR_Q5_1_Q8_1_MMVQ];",
      "    int vh[VDR_Q5_1_Q8_1_MMVQ];",
      "    int  u[2*VDR_Q5_1_Q8_1_MMVQ];",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < VDR_Q5_1_Q8_1_MMVQ; ++i) {",
      "        vl[i]   = get_int_from_uint8_aligned(bq5_1->qs, iqs + i);",
      "        vh[i]   = get_int_from_uint8_aligned(bq5_1->qh, 0) >> (4 * (iqs + i));",
      "        u[2*i+0] = get_int_from_int8_aligned(bq8_1->qs, iqs + i);",
      "        u[2*i+1] = get_int_from_int8_aligned(bq8_1->qs, iqs + i + QI5_1);",
      "    }",
      "",
      "    return vec_dot_q5_1_q8_1_impl<VDR_Q5_1_Q8_1_MMVQ>(vl, vh, u, bq5_1->dm, bq8_1->ds);",
      "}",
      "",
      "template <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q5_1(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {",
      "    __shared__ int   tile_x_ql[mmq_y * (2*WARP_SIZE_GGUF)     + mmq_y];",
      "    __shared__ half2 tile_x_dm[mmq_y * (WARP_SIZE_GGUF/QI5_1) + mmq_y/QI5_1];",
      "",
      "    *x_ql = tile_x_ql;",
      "    *x_dm = tile_x_dm;",
      "}",
      "",
      "template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q5_1(",
      "    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,",
      "    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {",
      "    const int kbx  = k / QI5_1;",
      "    const int kqsx = k % QI5_1;",
      "",
      "    const block_q5_1 * bx0 = (const block_q5_1 *) vx;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {",
      "        int i = i0 + i_offset;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "",
      "        const block_q5_1 * bxi = bx0 + i*blocks_per_row + kbx;",
      "",
      "        const int ql = get_int_from_uint8_aligned(bxi->qs, kqsx);",
      "        const int qh = get_int_from_uint8_aligned(bxi->qh, 0) >> (4 * (k % QI5_1));",
      "",
      "        int qs0 = (ql >>  0) & 0x0F0F0F0F;",
      "        qs0    |= (qh <<  4) & 0x00000010; // 0 ->  4",
      "        qs0    |= (qh << 11) & 0x00001000; // 1 -> 12",
      "        qs0    |= (qh << 18) & 0x00100000; // 2 -> 20",
      "        qs0    |= (qh << 25) & 0x10000000; // 3 -> 28",
      "",
      "        x_ql[i * (2*WARP_SIZE_GGUF + 1) + 2*k+0] = qs0;",
      "",
      "        int qs1 = (ql >>  4) & 0x0F0F0F0F;",
      "        qs1    |= (qh >> 12) & 0x00000010; // 16 ->  4",
      "        qs1    |= (qh >>  5) & 0x00001000; // 17 -> 12",
      "        qs1    |= (qh <<  2) & 0x00100000; // 18 -> 20",
      "        qs1    |= (qh <<  9) & 0x10000000; // 19 -> 28",
      "",
      "        x_ql[i * (2*WARP_SIZE_GGUF + 1) + 2*k+1] = qs1;",
      "    }",
      "",
      "    const int blocks_per_tile_x_row = WARP_SIZE_GGUF / QI5_1;",
      "    const int kbxd = k % blocks_per_tile_x_row;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI5_1) {",
      "        int i = i0 + i_offset * QI5_1 + k / blocks_per_tile_x_row;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "",
      "        const block_q5_1 * bxi = bx0 + i*blocks_per_row + kbxd;",
      "",
      "        x_dm[i * (WARP_SIZE_GGUF/QI5_1) + i / QI5_1 + kbxd] = bxi->dm;",
      "    }",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q5_1_q8_1_mul_mat(",
      "    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,",
      "    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {",
      "    const int kyqs = k % (QI8_1/2) + QI8_1 * (k / (QI8_1/2));",
      "    const int index_bx = i * (WARP_SIZE_GGUF/QI5_1) + + i/QI5_1 + k/QI5_1;",
      "",
      "    int u[2*VDR_Q5_1_Q8_1_MMQ];",
      "",
      "#pragma unroll",
      "    for (int l = 0; l < VDR_Q5_1_Q8_1_MMQ; ++l) {",
      "        u[2*l+0] = y_qs[j * WARP_SIZE_GGUF + (kyqs + l)         % WARP_SIZE_GGUF];",
      "        u[2*l+1] = y_qs[j * WARP_SIZE_GGUF + (kyqs + l + QI5_1) % WARP_SIZE_GGUF];",
      "    }",
      "",
      "    return vec_dot_q8_1_q8_1_impl<QR5_1*VDR_Q5_1_Q8_1_MMQ>",
      "        (&x_ql[i * (2*WARP_SIZE_GGUF + 1) + 2 * k], u, x_dm[index_bx], y_ds[j * (WARP_SIZE_GGUF/QI8_1) + (2*k/QI8_1) % (WARP_SIZE_GGUF/QI8_1)]);",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q8_0_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "",
      "    const block_q8_0 * bq8_0 = (const block_q8_0 *) vbq;",
      "",
      "    int v[VDR_Q8_0_Q8_1_MMVQ];",
      "    int u[VDR_Q8_0_Q8_1_MMVQ];",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < VDR_Q8_0_Q8_1_MMVQ; ++i) {",
      "        v[i] = get_int_from_int8(bq8_0->qs, iqs + i);",
      "        u[i] = get_int_from_int8_aligned(bq8_1->qs, iqs + i);",
      "    }",
      "",
      "    return vec_dot_q8_0_q8_1_impl<VDR_Q8_0_Q8_1_MMVQ>(v, u, __half2float(bq8_0->d), __low2float(bq8_1->ds));",
      "}",
      "",
      "template <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q8_0(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {",
      "    __shared__ int  tile_x_qs[mmq_y * (WARP_SIZE_GGUF)       + mmq_y];",
      "    __shared__ float tile_x_d[mmq_y * (WARP_SIZE_GGUF/QI8_0) + mmq_y/QI8_0];",
      "",
      "    *x_ql = tile_x_qs;",
      "    *x_dm = (half2 *) tile_x_d;",
      "}",
      "",
      "template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q8_0(",
      "    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,",
      "    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {",
      "    const int kbx  = k / QI8_0;",
      "    const int kqsx = k % QI8_0;",
      "    float * x_dmf = (float *) x_dm;",
      "",
      "    const block_q8_0 * bx0 = (const block_q8_0 *) vx;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {",
      "        int i = i0 + i_offset;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q8_0 * bxi = bx0 + i*blocks_per_row + kbx;",
      "        x_ql[i * (WARP_SIZE_GGUF + 1) + k] = get_int_from_int8(bxi->qs, kqsx);",
      "    }",
      "",
      "    const int blocks_per_tile_x_row = WARP_SIZE_GGUF / QI8_0;",
      "    const int kbxd = k % blocks_per_tile_x_row;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI8_0) {",
      "        int i = i0 + i_offset * QI8_0 + k / blocks_per_tile_x_row;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q8_0 * bxi = bx0 + i*blocks_per_row + kbxd;",
      "        x_dmf[i * (WARP_SIZE_GGUF/QI8_0) + i / QI8_0 + kbxd] = __half2float(bxi->d);",
      "    }",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q8_0_q8_1_mul_mat(",
      "    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,",
      "    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {",
      "    const float * x_dmf = (const float *) x_dm;",
      "    const float * y_df  = (const float *) y_ds;",
      "",
      "    return vec_dot_q8_0_q8_1_impl<VDR_Q8_0_Q8_1_MMQ>",
      "        (&x_ql[i * (WARP_SIZE_GGUF + 1) + k], &y_qs[j * WARP_SIZE_GGUF + k], x_dmf[i * (WARP_SIZE_GGUF/QI8_0) + i/QI8_0 + k/QI8_0],",
      "         y_df[j * (WARP_SIZE_GGUF/QI8_1) + k/QI8_1]);",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q2_K_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "",
      "    const block_q2_K * bq2_K = (const block_q2_K *) vbq;",
      "",
      "    const int bq8_offset = QR2_K * (iqs / QI8_1);",
      "    const int scale_offset = iqs - iqs % QI8_1 + (iqs % QI8_1) / (QI8_1/2);",
      "",
      "    const uint8_t * scales = bq2_K->scales + scale_offset;",
      "",
      "    const int v = get_int_from_uint8_aligned(bq2_K->qs, iqs);",
      "    int    u[QR2_K];",
      "    float d8[QR2_K];",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < QR2_K; ++ i) {",
      "        u[i]  = get_int_from_int8_aligned(bq8_1[bq8_offset + i].qs, iqs % QI8_1);",
      "        d8[i] = __low2float(bq8_1[bq8_offset + i].ds);",
      "    }",
      "",
      "    return vec_dot_q2_K_q8_1_impl_mmvq(v, u, scales, bq2_K->dm, d8);",
      "}",
      "",
      "template <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q2_K(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {",
      "    __shared__ int   tile_x_ql[mmq_y * (WARP_SIZE_GGUF)       + mmq_y];",
      "    __shared__ half2 tile_x_dm[mmq_y * (WARP_SIZE_GGUF/QI2_K) + mmq_y/QI2_K];",
      "    __shared__ int   tile_x_sc[mmq_y * (WARP_SIZE_GGUF/4)     + mmq_y/4];",
      "",
      "    *x_ql = tile_x_ql;",
      "    *x_dm = tile_x_dm;",
      "    *x_sc = tile_x_sc;",
      "}",
      "",
      "template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q2_K(",
      "    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,",
      "    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {",
      "    const int kbx  = k / QI2_K;",
      "    const int kqsx = k % QI2_K;",
      "",
      "    const block_q2_K * bx0 = (const block_q2_K *) vx;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {",
      "        int i = i0 + i_offset;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q2_K * bxi = bx0 + i*blocks_per_row + kbx;",
      "        x_ql[i * (WARP_SIZE_GGUF + 1) + k] = get_int_from_uint8_aligned(bxi->qs, kqsx);",
      "    }",
      "",
      "    const int blocks_per_tile_x_row = WARP_SIZE_GGUF / QI2_K;",
      "    const int kbxd = k % blocks_per_tile_x_row;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI2_K) {",
      "        int i = (i0 + i_offset * QI2_K + k / blocks_per_tile_x_row) % mmq_y;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q2_K * bxi = bx0 + i*blocks_per_row + kbxd;",
      "        x_dm[i * (WARP_SIZE_GGUF/QI2_K) + i / QI2_K + kbxd] = bxi->dm;",
      "    }",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * 4) {",
      "        int i = i0 + i_offset * 4 + k / (WARP_SIZE_GGUF/4);",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q2_K * bxi = bx0 + i*blocks_per_row + (k % (WARP_SIZE_GGUF/4)) / (QI2_K/4);",
      "        x_sc[i * (WARP_SIZE_GGUF/4) + i / 4 + k % (WARP_SIZE_GGUF/4)] = get_int_from_uint8_aligned(bxi->scales, k % (QI2_K/4));",
      "    }",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q2_K_q8_1_mul_mat(",
      "    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,",
      "    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {",
      "    const int kbx = k / QI2_K;",
      "    const int ky  = (k % QI2_K) * QR2_K;",
      "    const float * y_df = (const float *) y_ds;",
      "",
      "    int v[QR2_K*VDR_Q2_K_Q8_1_MMQ];",
      "",
      "    const int kqsx = i * (WARP_SIZE_GGUF + 1) + kbx*QI2_K + (QI2_K/2) * (ky/(2*QI2_K)) + ky % (QI2_K/2);",
      "    const int shift = 2 * ((ky % (2*QI2_K)) / (QI2_K/2));",
      "",
      "#pragma unroll",
      "    for (int l = 0; l < QR2_K*VDR_Q2_K_Q8_1_MMQ; ++l) {",
      "        v[l] = (x_ql[kqsx + l] >> shift) & 0x03030303;",
      "    }",
      "",
      "    const uint8_t * scales = ((const uint8_t *) &x_sc[i * (WARP_SIZE_GGUF/4) + i/4 + kbx*4]) + ky/4;",
      "",
      "    const int index_y = j * WARP_SIZE_GGUF + (QR2_K*k) % WARP_SIZE_GGUF;",
      "    return vec_dot_q2_K_q8_1_impl_mmq(v, &y_qs[index_y], scales, x_dm[i * (WARP_SIZE_GGUF/QI2_K) + i/QI2_K + kbx], y_df[index_y/QI8_1]);",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q3_K_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "",
      "    const block_q3_K * bq3_K = (const block_q3_K *) vbq;",
      "",
      "    const int bq8_offset = QR3_K * (iqs / (QI3_K/2));",
      "    const int scale_offset = iqs - iqs % QI8_1 + (iqs % QI8_1) / (QI8_1/2);",
      "",
      "    const float d = __half2float(bq3_K->d);",
      "",
      "    const int vl = get_int_from_uint8(bq3_K->qs, iqs);",
      "",
      "    // invert the mask with ~ so that a 0/1 results in 4/0 being subtracted",
      "    const int vh = ~get_int_from_uint8(bq3_K->hmask, iqs % (QI3_K/2)) >> bq8_offset;",
      "",
      "    int    u[QR3_K];",
      "    float d8[QR3_K];",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < QR3_K; ++i) {",
      "        u[i]  = get_int_from_int8_aligned(bq8_1[bq8_offset + i].qs, iqs % QI8_1);",
      "        d8[i] = __low2float(bq8_1[bq8_offset + i].ds);",
      "    }",
      "",
      "    return vec_dot_q3_K_q8_1_impl_mmvq(vl, vh, u, bq3_K->scales, scale_offset, d, d8);",
      "}",
      "",
      "template <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q3_K(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {",
      "    __shared__ int   tile_x_ql[mmq_y * (WARP_SIZE_GGUF)       + mmq_y];",
      "    __shared__ half2 tile_x_dm[mmq_y * (WARP_SIZE_GGUF/QI3_K) + mmq_y/QI3_K];",
      "    __shared__ int   tile_x_qh[mmq_y * (WARP_SIZE_GGUF/2)     + mmq_y/2];",
      "    __shared__ int   tile_x_sc[mmq_y * (WARP_SIZE_GGUF/4)     + mmq_y/4];",
      "",
      "    *x_ql = tile_x_ql;",
      "    *x_dm = tile_x_dm;",
      "    *x_qh = tile_x_qh;",
      "    *x_sc = tile_x_sc;",
      "}",
      "",
      "template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q3_K(",
      "    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,",
      "    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {",
      "    const int kbx  = k / QI3_K;",
      "    const int kqsx = k % QI3_K;",
      "",
      "    const block_q3_K * bx0 = (const block_q3_K *) vx;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {",
      "        int i = i0 + i_offset;",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q3_K * bxi = bx0 + i*blocks_per_row + kbx;",
      "        x_ql[i * (WARP_SIZE_GGUF + 1) + k] = get_int_from_uint8(bxi->qs, kqsx);",
      "    }",
      "",
      "    const int blocks_per_tile_x_row = WARP_SIZE_GGUF / QI3_K;",
      "    const int kbxd = k % blocks_per_tile_x_row;",
      "    float * x_dmf = (float *) x_dm;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI3_K) {",
      "        int i = (i0 + i_offset * QI3_K + k / blocks_per_tile_x_row) % mmq_y;",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q3_K * bxi = bx0 + i*blocks_per_row + kbxd;",
      "        x_dmf[i * (WARP_SIZE_GGUF/QI3_K) + i / QI3_K + kbxd] = __half2float(bxi->d);",
      "    }",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * 2) {",
      "        int i = i0 + i_offset * 2 + k / (WARP_SIZE_GGUF/2);",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q3_K * bxi = bx0 + i*blocks_per_row + (k % (WARP_SIZE_GGUF/2)) / (QI3_K/2);",
      "        // invert the mask with ~ so that a 0/1 results in 4/0 being subtracted",
      "        x_qh[i * (WARP_SIZE_GGUF/2) + i / 2 + k % (WARP_SIZE_GGUF/2)] = ~get_int_from_uint8(bxi->hmask, k % (QI3_K/2));",
      "    }",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * 4) {",
      "        int i = i0 + i_offset * 4 + k / (WARP_SIZE_GGUF/4);",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q3_K * bxi = bx0 + i*blocks_per_row + (k % (WARP_SIZE_GGUF/4)) / (QI3_K/4);",
      "",
      "        const int ksc = k % (QI3_K/4);",
      "",
      "        const int ksc_low = ksc % (QI3_K/8);",
      "        const int shift_low = 4 * (ksc / (QI3_K/8));",
      "        const int sc_low = (get_int_from_uint8(bxi->scales, ksc_low) >> shift_low) & 0x0F0F0F0F;",
      "",
      "        const int ksc_high = QI3_K/8;",
      "        const int shift_high = 2 * ksc;",
      "        const int sc_high = ((get_int_from_uint8(bxi->scales, ksc_high) >> shift_high) << 4) & 0x30303030;",
      "",
      "        const int sc = __vsubss4(sc_low | sc_high, 0x20202020);",
      "",
      "        x_sc[i * (WARP_SIZE_GGUF/4) + i / 4 + k % (WARP_SIZE_GGUF/4)] = sc;",
      "    }",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q3_K_q8_1_mul_mat(",
      "    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,",
      "    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {",
      "",
      "    const int kbx  = k / QI3_K;",
      "    const int ky  = (k % QI3_K) * QR3_K;",
      "    const float * x_dmf = (const float *) x_dm;",
      "    const float * y_df  = (const float *) y_ds;",
      "",
      "    const int8_t * scales = ((const int8_t *) (x_sc + i * (WARP_SIZE_GGUF/4) + i/4 + kbx*4)) + ky/4;",
      "",
      "    int v[QR3_K*VDR_Q3_K_Q8_1_MMQ];",
      "",
      "#pragma unroll",
      "    for (int l = 0; l < QR3_K*VDR_Q3_K_Q8_1_MMQ; ++l) {",
      "        const int kqsx = i * (WARP_SIZE_GGUF + 1) + kbx*QI3_K + (QI3_K/2) * (ky/(2*QI3_K)) + ky % (QI3_K/2);",
      "        const int shift = 2 * ((ky % 32) / 8);",
      "        const int vll = (x_ql[kqsx + l] >> shift) & 0x03030303;",
      "",
      "        const int vh = x_qh[i * (WARP_SIZE_GGUF/2) + i/2 + kbx * (QI3_K/2) + (ky+l)%8] >> ((ky+l) / 8);",
      "        const int vlh = (vh << 2) & 0x04040404;",
      "",
      "        v[l] = __vsubss4(vll, vlh);",
      "    }",
      "",
      "    const int index_y = j * WARP_SIZE_GGUF + (k*QR3_K) % WARP_SIZE_GGUF;",
      "    return vec_dot_q3_K_q8_1_impl_mmq(v, &y_qs[index_y], scales, x_dmf[i * (WARP_SIZE_GGUF/QI3_K) + i/QI3_K + kbx], y_df[index_y/QI8_1]);",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q4_K_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "    const block_q4_K * bq4_K = (const block_q4_K *) vbq;",
      "",
      "    int    v[2];",
      "    int    u[2*QR4_K];",
      "    float d8[QR4_K];",
      "",
      "    // iqs is in 0,2..30. bq8_offset = iqs/4 -> bq8_offset = 0, 2, 4, 6",
      "    const int bq8_offset = QR4_K * ((iqs/2) / (QI8_1/2));",
      "",
      "    // iqs = 0....3 -> bq8_offset = 0, want q4_offset = 0, 4, 8, 12",
      "    // iqs = 4....7 -> bq8_offset = 2, want q4_offset = 32, 36, 40, 44",
      "    // iqs = 8...11 -> bq8_offset = 4, want q4_offset = 64, 68, 72, 76",
      "    // iqs = 12..15 -> bq8_offset = 6, want q4_offset = 96, 100, 104, 108",
      "",
      "    const int * q4 = (const int *)(bq4_K->qs + 16 * bq8_offset + 4 * ((iqs/2)%4));",
      "    v[0] = q4[0];",
      "    v[1] = q4[4];",
      "",
      "    const uint16_t * scales = (const uint16_t *)bq4_K->scales;",
      "    uint16_t aux[2];",
      "    const int j = bq8_offset/2;",
      "    if (j < 2) {",
      "        aux[0] = scales[j+0] & 0x3f3f;",
      "        aux[1] = scales[j+2] & 0x3f3f;",
      "    } else {",
      "        aux[0] = ((scales[j+2] >> 0) & 0x0f0f) | ((scales[j-2] & 0xc0c0) >> 2);",
      "        aux[1] = ((scales[j+2] >> 4) & 0x0f0f) | ((scales[j-0] & 0xc0c0) >> 2);",
      "    }",
      "    const uint8_t * sc = (const uint8_t *)aux;",
      "    const uint8_t * m  = sc + 2;",
      "",
      "    for (int i = 0; i < QR4_K; ++i) {",
      "        const block_q8_1 * bq8i = bq8_1 + bq8_offset + i;",
      "        d8[i] = __low2float(bq8i->ds);",
      "",
      "        const int * q8 = (const int *)bq8i->qs + ((iqs/2)%4);",
      "        u[2*i+0] = q8[0];",
      "        u[2*i+1] = q8[4];",
      "    }",
      "",
      "    return vec_dot_q4_K_q8_1_impl_vmmq(v, u, sc, m, bq4_K->dm, d8);",
      "}",
      "",
      "template <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q4_K(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {",
      "    __shared__ int   tile_x_ql[mmq_y * (WARP_SIZE_GGUF)       + mmq_y];",
      "    __shared__ half2 tile_x_dm[mmq_y * (WARP_SIZE_GGUF/QI4_K) + mmq_y/QI4_K];",
      "    __shared__ int   tile_x_sc[mmq_y * (WARP_SIZE_GGUF/8)     + mmq_y/8];",
      "",
      "    *x_ql = tile_x_ql;",
      "    *x_dm = tile_x_dm;",
      "    *x_sc = tile_x_sc;",
      "}",
      "",
      "template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q4_K(",
      "    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,",
      "    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {",
      "    const int kbx  = k / QI4_K; // == 0 if QK_K == 256",
      "    const int kqsx = k % QI4_K; // == k if QK_K == 256",
      "",
      "    const block_q4_K * bx0 = (const block_q4_K *) vx;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {",
      "        int i = i0 + i_offset;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q4_K * bxi = bx0 + i*blocks_per_row + kbx;",
      "        x_ql[i * (WARP_SIZE_GGUF + 1) + k] = get_int_from_uint8_aligned(bxi->qs, kqsx);",
      "    }",
      "",
      "    const int blocks_per_tile_x_row = WARP_SIZE_GGUF / QI4_K; // == 1 if QK_K == 256",
      "    const int kbxd = k % blocks_per_tile_x_row;          // == 0 if QK_K == 256",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI4_K) {",
      "        int i = (i0 + i_offset * QI4_K + k / blocks_per_tile_x_row) % mmq_y;",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "        const block_q4_K * bxi = bx0 + i*blocks_per_row + kbxd;",
      "        x_dm[i * (WARP_SIZE_GGUF/QI4_K) + i / QI4_K + kbxd] = bxi->dm;",
      "    }",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * 8) {",
      "        int i = (i0 + i_offset * 8 + k / (WARP_SIZE_GGUF/8)) % mmq_y;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "",
      "        const block_q4_K * bxi = bx0 + i*blocks_per_row + (k % (WARP_SIZE_GGUF/8)) / (QI4_K/8);",
      "",
      "        const int * scales = (const int *) bxi->scales;",
      "",
      "        const int ksc = k % (WARP_SIZE_GGUF/8);",
      "        // scale arrangement after the following two lines: sc0,...,sc3, sc4,...,sc7, m0,...,m3, m4,...,m8",
      "        int scales8 = (scales[(ksc%2) + (ksc!=0)] >> (4 * (ksc & (ksc/2)))) & 0x0F0F0F0F; // lower 4 bits",
      "        scales8    |= (scales[ksc/2]              >> (2 * (ksc % 2)))       & 0x30303030; // upper 2 bits",
      "",
      "        x_sc[i * (WARP_SIZE_GGUF/8) + i / 8 + ksc] = scales8;",
      "    }",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q4_K_q8_1_mul_mat(",
      "    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,",
      "    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {",
      "    (void)x_qh;",
      "",
      "    const uint8_t * sc = ((const uint8_t *) &x_sc[i * (WARP_SIZE_GGUF/8) + i/8 + k/16]) + 2*((k % 16) / 8);",
      "",
      "    const int index_y = j * WARP_SIZE_GGUF + (QR4_K*k) % WARP_SIZE_GGUF;",
      "    return vec_dot_q4_K_q8_1_impl_mmq(&x_ql[i * (WARP_SIZE_GGUF + 1) + k], &y_qs[index_y], sc, sc+8,",
      "                                      x_dm[i * (WARP_SIZE_GGUF/QI4_K) + i/QI4_K], &y_ds[index_y/QI8_1]);",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q5_K_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "",
      "    const block_q5_K * bq5_K = (const block_q5_K *) vbq;",
      "",
      "    int   vl[2];",
      "    int   vh[2];",
      "    int    u[2*QR5_K];",
      "    float d8[QR5_K];",
      "",
      "    const int bq8_offset = QR5_K * ((iqs/2) / (QI8_1/2));",
      "    const int * ql = (const int *)(bq5_K->qs + 16 * bq8_offset + 4 * ((iqs/2)%4));",
      "    const int * qh = (const int *)(bq5_K->qh + 4 * ((iqs/2)%4));",
      "",
      "    vl[0] = ql[0];",
      "    vl[1] = ql[4];",
      "",
      "    vh[0] = qh[0] >> bq8_offset;",
      "    vh[1] = qh[4] >> bq8_offset;",
      "",
      "    const uint16_t * scales = (const uint16_t *)bq5_K->scales;",
      "    uint16_t aux[2];",
      "    const int j = bq8_offset/2;",
      "    if (j < 2) {",
      "        aux[0] = scales[j+0] & 0x3f3f;",
      "        aux[1] = scales[j+2] & 0x3f3f;",
      "    } else {",
      "        aux[0] = ((scales[j+2] >> 0) & 0x0f0f) | ((scales[j-2] & 0xc0c0) >> 2);",
      "        aux[1] = ((scales[j+2] >> 4) & 0x0f0f) | ((scales[j-0] & 0xc0c0) >> 2);",
      "    }",
      "    const uint8_t * sc = (const uint8_t *)aux;",
      "    const uint8_t * m  = sc + 2;",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < QR5_K; ++i) {",
      "        const block_q8_1 * bq8i = bq8_1 + bq8_offset + i;",
      "        d8[i] = __low2float(bq8i->ds);",
      "",
      "        const int * q8 = (const int *)bq8i->qs + ((iqs/2)%4);",
      "        u[2*i+0] = q8[0];",
      "        u[2*i+1] = q8[4];",
      "    }",
      "",
      "    return vec_dot_q5_K_q8_1_impl_vmmq(vl, vh, u, sc, m, bq5_K->dm, d8);",
      "}",
      "",
      "template <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q5_K(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {",
      "    __shared__ int   tile_x_ql[mmq_y * (2*WARP_SIZE_GGUF)     + mmq_y];",
      "    __shared__ half2 tile_x_dm[mmq_y * (WARP_SIZE_GGUF/QI5_K) + mmq_y/QI5_K];",
      "    __shared__ int   tile_x_sc[mmq_y * (WARP_SIZE_GGUF/8)     + mmq_y/8];",
      "",
      "    *x_ql = tile_x_ql;",
      "    *x_dm = tile_x_dm;",
      "    *x_sc = tile_x_sc;",
      "}",
      "",
      "template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q5_K(",
      "    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,",
      "    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {",
      "    const int kbx  = k / QI5_K; // == 0 if QK_K == 256",
      "    const int kqsx = k % QI5_K; // == k if QK_K == 256",
      "",
      "    const block_q5_K * bx0 = (const block_q5_K *) vx;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {",
      "        int i = i0 + i_offset;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "",
      "        const block_q5_K * bxi = bx0 + i*blocks_per_row + kbx;",
      "        const int ky = QR5_K*kqsx;",
      "",
      "        const int ql = get_int_from_uint8_aligned(bxi->qs, kqsx);",
      "        const int ql0 = (ql >> 0) & 0x0F0F0F0F;",
      "        const int ql1 = (ql >> 4) & 0x0F0F0F0F;",
      "",
      "        const int qh = get_int_from_uint8_aligned(bxi->qh, kqsx % (QI5_K/4));",
      "        const int qh0 = ((qh >> (2 * (kqsx / (QI5_K/4)) + 0)) << 4) & 0x10101010;",
      "        const int qh1 = ((qh >> (2 * (kqsx / (QI5_K/4)) + 1)) << 4) & 0x10101010;",
      "",
      "        const int kq0 = ky - ky % (QI5_K/2) + k % (QI5_K/4) + 0;",
      "        const int kq1 = ky - ky % (QI5_K/2) + k % (QI5_K/4) + (QI5_K/4);",
      "",
      "        x_ql[i * (2*WARP_SIZE_GGUF + 1) + kq0] = ql0 | qh0;",
      "        x_ql[i * (2*WARP_SIZE_GGUF + 1) + kq1] = ql1 | qh1;",
      "    }",
      "",
      "    const int blocks_per_tile_x_row = WARP_SIZE_GGUF / QI5_K; // == 1 if QK_K == 256",
      "    const int kbxd = k % blocks_per_tile_x_row;          // == 0 if QK_K == 256",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI5_K) {",
      "        int i = (i0 + i_offset * QI5_K + k / blocks_per_tile_x_row) % mmq_y;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "",
      "        const block_q5_K * bxi = bx0 + i*blocks_per_row + kbxd;",
      "        x_dm[i * (WARP_SIZE_GGUF/QI5_K) + i / QI5_K + kbxd] = bxi->dm;",
      "    }",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * 8) {",
      "        int i = (i0 + i_offset * 8 + k / (WARP_SIZE_GGUF/8)) % mmq_y;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "",
      "        const block_q5_K * bxi = bx0 + i*blocks_per_row + (k % (WARP_SIZE_GGUF/8)) / (QI5_K/8);",
      "",
      "        const int * scales = (const int *) bxi->scales;",
      "",
      "        const int ksc = k % (WARP_SIZE_GGUF/8);",
      "",
      "        // scale arrangement after the following two lines: sc0,...,sc3, sc4,...,sc7, m0,...,m3, m4,...,m8",
      "        int scales8 = (scales[(ksc%2) + (ksc!=0)] >> (4 * (ksc & (ksc/2)))) & 0x0F0F0F0F; // lower 4 bits",
      "        scales8    |= (scales[ksc/2]              >> (2 * (ksc % 2)))       & 0x30303030; // upper 2 bits",
      "",
      "        x_sc[i * (WARP_SIZE_GGUF/8) + i / 8 + ksc] = scales8;",
      "    }",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q5_K_q8_1_mul_mat(",
      "    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,",
      "    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {",
      "    const uint8_t * sc = ((const uint8_t *) &x_sc[i * (WARP_SIZE_GGUF/8) + i/8 + k/16]) + 2 * ((k % 16) / 8);",
      "",
      "    const int index_x = i * (QR5_K*WARP_SIZE_GGUF + 1) +  QR5_K*k;",
      "    const int index_y = j * WARP_SIZE_GGUF             + (QR5_K*k) % WARP_SIZE_GGUF;",
      "    return vec_dot_q5_K_q8_1_impl_mmq(&x_ql[index_x], &y_qs[index_y], sc, sc+8,",
      "                                      x_dm[i * (WARP_SIZE_GGUF/QI5_K) + i/QI5_K], &y_ds[index_y/QI8_1]);",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q6_K_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "",
      "    const block_q6_K * bq6_K = (const block_q6_K *) vbq;",
      "",
      "    const int bq8_offset = 2 * QR6_K * (iqs / (QI6_K/2)) + (iqs % (QI6_K/2)) / (QI6_K/4);",
      "    const int scale_offset = (QI6_K/4) * (iqs / (QI6_K/2)) + (iqs % (QI6_K/2)) / (QI6_K/8);",
      "    const int vh_shift = 2 * ((iqs % (QI6_K/2)) / (QI6_K/4));",
      "",
      "    const int vl = get_int_from_uint8(bq6_K->ql, iqs);",
      "    const int vh = get_int_from_uint8(bq6_K->qh, (QI6_K/4) * (iqs / (QI6_K/2)) + iqs % (QI6_K/4)) >> vh_shift;",
      "",
      "    const int8_t * scales = bq6_K->scales + scale_offset;",
      "",
      "    int    u[QR6_K];",
      "    float d8[QR6_K];",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < QR6_K; ++i) {",
      "        u[i]  = get_int_from_int8_aligned(bq8_1[bq8_offset + 2*i].qs, iqs % QI8_1);",
      "        d8[i] = __low2float(bq8_1[bq8_offset + 2*i].ds);",
      "    }",
      "",
      "    return vec_dot_q6_K_q8_1_impl_mmvq(vl, vh, u, scales, __half2float(bq6_K->d), d8);",
      "}",
      "",
      "template <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q6_K(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {",
      "    __shared__ int   tile_x_ql[mmq_y * (2*WARP_SIZE_GGUF)     + mmq_y];",
      "    __shared__ half2 tile_x_dm[mmq_y * (WARP_SIZE_GGUF/QI6_K) + mmq_y/QI6_K];",
      "    __shared__ int   tile_x_sc[mmq_y * (WARP_SIZE_GGUF/8)     + mmq_y/8];",
      "",
      "    *x_ql = tile_x_ql;",
      "    *x_dm = tile_x_dm;",
      "    *x_sc = tile_x_sc;",
      "}",
      "",
      "template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q6_K(",
      "    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,",
      "    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {",
      "    const int kbx  = k / QI6_K; // == 0 if QK_K == 256",
      "    const int kqsx = k % QI6_K; // == k if QK_K == 256",
      "",
      "    const block_q6_K * bx0 = (const block_q6_K *) vx;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {",
      "        int i = i0 + i_offset;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "",
      "        const block_q6_K * bxi = bx0 + i*blocks_per_row + kbx;",
      "        const int ky = QR6_K*kqsx;",
      "",
      "        const int ql = get_int_from_uint8(bxi->ql, kqsx);",
      "        const int ql0 = (ql >> 0) & 0x0F0F0F0F;",
      "        const int ql1 = (ql >> 4) & 0x0F0F0F0F;",
      "",
      "        const int qh = get_int_from_uint8(bxi->qh, (QI6_K/4) * (kqsx / (QI6_K/2)) + kqsx % (QI6_K/4));",
      "        const int qh0 = ((qh >> (2 * ((kqsx % (QI6_K/2)) / (QI6_K/4)))) << 4) & 0x30303030;",
      "        const int qh1 =  (qh >> (2 * ((kqsx % (QI6_K/2)) / (QI6_K/4))))       & 0x30303030;",
      "",
      "        const int kq0 = ky - ky % QI6_K + k % (QI6_K/2) + 0;",
      "        const int kq1 = ky - ky % QI6_K + k % (QI6_K/2) + (QI6_K/2);",
      "",
      "        x_ql[i * (2*WARP_SIZE_GGUF + 1) + kq0] = __vsubss4(ql0 | qh0, 0x20202020);",
      "        x_ql[i * (2*WARP_SIZE_GGUF + 1) + kq1] = __vsubss4(ql1 | qh1, 0x20202020);",
      "    }",
      "",
      "    const int blocks_per_tile_x_row = WARP_SIZE_GGUF / QI6_K; // == 1 if QK_K == 256",
      "    const int kbxd = k % blocks_per_tile_x_row;          // == 0 if QK_K == 256",
      "    float * x_dmf = (float *) x_dm;",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI6_K) {",
      "        int i = (i0 + i_offset * QI6_K + k / blocks_per_tile_x_row) % mmq_y;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "",
      "        const block_q6_K * bxi = bx0 + i*blocks_per_row + kbxd;",
      "",
      "        x_dmf[i * (WARP_SIZE_GGUF/QI6_K) + i / QI6_K + kbxd] = __half2float(bxi->d);",
      "    }",
      "",
      "#pragma unroll",
      "    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * 8) {",
      "        int i = (i0 + i_offset * 8 + k / (WARP_SIZE_GGUF/8)) % mmq_y;",
      "",
      "        if (need_check) {",
      "            i = min(i, i_max);",
      "        }",
      "",
      "        const block_q6_K * bxi = bx0 + i*blocks_per_row + (k % (WARP_SIZE_GGUF/8)) / 4;",
      "",
      "        x_sc[i * (WARP_SIZE_GGUF/8) + i / 8 + k % (WARP_SIZE_GGUF/8)] = get_int_from_int8(bxi->scales, k % (QI6_K/8));",
      "    }",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_q6_K_q8_1_mul_mat(",
      "    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,",
      "    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {",
      "    const float * x_dmf = (const float *) x_dm;",
      "    const float * y_df  = (const float *) y_ds;",
      "",
      "    const int8_t * sc = ((const int8_t *) &x_sc[i * (WARP_SIZE_GGUF/8) + i/8 + k/8]);",
      "",
      "    const int index_x = i * (QR6_K*WARP_SIZE_GGUF + 1) +  QR6_K*k;",
      "    const int index_y = j * WARP_SIZE_GGUF             + (QR6_K*k) % WARP_SIZE_GGUF;",
      "    return vec_dot_q6_K_q8_1_impl_mmq(&x_ql[index_x], &y_qs[index_y], sc, x_dmf[i * (WARP_SIZE_GGUF/QI6_K) + i/QI6_K], &y_df[index_y/QI8_1]);",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_iq2_xxs_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "    const block_iq2_xxs * bq2 = (const block_iq2_xxs *) vbq;",
      "",
      "    const int ib32 = iqs;",
      "    const uint16_t * q2 = bq2->qs + 4*ib32;",
      "    const uint8_t  * aux8 = (const uint8_t *)q2;",
      "    const int8_t   * q8 = bq8_1[ib32].qs;",
      "    uint32_t aux32 = q2[2] | (q2[3] << 16);",
      "    int sumi = 0;",
      "    for (int l = 0; l < 4; ++l) {",
      "        const uint8_t * grid = (const uint8_t *)(iq2xxs_grid + aux8[l]);",
      "        const uint8_t  signs = ksigns_iq2xs[aux32 & 127];",
      "        for (int j = 0; j < 8; ++j) {",
      "            sumi += q8[j] * grid[j] * (signs & kmask_iq2xs[j] ? -1 : 1);",
      "        }",
      "        q8 += 8;",
      "        aux32 >>= 7;",
      "    }",
      "    const float d = __half2float(bq2->d) * (0.5f + aux32) * __half2float(bq8_1[ib32].ds.x) * 0.25f;",
      "    return d * sumi;",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_iq2_xs_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "    const block_iq2_xs * bq2 = (const block_iq2_xs *) vbq;",
      "",
      "    const int ib32 = iqs;",
      "    const uint16_t * q2 = bq2->qs + 4*ib32;",
      "    const int8_t   * q8 = bq8_1[ib32].qs;",
      "    const uint8_t ls1 = bq2->scales[ib32] & 0xf;",
      "    const uint8_t ls2 = bq2->scales[ib32] >>  4;",
      "    int sumi1 = 0;",
      "    for (int l = 0; l < 2; ++l) {",
      "        const uint8_t * grid = (const uint8_t *)(iq2xs_grid + (q2[l] & 511));",
      "        const uint8_t  signs = ksigns_iq2xs[q2[l] >> 9];",
      "        for (int j = 0; j < 8; ++j) {",
      "            sumi1 += q8[j] * grid[j] * (signs & kmask_iq2xs[j] ? -1 : 1);",
      "        }",
      "        q8 += 8;",
      "    }",
      "    int sumi2 = 0;",
      "    for (int l = 2; l < 4; ++l) {",
      "        const uint8_t * grid = (const uint8_t *)(iq2xs_grid + (q2[l] & 511));",
      "        const uint8_t  signs = ksigns_iq2xs[q2[l] >> 9];",
      "        for (int j = 0; j < 8; ++j) {",
      "            sumi2 += q8[j] * grid[j] * (signs & kmask_iq2xs[j] ? -1 : 1);",
      "        }",
      "        q8 += 8;",
      "    }",
      "    const float d = __half2float(bq2->d) * __half2float(bq8_1[ib32].ds.x) * 0.25f;",
      "    return d * ((0.5f + ls1) * sumi1 + (0.5f + ls2) * sumi2);",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_iq2_s_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    const block_iq2_s * bq2 = (const block_iq2_s *) vbq;",
      "",
      "    const int ib32 = iqs;",
      "    const int8_t  * q8 = bq8_1[ib32].qs;",
      "    const uint8_t * signs = bq2->qs + QK_K/8 + 4*ib32;",
      "    const uint8_t ls1 = bq2->scales[ib32] & 0xf;",
      "    const uint8_t ls2 = bq2->scales[ib32] >>  4;",
      "    int sumi1 = 0;",
      "    for (int l = 0; l < 2; ++l) {",
      "        const uint32_t * grid = (const uint32_t *)(iq2s_grid + (bq2->qs[4*ib32+l] | ((bq2->qh[ib32] << (8-2*l)) & 0x300)));",
      "        const uint32_t signs0 = __vcmpeq4(((signs[l] & 0xf) * 0x01010101) & 0x08040201, 0x08040201);",
      "        const uint32_t signs1 = __vcmpeq4(((signs[l] >>  4) * 0x01010101) & 0x08040201, 0x08040201);",
      "        const int grid_l = __vsub4(grid[0] ^ signs0, signs0);",
      "        const int grid_h = __vsub4(grid[1] ^ signs1, signs1);",
      "        sumi1 = __dp4a(grid_l, *((const int *)q8 + 0), sumi1);",
      "        sumi1 = __dp4a(grid_h, *((const int *)q8 + 1), sumi1);",
      "        q8 += 8;",
      "    }",
      "    int sumi2 = 0;",
      "    for (int l = 2; l < 4; ++l) {",
      "        const uint32_t * grid = (const uint32_t *)(iq2s_grid + (bq2->qs[4*ib32+l] | ((bq2->qh[ib32] << (8-2*l)) & 0x300)));",
      "        const uint32_t signs0 = __vcmpeq4(((signs[l] & 0xf) * 0x01010101) & 0x08040201, 0x08040201);",
      "        const uint32_t signs1 = __vcmpeq4(((signs[l] >>  4) * 0x01010101) & 0x08040201, 0x08040201);",
      "        const int grid_l = __vsub4(grid[0] ^ signs0, signs0);",
      "        const int grid_h = __vsub4(grid[1] ^ signs1, signs1);",
      "        sumi2 = __dp4a(grid_l, *((const int *)q8 + 0), sumi2);",
      "        sumi2 = __dp4a(grid_h, *((const int *)q8 + 1), sumi2);",
      "        q8 += 8;",
      "    }",
      "    const float d = __half2float(bq2->d) * __low2float(bq8_1[ib32].ds) * 0.25f;",
      "    return d * ((0.5f + ls1) * sumi1 + (0.5f + ls2) * sumi2);",
      "#endif",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_iq3_xxs_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    const block_iq3_xxs * bq2 = (const block_iq3_xxs *) vbq;",
      "",
      "    const int ib32 = iqs;",
      "    const uint8_t  * q3 = bq2->qs + 8*ib32;",
      "    const uint16_t * gas = (const uint16_t *)(bq2->qs + QK_K/4) + 2*ib32;",
      "    const int8_t   * q8 = bq8_1[ib32].qs;",
      "    uint32_t aux32 = gas[0] | (gas[1] << 16);",
      "    int sumi = 0;",
      "    for (int l = 0; l < 4; ++l) {",
      "        const uint32_t * grid1 = iq3xxs_grid + q3[2*l+0];",
      "        const uint32_t * grid2 = iq3xxs_grid + q3[2*l+1];",
      "        const uint32_t * signs = (const uint32_t *)(ksigns64 + (aux32 & 127));",
      "        const int grid_l = __vsub4(grid1[0] ^ signs[0], signs[0]);",
      "        const int grid_h = __vsub4(grid2[0] ^ signs[1], signs[1]);",
      "        sumi = __dp4a(grid_l, *((int *)q8+0), sumi);",
      "        sumi = __dp4a(grid_h, *((int *)q8+1), sumi);",
      "        q8 += 8;",
      "        aux32 >>= 7;",
      "    }",
      "    const float d = __half2float(bq2->d) * (0.5f + aux32) * __low2float(bq8_1[ib32].ds) * 0.5f;",
      "    return d * sumi;",
      "#endif",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_iq3_s_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    const block_iq3_s * bq2 = (const block_iq3_s *) vbq;",
      "",
      "    const int ib32 = iqs;",
      "    const uint8_t  * qs = bq2->qs + 8*ib32;",
      "    const int8_t   * q8 = bq8_1[ib32].qs;",
      "    int sumi = 0;",
      "    for (int l = 0; l < 4; ++l) {",
      "        const uint32_t * grid1 = iq3xs_grid + (qs[2*l+0] | ((bq2->qh[ib32] << (8 - 2*l)) & 256));",
      "        const uint32_t * grid2 = iq3xs_grid + (qs[2*l+1] | ((bq2->qh[ib32] << (7 - 2*l)) & 256));",
      "        uint32_t signs0 = __vcmpeq4(((bq2->signs[4*ib32+l] & 0xf) * 0x01010101) & 0x08040201, 0x08040201);",
      "        uint32_t signs1 = __vcmpeq4(((bq2->signs[4*ib32+l] >>  4) * 0x01010101) & 0x08040201, 0x08040201);",
      "        const int grid_l = __vsub4(grid1[0] ^ signs0, signs0);",
      "        const int grid_h = __vsub4(grid2[0] ^ signs1, signs1);",
      "        sumi = __dp4a(grid_l, *((int *)q8+0), sumi);",
      "        sumi = __dp4a(grid_h, *((int *)q8+1), sumi);",
      "        q8 += 8;",
      "    }",
      "    const float d = __half2float(bq2->d) * (0.5f + ((bq2->scales[ib32/2] >> 4*(ib32%2)) & 0xf)) * __low2float(bq8_1[ib32].ds) * 0.5f;",
      "    return d * sumi;",
      "#endif",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_iq1_s_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    const block_iq1_s * bq1 = (const block_iq1_s *) vbq;",
      "",
      "    const int       qs_packed = get_int_b2(bq1->qs, iqs);",
      "    const uint8_t * qs        = (const uint8_t *) &qs_packed;",
      "",
      "    const int qh = bq1->qh[iqs];",
      "",
      "    int sumi = 0;",
      "#pragma unroll",
      "    for (int l0 = 0; l0 < 8; l0 += 2) {",
      "        const int grid = iq1s_grid_gpu[qs[l0/2] | (((qh >> 3*(l0/2)) & 0x07) << 8)];",
      "",
      "        const int grid0 = (grid >> 0) & 0x0F0F0F0F;",
      "        const int grid1 = (grid >> 4) & 0x0F0F0F0F;",
      "",
      "        const int u0 = get_int_b4(bq8_1[iqs].qs, l0 + 0);",
      "        const int u1 = get_int_b4(bq8_1[iqs].qs, l0 + 1);",
      "",
      "        sumi = __dp4a(grid0, u0, sumi);",
      "        sumi = __dp4a(grid1, u1, sumi);",
      "    }",
      "",
      "    const float  d1q   = __half2float(bq1->d) * (((qh >> 11) & 0x0E) + 1);",
      "    const float  delta = -1.0f + IQ1S_DELTA - (qh & 0x8000) * (2.0f*IQ1S_DELTA/0x8000);",
      "    const float2 ds    = __half22float2(bq8_1[iqs].ds);",
      "    return d1q * (ds.x*sumi + ds.y*delta);",
      "#endif",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_iq1_m_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "",
      "    const block_iq1_m * bq1 = (const block_iq1_m *) vbq;",
      "",
      "    const int       qs_packed = get_int_b4(bq1->qs, iqs);",
      "    const uint8_t * qs        = (const uint8_t *) &qs_packed;",
      "",
      "    int   sumi[2] = {0};",
      "    float sumf[2] = {0.0f};",
      "#pragma unroll",
      "    for (int l0 = 0; l0 < 8; l0 += 2) {",
      "        const int qhl = bq1->qh[2*iqs + l0/4] >> (4 * ((l0/2) % 2));",
      "",
      "        const int grid = iq1s_grid_gpu[qs[l0/2] | ((qhl & 0x07) << 8)];",
      "",
      "        const int grid0 = (grid >> 0) & 0x0F0F0F0F;",
      "        const int grid1 = (grid >> 4) & 0x0F0F0F0F;",
      "",
      "        const int u0 = get_int_b4(bq8_1[iqs].qs, l0 + 0);",
      "        const int u1 = get_int_b4(bq8_1[iqs].qs, l0 + 1);",
      "",
      "        sumi[l0/4] = __dp4a(grid0, u0, sumi[l0/4]);",
      "        sumi[l0/4] = __dp4a(grid1, u1, sumi[l0/4]);",
      "",
      "        const float delta = -1.0f + IQ1M_DELTA - (qhl & 0x08) * (2.0f*IQ1M_DELTA/0x08);",
      "        int sumy = 0;",
      "        sumy = __dp4a(u0, 0x01010101, sumy);",
      "        sumy = __dp4a(u1, 0x01010101, sumy);",
      "        sumf[l0/4] += delta*sumy;",
      "    }",
      "",
      "    const uint16_t * sc = (const uint16_t *) bq1->scales;",
      "",
      "    iq1m_scale_t scale;",
      "    scale.u16 = (sc[0] >> 12) | ((sc[1] >> 8) & 0x00F0) | ((sc[2] >> 4) & 0x0F00) | (sc[3] & 0xF000);",
      "    const float d = __half2float(scale.f16) * __low2float(bq8_1[iqs].ds);",
      "",
      "    const int tmp = sc[iqs/2] >> (6*(iqs%2));",
      "    const int sc0 = 2*((tmp >> 0) & 0x07) + 1;",
      "    const int sc1 = 2*((tmp >> 3) & 0x07) + 1;",
      "    return d * ((sumi[0] + sumf[0]) * sc0 + (sumi[1] + sumf[1]) * sc1);",
      "#endif",
      "}",
      "",
      "static __device__ __forceinline__ void get_int_from_table_16(const uint32_t & q4, const uint8_t * values,",
      "        int & val1, int & val2) {",
      "",
      "    uint32_t aux32; const uint8_t * q8 = (const uint8_t *)&aux32;",
      "    aux32 = q4 & 0x0f0f0f0f;",
      "    uint16_t v1 = values[q8[0]] | (values[q8[1]] << 8);",
      "    uint16_t v2 = values[q8[2]] | (values[q8[3]] << 8);",
      "    val1 = v1 | (v2 << 16);",
      "    aux32 = (q4 >> 4) & 0x0f0f0f0f;",
      "    v1 = values[q8[0]] | (values[q8[1]] << 8);",
      "    v2 = values[q8[2]] | (values[q8[3]] << 8);",
      "    val2 = v1 | (v2 << 16);",
      "}",
      "",
      "static __device__ __forceinline__ float vec_dot_iq4_nl_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "",
      "    const block_iq4_nl * bq = (const block_iq4_nl *) vbq;",
      "",
      "    const uint16_t * q4 = (const uint16_t *)bq->qs + 2*iqs;",
      "    const int32_t  * q8 = (const int32_t  *)bq8_1->qs + iqs;",
      "",
      "    const uint8_t * values = (const uint8_t *)kvalues_iq4nl;",
      "",
      "    int v1, v2;",
      "    int sumi1 = 0, sumi2 = 0;",
      "    for (int l = 0; l < VDR_Q4_0_Q8_1_MMVQ; ++l) {",
      "        const uint32_t aux = q4[2*l] | (q4[2*l+1] << 16);",
      "        get_int_from_table_16(aux, values, v1, v2);",
      "        sumi1 = __dp4a(v1, q8[l+0], sumi1);",
      "        sumi2 = __dp4a(v2, q8[l+4], sumi2);",
      "    }",
      "    const float d = __half2float(bq->d) * __low2float(bq8_1->ds);",
      "    return d * (sumi1 + sumi2);",
      "#endif",
      "}",
      "",
      "",
      "static __device__ __forceinline__ float vec_dot_iq4_xs_q8_1(",
      "    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 610 || defined USE_ROCM",
      "    const block_iq4_xs * bq4 = (const block_iq4_xs *) vbq;",
      "    const uint8_t * values = (const uint8_t *)kvalues_iq4nl;",
      "",
      "    // iqs is 0...7",
      "    const int ib32 = iqs;",
      "    const int32_t  * q8 = (const int *)bq8_1[ib32].qs;",
      "    const uint32_t * q4 = (const uint32_t *)bq4->qs + 4*ib32;",
      "    const int8_t ls = ((bq4->scales_l[ib32/2] >> 4*(ib32%2)) & 0xf) | (((bq4->scales_h >> 2*ib32) & 3) << 4);",
      "    const float d = __half2float(bq4->d) * (ls - 32) * __low2float(bq8_1[ib32].ds);",
      "    int v1, v2;",
      "    int sumi1 = 0, sumi2 = 0;",
      "    for (int j = 0; j < 4; ++j) {",
      "        get_int_from_table_16(q4[j], values, v1, v2);",
      "        sumi1 = __dp4a(v1, q8[j+0], sumi1);",
      "        sumi2 = __dp4a(v2, q8[j+4], sumi2);",
      "    }",
      "    return d * (sumi1 + sumi2);",
      "#endif",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gguf/gguf_kernel.cu",
    "source": [
      "#include <cuda_fp16.h>",
      "#include <cuda_runtime.h>",
      "",
      "#include <torch/all.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#include \"../../cuda_compat.h\"",
      "#include \"dispatch_utils.h\"",
      "",
      "#include \"ggml-common.h\"",
      "#include \"vecdotq.cuh\"",
      "#include \"dequantize.cuh\"",
      "#include \"mmvq.cuh\"",
      "#include \"mmq.cuh\"",
      "#include \"moe.cuh\"",
      "#include \"moe_vec.cuh\"",
      "",
      "// Q8 gemv",
      "template <typename scalar_t>",
      "static __global__ void quantize_q8_1(const scalar_t* __restrict__ x,",
      "                                     void* __restrict__ vy, const int kx,",
      "                                     const int kx_padded) {",
      "  const auto ix = blockDim.x * blockIdx.x + threadIdx.x;",
      "  if (ix >= kx_padded) {",
      "    return;",
      "  }",
      "  const auto iy = blockDim.y * blockIdx.y + threadIdx.y;",
      "  const int i_padded = iy * kx_padded + ix;",
      "",
      "  block_q8_1* y = (block_q8_1*)vy;",
      "",
      "  const int ib = i_padded / QK8_1;   // block index",
      "  const int iqs = i_padded % QK8_1;  // quant index",
      "",
      "  const float xi = ix < kx ? static_cast<float>(x[iy * kx + ix]) : 0.0f;",
      "  float amax = fabsf(xi);",
      "  float sum = xi;",
      "",
      "#pragma unroll",
      "  for (int mask = 16; mask > 0; mask >>= 1) {",
      "    amax = fmaxf(amax, VLLM_SHFL_XOR_SYNC_WIDTH(amax, mask, 32));",
      "    sum += VLLM_SHFL_XOR_SYNC_WIDTH(sum, mask, 32);",
      "  }",
      "",
      "  const float d = amax / 127;",
      "  const int8_t q = amax == 0.0f ? 0 : roundf(xi / d);",
      "",
      "  y[ib].qs[iqs] = q;",
      "",
      "  if (iqs > 0) {",
      "    return;",
      "  }",
      "",
      "  y[ib].ds.x = __float2half(d);",
      "  y[ib].ds.y = __float2half(sum);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void quantize_row_q8_1_cuda(const scalar_t* x, void* vy, const int kx,",
      "                                   const int ky, cudaStream_t stream) {",
      "  const int64_t kx_padded = (kx + 512 - 1) / 512 * 512;",
      "  const int block_num_x =",
      "      (kx_padded + CUDA_QUANTIZE_BLOCK_SIZE - 1) / CUDA_QUANTIZE_BLOCK_SIZE;",
      "  constexpr int MAX_BLOCK_SIZE = 65535;",
      "  for (int off = 0; off < ky; off += MAX_BLOCK_SIZE) {",
      "    const int num_blocks_y = std::min(ky, off + MAX_BLOCK_SIZE) - off;",
      "    const dim3 num_blocks(block_num_x, num_blocks_y, 1);",
      "    const dim3 block_size(CUDA_DEQUANTIZE_BLOCK_SIZE, 1, 1);",
      "    quantize_q8_1<<<num_blocks, block_size, 0, stream>>>(",
      "        &x[off * kx], (int32_t*)vy + off * (kx_padded / 32 * 9), kx, kx_padded);",
      "  }",
      "}",
      "",
      "torch::Tensor ggml_dequantize(torch::Tensor W,  // quant weight",
      "                              int64_t type, int64_t m, int64_t n,",
      "                              std::optional<at::ScalarType> const& dtype) {",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(W));",
      "  auto dtype_ = dtype.value_or(torch::kFloat16);",
      "  auto options = torch::TensorOptions().dtype(dtype_).device(W.device());",
      "  at::Tensor DW = torch::empty({m, n}, options);",
      "  cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();",
      "",
      "  VLLM_DISPATCH_FLOATING_TYPES(DW.scalar_type(), \"ggml_dequantize\", [&] {",
      "    auto to_cuda = ggml_get_to_cuda<scalar_t>(type);",
      "    to_cuda((void*)W.data_ptr(), (scalar_t*)DW.data_ptr(), m * n, stream);",
      "  });",
      "",
      "  return DW;",
      "}",
      "",
      "torch::Tensor ggml_mul_mat_vec_a8(torch::Tensor W,  // quant weight",
      "                                  torch::Tensor X,  // input",
      "                                  int64_t type, int64_t row) {",
      "  int col = X.sizes()[1];",
      "  int vecs = X.sizes()[0];",
      "  const int padded = (col + 512 - 1) / 512 * 512;",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(X));",
      "  auto options = torch::TensorOptions().dtype(X.dtype()).device(W.device());",
      "  at::Tensor Y = torch::empty({vecs, row}, options);",
      "  cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();",
      "  options = torch::TensorOptions().dtype(torch::kInt32).device(W.device());",
      "  at::Tensor quant_X = torch::empty({vecs, padded / 32 * 9}, options);",
      "  VLLM_DISPATCH_FLOATING_TYPES(X.scalar_type(), \"ggml_mul_mat_vec_a8\", [&] {",
      "    quantize_row_q8_1_cuda<scalar_t>(",
      "        (scalar_t*)X.data_ptr(), (void*)quant_X.data_ptr(), col, vecs, stream);",
      "    switch (type) {",
      "      case 2:",
      "        mul_mat_vec_q4_0_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 3:",
      "        mul_mat_vec_q4_1_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 6:",
      "        mul_mat_vec_q5_0_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 7:",
      "        mul_mat_vec_q5_1_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 8:",
      "        mul_mat_vec_q8_0_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 10:",
      "        mul_mat_vec_q2_K_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 11:",
      "        mul_mat_vec_q3_K_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 12:",
      "        mul_mat_vec_q4_K_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 13:",
      "        mul_mat_vec_q5_K_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 14:",
      "        mul_mat_vec_q6_K_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 16:",
      "        mul_mat_vec_iq2_xxs_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 17:",
      "        mul_mat_vec_iq2_xs_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 18:",
      "        mul_mat_vec_iq3_xxs_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 19:",
      "        mul_mat_vec_iq1_s_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 20:",
      "        mul_mat_vec_iq4_nl_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 21:",
      "        mul_mat_vec_iq3_s_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 22:",
      "        mul_mat_vec_iq2_s_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 23:",
      "        mul_mat_vec_iq4_xs_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "      case 29:",
      "        mul_mat_vec_iq1_m_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, vecs, stream);",
      "        break;",
      "    }",
      "  });",
      "  return Y;",
      "}",
      "",
      "torch::Tensor ggml_mul_mat_a8(torch::Tensor W,  // quant weight",
      "                              torch::Tensor X,  // input",
      "                              int64_t type, int64_t row) {",
      "  int col = X.sizes()[1];",
      "  int padded = (col + 512 - 1) / 512 * 512;",
      "  int batch = X.sizes()[0];",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(X));",
      "  auto options = torch::TensorOptions().dtype(X.dtype()).device(W.device());",
      "  at::Tensor Y = torch::empty({batch, row}, options);",
      "  cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();",
      "  options = torch::TensorOptions().dtype(torch::kInt32).device(W.device());",
      "  at::Tensor quant_X = torch::empty({batch, padded / 32 * 9}, options);",
      "  VLLM_DISPATCH_FLOATING_TYPES(X.scalar_type(), \"ggml_mul_mat_a8\", [&] {",
      "    quantize_row_q8_1_cuda((scalar_t*)X.data_ptr(), (void*)quant_X.data_ptr(),",
      "                           col, batch, stream);",
      "",
      "    switch (type) {",
      "      case 2:",
      "        ggml_mul_mat_q4_0_q8_1_cuda(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, batch, padded, row, stream);",
      "        break;",
      "      case 3:",
      "        ggml_mul_mat_q4_1_q8_1_cuda(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, batch, padded, row, stream);",
      "        break;",
      "      case 6:",
      "        ggml_mul_mat_q5_0_q8_1_cuda(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, batch, padded, row, stream);",
      "        break;",
      "      case 7:",
      "        ggml_mul_mat_q5_1_q8_1_cuda(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, batch, padded, row, stream);",
      "        break;",
      "      case 8:",
      "        ggml_mul_mat_q8_0_q8_1_cuda(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, batch, padded, row, stream);",
      "        break;",
      "      case 10:",
      "        ggml_mul_mat_q2_K_q8_1_cuda(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, batch, padded, row, stream);",
      "        break;",
      "      case 11:",
      "        ggml_mul_mat_q3_K_q8_1_cuda(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, batch, padded, row, stream);",
      "        break;",
      "      case 12:",
      "        ggml_mul_mat_q4_K_q8_1_cuda(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, batch, padded, row, stream);",
      "        break;",
      "      case 13:",
      "        ggml_mul_mat_q5_K_q8_1_cuda(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, batch, padded, row, stream);",
      "        break;",
      "      case 14:",
      "        ggml_mul_mat_q6_K_q8_1_cuda(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), col, row, batch, padded, row, stream);",
      "        break;",
      "    }",
      "  });",
      "  return Y;",
      "}",
      "",
      "torch::Tensor ggml_moe_a8(torch::Tensor X,  // input",
      "                          torch::Tensor W,  // expert weights",
      "                          torch::Tensor sorted_token_ids,",
      "                          torch::Tensor expert_ids,",
      "                          torch::Tensor num_tokens_post_padded, int64_t type,",
      "                          int64_t row, int64_t top_k, int64_t tokens) {",
      "  int col = X.sizes()[1];",
      "  int padded = (col + 512 - 1) / 512 * 512;",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(X));",
      "  auto options = torch::TensorOptions().dtype(X.dtype()).device(W.device());",
      "  at::Tensor Y = torch::empty({tokens * top_k, row}, options);",
      "  cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();",
      "  options = torch::TensorOptions().dtype(torch::kInt32).device(W.device());",
      "  at::Tensor quant_X = torch::empty({tokens, padded / 32 * 9}, options);",
      "  VLLM_DISPATCH_FLOATING_TYPES(X.scalar_type(), \"ggml_moe_a8\", [&] {",
      "    quantize_row_q8_1_cuda((scalar_t*)X.data_ptr(), (void*)quant_X.data_ptr(),",
      "                           col, tokens, stream);",
      "    switch (type) {",
      "      case 2:",
      "        ggml_moe_q4_0_q8_1_cuda(",
      "            (void*)quant_X.data_ptr(), (void*)W.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)sorted_token_ids.data_ptr(),",
      "            (int*)expert_ids.data_ptr(),",
      "            (int*)num_tokens_post_padded.data_ptr(), W.stride(0), col, row,",
      "            tokens, padded, row, top_k, sorted_token_ids.sizes()[0], stream);",
      "        break;",
      "      case 3:",
      "        ggml_moe_q4_1_q8_1_cuda(",
      "            (void*)quant_X.data_ptr(), (void*)W.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)sorted_token_ids.data_ptr(),",
      "            (int*)expert_ids.data_ptr(),",
      "            (int*)num_tokens_post_padded.data_ptr(), W.stride(0), col, row,",
      "            tokens, padded, row, top_k, sorted_token_ids.sizes()[0], stream);",
      "        break;",
      "      case 6:",
      "        ggml_moe_q5_0_q8_1_cuda(",
      "            (void*)quant_X.data_ptr(), (void*)W.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)sorted_token_ids.data_ptr(),",
      "            (int*)expert_ids.data_ptr(),",
      "            (int*)num_tokens_post_padded.data_ptr(), W.stride(0), col, row,",
      "            tokens, padded, row, top_k, sorted_token_ids.sizes()[0], stream);",
      "        break;",
      "      case 7:",
      "        ggml_moe_q5_1_q8_1_cuda(",
      "            (void*)quant_X.data_ptr(), (void*)W.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)sorted_token_ids.data_ptr(),",
      "            (int*)expert_ids.data_ptr(),",
      "            (int*)num_tokens_post_padded.data_ptr(), W.stride(0), col, row,",
      "            tokens, padded, row, top_k, sorted_token_ids.sizes()[0], stream);",
      "        break;",
      "      case 8:",
      "        ggml_moe_q8_0_q8_1_cuda(",
      "            (void*)quant_X.data_ptr(), (void*)W.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)sorted_token_ids.data_ptr(),",
      "            (int*)expert_ids.data_ptr(),",
      "            (int*)num_tokens_post_padded.data_ptr(), W.stride(0), col, row,",
      "            tokens, padded, row, top_k, sorted_token_ids.sizes()[0], stream);",
      "        break;",
      "      case 10:",
      "        ggml_moe_q2_K_q8_1_cuda(",
      "            (void*)quant_X.data_ptr(), (void*)W.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)sorted_token_ids.data_ptr(),",
      "            (int*)expert_ids.data_ptr(),",
      "            (int*)num_tokens_post_padded.data_ptr(), W.stride(0), col, row,",
      "            tokens, padded, row, top_k, sorted_token_ids.sizes()[0], stream);",
      "        break;",
      "      case 11:",
      "        ggml_moe_q3_K_q8_1_cuda(",
      "            (void*)quant_X.data_ptr(), (void*)W.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)sorted_token_ids.data_ptr(),",
      "            (int*)expert_ids.data_ptr(),",
      "            (int*)num_tokens_post_padded.data_ptr(), W.stride(0), col, row,",
      "            tokens, padded, row, top_k, sorted_token_ids.sizes()[0], stream);",
      "        break;",
      "      case 12:",
      "        ggml_moe_q4_K_q8_1_cuda(",
      "            (void*)quant_X.data_ptr(), (void*)W.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)sorted_token_ids.data_ptr(),",
      "            (int*)expert_ids.data_ptr(),",
      "            (int*)num_tokens_post_padded.data_ptr(), W.stride(0), col, row,",
      "            tokens, padded, row, top_k, sorted_token_ids.sizes()[0], stream);",
      "        break;",
      "      case 13:",
      "        ggml_moe_q5_K_q8_1_cuda(",
      "            (void*)quant_X.data_ptr(), (void*)W.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)sorted_token_ids.data_ptr(),",
      "            (int*)expert_ids.data_ptr(),",
      "            (int*)num_tokens_post_padded.data_ptr(), W.stride(0), col, row,",
      "            tokens, padded, row, top_k, sorted_token_ids.sizes()[0], stream);",
      "        break;",
      "      case 14:",
      "        ggml_moe_q6_K_q8_1_cuda(",
      "            (void*)quant_X.data_ptr(), (void*)W.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)sorted_token_ids.data_ptr(),",
      "            (int*)expert_ids.data_ptr(),",
      "            (int*)num_tokens_post_padded.data_ptr(), W.stride(0), col, row,",
      "            tokens, padded, row, top_k, sorted_token_ids.sizes()[0], stream);",
      "        break;",
      "    }",
      "  });",
      "  return Y;",
      "}",
      "",
      "torch::Tensor ggml_moe_a8_vec(torch::Tensor X,  // input",
      "                              torch::Tensor W,  // expert weights",
      "                              torch::Tensor topk_ids, int64_t top_k,",
      "                              int64_t type, int64_t row, int64_t tokens) {",
      "  int col = X.sizes()[1];",
      "  const int padded = (col + 512 - 1) / 512 * 512;",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(X));",
      "  auto options = torch::TensorOptions().dtype(X.dtype()).device(W.device());",
      "  at::Tensor Y = torch::zeros({tokens * top_k, row}, options);",
      "  cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();",
      "  options = torch::TensorOptions().dtype(torch::kInt32).device(W.device());",
      "  at::Tensor quant_X = torch::empty({tokens, padded / 32 * 9}, options);",
      "  VLLM_DISPATCH_FLOATING_TYPES(X.scalar_type(), \"ggml_moe_vec_a8\", [&] {",
      "    quantize_row_q8_1_cuda<scalar_t>((scalar_t*)X.data_ptr(),",
      "                                     (void*)quant_X.data_ptr(), col, tokens,",
      "                                     stream);",
      "    switch (type) {",
      "      case 2:",
      "        moe_vec_q4_0_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 3:",
      "        moe_vec_q4_1_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 6:",
      "        moe_vec_q5_0_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 7:",
      "        moe_vec_q5_1_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 8:",
      "        moe_vec_q8_0_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 10:",
      "        moe_vec_q2_K_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 11:",
      "        moe_vec_q3_K_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 12:",
      "        moe_vec_q4_K_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 13:",
      "        moe_vec_q5_K_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 14:",
      "        moe_vec_q6_K_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 16:",
      "        moe_vec_iq2_xxs_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 17:",
      "        moe_vec_iq2_xs_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 18:",
      "        moe_vec_iq3_xxs_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 19:",
      "        moe_vec_iq1_s_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 20:",
      "        moe_vec_iq4_nl_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 21:",
      "        moe_vec_iq3_s_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 22:",
      "        moe_vec_iq2_s_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 23:",
      "        moe_vec_iq4_xs_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "      case 29:",
      "        moe_vec_iq1_m_q8_1_cuda<scalar_t>(",
      "            (void*)W.data_ptr(), (void*)quant_X.data_ptr(),",
      "            (scalar_t*)Y.data_ptr(), (int*)topk_ids.data_ptr(), top_k, tokens,",
      "            col, row, quant_X.stride(0), stream);",
      "        break;",
      "    }",
      "  });",
      "  return Y;",
      "}",
      "",
      "int64_t ggml_moe_get_block_size(int64_t type) {",
      "  switch (type) {",
      "    case 2:",
      "      return MOE_X_Q4_0;",
      "    case 3:",
      "      return MOE_X_Q4_1;",
      "    case 6:",
      "      return MOE_X_Q5_0;",
      "    case 7:",
      "      return MOE_X_Q5_1;",
      "    case 8:",
      "      return MOE_X_Q8_0;",
      "    case 10:",
      "      return MOE_X_Q2_K;",
      "    case 11:",
      "      return MOE_X_Q3_K;",
      "    case 12:",
      "      return MOE_X_Q4_K;",
      "    case 13:",
      "      return MOE_X_Q5_K;",
      "    case 14:",
      "      return MOE_X_Q6_K;",
      "  }",
      "  return 0;",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gguf/dequantize.cuh",
    "source": [
      "// copied and adapted from https://github.com/ggerganov/llama.cpp/blob/b2899/ggml-cuda/convert.cu",
      "// Dequant functions",
      "static __device__ __forceinline__ void dequantize_q4_0(const void * vx, const int ib, const int iqs, dfloat2 & v){",
      "    const block_q4_0 * x = (const block_q4_0 *) vx;",
      "",
      "    const dfloat d = x[ib].d;",
      "",
      "    const int vui = x[ib].qs[iqs];",
      "",
      "    v.x = __int2half_rn(vui & 0xF);",
      "    v.y = __int2half_rn(vui >> 4);",
      "",
      "    v = __hsub2(v, __floats2half2_rn(8.0f, 8.0f));",
      "    v = __hmul2(v, {d, d});",
      "}",
      "",
      "static __device__ __forceinline__ void dequantize_q4_1(const void * vx, const int ib, const int iqs, dfloat2 & v){",
      "    const block_q4_1 * x = (const block_q4_1 *) vx;",
      "",
      "    const dfloat d = __low2half(x[ib].dm);",
      "    const dfloat m = __high2half(x[ib].dm);",
      "",
      "    const int vui = x[ib].qs[iqs];",
      "",
      "    v.x = __int2half_rn(vui & 0xF);",
      "    v.y = __int2half_rn(vui >> 4);",
      "",
      "    v = __hmul2(v, {d, d});",
      "    v = __hadd2(v, {m, m});",
      "}",
      "",
      "static __device__ __forceinline__ void dequantize_q5_0(const void * vx, const int ib, const int iqs, dfloat2 & v){",
      "    const block_q5_0 * x = (const block_q5_0 *) vx;",
      "",
      "    const dfloat d = x[ib].d;",
      "",
      "    uint32_t qh;",
      "    memcpy(&qh, x[ib].qh, sizeof(qh));",
      "",
      "    const int xh_0 = ((qh >> (iqs +  0)) << 4) & 0x10;",
      "    const int xh_1 = ((qh >> (iqs + 12))     ) & 0x10;",
      "",
      "    v.x = __int2half_rn((x[ib].qs[iqs] & 0xf) | xh_0);",
      "    v.y = __int2half_rn((x[ib].qs[iqs] >>  4) | xh_1);",
      "",
      "    v = __hsub2(v, __floats2half2_rn(16.0f, 16.0f));",
      "    v = __hmul2(v, {d, d});",
      "}",
      "",
      "static __device__ __forceinline__ void dequantize_q5_1(const void * vx, const int ib, const int iqs, dfloat2 & v){",
      "    const block_q5_1 * x = (const block_q5_1 *) vx;",
      "",
      "    const dfloat d = __low2half(x[ib].dm);",
      "    const dfloat m = __high2half(x[ib].dm);",
      "",
      "    uint32_t qh;",
      "    memcpy(&qh, x[ib].qh, sizeof(qh));",
      "",
      "    const int xh_0 = ((qh >> (iqs +  0)) << 4) & 0x10;",
      "    const int xh_1 = ((qh >> (iqs + 12))     ) & 0x10;",
      "",
      "    v.x = __int2half_rn((x[ib].qs[iqs] & 0xf) | xh_0);",
      "    v.y = __int2half_rn((x[ib].qs[iqs] >>  4) | xh_1);",
      "",
      "    v = __hmul2(v, {d, d});",
      "    v = __hadd2(v, {m, m});",
      "}",
      "",
      "static __device__ __forceinline__ void dequantize_q8_0(const void * vx, const int ib, const int iqs, dfloat2 & v){",
      "    const block_q8_0 * x = (const block_q8_0 *) vx;",
      "",
      "    const dfloat d = x[ib].d;",
      "",
      "    v.x = __int2half_rn(x[ib].qs[iqs + 0]);",
      "    v.y = __int2half_rn(x[ib].qs[iqs + 1]);",
      "",
      "    v = __hmul2(v, {d, d});",
      "}",
      "",
      "template <int qk, int qr, dequantize_kernel_t dequantize_kernel, typename dst_t>",
      "static __global__ void dequantize_block(const void * __restrict__ vx, dst_t * __restrict__ y, const int k) {",
      "    const int i = 2*(blockDim.x*blockIdx.x + threadIdx.x);",
      "",
      "    if (i >= k) {",
      "        return;",
      "    }",
      "",
      "    const int ib = i/qk; // block index",
      "    const int iqs = (i%qk)/qr; // quant index",
      "    const int iybs = i - i%qk; // y block start index",
      "    const int y_offset = qr == 1 ? 1 : qk/2;",
      "",
      "    // dequantize",
      "    dfloat2 v;",
      "    dequantize_kernel(vx, ib, iqs, v);",
      "",
      "    y[iybs + iqs + 0]        = convert_from_half<dst_t>(v.x);",
      "    y[iybs + iqs + y_offset] = convert_from_half<dst_t>(v.y);",
      "}",
      "",
      "template<typename dst_t>",
      "static __global__ void dequantize_block_q2_K(const void * __restrict__ vx, dst_t * __restrict__ yy) {",
      "",
      "    const auto i   = blockIdx.x;",
      "    const block_q2_K * x = (const block_q2_K *) vx;",
      "",
      "    const auto tid = threadIdx.x;",
      "    const int n   = tid/32;",
      "    const int l   = tid - 32*n;",
      "    const int is  = 8*n + l/16;",
      "",
      "    const uint8_t q = x[i].qs[32*n + l];",
      "    dst_t * y = yy + i*QK_K + 128*n;",
      "",
      "    half dall = __low2half(x[i].dm);",
      "    half dmin = __high2half(x[i].dm);",
      "    y[l+ 0] = convert_from_half<dst_t>(__hsub(__hmul(dall, __int2half_rn((x[i].scales[is+0] & 0xF) * ((q >> 0) & 3))), __hmul(dmin,  __int2half_rn(x[i].scales[is+0] >> 4))));",
      "    y[l+32] = convert_from_half<dst_t>(__hsub(__hmul(dall, __int2half_rn((x[i].scales[is+2] & 0xF) * ((q >> 2) & 3))), __hmul(dmin,  __int2half_rn(x[i].scales[is+2] >> 4))));",
      "    y[l+64] = convert_from_half<dst_t>(__hsub(__hmul(dall, __int2half_rn((x[i].scales[is+4] & 0xF) * ((q >> 4) & 3))), __hmul(dmin,  __int2half_rn(x[i].scales[is+4] >> 4))));",
      "    y[l+96] = convert_from_half<dst_t>(__hsub(__hmul(dall, __int2half_rn((x[i].scales[is+6] & 0xF) * ((q >> 6) & 3))), __hmul(dmin,  __int2half_rn(x[i].scales[is+6] >> 4))));",
      "}",
      "",
      "template<typename dst_t>",
      "static __global__ void dequantize_block_q3_K(const void * __restrict__ vx, dst_t * __restrict__ yy) {",
      "",
      "    const auto i = blockIdx.x;",
      "    const block_q3_K * x = (const block_q3_K *) vx;",
      "",
      "    const auto r = threadIdx.x/4;",
      "    const int tid = r/2;",
      "    const int is0 = r%2;",
      "    const int l0 = 16*is0 + 4*(threadIdx.x%4);",
      "    const int n = tid / 4;",
      "    const int j = tid - 4*n;",
      "",
      "    uint8_t m = 1 << (4*n + j);",
      "    int is = 8*n + 2*j + is0;",
      "    int shift = 2*j;",
      "",
      "    int8_t us = is <  4 ? (x[i].scales[is-0] & 0xF) | (((x[i].scales[is+8] >> 0) & 3) << 4) :",
      "                is <  8 ? (x[i].scales[is-0] & 0xF) | (((x[i].scales[is+4] >> 2) & 3) << 4) :",
      "                is < 12 ? (x[i].scales[is-8] >>  4) | (((x[i].scales[is+0] >> 4) & 3) << 4) :",
      "                          (x[i].scales[is-8] >>  4) | (((x[i].scales[is-4] >> 6) & 3) << 4);",
      "    half d_all = x[i].d;",
      "    half dl = __hmul(d_all,  __int2half_rn(us - 32));",
      "",
      "    dst_t * y = yy + i*QK_K + 128*n + 32*j;",
      "    const uint8_t * q = x[i].qs + 32*n;",
      "    const uint8_t * hm = x[i].hmask;",
      "",
      "    for (int l = l0; l < l0+4; ++l) {",
      "        y[l] = convert_from_half<dst_t>(__hmul(dl,  __int2half_rn((int8_t)((q[l] >> shift) & 3) - ((hm[l] & m) ? 0 : 4))));",
      "    }",
      "}",
      "",
      "static inline __device__ void get_scale_min_k4(int j, const uint8_t * q, uint8_t & d, uint8_t & m) {",
      "    if (j < 4) {",
      "        d = q[j] & 63; m = q[j + 4] & 63;",
      "    } else {",
      "        d = (q[j+4] & 0xF) | ((q[j-4] >> 6) << 4);",
      "        m = (q[j+4] >>  4) | ((q[j-0] >> 6) << 4);",
      "    }",
      "}",
      "",
      "template<typename dst_t>",
      "static __global__ void dequantize_block_q4_K(const void * __restrict__ vx, dst_t * __restrict__ yy) {",
      "    const block_q4_K * x = (const block_q4_K *) vx;",
      "",
      "    const auto i = blockIdx.x;",
      "",
      "    // assume 32 threads",
      "    const auto tid = threadIdx.x;",
      "    const int il  = tid/8;",
      "    const int ir  = tid%8;",
      "    const int is  = 2*il;",
      "    const int n   = 4;",
      "",
      "    dst_t * y = yy + i*QK_K + 64*il + n*ir;",
      "",
      "    const half dall = __low2half(x[i].dm);",
      "    const half dmin = __high2half(x[i].dm);",
      "",
      "    const uint8_t * q = x[i].qs + 32*il + n*ir;",
      "",
      "    uint8_t sc, m;",
      "    get_scale_min_k4(is + 0, x[i].scales, sc, m);",
      "    const half d1 = __hmul(dall, __int2half_rn(sc));",
      "    const half m1 = __hmul(dmin,  __int2half_rn(m));",
      "    get_scale_min_k4(is + 1, x[i].scales, sc, m);",
      "    const half d2 = __hmul(dall, __int2half_rn(sc));",
      "    const half m2 = __hmul(dmin, __int2half_rn(m));",
      "    for (int l = 0; l < n; ++l) {",
      "        y[l + 0] = convert_from_half<dst_t>(__hsub(__hmul(d1, __int2half_rn(q[l] & 0xF)), m1));",
      "        y[l +32] = convert_from_half<dst_t>(__hsub(__hmul(d2,  __int2half_rn(q[l] >> 4)), m2));",
      "    }",
      "}",
      "",
      "template<typename dst_t>",
      "static __global__ void dequantize_block_q5_K(const void * __restrict__ vx, dst_t * __restrict__ yy) {",
      "    const block_q5_K * x = (const block_q5_K *) vx;",
      "",
      "    const auto i = blockIdx.x;",
      "",
      "    // assume 64 threads - this is very slightly better than the one below",
      "    const auto tid = threadIdx.x;",
      "    const int il  = tid/16;   // il is in 0...3",
      "    const int ir  = tid%16;   // ir is in 0...15",
      "    const int is  = 2*il;     // is is in 0...6",
      "",
      "    dst_t * y = yy + i*QK_K + 64*il + 2*ir;",
      "",
      "    const half dall = __low2half(x[i].dm);",
      "    const half dmin = __high2half(x[i].dm);",
      "",
      "    const uint8_t * ql = x[i].qs + 32*il + 2*ir;",
      "    const uint8_t * qh = x[i].qh + 2*ir;",
      "",
      "    uint8_t sc, m;",
      "    get_scale_min_k4(is + 0, x[i].scales, sc, m);",
      "    const half d1 = __hmul(dall, __int2half_rn(sc)); const half m1 = __hmul(dmin, __int2half_rn(m));",
      "    get_scale_min_k4(is + 1, x[i].scales, sc, m);",
      "    const half d2 = __hmul(dall, __int2half_rn(sc)); const half m2 = __hmul(dmin, __int2half_rn(m));",
      "",
      "    uint8_t   hm  = 1 << (2*il);",
      "    y[ 0] = convert_from_half<dst_t>(__hsub(__hmul(d1, __int2half_rn((ql[0] & 0xF) + (qh[0] & hm ? 16 : 0))), m1));",
      "    y[ 1] = convert_from_half<dst_t>(__hsub(__hmul(d1, __int2half_rn((ql[1] & 0xF) + (qh[1] & hm ? 16 : 0))), m1));",
      "    hm <<= 1;",
      "    y[32] = convert_from_half<dst_t>(__hsub(__hmul(d2, __int2half_rn((ql[0] >>  4) + (qh[0] & hm ? 16 : 0))), m2));",
      "    y[33] = convert_from_half<dst_t>(__hsub(__hmul(d2, __int2half_rn((ql[1] >>  4) + (qh[1] & hm ? 16 : 0))), m2));",
      "}",
      "",
      "template<typename dst_t>",
      "static __global__ void dequantize_block_q6_K(const void * __restrict__ vx, dst_t * __restrict__ yy) {",
      "    const block_q6_K * x = (const block_q6_K *) vx;",
      "",
      "    const auto i = blockIdx.x;",
      "",
      "    // assume 64 threads - this is very slightly better than the one below",
      "    const auto tid = threadIdx.x;",
      "    const int ip  = tid/32;   // ip is 0 or 1",
      "    const int il  = tid - 32*ip; // 0...32",
      "    const int is  = 8*ip + il/16;",
      "",
      "    dst_t * y = yy + i*QK_K + 128*ip + il;",
      "",
      "    const half d = x[i].d;",
      "",
      "    const uint8_t * ql = x[i].ql + 64*ip + il;",
      "    const uint8_t   qh = x[i].qh[32*ip + il];",
      "    const int8_t  * sc = x[i].scales + is;",
      "",
      "    y[ 0] = convert_from_half<dst_t>(__hmul(d, __int2half_rn(sc[0] * ((int8_t)((ql[ 0] & 0xF) | (((qh >> 0) & 3) << 4)) - 32))));",
      "    y[32] = convert_from_half<dst_t>(__hmul(d, __int2half_rn(sc[2] * ((int8_t)((ql[32] & 0xF) | (((qh >> 2) & 3) << 4)) - 32))));",
      "    y[64] = convert_from_half<dst_t>(__hmul(d, __int2half_rn(sc[4] * ((int8_t)((ql[ 0]  >> 4) | (((qh >> 4) & 3) << 4)) - 32))));",
      "    y[96] = convert_from_half<dst_t>(__hmul(d, __int2half_rn(sc[6] * ((int8_t)((ql[32]  >> 4) | (((qh >> 6) & 3) << 4)) - 32))));",
      "}",
      "",
      "template<typename dst_t>",
      "static __global__ void dequantize_block_iq2_xxs(const void * __restrict__ vx, dst_t * __restrict__ yy) {",
      "",
      "    const auto i   = blockIdx.x;",
      "    const block_iq2_xxs * x = (const block_iq2_xxs  *) vx;",
      "",
      "    const auto tid = threadIdx.x;",
      "    const int il = tid/8; // 0...3",
      "    const int ib = tid%8; // 0...7",
      "    dst_t * y = yy + i*QK_K + 32*ib + 8*il;",
      "    const uint16_t * q2 = x[i].qs + 4*ib;",
      "    const uint8_t  * aux8 = (const uint8_t *)q2;",
      "    const uint8_t  * grid = (const uint8_t *)(iq2xxs_grid + aux8[il]);",
      "    const uint32_t aux32 = q2[2] | (q2[3] << 16);",
      "    const float d = __half2float(x[i].d) * (0.5f + (aux32 >> 28)) * 0.25f;",
      "    const uint8_t signs = ksigns_iq2xs[(aux32 >> 7*il) & 127];",
      "    for (int j = 0; j < 8; ++j) y[j] = d * grid[j] * (signs & kmask_iq2xs[j] ? -1.f : 1.f);",
      "}",
      "",
      "template<typename dst_t>",
      "static __global__ void dequantize_block_iq2_xs(const void * __restrict__ vx, dst_t * __restrict__ yy) {",
      "",
      "    const auto i   = blockIdx.x;",
      "    const block_iq2_xs * x = (const block_iq2_xs *) vx;",
      "",
      "    const auto tid = threadIdx.x;",
      "    const int il = tid/8; // 0...3",
      "    const int ib = tid%8; // 0...7",
      "    dst_t * y = yy + i*QK_K + 32*ib + 8*il;",
      "    const uint16_t * q2 = x[i].qs + 4*ib;",
      "    const uint8_t  * grid = (const uint8_t *)(iq2xs_grid + (q2[il] & 511));",
      "    const float d = __half2float(x[i].d) * (0.5f + ((x[i].scales[ib] >> 4*(il/2)) & 0xf)) * 0.25f;",
      "    const uint8_t signs = ksigns_iq2xs[q2[il] >> 9];",
      "    for (int j = 0; j < 8; ++j) y[j] = d * grid[j] * (signs & kmask_iq2xs[j] ? -1.f : 1.f);",
      "",
      "}",
      "",
      "template<typename dst_t>",
      "static __global__ void dequantize_block_iq2_s(const void * __restrict__ vx, dst_t * __restrict__ yy) {",
      "",
      "    const auto i   = blockIdx.x;",
      "    const block_iq2_s * x = (const block_iq2_s *) vx;",
      "",
      "    const auto tid = threadIdx.x;",
      "    const int il = tid/8; // 0...3",
      "    const int ib = tid%8; // 0...7",
      "    dst_t * y = yy + i*QK_K + 32*ib + 8*il;",
      "    const uint8_t * grid = (const uint8_t *)(iq2s_grid + (x[i].qs[4*ib+il] | ((x[i].qh[ib] << (8-2*il)) & 0x300)));",
      "    const float d = __half2float(x[i].d) * (0.5f + ((x[i].scales[ib] >> 4*(il/2)) & 0xf)) * 0.25f;",
      "    const uint8_t signs = x[i].qs[QK_K/8+4*ib+il];",
      "    for (int j = 0; j < 8; ++j) y[j] = d * grid[j] * (signs & kmask_iq2xs[j] ? -1.f : 1.f);",
      "}",
      "",
      "template<typename dst_t>",
      "static __global__ void dequantize_block_iq3_xxs(const void * __restrict__ vx, dst_t * __restrict__ yy) {",
      "",
      "    const auto i   = blockIdx.x;",
      "    const block_iq3_xxs * x = (const block_iq3_xxs  *) vx;",
      "",
      "    const auto tid = threadIdx.x;",
      "    const int il = tid/8; // 0...3",
      "    const int ib = tid%8; // 0...7",
      "    dst_t * y = yy + i*QK_K + 32*ib + 8*il;",
      "    const uint8_t  * q3 = x[i].qs + 8*ib;",
      "    const uint16_t * gas = (const uint16_t *)(x[i].qs + QK_K/4) + 2*ib;",
      "    const uint8_t  * grid1 = (const uint8_t *)(iq3xxs_grid + q3[2*il+0]);",
      "    const uint8_t  * grid2 = (const uint8_t *)(iq3xxs_grid + q3[2*il+1]);",
      "    const uint32_t aux32 = gas[0] | (gas[1] << 16);",
      "    const float d = __half2float(x[i].d) * (0.5f + (aux32 >> 28)) * 0.5f;",
      "    const uint8_t signs = ksigns_iq2xs[(aux32 >> 7*il) & 127];",
      "    for (int j = 0; j < 4; ++j) {",
      "        y[j+0] = d * grid1[j] * (signs & kmask_iq2xs[j+0] ? -1.f : 1.f);",
      "        y[j+4] = d * grid2[j] * (signs & kmask_iq2xs[j+4] ? -1.f : 1.f);",
      "    }",
      "}",
      "",
      "template<typename dst_t>",
      "static __global__ void dequantize_block_iq3_s(const void * __restrict__ vx, dst_t * __restrict__ yy) {",
      "",
      "    const auto i   = blockIdx.x;",
      "    const block_iq3_s * x = (const block_iq3_s *) vx;",
      "",
      "    const auto tid = threadIdx.x;",
      "    const int il = tid/8; // 0...3",
      "    const int ib = tid%8; // 0...7",
      "    dst_t * y = yy + i*QK_K + 32*ib + 8*il;",
      "    const uint8_t * qs = x[i].qs + 8*ib;",
      "    const uint8_t * grid1 = (const uint8_t *)(iq3xs_grid + (qs[2*il+0] | ((x[i].qh[ib] << (8-2*il)) & 256)));",
      "    const uint8_t * grid2 = (const uint8_t *)(iq3xs_grid + (qs[2*il+1] | ((x[i].qh[ib] << (7-2*il)) & 256)));",
      "    const float d = __half2float(x[i].d) * (0.5f + ((x[i].scales[ib/2] >> 4*(ib%2)) & 0xf)) * 0.5f;",
      "    const uint8_t signs = x[i].signs[4*ib + il];",
      "    for (int j = 0; j < 4; ++j) {",
      "        y[j+0] = d * grid1[j] * (signs & kmask_iq2xs[j+0] ? -1.f : 1.f);",
      "        y[j+4] = d * grid2[j] * (signs & kmask_iq2xs[j+4] ? -1.f : 1.f);",
      "    }",
      "}",
      "",
      "template<typename dst_t>",
      "static __global__ void dequantize_block_iq1_s(const void * __restrict__ vx, dst_t * __restrict__ yy) {",
      "",
      "    const int64_t i   = blockIdx.x;",
      "    const block_iq1_s * x = (const block_iq1_s  *) vx;",
      "",
      "    const int64_t tid = threadIdx.x;",
      "    const int64_t il = tid/8; // 0...3",
      "    const int64_t ib = tid%8; // 0...7",
      "    dst_t * y = yy + i*QK_K + 32*ib + 8*il;",
      "    const float delta = x[i].qh[ib] & 0x8000 ? -1 - IQ1S_DELTA : -1 + IQ1S_DELTA;",
      "    const float d = __half2float(x[i].d) * (2*((x[i].qh[ib] >> 12) & 7) + 1);",
      "    uint32_t grid32[2]; const int8_t * q = (const int8_t *)grid32;",
      "    grid32[0] = iq1s_grid_gpu[x[i].qs[4*ib+il] | (((x[i].qh[ib] >> 3*il) & 7) << 8)];",
      "    grid32[1] = (grid32[0] >> 4) & 0x0f0f0f0f;",
      "    grid32[0] &= 0x0f0f0f0f;",
      "    for (int j = 0; j < 8; ++j) {",
      "        y[j] = d * (q[j] + delta);",
      "    }",
      "}",
      "",
      "template<typename dst_t>",
      "static __global__ void dequantize_block_iq1_m(const void * __restrict__ vx, dst_t * __restrict__ yy) {",
      "",
      "    const int64_t i   = blockIdx.x;",
      "    const block_iq1_m * x = (const block_iq1_m  *) vx;",
      "",
      "    const int64_t tid = threadIdx.x;",
      "    const int64_t il = tid/8; // 0...3",
      "    const int64_t ib = tid%8; // 0...7",
      "    dst_t * y = yy + i*QK_K + 32*ib + 8*il;",
      "    const uint16_t * sc = (const uint16_t *)x[i].scales;",
      "    iq1m_scale_t scale;",
      "    scale.u16 = (sc[0] >> 12) | ((sc[1] >> 8) & 0x00f0) | ((sc[2] >> 4) & 0x0f00) | (sc[3] & 0xf000);",
      "    const int64_t ib16 = 2*ib + il/2; // sc[ib16/4] >> 3*(ib16%4) -> sc[ib/2] >> 3*((2*ib+il/2)%4);",
      "    const float d = __half2float(scale.f16) * (2*((sc[ib16/4] >> 3*(ib16%4)) & 0x7) + 1);",
      "    const float delta = x[i].qh[2*ib+il/2] & (0x08 << 4*(il%2)) ? -1 - IQ1M_DELTA : -1 + IQ1M_DELTA;",
      "    uint32_t grid32[2]; const int8_t * q = (const int8_t *)grid32;",
      "    grid32[0] = iq1s_grid_gpu[x[i].qs[4*ib+il] | (((x[i].qh[2*ib+il/2] >> 4*(il%2)) & 7) << 8)];",
      "    grid32[1] = (grid32[0] >> 4) & 0x0f0f0f0f;",
      "    grid32[0] &= 0x0f0f0f0f;",
      "    for (int j = 0; j < 8; ++j) {",
      "        y[j] = d * (q[j] + delta);",
      "    }",
      "}",
      "",
      "template<typename dst_t>",
      "static __global__ void dequantize_block_iq4_nl(const void * __restrict__ vx, dst_t * __restrict__ yy) {",
      "",
      "    const auto i   = blockIdx.x;",
      "    const block_iq4_nl * x = (const block_iq4_nl *) vx + i*(QK_K/QK4_NL);",
      "",
      "    const auto tid = threadIdx.x;",
      "    const int il = tid/8; // 0...3",
      "    const int ib = tid%8; // 0...7",
      "    dst_t * y = yy + i*QK_K + 32*ib + 4*il;",
      "    const uint8_t  * q4 = x[ib].qs + 4*il;",
      "    const float d = __half2float(x[ib].d);",
      "    for (int j = 0; j < 4; ++j) {",
      "        y[j+ 0] = d * kvalues_iq4nl[q4[j] & 0xf];",
      "        y[j+16] = d * kvalues_iq4nl[q4[j] >>  4];",
      "    }",
      "",
      "}",
      "",
      "template<typename dst_t>",
      "static __global__ void dequantize_block_iq4_xs(const void * __restrict__ vx, dst_t * __restrict__ yy) {",
      "    const auto i   = blockIdx.x;",
      "    const block_iq4_xs * x = (const block_iq4_xs *)vx;",
      "",
      "    const auto tid = threadIdx.x;",
      "    const int il = tid/8; // 0...3",
      "    const int ib = tid%8; // 0...7",
      "    dst_t * y = yy + i*QK_K + 32*ib + 4*il;",
      "    const uint8_t  * q4 = x[i].qs + 16*ib + 4*il;",
      "    const float d = __half2float(x[i].d) * ((((x[i].scales_l[ib/2] >> 4*(ib%2)) & 0xf) | (((x[i].scales_h >> 2*ib) & 3) << 4)) - 32);",
      "    for (int j = 0; j < 4; ++j) {",
      "        y[j+ 0] = d * kvalues_iq4nl[q4[j] & 0xf];",
      "        y[j+16] = d * kvalues_iq4nl[q4[j] >>  4];",
      "    }",
      "}",
      "",
      "template <int qk, int qr, dequantize_kernel_t dequantize_kernel, typename dst_t>",
      "static void dequantize_block_cuda(const void * __restrict__ vx, dst_t * __restrict__ y, const int k, cudaStream_t stream) {",
      "    const int num_blocks = (k + 2*CUDA_DEQUANTIZE_BLOCK_SIZE - 1) / (2*CUDA_DEQUANTIZE_BLOCK_SIZE);",
      "    dequantize_block<qk, qr, dequantize_kernel><<<num_blocks, CUDA_DEQUANTIZE_BLOCK_SIZE, 0, stream>>>(vx, y, k);",
      "}",
      "",
      "template<typename dst_t>",
      "static void dequantize_row_q2_K_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {",
      "    const int nb = k / QK_K;",
      "    dequantize_block_q2_K<<<nb, 64, 0, stream>>>(vx, y);",
      "}",
      "",
      "template<typename dst_t>",
      "static void dequantize_row_q3_K_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {",
      "    const int nb = k / QK_K;",
      "    dequantize_block_q3_K<<<nb, 64, 0, stream>>>(vx, y);",
      "}",
      "",
      "template<typename dst_t>",
      "static void dequantize_row_q4_K_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {",
      "    const int nb = k / QK_K;",
      "    dequantize_block_q4_K<<<nb, 32, 0, stream>>>(vx, y);",
      "}",
      "",
      "template<typename dst_t>",
      "static void dequantize_row_q5_K_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {",
      "    const int nb = k / QK_K;",
      "    dequantize_block_q5_K<<<nb, 64, 0, stream>>>(vx, y);",
      "}",
      "",
      "template<typename dst_t>",
      "static void dequantize_row_q6_K_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {",
      "    const int nb = k / QK_K;",
      "    dequantize_block_q6_K<<<nb, 64, 0, stream>>>(vx, y);",
      "}",
      "",
      "template<typename dst_t>",
      "static void dequantize_row_iq2_xxs_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {",
      "    const int nb = k / QK_K;",
      "    dequantize_block_iq2_xxs<<<nb, 32, 0, stream>>>(vx, y);",
      "}",
      "",
      "template<typename dst_t>",
      "static void dequantize_row_iq2_xs_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {",
      "    const int nb = k / QK_K;",
      "    dequantize_block_iq2_xs<<<nb, 32, 0, stream>>>(vx, y);",
      "}",
      "",
      "template<typename dst_t>",
      "static void dequantize_row_iq2_s_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {",
      "    const int nb = k / QK_K;",
      "    dequantize_block_iq2_s<<<nb, 32, 0, stream>>>(vx, y);",
      "}",
      "",
      "template<typename dst_t>",
      "static void dequantize_row_iq3_xxs_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {",
      "    const int nb = k / QK_K;",
      "    dequantize_block_iq3_xxs<<<nb, 32, 0, stream>>>(vx, y);",
      "}",
      "",
      "template<typename dst_t>",
      "static void dequantize_row_iq3_s_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {",
      "    const int nb = k / QK_K;",
      "    dequantize_block_iq3_s<<<nb, 32, 0, stream>>>(vx, y);",
      "}",
      "",
      "template<typename dst_t>",
      "static void dequantize_row_iq1_s_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {",
      "    const int nb = k / QK_K;",
      "    dequantize_block_iq1_s<<<nb, 32, 0, stream>>>(vx, y);",
      "}",
      "",
      "template<typename dst_t>",
      "static void dequantize_row_iq1_m_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {",
      "    const int nb = k / QK_K;",
      "    dequantize_block_iq1_m<<<nb, 32, 0, stream>>>(vx, y);",
      "}",
      "",
      "template<typename dst_t>",
      "static void dequantize_row_iq4_nl_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {",
      "    const int nb = (k + QK_K - 1) / QK_K;",
      "    dequantize_block_iq4_nl<<<nb, 32, 0, stream>>>(vx, y);",
      "}",
      "",
      "template<typename dst_t>",
      "static void dequantize_row_iq4_xs_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {",
      "    const int nb = (k + QK_K - 1) / QK_K;",
      "    dequantize_block_iq4_xs<<<nb, 32, 0, stream>>>(vx, y);",
      "}",
      "",
      "template<typename dst_t>",
      "static to_cuda_ggml_t<dst_t> ggml_get_to_cuda(int64_t type) {",
      "    switch (type) {",
      "        case 2:",
      "            return dequantize_block_cuda<QK4_0, QR4_0, dequantize_q4_0>;",
      "        case 3:",
      "            return dequantize_block_cuda<QK4_1, QR4_1, dequantize_q4_1>;",
      "        case 6:",
      "            return dequantize_block_cuda<QK5_0, QR5_0, dequantize_q5_0>;",
      "        case 7:",
      "            return dequantize_block_cuda<QK5_1, QR5_1, dequantize_q5_1>;",
      "        case 8:",
      "            return dequantize_block_cuda<QK8_0, QR8_0, dequantize_q8_0>;",
      "        case 10:",
      "            return dequantize_row_q2_K_cuda;",
      "        case 11:",
      "            return dequantize_row_q3_K_cuda;",
      "        case 12:",
      "            return dequantize_row_q4_K_cuda;",
      "        case 13:",
      "            return dequantize_row_q5_K_cuda;",
      "        case 14:",
      "            return dequantize_row_q6_K_cuda;",
      "        case 16:",
      "            return dequantize_row_iq2_xxs_cuda;",
      "        case 17:",
      "            return dequantize_row_iq2_xs_cuda;",
      "        case 18:",
      "            return dequantize_row_iq3_xxs_cuda;",
      "        case 19:",
      "            return dequantize_row_iq1_s_cuda;",
      "        case 20:",
      "            return dequantize_row_iq4_nl_cuda;",
      "        case 21:",
      "            return dequantize_row_iq3_s_cuda;",
      "        case 22:",
      "            return dequantize_row_iq2_s_cuda;",
      "        case 23:",
      "            return dequantize_row_iq4_xs_cuda;",
      "        case 29:",
      "            return dequantize_row_iq1_m_cuda;",
      "        default:",
      "            return nullptr;",
      "    }",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gguf/mmvq.cuh",
    "source": [
      "// copied and adapted from https://github.com/ggerganov/llama.cpp/blob/b2899/ggml-cuda/mmvq.cu",
      "template <typename scalar_t, int qk, int qi, typename block_q_t, int vdr, vec_dot_q_cuda_t vec_dot_q_cuda>",
      "static __global__ void mul_mat_vec_q(const void * __restrict__ vx, const void * __restrict__ vy, scalar_t * __restrict__ dst, const int ncols, const int nrows, const int nvecs) {",
      "    const auto row = blockIdx.x*blockDim.y + threadIdx.y;",
      "    const auto vec = blockIdx.y;",
      "",
      "    if (row >= nrows || vec >= nvecs) {",
      "        return;",
      "    }",
      "",
      "    const int blocks_per_row = ncols / qk;",
      "    const int blocks_per_warp = vdr * WARP_SIZE / qi;",
      "    const int nrows_y = (ncols + 512 - 1) / 512 * 512;",
      "",
      "",
      "    // partial sum for each thread",
      "    float tmp = 0.0f;",
      "",
      "    const block_q_t  * x = (const block_q_t  *) vx;",
      "    const block_q8_1 * y = (const block_q8_1 *) vy;",
      "",
      "    for (auto i = threadIdx.x / (qi/vdr); i < blocks_per_row; i += blocks_per_warp) {",
      "        const int ibx = row*blocks_per_row + i; // x block index",
      "",
      "        const int iby = vec*(nrows_y/QK8_1) + i * (qk/QK8_1); // y block index that aligns with ibx",
      "",
      "        const int iqs  = vdr * (threadIdx.x % (qi/vdr)); // x block quant index when casting the quants to int",
      "",
      "        tmp += vec_dot_q_cuda(&x[ibx], &y[iby], iqs);",
      "    }",
      "",
      "    // sum up partial sums and write back result",
      "#pragma unroll",
      "    for (int mask = WARP_SIZE/2; mask > 0; mask >>= 1) {",
      "        tmp += VLLM_SHFL_XOR_SYNC(tmp, mask);",
      "    }",
      "",
      "    if (threadIdx.x == 0) {",
      "        dst[vec*nrows + row] = tmp;",
      "    }",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_q4_0_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK4_0, QI4_0, block_q4_0, VDR_Q4_0_Q8_1_MMVQ, vec_dot_q4_0_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_q4_1_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK4_0, QI4_1, block_q4_1, VDR_Q4_1_Q8_1_MMVQ, vec_dot_q4_1_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_q5_0_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK5_0, QI5_0, block_q5_0, VDR_Q5_0_Q8_1_MMVQ, vec_dot_q5_0_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_q5_1_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK5_1, QI5_1, block_q5_1, VDR_Q5_1_Q8_1_MMVQ, vec_dot_q5_1_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_q8_0_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK8_0, QI8_0, block_q8_0, VDR_Q8_0_Q8_1_MMVQ, vec_dot_q8_0_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_q2_K_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK_K, QI2_K, block_q2_K, VDR_Q2_K_Q8_1_MMVQ, vec_dot_q2_K_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_q3_K_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK_K, QI3_K, block_q3_K, VDR_Q3_K_Q8_1_MMVQ, vec_dot_q3_K_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_q4_K_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK_K, QI4_K, block_q4_K, VDR_Q4_K_Q8_1_MMVQ, vec_dot_q4_K_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_q5_K_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK_K, QI5_K, block_q5_K, VDR_Q5_K_Q8_1_MMVQ, vec_dot_q5_K_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_q6_K_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK_K, QI6_K, block_q6_K, VDR_Q6_K_Q8_1_MMVQ, vec_dot_q6_K_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_iq2_xxs_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK_K, QI2_XXS, block_iq2_xxs, 1, vec_dot_iq2_xxs_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_iq2_xs_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK_K, QI2_XS, block_iq2_xs, 1, vec_dot_iq2_xs_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_iq2_s_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK_K, QI2_S, block_iq2_s, 1, vec_dot_iq2_s_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_iq3_xxs_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK_K, QI3_XXS, block_iq3_xxs, 1, vec_dot_iq3_xxs_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_iq1_s_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK_K, QI1_S, block_iq1_s, 1, vec_dot_iq1_s_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_iq1_m_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK_K, QI1_M, block_iq1_m, 1, vec_dot_iq1_m_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_iq4_nl_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK4_NL, QI4_NL, block_iq4_nl, VDR_Q4_0_Q8_1_MMVQ, vec_dot_iq4_nl_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_iq4_xs_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK_K, QI4_XS, block_iq4_xs, 1, vec_dot_iq4_xs_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void mul_mat_vec_iq3_s_q8_1_cuda(const void * vx, const void * vy, scalar_t * dst, const int ncols, const int nrows, const int nvecs, cudaStream_t stream) {",
      "    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;",
      "    const dim3 block_nums(block_num_y, nvecs, 1);",
      "    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);",
      "    mul_mat_vec_q<scalar_t, QK_K, QI3_XS, block_iq3_s, 1, vec_dot_iq3_s_q8_1>",
      "        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, nvecs);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gguf/moe.cuh",
    "source": [
      "#include <cstdint>",
      "",
      "/* Adapted from ./csrc/quantization/gguf/mmq.cuh",
      "   based on ./vllm/model_executor/layers/fused_moe/fused_moe.py */",
      "template <typename scalar_t, int qk, int qr, int qi, bool need_sum,",
      "          typename block_q_t, int mmq_x, int mmq_y, int nwarps,",
      "          allocate_tiles_cuda_t allocate_tiles, load_tiles_cuda_t load_tiles,",
      "          int vdr, vec_dot_q_mul_mat_cuda_t vec_dot>",
      "static __device__ __forceinline__ void moe_q(",
      "    const void* __restrict__ vx, const void* __restrict__ vy,",
      "    scalar_t* __restrict__ dst, const int* __restrict__ sorted_token_ids,",
      "    const int* __restrict__ expert_ids,",
      "    const int* __restrict__ num_tokens_post_padded, const int exp_stride,",
      "    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y,",
      "    const int nrows_dst, const int top_k) {",
      "  const int blocks_per_row_x = ncols_x / qk;",
      "  const int blocks_per_col_y = nrows_y / QK8_1;",
      "  const int blocks_per_warp = WARP_SIZE_GGUF / qi;",
      "",
      "  const int ncols_dst = ncols_y * top_k;",
      "",
      "  const auto row_dst_0 = blockIdx.x * mmq_y;",
      "  const int& row_x_0 = row_dst_0;",
      "",
      "  const auto col_dst_0 = blockIdx.y * mmq_x;",
      "",
      "  int token_offs[mmq_x / nwarps];",
      "  for (int i = 0; i < mmq_x; i += nwarps) {",
      "    token_offs[i / nwarps] = sorted_token_ids[col_dst_0 + threadIdx.y + i];",
      "  }",
      "",
      "  const int exp_idx = expert_ids[blockIdx.y];",
      "  if (exp_idx > 255 || exp_idx < 0) return;",
      "  if (blockIdx.y * mmq_x > num_tokens_post_padded[0]) return;",
      "",
      "  const block_q_t* x = (const block_q_t*)((char*)vx + exp_idx * exp_stride);",
      "  const block_q8_1* y = (const block_q8_1*)(vy);",
      "",
      "  int* tile_x_ql = nullptr;",
      "  half2* tile_x_dm = nullptr;",
      "  int* tile_x_qh = nullptr;",
      "  int* tile_x_sc = nullptr;",
      "",
      "  allocate_tiles(&tile_x_ql, &tile_x_dm, &tile_x_qh, &tile_x_sc);",
      "",
      "  __shared__ int tile_y_qs[mmq_x * WARP_SIZE_GGUF];",
      "  __shared__ half2 tile_y_ds[mmq_x * WARP_SIZE_GGUF / QI8_1];",
      "",
      "  float sum[mmq_y / WARP_SIZE_GGUF][mmq_x / nwarps] = {{0.0f}};",
      "",
      "  for (int ib0 = 0; ib0 < blocks_per_row_x; ib0 += blocks_per_warp) {",
      "    load_tiles(x + row_x_0 * blocks_per_row_x + ib0, tile_x_ql, tile_x_dm,",
      "               tile_x_qh, tile_x_sc, threadIdx.y, nrows_x - row_x_0 - 1,",
      "               threadIdx.x, blocks_per_row_x);",
      "",
      "    const int n_per_r = ((qk * blocks_per_warp) / qr);",
      "#pragma unroll",
      "    for (int ir = 0; ir < qr && ib0 * qk + ir * n_per_r < ncols_x; ++ir) {",
      "      const auto kqs = ir * WARP_SIZE_GGUF + threadIdx.x;",
      "      const int kbxd = kqs / QI8_1;",
      "",
      "#pragma unroll",
      "      for (int i = 0; i < mmq_x; i += nwarps) {",
      "        const int col_y_eff = token_offs[i / nwarps] / top_k;",
      "        const int block_x = ib0 * (qk / QK8_1) + kbxd;",
      "        if (col_y_eff < ncols_y && block_x < blocks_per_col_y) {",
      "          const block_q8_1* by0 = &y[col_y_eff * blocks_per_col_y + block_x];",
      "          const int index_y =",
      "              (threadIdx.y + i) * WARP_SIZE_GGUF + kqs % WARP_SIZE_GGUF;",
      "          tile_y_qs[index_y] =",
      "              get_int_from_int8_aligned(by0->qs, threadIdx.x % QI8_1);",
      "        }",
      "      }",
      "",
      "      if (threadIdx.x < n_per_r / QK8_1) {",
      "        const auto kby = threadIdx.x % (WARP_SIZE_GGUF / QI8_1);",
      "        const int col_y_eff = token_offs[threadIdx.y] / top_k;",
      "        const int block_x =",
      "            ib0 * (qk / QK8_1) + ir * (WARP_SIZE_GGUF / QI8_1) + kby;",
      "",
      "        if (col_y_eff < ncols_y && block_x < blocks_per_col_y) {",
      "          const half2* dsi_src = &y[col_y_eff * blocks_per_col_y + block_x].ds;",
      "          half2* dsi_dst =",
      "              &tile_y_ds[threadIdx.y * (WARP_SIZE_GGUF / QI8_1) + kby];",
      "",
      "          if (need_sum) {",
      "            *dsi_dst = *dsi_src;",
      "          } else {",
      "            float* dfi_dst = (float*)dsi_dst;",
      "            *dfi_dst = __low2float(*dsi_src);",
      "          }",
      "        }",
      "      }",
      "      __syncthreads();",
      "",
      "      // #pragma unroll // unrolling this loop causes too much register pressure",
      "      for (int k = ir * WARP_SIZE_GGUF / qr; k < (ir + 1) * WARP_SIZE_GGUF / qr;",
      "           k += vdr) {",
      "#pragma unroll",
      "        for (int j = 0; j < mmq_x; j += nwarps) {",
      "#pragma unroll",
      "          for (int i = 0; i < mmq_y; i += WARP_SIZE_GGUF) {",
      "            sum[i / WARP_SIZE_GGUF][j / nwarps] +=",
      "                vec_dot(tile_x_ql, tile_x_dm, tile_x_qh, tile_x_sc, tile_y_qs,",
      "                        tile_y_ds, threadIdx.x + i, threadIdx.y + j, k);",
      "          }",
      "        }",
      "      }",
      "      __syncthreads();",
      "    }",
      "  }",
      "",
      "#pragma unroll",
      "  for (int j = 0; j < mmq_x; j += nwarps) {",
      "    const int col_dst = token_offs[j / nwarps];",
      "    if (col_dst >= ncols_dst) {",
      "      return;",
      "    }",
      "",
      "#pragma unroll",
      "    for (int i = 0; i < mmq_y; i += WARP_SIZE_GGUF) {",
      "      const auto row_dst = row_dst_0 + threadIdx.x + i;",
      "      if (row_dst >= nrows_dst) {",
      "        continue;",
      "      }",
      "      dst[col_dst * nrows_dst + row_dst] = sum[i / WARP_SIZE_GGUF][j / nwarps];",
      "    }",
      "  }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "  #define MOE_X_Q4_0 8",
      "  #define MOE_Y_Q4_0 128",
      "  #define NWARPS_Q4_0 8",
      "#else",
      "  #define MOE_X_Q4_0 4",
      "  #define MOE_Y_Q4_0 32",
      "  #define NWARPS_Q4_0 4",
      "#endif",
      "",
      "template <typename scalar_t, bool need_check>",
      "static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF* NWARPS_Q4_0, 2)",
      "#endif",
      "    moe_q4_0(const void* __restrict__ vx, const void* __restrict__ vy,",
      "             scalar_t* __restrict__ dst, const int* sorted_token_ids,",
      "             const int* expert_ids, const int* num_tokens_post_padded,",
      "             const int exp_stride, const int ncols_x, const int nrows_x,",
      "             const int ncols_y, const int nrows_y, const int nrows_dst,",
      "             const int top_k) {",
      "  const int mmq_x = MOE_X_Q4_0;",
      "  const int mmq_y = MOE_Y_Q4_0;",
      "  const int nwarps = NWARPS_Q4_0;",
      "",
      "  moe_q<scalar_t, QK4_0, QR4_0, QI4_0, true, block_q4_0, mmq_x, mmq_y, nwarps,",
      "        allocate_tiles_q4_0<mmq_y>, load_tiles_q4_0<mmq_y, nwarps, need_check>,",
      "        VDR_Q4_0_Q8_1_MMQ, vec_dot_q4_0_q8_1_mul_mat>(",
      "      vx, vy, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "      exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void ggml_moe_q4_0_q8_1_cuda(",
      "    const void* inp, const void* w, scalar_t* dst, const int* sorted_token_ids,",
      "    const int* expert_ids, const int* num_tokens_post_padded,",
      "    const int exp_stride, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, const int top_k,",
      "    const int tokens_post_padded, cudaStream_t stream) {",
      "  int mmq_x = MOE_X_Q4_0;",
      "  int mmq_y = MOE_Y_Q4_0;",
      "  int nwarps = NWARPS_Q4_0;",
      "",
      "  const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "  const int block_num_y = (tokens_post_padded) / mmq_x;",
      "  const dim3 block_nums(block_num_x, block_num_y, 1);",
      "  const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "  if (nrows_x % mmq_y == 0) {",
      "    constexpr bool need_check = false;",
      "    moe_q4_0<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  } else {",
      "    constexpr bool need_check = true;",
      "    moe_q4_0<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "  #define MOE_X_Q4_1 8",
      "  #define MOE_Y_Q4_1 128",
      "  #define NWARPS_Q4_1 8",
      "#else",
      "  #define MOE_X_Q4_1 4",
      "  #define MOE_Y_Q4_1 32",
      "  #define NWARPS_Q4_1 4",
      "#endif",
      "",
      "template <typename scalar_t, bool need_check>",
      "static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF* NWARPS_Q4_1, 2)",
      "#endif",
      "    moe_q4_1(const void* __restrict__ vx, const void* __restrict__ vy,",
      "             scalar_t* __restrict__ dst, const int* sorted_token_ids,",
      "             const int* expert_ids, const int* num_tokens_post_padded,",
      "             const int exp_stride, const int ncols_x, const int nrows_x,",
      "             const int ncols_y, const int nrows_y, const int nrows_dst,",
      "             const int top_k) {",
      "  const int mmq_x = MOE_X_Q4_1;",
      "  const int mmq_y = MOE_Y_Q4_1;",
      "  const int nwarps = NWARPS_Q4_1;",
      "",
      "  moe_q<scalar_t, QK4_1, QR4_1, QI4_1, true, block_q4_1, mmq_x, mmq_y, nwarps,",
      "        allocate_tiles_q4_1<mmq_y>, load_tiles_q4_1<mmq_y, nwarps, need_check>,",
      "        VDR_Q4_1_Q8_1_MMQ, vec_dot_q4_1_q8_1_mul_mat>(",
      "      vx, vy, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "      exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void ggml_moe_q4_1_q8_1_cuda(",
      "    const void* inp, const void* w, scalar_t* dst, const int* sorted_token_ids,",
      "    const int* expert_ids, const int* num_tokens_post_padded,",
      "    const int exp_stride, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, const int top_k,",
      "    const int tokens_post_padded, cudaStream_t stream) {",
      "  int mmq_x = MOE_X_Q4_1;",
      "  int mmq_y = MOE_Y_Q4_1;",
      "  int nwarps = NWARPS_Q4_1;",
      "",
      "  const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "  const int block_num_y = (tokens_post_padded) / mmq_x;",
      "  const dim3 block_nums(block_num_x, block_num_y, 1);",
      "  const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "  if (nrows_x % mmq_y == 0) {",
      "    constexpr bool need_check = false;",
      "    moe_q4_1<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  } else {",
      "    constexpr bool need_check = true;",
      "    moe_q4_1<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "  #define MOE_X_Q5_0 8",
      "  #define MOE_Y_Q5_0 128",
      "  #define NWARPS_Q5_0 8",
      "#else",
      "  #define MOE_X_Q5_0 4",
      "  #define MOE_Y_Q5_0 32",
      "  #define NWARPS_Q5_0 4",
      "#endif",
      "",
      "template <typename scalar_t, bool need_check>",
      "static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF* NWARPS_Q5_0, 2)",
      "#endif",
      "    moe_q5_0(const void* __restrict__ vx, const void* __restrict__ vy,",
      "             scalar_t* __restrict__ dst, const int* sorted_token_ids,",
      "             const int* expert_ids, const int* num_tokens_post_padded,",
      "             const int exp_stride, const int ncols_x, const int nrows_x,",
      "             const int ncols_y, const int nrows_y, const int nrows_dst,",
      "             const int top_k) {",
      "  const int mmq_x = MOE_X_Q5_0;",
      "  const int mmq_y = MOE_Y_Q5_0;",
      "  const int nwarps = NWARPS_Q5_0;",
      "",
      "  moe_q<scalar_t, QK5_0, QR5_0, QI5_0, false, block_q5_0, mmq_x, mmq_y, nwarps,",
      "        allocate_tiles_q5_0<mmq_y>, load_tiles_q5_0<mmq_y, nwarps, need_check>,",
      "        VDR_Q5_0_Q8_1_MMQ, vec_dot_q5_0_q8_1_mul_mat>(",
      "      vx, vy, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "      exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void ggml_moe_q5_0_q8_1_cuda(",
      "    const void* inp, const void* w, scalar_t* dst, const int* sorted_token_ids,",
      "    const int* expert_ids, const int* num_tokens_post_padded,",
      "    const int exp_stride, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, const int top_k,",
      "    const int tokens_post_padded, cudaStream_t stream) {",
      "  const int mmq_x = MOE_X_Q5_0;",
      "  const int mmq_y = MOE_Y_Q5_0;",
      "  const int nwarps = NWARPS_Q5_0;",
      "",
      "  const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "  const int block_num_y = (tokens_post_padded) / mmq_x;",
      "  const dim3 block_nums(block_num_x, block_num_y, 1);",
      "  const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "  if (nrows_x % mmq_y == 0) {",
      "    constexpr bool need_check = false;",
      "    moe_q5_0<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  } else {",
      "    constexpr bool need_check = true;",
      "    moe_q5_0<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "  #define MOE_X_Q5_1 8",
      "  #define MOE_Y_Q5_1 128",
      "  #define NWARPS_Q5_1 8",
      "#else",
      "  #define MOE_X_Q5_1 4",
      "  #define MOE_Y_Q5_1 32",
      "  #define NWARPS_Q5_1 4",
      "#endif",
      "",
      "template <typename scalar_t, bool need_check>",
      "static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF* NWARPS_Q5_1, 2)",
      "#endif",
      "    moe_q5_1(const void* __restrict__ vx, const void* __restrict__ vy,",
      "             scalar_t* __restrict__ dst, const int* sorted_token_ids,",
      "             const int* expert_ids, const int* num_tokens_post_padded,",
      "             const int exp_stride, const int ncols_x, const int nrows_x,",
      "             const int ncols_y, const int nrows_y, const int nrows_dst,",
      "             const int top_k) {",
      "  const int mmq_x = MOE_X_Q5_1;",
      "  const int mmq_y = MOE_Y_Q5_1;",
      "  const int nwarps = NWARPS_Q5_1;",
      "",
      "  moe_q<scalar_t, QK5_1, QR5_1, QI5_1, true, block_q5_1, mmq_x, mmq_y, nwarps,",
      "        allocate_tiles_q5_1<mmq_y>, load_tiles_q5_1<mmq_y, nwarps, need_check>,",
      "        VDR_Q5_1_Q8_1_MMQ, vec_dot_q5_1_q8_1_mul_mat>(",
      "      vx, vy, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "      exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void ggml_moe_q5_1_q8_1_cuda(",
      "    const void* inp, const void* w, scalar_t* dst, const int* sorted_token_ids,",
      "    const int* expert_ids, const int* num_tokens_post_padded,",
      "    const int exp_stride, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, const int top_k,",
      "    const int tokens_post_padded, cudaStream_t stream) {",
      "  const int mmq_x = MOE_X_Q5_1;",
      "  const int mmq_y = MOE_Y_Q5_1;",
      "  const int nwarps = NWARPS_Q5_1;",
      "",
      "  const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "  const int block_num_y = (tokens_post_padded) / mmq_x;",
      "  const dim3 block_nums(block_num_x, block_num_y, 1);",
      "  const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "  if (nrows_x % mmq_y == 0) {",
      "    constexpr bool need_check = false;",
      "    moe_q5_1<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  } else {",
      "    constexpr bool need_check = true;",
      "    moe_q5_1<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "  #define MOE_X_Q8_0 8",
      "  #define MOE_Y_Q8_0 128",
      "  #define NWARPS_Q8_0 8",
      "#else",
      "  #define MOE_X_Q8_0 4",
      "  #define MOE_Y_Q8_0 32",
      "  #define NWARPS_Q8_0 4",
      "#endif",
      "",
      "template <typename scalar_t, bool need_check>",
      "static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF* NWARPS_Q8_0, 2)",
      "#endif",
      "    moe_q8_0(const void* __restrict__ vx, const void* __restrict__ vy,",
      "             scalar_t* __restrict__ dst, const int* sorted_token_ids,",
      "             const int* expert_ids, const int* num_tokens_post_padded,",
      "             const int exp_stride, const int ncols_x, const int nrows_x,",
      "             const int ncols_y, const int nrows_y, const int nrows_dst,",
      "             const int top_k) {",
      "  const int mmq_x = MOE_X_Q8_0;",
      "  const int mmq_y = MOE_Y_Q8_0;",
      "  const int nwarps = NWARPS_Q8_0;",
      "",
      "  moe_q<scalar_t, QK8_0, QR8_0, QI8_0, false, block_q8_0, mmq_x, mmq_y, nwarps,",
      "        allocate_tiles_q8_0<mmq_y>, load_tiles_q8_0<mmq_y, nwarps, need_check>,",
      "        VDR_Q8_0_Q8_1_MMQ, vec_dot_q8_0_q8_1_mul_mat>(",
      "      vx, vy, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "      exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void ggml_moe_q8_0_q8_1_cuda(",
      "    const void* inp, const void* w, scalar_t* dst, const int* sorted_token_ids,",
      "    const int* expert_ids, const int* num_tokens_post_padded,",
      "    const int exp_stride, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, const int top_k,",
      "    const int tokens_post_padded, cudaStream_t stream) {",
      "  const int mmq_x = MOE_X_Q8_0;",
      "  const int mmq_y = MOE_Y_Q8_0;",
      "  const int nwarps = NWARPS_Q8_0;",
      "",
      "  const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "  const int block_num_y = (tokens_post_padded) / mmq_x;",
      "  const dim3 block_nums(block_num_x, block_num_y, 1);",
      "  const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "  if (nrows_x % mmq_y == 0) {",
      "    constexpr bool need_check = false;",
      "    moe_q8_0<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  } else {",
      "    constexpr bool need_check = true;",
      "    moe_q8_0<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "  #define MOE_X_Q2_K 8",
      "  #define MOE_Y_Q2_K 128",
      "  #define NWARPS_Q2_K 8",
      "#else",
      "  #define MOE_X_Q2_K 4",
      "  #define MOE_Y_Q2_K 32",
      "  #define NWARPS_Q2_K 4",
      "#endif",
      "",
      "template <typename scalar_t, bool need_check>",
      "static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF* NWARPS_Q2_K, 2)",
      "#endif",
      "    moe_q2_K(const void* __restrict__ vx, const void* __restrict__ vy,",
      "             scalar_t* __restrict__ dst, const int* sorted_token_ids,",
      "             const int* expert_ids, const int* num_tokens_post_padded,",
      "             const int exp_stride, const int ncols_x, const int nrows_x,",
      "             const int ncols_y, const int nrows_y, const int nrows_dst,",
      "             const int top_k) {",
      "  const int mmq_x = MOE_X_Q2_K;",
      "  const int mmq_y = MOE_Y_Q2_K;",
      "  const int nwarps = NWARPS_Q2_K;",
      "",
      "  moe_q<scalar_t, QK_K, QR2_K, QI2_K, false, block_q2_K, mmq_x, mmq_y, nwarps,",
      "        allocate_tiles_q2_K<mmq_y>, load_tiles_q2_K<mmq_y, nwarps, need_check>,",
      "        VDR_Q2_K_Q8_1_MMQ, vec_dot_q2_K_q8_1_mul_mat>(",
      "      vx, vy, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "      exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void ggml_moe_q2_K_q8_1_cuda(",
      "    const void* inp, const void* w, scalar_t* dst, const int* sorted_token_ids,",
      "    const int* expert_ids, const int* num_tokens_post_padded,",
      "    const int exp_stride, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, const int top_k,",
      "    const int tokens_post_padded, cudaStream_t stream) {",
      "  const int mmq_x = MOE_X_Q2_K;",
      "  const int mmq_y = MOE_Y_Q2_K;",
      "  const int nwarps = NWARPS_Q2_K;",
      "",
      "  const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "  const int block_num_y = (tokens_post_padded) / mmq_x;",
      "  const dim3 block_nums(block_num_x, block_num_y, 1);",
      "  const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "  if (nrows_x % mmq_y == 0) {",
      "    constexpr bool need_check = false;",
      "    moe_q2_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  } else {",
      "    constexpr bool need_check = true;",
      "    moe_q2_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "  #define MOE_X_Q3_K 8",
      "  #define MOE_Y_Q3_K 128",
      "  #define NWARPS_Q3_K 8",
      "#else",
      "  #define MOE_X_Q3_K 4",
      "  #define MOE_Y_Q3_K 32",
      "  #define NWARPS_Q3_K 4",
      "#endif",
      "",
      "template <typename scalar_t, bool need_check>",
      "static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF* NWARPS_Q3_K, 2)",
      "#endif",
      "    moe_q3_K(const void* __restrict__ vx, const void* __restrict__ vy,",
      "             scalar_t* __restrict__ dst, const int* sorted_token_ids,",
      "             const int* expert_ids, const int* num_tokens_post_padded,",
      "             const int exp_stride, const int ncols_x, const int nrows_x,",
      "             const int ncols_y, const int nrows_y, const int nrows_dst,",
      "             const int top_k) {",
      "",
      "  const int mmq_x = MOE_X_Q3_K;",
      "  const int mmq_y = MOE_Y_Q3_K;",
      "  const int nwarps = NWARPS_Q3_K;",
      "",
      "  moe_q<scalar_t, QK_K, QR3_K, QI3_K, false, block_q3_K, mmq_x, mmq_y, nwarps,",
      "        allocate_tiles_q3_K<mmq_y>, load_tiles_q3_K<mmq_y, nwarps, need_check>,",
      "        VDR_Q3_K_Q8_1_MMQ, vec_dot_q3_K_q8_1_mul_mat>(",
      "      vx, vy, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "      exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "}",
      "template <typename scalar_t>",
      "static void ggml_moe_q3_K_q8_1_cuda(",
      "    const void* inp, const void* w, scalar_t* dst, const int* sorted_token_ids,",
      "    const int* expert_ids, const int* num_tokens_post_padded,",
      "    const int exp_stride, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, const int top_k,",
      "    const int tokens_post_padded, cudaStream_t stream) {",
      "  const int mmq_x = MOE_X_Q3_K;",
      "  const int mmq_y = MOE_Y_Q3_K;",
      "  const int nwarps = NWARPS_Q3_K;",
      "",
      "  const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "  const int block_num_y = (tokens_post_padded) / mmq_x;",
      "  const dim3 block_nums(block_num_x, block_num_y, 1);",
      "  const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "  if (nrows_x % mmq_y == 0) {",
      "    constexpr bool need_check = false;",
      "    moe_q3_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  } else {",
      "    constexpr bool need_check = true;",
      "    moe_q3_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "  #define MOE_X_Q4_K 8",
      "  #define MOE_Y_Q4_K 128",
      "  #define NWARPS_Q4_K 8",
      "#else",
      "  #define MOE_X_Q4_K 4",
      "  #define MOE_Y_Q4_K 32",
      "  #define NWARPS_Q4_K 4",
      "#endif",
      "",
      "template <typename scalar_t, bool need_check>",
      "static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF* NWARPS_Q4_K, 2)",
      "#endif",
      "    moe_q4_K(const void* __restrict__ vx, const void* __restrict__ vy,",
      "             scalar_t* __restrict__ dst, const int* sorted_token_ids,",
      "             const int* expert_ids, const int* num_tokens_post_padded,",
      "             const int exp_stride, const int ncols_x, const int nrows_x,",
      "             const int ncols_y, const int nrows_y, const int nrows_dst,",
      "             const int top_k) {",
      "  const int mmq_x = MOE_X_Q4_K;",
      "  const int mmq_y = MOE_Y_Q4_K;",
      "  const int nwarps = NWARPS_Q4_K;",
      "",
      "  moe_q<scalar_t, QK_K, QR4_K, QI4_K, true, block_q4_K, mmq_x, mmq_y, nwarps,",
      "        allocate_tiles_q4_K<mmq_y>, load_tiles_q4_K<mmq_y, nwarps, need_check>,",
      "        VDR_Q4_K_Q8_1_MMQ, vec_dot_q4_K_q8_1_mul_mat>(",
      "      vx, vy, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "      exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void ggml_moe_q4_K_q8_1_cuda(",
      "    const void* inp, const void* w, scalar_t* dst, const int* sorted_token_ids,",
      "    const int* expert_ids, const int* num_tokens_post_padded,",
      "    const int exp_stride, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, const int top_k,",
      "    const int tokens_post_padded, cudaStream_t stream) {",
      "  const int mmq_x = MOE_X_Q4_K;",
      "  const int mmq_y = MOE_Y_Q4_K;",
      "  const int nwarps = NWARPS_Q4_K;",
      "",
      "  const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "  const int block_num_y = (tokens_post_padded) / mmq_x;",
      "  const dim3 block_nums(block_num_x, block_num_y, 1);",
      "  const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "  if (nrows_x % mmq_y == 0) {",
      "    constexpr bool need_check = false;",
      "    moe_q4_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  } else {",
      "    constexpr bool need_check = true;",
      "    moe_q4_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "  #define MOE_X_Q5_K 8",
      "  #define MOE_Y_Q5_K 128",
      "  #define NWARPS_Q5_K 8",
      "#else",
      "  #define MOE_X_Q5_K 4",
      "  #define MOE_Y_Q5_K 32",
      "  #define NWARPS_Q5_K 4",
      "#endif",
      "",
      "template <typename scalar_t, bool need_check>",
      "static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF* NWARPS_Q5_K, 2)",
      "#endif",
      "    moe_q5_K(const void* __restrict__ vx, const void* __restrict__ vy,",
      "             scalar_t* __restrict__ dst, const int* sorted_token_ids,",
      "             const int* expert_ids, const int* num_tokens_post_padded,",
      "             const int exp_stride, const int ncols_x, const int nrows_x,",
      "             const int ncols_y, const int nrows_y, const int nrows_dst,",
      "             const int top_k) {",
      "  const int mmq_x = MOE_X_Q5_K;",
      "  const int mmq_y = MOE_Y_Q5_K;",
      "  const int nwarps = NWARPS_Q5_K;",
      "",
      "  moe_q<scalar_t, QK_K, QR5_K, QI5_K, true, block_q5_K, mmq_x, mmq_y, nwarps,",
      "        allocate_tiles_q5_K<mmq_y>, load_tiles_q5_K<mmq_y, nwarps, need_check>,",
      "        VDR_Q5_K_Q8_1_MMQ, vec_dot_q5_K_q8_1_mul_mat>(",
      "      vx, vy, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "      exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void ggml_moe_q5_K_q8_1_cuda(",
      "    const void* inp, const void* w, scalar_t* dst, const int* sorted_token_ids,",
      "    const int* expert_ids, const int* num_tokens_post_padded,",
      "    const int exp_stride, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, const int top_k,",
      "    const int tokens_post_padded, cudaStream_t stream) {",
      "  const int mmq_x = MOE_X_Q5_K;",
      "  const int mmq_y = MOE_Y_Q5_K;",
      "  const int nwarps = NWARPS_Q5_K;",
      "",
      "  const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "  const int block_num_y = (tokens_post_padded) / mmq_x;",
      "  const dim3 block_nums(block_num_x, block_num_y, 1);",
      "  const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "  if (nrows_x % mmq_y == 0) {",
      "    constexpr bool need_check = false;",
      "    moe_q5_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  } else {",
      "    constexpr bool need_check = true;",
      "    moe_q5_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "  #define MOE_X_Q6_K 8",
      "  #define MOE_Y_Q6_K 128",
      "  #define NWARPS_Q6_K 8",
      "#else",
      "  #define MOE_X_Q6_K 4",
      "  #define MOE_Y_Q6_K 32",
      "  #define NWARPS_Q6_K 4",
      "#endif",
      "",
      "template <typename scalar_t, bool need_check>",
      "static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF* NWARPS_Q6_K, 2)",
      "#endif",
      "    moe_q6_K(const void* __restrict__ vx, const void* __restrict__ vy,",
      "             scalar_t* __restrict__ dst, const int* sorted_token_ids,",
      "             const int* expert_ids, const int* num_tokens_post_padded,",
      "             const int exp_stride, const int ncols_x, const int nrows_x,",
      "             const int ncols_y, const int nrows_y, const int nrows_dst,",
      "             const int top_k) {",
      "  const int mmq_x = MOE_X_Q6_K;",
      "  const int mmq_y = MOE_Y_Q6_K;",
      "  const int nwarps = NWARPS_Q6_K;",
      "",
      "  moe_q<scalar_t, QK_K, QR6_K, QI6_K, false, block_q6_K, mmq_x, mmq_y, nwarps,",
      "        allocate_tiles_q6_K<mmq_y>, load_tiles_q6_K<mmq_y, nwarps, need_check>,",
      "        VDR_Q6_K_Q8_1_MMQ, vec_dot_q6_K_q8_1_mul_mat>(",
      "      vx, vy, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "      exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "}",
      "",
      "template <typename scalar_t>",
      "static void ggml_moe_q6_K_q8_1_cuda(",
      "    const void* inp, const void* w, scalar_t* dst, const int* sorted_token_ids,",
      "    const int* expert_ids, const int* num_tokens_post_padded,",
      "    const int exp_stride, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, const int top_k,",
      "    const int tokens_post_padded, cudaStream_t stream) {",
      "  const int mmq_x = MOE_X_Q6_K;",
      "  const int mmq_y = MOE_Y_Q6_K;",
      "  const int nwarps = NWARPS_Q6_K;",
      "",
      "  const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "  const int block_num_y = (tokens_post_padded) / mmq_x;",
      "  const dim3 block_nums(block_num_x, block_num_y, 1);",
      "  const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "  if (nrows_x % mmq_y == 0) {",
      "    constexpr bool need_check = false;",
      "    moe_q6_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  } else {",
      "    constexpr bool need_check = true;",
      "    moe_q6_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>(",
      "        w, inp, dst, sorted_token_ids, expert_ids, num_tokens_post_padded,",
      "        exp_stride, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst, top_k);",
      "  }",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gguf/mmq.cuh",
    "source": [
      "// copied from https://github.com/ggerganov/llama.cpp/blob/b2899/ggml-cuda/mmq.cu",
      "template <typename scalar_t, int qk, int qr, int qi, bool need_sum, typename block_q_t, int mmq_x, int mmq_y, int nwarps,",
      "              allocate_tiles_cuda_t allocate_tiles, load_tiles_cuda_t load_tiles, int vdr, vec_dot_q_mul_mat_cuda_t vec_dot>",
      "static __device__ __forceinline__ void mul_mat_q(",
      "    const void * __restrict__ vx, const void * __restrict__ vy, scalar_t * __restrict__ dst,",
      "    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {",
      "",
      "    const block_q_t  * x = (const block_q_t  *) vx;",
      "    const block_q8_1 * y = (const block_q8_1 *) vy;",
      "",
      "    const int blocks_per_row_x = ncols_x / qk;",
      "    const int blocks_per_col_y = nrows_y / QK8_1;",
      "    const int blocks_per_warp = WARP_SIZE_GGUF / qi;",
      "",
      "    const int & ncols_dst = ncols_y;",
      "",
      "    const auto row_dst_0 = blockIdx.x*mmq_y;",
      "    const int & row_x_0 = row_dst_0;",
      "",
      "    const auto col_dst_0 = blockIdx.y*mmq_x;",
      "    const int & col_y_0 = col_dst_0;",
      "",
      "    int   * tile_x_ql = nullptr;",
      "    half2 * tile_x_dm = nullptr;",
      "    int   * tile_x_qh = nullptr;",
      "    int   * tile_x_sc = nullptr;",
      "",
      "    allocate_tiles(&tile_x_ql, &tile_x_dm, &tile_x_qh, &tile_x_sc);",
      "",
      "    __shared__ int    tile_y_qs[mmq_x * WARP_SIZE_GGUF];",
      "    __shared__ half2  tile_y_ds[mmq_x * WARP_SIZE_GGUF/QI8_1];",
      "",
      "    float sum[mmq_y/WARP_SIZE_GGUF][mmq_x/nwarps] = {{0.0f}};",
      "",
      "    for (int ib0 = 0; ib0 < blocks_per_row_x; ib0 += blocks_per_warp) {",
      "",
      "        load_tiles(x + row_x_0*blocks_per_row_x + ib0, tile_x_ql, tile_x_dm, tile_x_qh, tile_x_sc,",
      "                   threadIdx.y, nrows_x-row_x_0-1, threadIdx.x, blocks_per_row_x);",
      "",
      "#pragma unroll",
      "        for (int ir = 0; ir < qr && ib0 + ir * blocks_per_warp/qr < blocks_per_row_x; ++ir) {",
      "            const auto kqs = ir*WARP_SIZE_GGUF + threadIdx.x;",
      "            const int kbxd = kqs / QI8_1;",
      "",
      "#pragma unroll",
      "            for (int i = 0; i < mmq_x; i += nwarps) {",
      "                const int col_y_eff = min(col_y_0 + threadIdx.y + i, ncols_y-1); // to prevent out-of-bounds memory accesses",
      "                const block_q8_1 * by0 = &y[col_y_eff*blocks_per_col_y + ib0 * (qk/QK8_1) + kbxd];",
      "                const int index_y = (threadIdx.y + i) * WARP_SIZE_GGUF + kqs % WARP_SIZE_GGUF;",
      "                tile_y_qs[index_y] = get_int_from_int8_aligned(by0->qs, threadIdx.x % QI8_1);",
      "            }",
      "",
      "#pragma unroll",
      "            for (int ids0 = 0; ids0 < mmq_x; ids0 += nwarps * QI8_1) {",
      "                const int ids = (ids0 + threadIdx.y * QI8_1 + threadIdx.x / (WARP_SIZE_GGUF/QI8_1)) % mmq_x;",
      "                const auto kby = threadIdx.x % (WARP_SIZE_GGUF/QI8_1);",
      "                const int col_y_eff = min(col_y_0 + ids, ncols_y-1);",
      "",
      "                // if the sum is not needed it's faster to transform the scale to f32 ahead of time",
      "                const half2 * dsi_src = &y[col_y_eff*blocks_per_col_y + ib0 * (qk/QK8_1) + ir*(WARP_SIZE_GGUF/QI8_1) + kby].ds;",
      "                half2       * dsi_dst = &tile_y_ds[ids * (WARP_SIZE_GGUF/QI8_1) + kby];",
      "                if (need_sum) {",
      "                    *dsi_dst = *dsi_src;",
      "                } else {",
      "                    float * dfi_dst = (float *) dsi_dst;",
      "                    *dfi_dst = __low2float(*dsi_src);",
      "                }",
      "            }",
      "",
      "            __syncthreads();",
      "",
      "// #pragma unroll // unrolling this loop causes too much register pressure",
      "            for (int k = ir*WARP_SIZE_GGUF/qr; k < (ir+1)*WARP_SIZE_GGUF/qr; k += vdr) {",
      "#pragma unroll",
      "                for (int j = 0; j < mmq_x; j += nwarps) {",
      "#pragma unroll",
      "                    for (int i = 0; i < mmq_y; i += WARP_SIZE_GGUF) {",
      "                        sum[i/WARP_SIZE_GGUF][j/nwarps] += vec_dot(",
      "                            tile_x_ql, tile_x_dm, tile_x_qh, tile_x_sc, tile_y_qs, tile_y_ds,",
      "                            threadIdx.x + i, threadIdx.y + j, k);",
      "                    }",
      "                }",
      "            }",
      "            __syncthreads();",
      "        }",
      "    }",
      "",
      "#pragma unroll",
      "    for (int j = 0; j < mmq_x; j += nwarps) {",
      "        const auto col_dst = col_dst_0 + j + threadIdx.y;",
      "        if (col_dst >= ncols_dst) {",
      "            return;",
      "        }",
      "",
      "#pragma unroll",
      "        for (int i = 0; i < mmq_y; i += WARP_SIZE_GGUF) {",
      "            const auto row_dst = row_dst_0 + threadIdx.x + i;",
      "            if (row_dst >= nrows_dst) {",
      "                continue;",
      "            }",
      "            dst[col_dst*nrows_dst + row_dst] = sum[i/WARP_SIZE_GGUF][j/nwarps];",
      "        }",
      "    }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "#define  MMQ_X_Q4_0  64",
      "#define  MMQ_Y_Q4_0  128",
      "#define NWARPS_Q4_0  8",
      "#else",
      "#define  MMQ_X_Q4_0 4",
      "#define  MMQ_Y_Q4_0 32",
      "#define NWARPS_Q4_0 4",
      "#endif",
      "",
      "template<typename scalar_t, bool need_check> static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF*NWARPS_Q4_0, 2)",
      "#endif",
      "mul_mat_q4_0(",
      "    const void * __restrict__ vx, const void * __restrict__ vy, scalar_t * __restrict__ dst,",
      "    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {",
      "    const int mmq_x  =  MMQ_X_Q4_0;",
      "    const int mmq_y  =  MMQ_Y_Q4_0;",
      "    const int nwarps = NWARPS_Q4_0;",
      "",
      "    mul_mat_q<scalar_t, QK4_0, QR4_0, QI4_0, true, block_q4_0, mmq_x, mmq_y, nwarps, allocate_tiles_q4_0<mmq_y>,",
      "        load_tiles_q4_0<mmq_y, nwarps, need_check>, VDR_Q4_0_Q8_1_MMQ, vec_dot_q4_0_q8_1_mul_mat>",
      "        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void ggml_mul_mat_q4_0_q8_1_cuda(",
      "    const void * vx, const void * vy, scalar_t * dst, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {",
      "",
      "    int mmq_x  =  MMQ_X_Q4_0;",
      "    int mmq_y  =  MMQ_Y_Q4_0;",
      "    int nwarps = NWARPS_Q4_0;",
      "",
      "    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;",
      "    const dim3 block_nums(block_num_x, block_num_y, 1);",
      "    const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "    if (nrows_x % mmq_y == 0) {",
      "        const bool need_check = false;",
      "        mul_mat_q4_0<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    } else {",
      "        const bool need_check = true;",
      "        mul_mat_q4_0<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "#define  MMQ_X_Q4_1 64",
      "#define  MMQ_Y_Q4_1 128",
      "#define NWARPS_Q4_1 8",
      "#else",
      "#define  MMQ_X_Q4_1 4",
      "#define  MMQ_Y_Q4_1 32",
      "#define NWARPS_Q4_1 4",
      "#endif",
      "",
      "template<typename scalar_t, bool need_check> static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF*NWARPS_Q4_1, 2)",
      "#endif",
      "mul_mat_q4_1(",
      "    const void * __restrict__ vx, const void * __restrict__ vy, scalar_t * __restrict__ dst,",
      "    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {",
      "    const int mmq_x  =  MMQ_X_Q4_1;",
      "    const int mmq_y  =  MMQ_Y_Q4_1;",
      "    const int nwarps = NWARPS_Q4_1;",
      "",
      "    mul_mat_q<scalar_t, QK4_1, QR4_1, QI4_1, true, block_q4_1, mmq_x, mmq_y, nwarps, allocate_tiles_q4_1<mmq_y>,",
      "        load_tiles_q4_1<mmq_y, nwarps, need_check>, VDR_Q4_1_Q8_1_MMQ, vec_dot_q4_1_q8_1_mul_mat>",
      "        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void ggml_mul_mat_q4_1_q8_1_cuda(",
      "    const void * vx, const void * vy, scalar_t * dst, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {",
      "",
      "    int mmq_x  =  MMQ_X_Q4_1;",
      "    int mmq_y  =  MMQ_Y_Q4_1;",
      "    int nwarps = NWARPS_Q4_1;",
      "",
      "    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;",
      "    const dim3 block_nums(block_num_x, block_num_y, 1);",
      "    const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "    if (nrows_x % mmq_y == 0) {",
      "        const bool need_check = false;",
      "        mul_mat_q4_1<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    } else {",
      "        const bool need_check = true;",
      "        mul_mat_q4_1<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "#define  MMQ_X_Q5_0 64",
      "#define  MMQ_Y_Q5_0 128",
      "#define NWARPS_Q5_0 8",
      "#else",
      "#define  MMQ_X_Q5_0 4",
      "#define  MMQ_Y_Q5_0 32",
      "#define NWARPS_Q5_0 4",
      "#endif",
      "",
      "template<typename scalar_t, bool need_check> static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF*NWARPS_Q5_0, 2)",
      "#endif",
      "mul_mat_q5_0(",
      "    const void * __restrict__ vx, const void * __restrict__ vy, scalar_t * __restrict__ dst,",
      "    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {",
      "    const int mmq_x  =  MMQ_X_Q5_0;",
      "    const int mmq_y  =  MMQ_Y_Q5_0;",
      "    const int nwarps = NWARPS_Q5_0;",
      "",
      "    mul_mat_q<scalar_t, QK5_0, QR5_0, QI5_0, false, block_q5_0, mmq_x, mmq_y, nwarps, allocate_tiles_q5_0<mmq_y>,",
      "        load_tiles_q5_0<mmq_y, nwarps, need_check>, VDR_Q5_0_Q8_1_MMQ, vec_dot_q5_0_q8_1_mul_mat>",
      "        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void ggml_mul_mat_q5_0_q8_1_cuda(",
      "    const void * vx, const void * vy, scalar_t * dst, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {",
      "",
      "    const int mmq_x  =  MMQ_X_Q5_0;",
      "    const int mmq_y  =  MMQ_Y_Q5_0;",
      "    const int nwarps = NWARPS_Q5_0;",
      "",
      "    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;",
      "    const dim3 block_nums(block_num_x, block_num_y, 1);",
      "    const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "    if (nrows_x % mmq_y == 0) {",
      "        const bool need_check = false;",
      "        mul_mat_q5_0<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    } else {",
      "        const bool need_check = true;",
      "        mul_mat_q5_0<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "#define  MMQ_X_Q5_1 64",
      "#define  MMQ_Y_Q5_1 128",
      "#define NWARPS_Q5_1 8",
      "#else",
      "#define  MMQ_X_Q5_1 4",
      "#define  MMQ_Y_Q5_1 32",
      "#define NWARPS_Q5_1 4",
      "#endif",
      "",
      "template<typename scalar_t, bool need_check> static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF*NWARPS_Q5_1, 2)",
      "#endif",
      "mul_mat_q5_1(",
      "    const void * __restrict__ vx, const void * __restrict__ vy, scalar_t * __restrict__ dst,",
      "    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {",
      "    const int mmq_x  =  MMQ_X_Q5_1;",
      "    const int mmq_y  =  MMQ_Y_Q5_1;",
      "    const int nwarps = NWARPS_Q5_1;",
      "",
      "    mul_mat_q<scalar_t, QK5_1, QR5_1, QI5_1, true, block_q5_1, mmq_x, mmq_y, nwarps, allocate_tiles_q5_1<mmq_y>,",
      "        load_tiles_q5_1<mmq_y, nwarps, need_check>, VDR_Q5_1_Q8_1_MMQ, vec_dot_q5_1_q8_1_mul_mat>",
      "        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void ggml_mul_mat_q5_1_q8_1_cuda(",
      "    const void * vx, const void * vy, scalar_t * dst, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {",
      "    const int mmq_x  =  MMQ_X_Q5_1;",
      "    const int mmq_y  =  MMQ_Y_Q5_1;",
      "    const int nwarps = NWARPS_Q5_1;",
      "",
      "    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;",
      "    const dim3 block_nums(block_num_x, block_num_y, 1);",
      "    const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "    if (nrows_x % mmq_y == 0) {",
      "        const bool need_check = false;",
      "        mul_mat_q5_1<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    } else {",
      "        const bool need_check = true;",
      "        mul_mat_q5_1<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "#define  MMQ_X_Q8_0 64",
      "#define  MMQ_Y_Q8_0 128",
      "#define NWARPS_Q8_0 8",
      "#else",
      "#define  MMQ_X_Q8_0 4",
      "#define  MMQ_Y_Q8_0 32",
      "#define NWARPS_Q8_0 4",
      "#endif",
      "",
      "template<typename scalar_t, bool need_check> static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF*NWARPS_Q8_0, 2)",
      "#endif",
      "mul_mat_q8_0(",
      "    const void * __restrict__ vx, const void * __restrict__ vy, scalar_t * __restrict__ dst,",
      "    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {",
      "    const int mmq_x  =  MMQ_X_Q8_0;",
      "    const int mmq_y  =  MMQ_Y_Q8_0;",
      "    const int nwarps = NWARPS_Q8_0;",
      "",
      "    mul_mat_q<scalar_t, QK8_0, QR8_0, QI8_0, false, block_q8_0, mmq_x, mmq_y, nwarps, allocate_tiles_q8_0<mmq_y>,",
      "        load_tiles_q8_0<mmq_y, nwarps, need_check>, VDR_Q8_0_Q8_1_MMQ, vec_dot_q8_0_q8_1_mul_mat>",
      "        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void ggml_mul_mat_q8_0_q8_1_cuda(",
      "    const void * vx, const void * vy, scalar_t * dst, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {",
      "    const int mmq_x  =  MMQ_X_Q8_0;",
      "    const int mmq_y  =  MMQ_Y_Q8_0;",
      "    const int nwarps = NWARPS_Q8_0;",
      "",
      "    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;",
      "    const dim3 block_nums(block_num_x, block_num_y, 1);",
      "    const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "    if (nrows_x % mmq_y == 0) {",
      "        const bool need_check = false;",
      "        mul_mat_q8_0<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    } else {",
      "        const bool need_check = true;",
      "        mul_mat_q8_0<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "#define  MMQ_X_Q2_K 64",
      "#define  MMQ_Y_Q2_K 128",
      "#define NWARPS_Q2_K 8",
      "#else",
      "#define  MMQ_X_Q2_K 4",
      "#define  MMQ_Y_Q2_K 32",
      "#define NWARPS_Q2_K 4",
      "#endif",
      "",
      "template<typename scalar_t, bool need_check> static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF*NWARPS_Q2_K, 2)",
      "#endif",
      "mul_mat_q2_K(",
      "    const void * __restrict__ vx, const void * __restrict__ vy, scalar_t * __restrict__ dst,",
      "    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {",
      "    const int mmq_x  =  MMQ_X_Q2_K;",
      "    const int mmq_y  =  MMQ_Y_Q2_K;",
      "    const int nwarps = NWARPS_Q2_K;",
      "",
      "    mul_mat_q<scalar_t, QK_K, QR2_K, QI2_K, false, block_q2_K, mmq_x, mmq_y, nwarps, allocate_tiles_q2_K<mmq_y>,",
      "        load_tiles_q2_K<mmq_y, nwarps, need_check>, VDR_Q2_K_Q8_1_MMQ, vec_dot_q2_K_q8_1_mul_mat>",
      "        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void ggml_mul_mat_q2_K_q8_1_cuda(",
      "    const void * vx, const void * vy, scalar_t * dst, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {",
      "    const int mmq_x  =  MMQ_X_Q2_K;",
      "    const int mmq_y  =  MMQ_Y_Q2_K;",
      "    const int nwarps = NWARPS_Q2_K;",
      "",
      "    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;",
      "    const dim3 block_nums(block_num_x, block_num_y, 1);",
      "    const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "    if (nrows_x % mmq_y == 0) {",
      "        const bool need_check = false;",
      "        mul_mat_q2_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    } else {",
      "        const bool need_check = true;",
      "        mul_mat_q2_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "#define  MMQ_X_Q3_K 64",
      "#define  MMQ_Y_Q3_K 128",
      "#define NWARPS_Q3_K 8",
      "#else",
      "#define  MMQ_X_Q3_K 4",
      "#define  MMQ_Y_Q3_K 32",
      "#define NWARPS_Q3_K 4",
      "#endif",
      "",
      "template<typename scalar_t, bool need_check> static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF*NWARPS_Q3_K, 2)",
      "#endif",
      "mul_mat_q3_K(",
      "    const void * __restrict__ vx, const void * __restrict__ vy, scalar_t * __restrict__ dst,",
      "    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {",
      "",
      "    const int mmq_x  =  MMQ_X_Q3_K;",
      "    const int mmq_y  =  MMQ_Y_Q3_K;",
      "    const int nwarps = NWARPS_Q3_K;",
      "",
      "    mul_mat_q<scalar_t, QK_K, QR3_K, QI3_K, false, block_q3_K, mmq_x, mmq_y, nwarps, allocate_tiles_q3_K<mmq_y>,",
      "        load_tiles_q3_K<mmq_y, nwarps, need_check>, VDR_Q3_K_Q8_1_MMQ, vec_dot_q3_K_q8_1_mul_mat>",
      "        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void ggml_mul_mat_q3_K_q8_1_cuda(",
      "    const void * vx, const void * vy, scalar_t * dst, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {",
      "",
      "    const int mmq_x  =  MMQ_X_Q3_K;",
      "    const int mmq_y  =  MMQ_Y_Q3_K;",
      "    const int nwarps = NWARPS_Q3_K;",
      "",
      "    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;",
      "    const dim3 block_nums(block_num_x, block_num_y, 1);",
      "    const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "    if (nrows_x % mmq_y == 0) {",
      "        const bool need_check = false;",
      "        mul_mat_q3_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    } else {",
      "        const bool need_check = true;",
      "        mul_mat_q3_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "#define  MMQ_X_Q4_K 64",
      "#define  MMQ_Y_Q4_K 128",
      "#define NWARPS_Q4_K 8",
      "#else",
      "#define  MMQ_X_Q4_K 4",
      "#define  MMQ_Y_Q4_K 32",
      "#define NWARPS_Q4_K 4",
      "#endif",
      "",
      "template<typename scalar_t, bool need_check> static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF*NWARPS_Q4_K, 2)",
      "#endif",
      "mul_mat_q4_K(",
      "    const void * __restrict__ vx, const void * __restrict__ vy, scalar_t * __restrict__ dst,",
      "    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {",
      "    const int mmq_x  =  MMQ_X_Q4_K;",
      "    const int mmq_y  =  MMQ_Y_Q4_K;",
      "    const int nwarps = NWARPS_Q4_K;",
      "",
      "    mul_mat_q<scalar_t, QK_K, QR4_K, QI4_K, true, block_q4_K, mmq_x, mmq_y, nwarps, allocate_tiles_q4_K<mmq_y>,",
      "        load_tiles_q4_K<mmq_y, nwarps, need_check>, VDR_Q4_K_Q8_1_MMQ, vec_dot_q4_K_q8_1_mul_mat>",
      "        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void ggml_mul_mat_q4_K_q8_1_cuda(",
      "    const void * vx, const void * vy, scalar_t * dst, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {",
      "    const int mmq_x  =  MMQ_X_Q4_K;",
      "    const int mmq_y  =  MMQ_Y_Q4_K;",
      "    const int nwarps = NWARPS_Q4_K;",
      "",
      "    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;",
      "    const dim3 block_nums(block_num_x, block_num_y, 1);",
      "    const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "    if (nrows_x % mmq_y == 0) {",
      "        const bool need_check = false;",
      "        mul_mat_q4_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    } else {",
      "        const bool need_check = true;",
      "        mul_mat_q4_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "#define  MMQ_X_Q5_K 64",
      "#define  MMQ_Y_Q5_K 128",
      "#define NWARPS_Q5_K 8",
      "#else",
      "#define  MMQ_X_Q5_K 4",
      "#define  MMQ_Y_Q5_K 32",
      "#define NWARPS_Q5_K 4",
      "#endif",
      "",
      "template<typename scalar_t, bool need_check> static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF*NWARPS_Q5_K, 2)",
      "#endif",
      "mul_mat_q5_K(",
      "    const void * __restrict__ vx, const void * __restrict__ vy, scalar_t * __restrict__ dst,",
      "    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {",
      "    const int mmq_x  =  MMQ_X_Q5_K;",
      "    const int mmq_y  =  MMQ_Y_Q5_K;",
      "    const int nwarps = NWARPS_Q5_K;",
      "",
      "    mul_mat_q<scalar_t, QK_K, QR5_K, QI5_K, true, block_q5_K, mmq_x, mmq_y, nwarps, allocate_tiles_q5_K<mmq_y>,",
      "        load_tiles_q5_K<mmq_y, nwarps, need_check>, VDR_Q5_K_Q8_1_MMQ, vec_dot_q5_K_q8_1_mul_mat>",
      "        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void ggml_mul_mat_q5_K_q8_1_cuda(",
      "    const void * vx, const void * vy, scalar_t * dst, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {",
      "",
      "    const int mmq_x  =  MMQ_X_Q5_K;",
      "    const int mmq_y  =  MMQ_Y_Q5_K;",
      "    const int nwarps = NWARPS_Q5_K;",
      "",
      "    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;",
      "    const dim3 block_nums(block_num_x, block_num_y, 1);",
      "    const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "    if (nrows_x % mmq_y == 0) {",
      "        const bool need_check = false;",
      "        mul_mat_q5_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    } else {",
      "        const bool need_check = true;",
      "        mul_mat_q5_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    }",
      "}",
      "",
      "#if defined(USE_ROCM)",
      "#define  MMQ_X_Q6_K 64",
      "#define  MMQ_Y_Q6_K 128",
      "#define NWARPS_Q6_K 8",
      "#else",
      "#define  MMQ_X_Q6_K 4",
      "#define  MMQ_Y_Q6_K 32",
      "#define NWARPS_Q6_K 4",
      "#endif",
      "",
      "template<typename scalar_t, bool need_check> static __global__ void",
      "#if defined(USE_ROCM)",
      "__launch_bounds__(WARP_SIZE_GGUF*NWARPS_Q6_K, 2)",
      "#endif",
      "mul_mat_q6_K(",
      "    const void * __restrict__ vx, const void * __restrict__ vy, scalar_t * __restrict__ dst,",
      "    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {",
      "    const int mmq_x  =  MMQ_X_Q6_K;",
      "    const int mmq_y  =  MMQ_Y_Q6_K;",
      "    const int nwarps = NWARPS_Q6_K;",
      "",
      "    mul_mat_q<scalar_t, QK_K, QR6_K, QI6_K, false, block_q6_K, mmq_x, mmq_y, nwarps, allocate_tiles_q6_K<mmq_y>,",
      "        load_tiles_q6_K<mmq_y, nwarps, need_check>, VDR_Q6_K_Q8_1_MMQ, vec_dot_q6_K_q8_1_mul_mat>",
      "        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "}",
      "",
      "template<typename scalar_t>",
      "static void ggml_mul_mat_q6_K_q8_1_cuda(",
      "    const void * vx, const void * vy, scalar_t * dst, const int ncols_x, const int nrows_x,",
      "    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {",
      "    const int mmq_x  =  MMQ_X_Q6_K;",
      "    const int mmq_y  =  MMQ_Y_Q6_K;",
      "    const int nwarps = NWARPS_Q6_K;",
      "",
      "    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;",
      "    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;",
      "    const dim3 block_nums(block_num_x, block_num_y, 1);",
      "    const dim3 block_dims(WARP_SIZE_GGUF, nwarps, 1);",
      "",
      "    if (nrows_x % mmq_y == 0) {",
      "        const bool need_check = false;",
      "        mul_mat_q6_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    } else {",
      "        const bool need_check = true;",
      "        mul_mat_q6_K<scalar_t, need_check><<<block_nums, block_dims, 0, stream>>>",
      "            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);",
      "    }",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"scaled_mm_c2x.cuh\"",
      "",
      "/**",
      " * This file defines Gemm kernel configurations for SM75 based on the Gemm",
      " * shape.",
      " */",
      "",
      "namespace vllm {",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename> typename Epilogue>",
      "struct sm75_config_default {",
      "  // This config is used in 2 cases,",
      "  // - M in (256, inf]",
      "  // - M in (64, 128]",
      "  // Shared memory required by this Gemm 32768",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using TileShape = typename cutlass::gemm::GemmShape<128, 128, 64>;",
      "  using WarpShape = typename cutlass::gemm::GemmShape<64, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<8, 8, 16>;",
      "  using Cutlass2xGemm =",
      "      cutlass_2x_gemm<cutlass::arch::Sm75, enable_sm75_to_sm80, InType, OutType,",
      "                      Epilogue, TileShape, WarpShape, InstructionShape, 2>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename> typename Epilogue>",
      "struct sm75_config_M256 {",
      "  // M in (128, 256]",
      "  // Shared memory required by this Gemm 65536",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using TileShape = typename cutlass::gemm::GemmShape<128, 128, 128>;",
      "  using WarpShape = typename cutlass::gemm::GemmShape<64, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<8, 8, 16>;",
      "  using Cutlass2xGemm =",
      "      cutlass_2x_gemm<cutlass::arch::Sm75, enable_sm75_to_sm80, InType, OutType,",
      "                      Epilogue, TileShape, WarpShape, InstructionShape, 2>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename> typename Epilogue>",
      "struct sm75_config_M64 {",
      "  // M in (32, 64]",
      "  // Shared memory required by this Gemm 49152",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using TileShape = typename cutlass::gemm::GemmShape<64, 128, 128>;",
      "  using WarpShape = typename cutlass::gemm::GemmShape<64, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<8, 8, 16>;",
      "  using Cutlass2xGemm =",
      "      cutlass_2x_gemm<cutlass::arch::Sm75, enable_sm75_to_sm80, InType, OutType,",
      "                      Epilogue, TileShape, WarpShape, InstructionShape, 2>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename> typename Epilogue>",
      "struct sm75_config_M32 {",
      "  // M in [1, 32]",
      "  // Shared memory required by this Gemm 49152",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using TileShape = typename cutlass::gemm::GemmShape<32, 128, 64>;",
      "  using WarpShape = typename cutlass::gemm::GemmShape<32, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<8, 8, 16>;",
      "  using Cutlass2xGemm =",
      "      cutlass_2x_gemm<cutlass::arch::Sm75, enable_sm75_to_sm80, InType, OutType,",
      "                      Epilogue, TileShape, WarpShape, InstructionShape, 2>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename> typename Epilogue,",
      "          typename... EpilogueArgs>",
      "inline void cutlass_gemm_sm75_dispatch(torch::Tensor& out,",
      "                                       torch::Tensor const& a,",
      "                                       torch::Tensor const& b,",
      "                                       EpilogueArgs&&... args) {",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  TORCH_CHECK(a.dtype() == torch::kInt8);",
      "  TORCH_CHECK(b.dtype() == torch::kInt8);",
      "",
      "  using Cutlass2xGemmDefault =",
      "      typename sm75_config_default<InType, OutType, Epilogue>::Cutlass2xGemm;",
      "  using Cutlass2xGemmM256 =",
      "      typename sm75_config_M256<InType, OutType, Epilogue>::Cutlass2xGemm;",
      "  using Cutlass2xGemmM128 = Cutlass2xGemmDefault;",
      "  using Cutlass2xGemmM64 =",
      "      typename sm75_config_M64<InType, OutType, Epilogue>::Cutlass2xGemm;",
      "  using Cutlass2xGemmM32 =",
      "      typename sm75_config_M32<InType, OutType, Epilogue>::Cutlass2xGemm;",
      "",
      "  // Due to shared memory requirements, some Gemms may fail to run on some",
      "  // GPUs. As the name indicates, the Fallback Gemm is used as an alternative",
      "  // in such cases.",
      "  // sm75_config_default has the least shared-memory requirements.",
      "  using FallbackGemm = Cutlass2xGemmDefault;",
      "",
      "  uint32_t const m = a.size(0);",
      "  uint32_t const mp2 =",
      "      std::max(static_cast<uint32_t>(32), next_pow_2(m));  // next power of 2",
      "  if (mp2 <= 32) {",
      "    // M in [1, 32]",
      "    return fallback_cutlass_gemm_caller<Cutlass2xGemmM32, FallbackGemm>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 64) {",
      "    // M in (32, 64]",
      "    return fallback_cutlass_gemm_caller<Cutlass2xGemmM64, FallbackGemm>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 128) {",
      "    // M in (64, 128]",
      "    return fallback_cutlass_gemm_caller<Cutlass2xGemmM128, FallbackGemm>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 256) {",
      "    // M in (128, 256]",
      "    return fallback_cutlass_gemm_caller<Cutlass2xGemmM256, FallbackGemm>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else {",
      "    // M in (256, inf)",
      "    return fallback_cutlass_gemm_caller<Cutlass2xGemmDefault, FallbackGemm>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm90.cu",
    "source": [
      "#include \"c3x/scaled_mm_helper.hpp\"",
      "#include \"c3x/scaled_mm_kernels.hpp\"",
      "",
      "/*",
      "   This file defines quantized GEMM operations using the CUTLASS 3.x API, for",
      "   NVIDIA GPUs with sm90a (Hopper).",
      "*/",
      "",
      "#if defined ENABLE_SCALED_MM_SM90 && ENABLE_SCALED_MM_SM90",
      "",
      "void cutlass_scaled_mm_sm90(torch::Tensor& c, torch::Tensor const& a,",
      "                            torch::Tensor const& b,",
      "                            torch::Tensor const& a_scales,",
      "                            torch::Tensor const& b_scales,",
      "                            std::optional<torch::Tensor> const& bias) {",
      "  dispatch_scaled_mm(c, a, b, a_scales, b_scales, bias,",
      "                     vllm::cutlass_scaled_mm_sm90_fp8,",
      "                     vllm::cutlass_scaled_mm_sm90_int8,",
      "                     vllm::cutlass_scaled_mm_blockwise_sm90_fp8);",
      "}",
      "",
      "void cutlass_scaled_mm_azp_sm90(torch::Tensor& out, torch::Tensor const& a,",
      "                                torch::Tensor const& b,",
      "                                torch::Tensor const& a_scales,",
      "                                torch::Tensor const& b_scales,",
      "                                torch::Tensor const& azp_adj,",
      "                                std::optional<torch::Tensor> const& azp,",
      "                                std::optional<torch::Tensor> const& bias) {",
      "  TORCH_CHECK(a_scales.dtype() == torch::kFloat32);",
      "  TORCH_CHECK(b_scales.dtype() == torch::kFloat32);",
      "",
      "  vllm::cutlass_scaled_mm_azp_sm90_int8(out, a, b, a_scales, b_scales, azp_adj,",
      "                                        azp, bias);",
      "}",
      "",
      "#endif"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm89_int8_dispatch.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"scaled_mm_c2x.cuh\"",
      "",
      "/**",
      " * This file defines Gemm kernel configurations for SM89 (int8) based on the",
      " * Gemm shape.",
      " */",
      "",
      "namespace vllm {",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename> typename Epilogue>",
      "struct sm89_int8_fallback_gemm {",
      "  // Shared mem requirement : 61440",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using TileShape = cutlass::gemm::GemmShape<32, 64, 128>;",
      "  using WarpShape = cutlass::gemm::GemmShape<16, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "  static int32_t const MainLoopStages = 5;",
      "",
      "  using Cutlass2xGemm =",
      "      cutlass_2x_gemm<cutlass::arch::Sm89, enable_sm89_to_sm90, InType, OutType,",
      "                      Epilogue, TileShape, WarpShape, InstructionShape, 5>;",
      "};",
      "",
      "struct sm89_int8_config_default {",
      "  // M in (256, inf)",
      "  using WarpShape = typename cutlass::gemm::GemmShape<64, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "",
      "  template <typename InType, typename OutType,",
      "            template <typename, typename> typename Epilogue,",
      "            typename... EpilogueArgs>",
      "  static void dispatch(torch::Tensor& out, torch::Tensor const& a,",
      "                       torch::Tensor const& b, EpilogueArgs&&... args) {",
      "    static_assert(std::is_same<InType, int8_t>());",
      "    TORCH_CHECK(a.dtype() == torch::kInt8);",
      "",
      "    using FallbackGemm =",
      "        typename sm89_int8_fallback_gemm<InType, OutType,",
      "                                         Epilogue>::Cutlass2xGemm;",
      "",
      "    uint32_t const n = out.size(1);",
      "    uint32_t const np2 = next_pow_2(n);",
      "",
      "    if (np2 <= 4096) {",
      "      using TileShape = cutlass::gemm::GemmShape<128, 128, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else if (np2 <= 8192) {",
      "      using TileShape = cutlass::gemm::GemmShape<256, 128, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 3>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else if (np2 <= 16384) {",
      "      using TileShape = cutlass::gemm::GemmShape<128, 128, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else {",
      "      using TileShape = cutlass::gemm::GemmShape<256, 128, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 3>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "  }",
      "};",
      "",
      "struct sm89_int8_config_M256 {",
      "  // M in (128, 256]",
      "  using WarpShape = typename cutlass::gemm::GemmShape<64, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "",
      "  template <typename InType, typename OutType,",
      "            template <typename, typename> typename Epilogue,",
      "            typename... EpilogueArgs>",
      "  static void dispatch(torch::Tensor& out, torch::Tensor const& a,",
      "                       torch::Tensor const& b, EpilogueArgs&&... args) {",
      "    static_assert(std::is_same<InType, int8_t>());",
      "    TORCH_CHECK(a.dtype() == torch::kInt8);",
      "",
      "    using FallbackGemm =",
      "        typename sm89_int8_fallback_gemm<InType, OutType,",
      "                                         Epilogue>::Cutlass2xGemm;",
      "",
      "    uint32_t const n = out.size(1);",
      "    uint32_t const np2 = next_pow_2(n);",
      "",
      "    if (np2 <= 4096) {",
      "      using TileShape = cutlass::gemm::GemmShape<64, 128, 128>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 3>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else if (np2 <= 8192) {",
      "      using TileShape = cutlass::gemm::GemmShape<128, 128, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else if (np2 <= 16384) {",
      "      using TileShape = cutlass::gemm::GemmShape<256, 128, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 3>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else {",
      "      using TileShape = cutlass::gemm::GemmShape<128, 128, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "  }",
      "};",
      "",
      "struct sm89_int8_config_M128 {",
      "  // M in (64, 128]",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "",
      "  template <typename InType, typename OutType,",
      "            template <typename, typename> typename Epilogue,",
      "            typename... EpilogueArgs>",
      "  static void dispatch(torch::Tensor& out, torch::Tensor const& a,",
      "                       torch::Tensor const& b, EpilogueArgs&&... args) {",
      "    static_assert(std::is_same<InType, int8_t>());",
      "    TORCH_CHECK(a.dtype() == torch::kInt8);",
      "",
      "    using FallbackGemm =",
      "        typename sm89_int8_fallback_gemm<InType, OutType,",
      "                                         Epilogue>::Cutlass2xGemm;",
      "",
      "    uint32_t const n = out.size(1);",
      "    uint32_t const np2 = next_pow_2(n);",
      "",
      "    if (np2 <= 8192) {",
      "      using TileShape = cutlass::gemm::GemmShape<64, 128, 128>;",
      "      using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 3>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else if (np2 <= 16384) {",
      "      using TileShape = cutlass::gemm::GemmShape<128, 128, 64>;",
      "      using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else {",
      "      using TileShape = cutlass::gemm::GemmShape<64, 64, 128>;",
      "      using WarpShape = cutlass::gemm::GemmShape<32, 64, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "  }",
      "};",
      "",
      "struct sm89_int8_config_M64 {",
      "  // M in (32, 64]",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "",
      "  template <typename InType, typename OutType,",
      "            template <typename, typename> typename Epilogue,",
      "            typename... EpilogueArgs>",
      "  static void dispatch(torch::Tensor& out, torch::Tensor const& a,",
      "                       torch::Tensor const& b, EpilogueArgs&&... args) {",
      "    static_assert(std::is_same<InType, int8_t>());",
      "    TORCH_CHECK(a.dtype() == torch::kInt8);",
      "",
      "    using FallbackGemm =",
      "        typename sm89_int8_fallback_gemm<InType, OutType,",
      "                                         Epilogue>::Cutlass2xGemm;",
      "",
      "    uint32_t const n = out.size(1);",
      "    uint32_t const np2 = next_pow_2(n);",
      "",
      "    if (np2 <= 8192) {",
      "      using TileShape = cutlass::gemm::GemmShape<64, 64, 128>;",
      "      using WarpShape = cutlass::gemm::GemmShape<32, 64, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else {",
      "      using TileShape = cutlass::gemm::GemmShape<64, 128, 128>;",
      "      using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 3>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "  }",
      "};",
      "",
      "struct sm89_int8_config_M32 {",
      "  // M in (16, 32]",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "",
      "  template <typename InType, typename OutType,",
      "            template <typename, typename> typename Epilogue,",
      "            typename... EpilogueArgs>",
      "  static void dispatch(torch::Tensor& out, torch::Tensor const& a,",
      "                       torch::Tensor const& b, EpilogueArgs&&... args) {",
      "    static_assert(std::is_same<InType, int8_t>());",
      "    TORCH_CHECK(a.dtype() == torch::kInt8);",
      "",
      "    using FallbackGemm =",
      "        typename sm89_int8_fallback_gemm<InType, OutType,",
      "                                         Epilogue>::Cutlass2xGemm;",
      "",
      "    uint32_t const n = out.size(1);",
      "    uint32_t const np2 = next_pow_2(n);",
      "",
      "    if (np2 <= 8192) {",
      "      using TileShape = cutlass::gemm::GemmShape<32, 64, 128>;",
      "      using WarpShape = cutlass::gemm::GemmShape<16, 64, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else {",
      "      using TileShape = cutlass::gemm::GemmShape<32, 128, 128>;",
      "      using WarpShape = cutlass::gemm::GemmShape<32, 64, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 4>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "  }",
      "};",
      "",
      "struct sm89_int8_config_M16 {",
      "  // M in [1, 16]",
      "  using WarpShape = typename cutlass::gemm::GemmShape<16, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "",
      "  template <typename InType, typename OutType,",
      "            template <typename, typename> typename Epilogue,",
      "            typename... EpilogueArgs>",
      "  static void dispatch(torch::Tensor& out, torch::Tensor const& a,",
      "                       torch::Tensor const& b, EpilogueArgs&&... args) {",
      "    static_assert(std::is_same<InType, int8_t>());",
      "    TORCH_CHECK(a.dtype() == torch::kInt8);",
      "",
      "    using FallbackGemm =",
      "        typename sm89_int8_fallback_gemm<InType, OutType,",
      "                                         Epilogue>::Cutlass2xGemm;",
      "",
      "    uint32_t const n = out.size(1);",
      "    uint32_t const np2 = next_pow_2(n);",
      "",
      "    if (np2 <= 8192) {",
      "      using TileShape = cutlass::gemm::GemmShape<16, 64, 128>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else {",
      "      using TileShape = cutlass::gemm::GemmShape<16, 128, 128>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 4>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "  }",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename> typename Epilogue,",
      "          typename... EpilogueArgs>",
      "inline void cutlass_gemm_sm89_int8_dispatch(torch::Tensor& out,",
      "                                            torch::Tensor const& a,",
      "                                            torch::Tensor const& b,",
      "                                            EpilogueArgs&&... args) {",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  TORCH_CHECK(a.dtype() == torch::kInt8);",
      "  TORCH_CHECK(b.dtype() == torch::kInt8);",
      "",
      "  uint32_t const m = a.size(0);",
      "  uint32_t const mp2 =",
      "      std::max(static_cast<uint32_t>(16), next_pow_2(m));  // next power of 2",
      "",
      "  if (mp2 <= 16) {",
      "    // M in [1, 16]",
      "    return sm89_int8_config_M16::dispatch<InType, OutType, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 32) {",
      "    // M in (16, 32]",
      "    return sm89_int8_config_M32::dispatch<InType, OutType, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 64) {",
      "    // M in (32, 64]",
      "    return sm89_int8_config_M64::dispatch<InType, OutType, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 128) {",
      "    // M in (64, 128]",
      "    return sm89_int8_config_M128::dispatch<InType, OutType, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 256) {",
      "    // M in (128, 256]",
      "    return sm89_int8_config_M256::dispatch<InType, OutType, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else {",
      "    // M in (256, inf)",
      "    return sm89_int8_config_default::dispatch<InType, OutType, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh",
    "source": [
      "#pragma once",
      "#include <stddef.h>",
      "#include <torch/all.h>",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "",
      "// clang-format will break include orders",
      "// clang-format off",
      "#include \"cute/tensor.hpp\"",
      "#include \"cute/atom/mma_atom.hpp\"",
      "#include \"cutlass/numeric_types.h\"",
      "",
      "#include \"cutlass/cutlass.h\"",
      "#include \"cutlass/gemm_coord.h\"",
      "#include \"cutlass/arch/mma_sm75.h\"",
      "#include \"cutlass/arch/arch.h\"",
      "#include \"cutlass/arch/mma.h\"",
      "#include \"cutlass/gemm/device/gemm.h\"",
      "#include \"cutlass/gemm/device/gemm_universal_adapter.h\"",
      "",
      "#include \"cutlass/epilogue/threadblock/fusion/visitors.hpp\"",
      "#include \"cutlass/gemm/kernel/default_gemm_universal_with_visitor.h\"",
      "",
      "#include \"core/math.hpp\"",
      "#include \"cutlass_extensions/common.hpp\"",
      "// clang-format on",
      "",
      "using namespace cute;",
      "",
      "/*",
      "   Epilogues defined in,",
      "   csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c2x.hpp",
      "   must contain a public type named EVTCompute of type Sm80EVT,",
      "   as well as a static prepare_args function that constructs an",
      "   EVTCompute::Arguments struct.",
      "*/",
      "",
      "namespace vllm {",
      "",
      "// Wrappers for the GEMM kernel that is used to guard against compilation on",
      "// architectures that will never use the kernel. The purpose of this is to",
      "// reduce the size of the compiled binary.",
      "// __CUDA_ARCH__ is not defined in host code, so this lets us smuggle the ifdef",
      "// into code that will be executed on the device where it is defined.",
      "template <typename Kernel>",
      "struct enable_sm75_to_sm80 : Kernel {",
      "  template <typename... Args>",
      "  CUTLASS_DEVICE static void invoke(Args&&... args) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 750 && __CUDA_ARCH__ < 800",
      "    Kernel::invoke(std::forward<Args>(args)...);",
      "#endif",
      "  }",
      "};",
      "",
      "template <typename Kernel>",
      "struct enable_sm80_to_sm89 : Kernel {",
      "  template <typename... Args>",
      "  CUTLASS_DEVICE static void invoke(Args&&... args) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 800 && __CUDA_ARCH__ < 890",
      "    Kernel::invoke(std::forward<Args>(args)...);",
      "#endif",
      "  }",
      "};",
      "",
      "template <typename Kernel>",
      "struct enable_sm89_to_sm90 : Kernel {",
      "  template <typename... Args>",
      "  CUTLASS_DEVICE static void invoke(Args&&... args) {",
      "#if defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 890 && __CUDA_ARCH__ < 900",
      "    Kernel::invoke(std::forward<Args>(args)...);",
      "#endif",
      "  }",
      "};",
      "template <typename Arch, template <typename> typename ArchGuard,",
      "          typename ElementAB_, typename ElementD_,",
      "          template <typename, typename> typename Epilogue_, typename TileShape,",
      "          typename WarpShape, typename InstructionShape, int32_t MainLoopStages,",
      "          typename FP8MathOperator = cutlass::arch::OpMultiplyAdd>",
      "struct cutlass_2x_gemm {",
      "  using ElementAB = ElementAB_;",
      "  using ElementD = ElementD_;",
      "",
      "  using ElementAcc =",
      "      typename std::conditional<std::is_same_v<ElementAB, int8_t>, int32_t,",
      "                                float>::type;",
      "",
      "  using Operator =",
      "      typename std::conditional<std::is_same_v<ElementAB, int8_t>,",
      "                                cutlass::arch::OpMultiplyAddSaturate,",
      "                                FP8MathOperator>::type;",
      "",
      "  using OutputTileThreadMap =",
      "      cutlass::epilogue::threadblock::OutputTileThreadLayout<",
      "          TileShape, WarpShape, float, 4, 1 /* epilogue stages */",
      "          >;",
      "",
      "  using Epilogue = Epilogue_<ElementD, OutputTileThreadMap>;",
      "  using EVTCompute = typename Epilogue::EVTCompute;",
      "",
      "  using D = cutlass::epilogue::threadblock::VisitorAuxStore<",
      "      OutputTileThreadMap, ElementD, cutlass::FloatRoundStyle::round_to_nearest,",
      "      Stride<int64_t, Int<1>, Int<0>>>;",
      "",
      "  using EVTD = cutlass::epilogue::threadblock::Sm80EVT<D, EVTCompute>;",
      "",
      "  // These are the minimum alignments needed for the kernels to compile",
      "  static constexpr int AlignmentAB =",
      "      128 / cutlass::sizeof_bits<ElementAB>::value;",
      "  static constexpr int AlignmentCD = 4;",
      "",
      "  // clang-format off",
      "  using RowMajor = typename cutlass::layout::RowMajor;",
      "  using ColumnMajor = typename cutlass::layout::ColumnMajor;",
      "  using KernelType =",
      "    ArchGuard<typename cutlass::gemm::kernel::DefaultGemmWithVisitor<",
      "      ElementAB, RowMajor, cutlass::ComplexTransform::kNone, AlignmentAB,",
      "      ElementAB, ColumnMajor, cutlass::ComplexTransform::kNone, AlignmentAB,",
      "      float, cutlass::layout::RowMajor, AlignmentCD,",
      "      ElementAcc, float, cutlass::arch::OpClassTensorOp,",
      "      Arch,",
      "      TileShape, WarpShape, InstructionShape,",
      "      EVTD,",
      "      cutlass::gemm::threadblock::ThreadblockSwizzleStreamK,",
      "      MainLoopStages, Operator,",
      "      1 /* epilogue stages */",
      "      >::GemmKernel>;",
      "  // clang-format on",
      "",
      "  using Op = cutlass::gemm::device::GemmUniversalAdapter<KernelType>;",
      "};",
      "",
      "template <typename Gemm, typename... EpilogueArgs>",
      "inline void cutlass_gemm_caller(torch::Tensor& out, torch::Tensor const& a,",
      "                                torch::Tensor const& b,",
      "                                EpilogueArgs&&... epilogue_params) {",
      "  using ElementAB = typename Gemm::ElementAB;",
      "  using ElementD = typename Gemm::ElementD;",
      "",
      "  int32_t m = a.size(0);",
      "  int32_t n = b.size(1);",
      "  int32_t k = a.size(1);",
      "  cutlass::gemm::GemmCoord problem_size{m, n, k};",
      "",
      "  int64_t lda = a.stride(0);",
      "  int64_t ldb = b.stride(1);",
      "  int64_t ldc = out.stride(0);",
      "",
      "  using StrideC = Stride<int64_t, Int<1>, Int<0>>;",
      "  StrideC c_stride{ldc, Int<1>{}, Int<0>{}};",
      "",
      "  auto a_ptr = static_cast<ElementAB const*>(a.data_ptr());",
      "  auto b_ptr = static_cast<ElementAB const*>(b.data_ptr());",
      "  auto c_ptr = static_cast<ElementD*>(out.data_ptr());",
      "",
      "  typename Gemm::D::Arguments d_args{c_ptr, c_stride};",
      "",
      "  using Epilogue = typename Gemm::Epilogue;",
      "  auto evt_args =",
      "      Epilogue::prepare_args(std::forward<EpilogueArgs>(epilogue_params)...);",
      "",
      "  typename Gemm::EVTD::Arguments epilogue_args{",
      "      evt_args,",
      "      d_args,",
      "  };",
      "",
      "  typename Gemm::Op::Arguments args{",
      "      cutlass::gemm::GemmUniversalMode::kGemmSplitKParallel,  // universal mode",
      "      problem_size,                                           // problem size",
      "      1,                                                      // batch count",
      "      epilogue_args,",
      "      a_ptr,",
      "      b_ptr,",
      "      nullptr,",
      "      nullptr,",
      "      0,",
      "      0,",
      "      0,",
      "      0,",
      "      lda,",
      "      ldb,",
      "      ldc,",
      "      ldc};",
      "",
      "  // Launch the CUTLASS GEMM kernel.",
      "  typename Gemm::Op gemm_op;",
      "  size_t workspace_size = gemm_op.get_workspace_size(args);",
      "  auto const workspace_options =",
      "      torch::TensorOptions().dtype(torch::kUInt8).device(a.device());",
      "  auto workspace = torch::empty(workspace_size, workspace_options);",
      "",
      "  auto stream = at::cuda::getCurrentCUDAStream(a.get_device());",
      "",
      "  CUTLASS_CHECK(gemm_op.can_implement(args));",
      "  cutlass::Status status = gemm_op(args, workspace.data_ptr(), stream);",
      "  CUTLASS_CHECK(status);",
      "}",
      "",
      "template <typename Gemm, typename FallbackGemm, typename... EpilogueArgs>",
      "inline void fallback_cutlass_gemm_caller(torch::Tensor& out,",
      "                                         torch::Tensor const& a,",
      "                                         torch::Tensor const& b,",
      "                                         EpilogueArgs&&... args) {",
      "  // In some cases, the GPU isn't able to accommodate the",
      "  // shared memory requirements of the Gemm. In such cases, use",
      "  // the FallbackGemm instead.",
      "  static const int max_shared_mem_per_block_opt_in =",
      "      get_cuda_max_shared_memory_per_block_opt_in(0);",
      "",
      "  size_t const gemm_shared_mem_size =",
      "      sizeof(typename Gemm::KernelType::SharedStorage);",
      "  size_t const fallback_gemm_shared_mem_size =",
      "      sizeof(typename FallbackGemm::KernelType::SharedStorage);",
      "",
      "  if (gemm_shared_mem_size <= max_shared_mem_per_block_opt_in) {",
      "    return cutlass_gemm_caller<Gemm>(out, a, b,",
      "                                     std::forward<EpilogueArgs>(args)...);",
      "  } else {",
      "    TORCH_CHECK(fallback_gemm_shared_mem_size <=",
      "                max_shared_mem_per_block_opt_in);",
      "    return cutlass_gemm_caller<FallbackGemm>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm120.cu",
    "source": [
      "#include \"c3x/scaled_mm_helper.hpp\"",
      "#include \"c3x/scaled_mm_kernels.hpp\"",
      "",
      "/*",
      "   This file defines quantized GEMM operations using the CUTLASS 3.x API, for",
      "   NVIDIA GPUs with sm120 (Blackwell).",
      "*/",
      "",
      "#if defined ENABLE_SCALED_MM_SM120 && ENABLE_SCALED_MM_SM120",
      "",
      "void cutlass_scaled_mm_sm120(torch::Tensor& c, torch::Tensor const& a,",
      "                             torch::Tensor const& b,",
      "                             torch::Tensor const& a_scales,",
      "                             torch::Tensor const& b_scales,",
      "                             std::optional<torch::Tensor> const& bias) {",
      "  dispatch_scaled_mm(c, a, b, a_scales, b_scales, bias,",
      "                     vllm::cutlass_scaled_mm_sm120_fp8,",
      "                     nullptr,  // int8 not supported on SM120",
      "                     vllm::cutlass_scaled_mm_blockwise_sm120_fp8);",
      "}",
      "",
      "#endif"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu",
    "source": [
      "#include <cudaTypedefs.h>",
      "",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <torch/all.h>",
      "",
      "#include \"cutlass_extensions/common.hpp\"",
      "",
      "void cutlass_scaled_mm_sm75(torch::Tensor& c, torch::Tensor const& a,",
      "                            torch::Tensor const& b,",
      "                            torch::Tensor const& a_scales,",
      "                            torch::Tensor const& b_scales,",
      "                            std::optional<torch::Tensor> const& bias);",
      "",
      "void cutlass_scaled_mm_sm80(torch::Tensor& c, torch::Tensor const& a,",
      "                            torch::Tensor const& b,",
      "                            torch::Tensor const& a_scales,",
      "                            torch::Tensor const& b_scales,",
      "                            std::optional<torch::Tensor> const& bias);",
      "",
      "void cutlass_scaled_mm_sm89(torch::Tensor& c, torch::Tensor const& a,",
      "                            torch::Tensor const& b,",
      "                            torch::Tensor const& a_scales,",
      "                            torch::Tensor const& b_scales,",
      "                            std::optional<torch::Tensor> const& bias);",
      "",
      "#if defined ENABLE_SCALED_MM_SM90 && ENABLE_SCALED_MM_SM90",
      "void cutlass_scaled_mm_sm90(torch::Tensor& c, torch::Tensor const& a,",
      "                            torch::Tensor const& b,",
      "                            torch::Tensor const& a_scales,",
      "                            torch::Tensor const& b_scales,",
      "                            std::optional<torch::Tensor> const& bias);",
      "#endif",
      "#if defined ENABLE_CUTLASS_MOE_SM90 && ENABLE_CUTLASS_MOE_SM90",
      "void cutlass_moe_mm_sm90(",
      "    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,",
      "    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,",
      "    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,",
      "    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,",
      "    torch::Tensor const& b_strides, torch::Tensor const& c_strides,",
      "    bool per_act_token, bool per_out_ch);",
      "",
      "#endif",
      "",
      "#if defined ENABLE_CUTLASS_MOE_SM100 && ENABLE_CUTLASS_MOE_SM100",
      "void cutlass_moe_mm_sm100(",
      "    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,",
      "    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,",
      "    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,",
      "    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,",
      "    torch::Tensor const& b_strides, torch::Tensor const& c_strides,",
      "    bool per_act_token, bool per_out_ch);",
      "#endif",
      "",
      "#if defined ENABLE_SCALED_MM_SM120 && ENABLE_SCALED_MM_SM120",
      "void cutlass_scaled_mm_sm120(torch::Tensor& c, torch::Tensor const& a,",
      "                             torch::Tensor const& b,",
      "                             torch::Tensor const& a_scales,",
      "                             torch::Tensor const& b_scales,",
      "                             std::optional<torch::Tensor> const& bias);",
      "#endif",
      "",
      "#if defined ENABLE_SCALED_MM_SM100 && ENABLE_SCALED_MM_SM100",
      "void cutlass_scaled_mm_sm100(torch::Tensor& c, torch::Tensor const& a,",
      "                             torch::Tensor const& b,",
      "                             torch::Tensor const& a_scales,",
      "                             torch::Tensor const& b_scales,",
      "                             std::optional<torch::Tensor> const& bias);",
      "#endif",
      "",
      "#if defined(ENABLE_SCALED_MM_SM90) && ENABLE_SCALED_MM_SM90 || \\",
      "    defined(ENABLE_SCALED_MM_SM100) && ENABLE_SCALED_MM_SM100",
      "void get_cutlass_moe_mm_data_caller(",
      "    const torch::Tensor& topk_ids, torch::Tensor& expert_offsets,",
      "    torch::Tensor& problem_sizes1, torch::Tensor& problem_sizes2,",
      "    torch::Tensor& input_permutation, torch::Tensor& output_permutation,",
      "    const int64_t num_experts, const int64_t n, const int64_t k,",
      "    const std::optional<torch::Tensor>& blockscale_offsets);",
      "",
      "void get_cutlass_moe_mm_problem_sizes_caller(",
      "    const torch::Tensor& topk_ids, torch::Tensor& problem_sizes1,",
      "    torch::Tensor& problem_sizes2, const int64_t num_experts, const int64_t n,",
      "    const int64_t k, const std::optional<torch::Tensor>& blockscale_offsets);",
      "",
      "void get_cutlass_pplx_moe_mm_data_caller(torch::Tensor& expert_offsets,",
      "                                         torch::Tensor& problem_sizes1,",
      "                                         torch::Tensor& problem_sizes2,",
      "                                         const torch::Tensor& expert_num_tokens,",
      "                                         const int64_t num_local_experts,",
      "                                         const int64_t padded_m,",
      "                                         const int64_t n, const int64_t k);",
      "#endif",
      "",
      "void cutlass_scaled_mm_azp_sm75(torch::Tensor& c, torch::Tensor const& a,",
      "                                torch::Tensor const& b,",
      "                                torch::Tensor const& a_scales,",
      "                                torch::Tensor const& b_scales,",
      "                                torch::Tensor const& azp_adj,",
      "                                std::optional<torch::Tensor> const& azp,",
      "                                std::optional<torch::Tensor> const& bias);",
      "",
      "void cutlass_scaled_mm_azp_sm80(torch::Tensor& c, torch::Tensor const& a,",
      "                                torch::Tensor const& b,",
      "                                torch::Tensor const& a_scales,",
      "                                torch::Tensor const& b_scales,",
      "                                torch::Tensor const& azp_adj,",
      "                                std::optional<torch::Tensor> const& azp,",
      "                                std::optional<torch::Tensor> const& bias);",
      "",
      "void cutlass_scaled_mm_azp_sm89(torch::Tensor& c, torch::Tensor const& a,",
      "                                torch::Tensor const& b,",
      "                                torch::Tensor const& a_scales,",
      "                                torch::Tensor const& b_scales,",
      "                                torch::Tensor const& azp_adj,",
      "                                std::optional<torch::Tensor> const& azp,",
      "                                std::optional<torch::Tensor> const& bias);",
      "",
      "#if defined ENABLE_SCALED_MM_SM90 && ENABLE_SCALED_MM_SM90",
      "void cutlass_scaled_mm_azp_sm90(torch::Tensor& c, torch::Tensor const& a,",
      "                                torch::Tensor const& b,",
      "                                torch::Tensor const& a_scales,",
      "                                torch::Tensor const& b_scales,",
      "                                torch::Tensor const& azp_adj,",
      "                                std::optional<torch::Tensor> const& azp,",
      "                                std::optional<torch::Tensor> const& bias);",
      "#endif",
      "",
      "bool cutlass_scaled_mm_supports_fp8(int64_t cuda_device_capability) {",
      "  // CUTLASS FP8 kernels need at least",
      "  //   CUDA 12.0 on SM90 systems (Hopper)",
      "  //   CUDA 12.4 on SM89 systems (Lovelace)",
      "",
      "#if defined CUDA_VERSION",
      "  if (cuda_device_capability >= 90) {",
      "    return CUDA_VERSION >= 12000;",
      "  } else if (cuda_device_capability >= 89) {",
      "    return CUDA_VERSION >= 12040;",
      "  }",
      "#endif",
      "",
      "  return false;",
      "}",
      "",
      "bool cutlass_scaled_mm_supports_block_fp8(int64_t cuda_device_capability) {",
      "  // CUTLASS block-quantized FP8 kernels need at least CUDA 12.0",
      "  // and at least SM90 (Hopper)",
      "",
      "#if defined CUDA_VERSION",
      "  if (cuda_device_capability >= 100) {",
      "    return CUDA_VERSION >= 12080;",
      "  } else if (cuda_device_capability >= 90) {",
      "    return CUDA_VERSION >= 12000;",
      "  }",
      "#endif",
      "",
      "  return false;",
      "}",
      "",
      "bool cutlass_group_gemm_supported(int64_t cuda_device_capability) {",
      "  // CUTLASS grouped FP8 kernels need at least CUDA 12.3 and SM90 (Hopper)",
      "  // or CUDA 12.8 and SM100 (Blackwell)",
      "",
      "#if defined CUDA_VERSION",
      "  if (cuda_device_capability >= 100) {",
      "    return CUDA_VERSION >= 12080;",
      "  }",
      "  if (cuda_device_capability >= 90) {",
      "    return CUDA_VERSION >= 12030;",
      "  }",
      "#endif",
      "",
      "  return false;",
      "}",
      "",
      "void cutlass_scaled_mm(torch::Tensor& c, torch::Tensor const& a,",
      "                       torch::Tensor const& b, torch::Tensor const& a_scales,",
      "                       torch::Tensor const& b_scales,",
      "                       std::optional<torch::Tensor> const& bias) {",
      "  // Checks for conformality",
      "  TORCH_CHECK(a.dim() == 2 && b.dim() == 2 && c.dim() == 2);",
      "  TORCH_CHECK(c.size(0) == a.size(0) && a.size(1) == b.size(0) &&",
      "              b.size(1) == c.size(1));",
      "",
      "  // Check for strides and alignment",
      "  TORCH_CHECK(a.stride(1) == 1 && c.stride(1) == 1);  // Row-major",
      "  TORCH_CHECK(b.stride(0) == 1);                      // Column-major",
      "  TORCH_CHECK(c.stride(0) % 16 == 0 &&",
      "              b.stride(1) % 16 == 0);  // 16 Byte Alignment",
      "",
      "  if (bias) {",
      "    TORCH_CHECK(bias->numel() == b.size(1) && bias->is_contiguous() &&",
      "                bias->dim() == 1);",
      "  }",
      "",
      "  at::cuda::OptionalCUDAGuard const device_guard(device_of(a));",
      "  int32_t version_num = get_sm_version_num();",
      "",
      "#if defined ENABLE_SCALED_MM_SM120 && ENABLE_SCALED_MM_SM120",
      "  if (version_num >= 120) {",
      "    cutlass_scaled_mm_sm120(c, a, b, a_scales, b_scales, bias);",
      "    return;",
      "  }",
      "#endif",
      "",
      "#if defined ENABLE_SCALED_MM_SM100 && ENABLE_SCALED_MM_SM100",
      "  if (version_num >= 100 && version_num < 120) {",
      "    cutlass_scaled_mm_sm100(c, a, b, a_scales, b_scales, bias);",
      "    return;",
      "  }",
      "#endif",
      "",
      "  // Guard against compilation issues for sm90 kernels",
      "#if defined ENABLE_SCALED_MM_SM90 && ENABLE_SCALED_MM_SM90",
      "  if (version_num >= 90 && version_num < 100) {",
      "    // Hopper",
      "    cutlass_scaled_mm_sm90(c, a, b, a_scales, b_scales, bias);",
      "    return;",
      "  }",
      "#endif",
      "",
      "#if defined ENABLE_SCALED_MM_C2X && ENABLE_SCALED_MM_C2X",
      "  if (version_num == 89) {",
      "    // Ada Lovelace",
      "    cutlass_scaled_mm_sm89(c, a, b, a_scales, b_scales, bias);",
      "    return;",
      "  }",
      "",
      "  if (version_num >= 80) {",
      "    // Ampere",
      "    cutlass_scaled_mm_sm80(c, a, b, a_scales, b_scales, bias);",
      "    return;",
      "  }",
      "",
      "  if (version_num >= 75) {",
      "    // Turing",
      "    cutlass_scaled_mm_sm75(c, a, b, a_scales, b_scales, bias);",
      "    return;",
      "  }",
      "#endif",
      "",
      "  TORCH_CHECK_NOT_IMPLEMENTED(",
      "      false,",
      "      \"No compiled cutlass_scaled_mm for a compute capability less than \"",
      "      \"CUDA device capability: \",",
      "      version_num);",
      "}",
      "",
      "void cutlass_moe_mm(",
      "    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,",
      "    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,",
      "    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,",
      "    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,",
      "    torch::Tensor const& b_strides, torch::Tensor const& c_strides,",
      "    bool per_act_token, bool per_out_ch) {",
      "  int32_t version_num = get_sm_version_num();",
      "#if defined ENABLE_CUTLASS_MOE_SM100 && ENABLE_CUTLASS_MOE_SM100",
      "  if (version_num >= 100) {",
      "    cutlass_moe_mm_sm100(out_tensors, a_tensors, b_tensors, a_scales, b_scales,",
      "                         expert_offsets, problem_sizes, a_strides, b_strides,",
      "                         c_strides, per_act_token, per_out_ch);",
      "    return;",
      "  }",
      "#endif",
      "#if defined ENABLE_CUTLASS_MOE_SM90 && ENABLE_CUTLASS_MOE_SM90",
      "  if (version_num >= 90) {",
      "    cutlass_moe_mm_sm90(out_tensors, a_tensors, b_tensors, a_scales, b_scales,",
      "                        expert_offsets, problem_sizes, a_strides, b_strides,",
      "                        c_strides, per_act_token, per_out_ch);",
      "    return;",
      "  }",
      "#endif",
      "  TORCH_CHECK_NOT_IMPLEMENTED(",
      "      false,",
      "      \"No compiled cutlass_scaled_mm for CUDA device capability: \", version_num,",
      "      \". Required capability: 90 or 100\");",
      "}",
      "",
      "void get_cutlass_moe_mm_data(",
      "    const torch::Tensor& topk_ids, torch::Tensor& expert_offsets,",
      "    torch::Tensor& problem_sizes1, torch::Tensor& problem_sizes2,",
      "    torch::Tensor& input_permutation, torch::Tensor& output_permutation,",
      "    const int64_t num_experts, const int64_t n, const int64_t k,",
      "    const std::optional<torch::Tensor>& blockscale_offsets) {",
      "  // This function currently gets compiled only if we have a valid cutlass moe",
      "  // mm to run it for.",
      "  int32_t version_num = get_sm_version_num();",
      "#if (defined ENABLE_CUTLASS_MOE_SM90 && ENABLE_CUTLASS_MOE_SM90) || \\",
      "    (defined ENABLE_CUTLASS_MOE_SM100 && ENABLE_CUTLASS_MOE_SM100)",
      "  get_cutlass_moe_mm_data_caller(topk_ids, expert_offsets, problem_sizes1,",
      "                                 problem_sizes2, input_permutation,",
      "                                 output_permutation, num_experts, n, k,",
      "                                 blockscale_offsets);",
      "  return;",
      "#endif",
      "  TORCH_CHECK_NOT_IMPLEMENTED(",
      "      false,",
      "      \"No compiled get_cutlass_moe_mm_data: no cutlass_scaled_mm kernel for \"",
      "      \"CUDA device capability: \",",
      "      version_num, \". Required capability: 90 or 100\");",
      "}",
      "",
      "void get_cutlass_moe_mm_problem_sizes(",
      "    const torch::Tensor& topk_ids, torch::Tensor& problem_sizes1,",
      "    torch::Tensor& problem_sizes2, const int64_t num_experts, const int64_t n,",
      "    const int64_t k, const std::optional<torch::Tensor>& blockscale_offsets) {",
      "  int32_t version_num = get_sm_version_num();",
      "#if (defined ENABLE_CUTLASS_MOE_SM90 && ENABLE_CUTLASS_MOE_SM90) || \\",
      "    (defined ENABLE_CUTLASS_MOE_SM100 && ENABLE_CUTLASS_MOE_SM100)",
      "  get_cutlass_moe_mm_problem_sizes_caller(topk_ids, problem_sizes1,",
      "                                          problem_sizes2, num_experts, n, k,",
      "                                          blockscale_offsets);",
      "  return;",
      "#endif",
      "  TORCH_CHECK_NOT_IMPLEMENTED(",
      "      false,",
      "      \"No compiled get_cutlass_moe_mm_problem_sizes: no cutlass_scaled_mm \"",
      "      \"kernel for CUDA device capability: \",",
      "      version_num, \". Required capability: 90 or 100\");",
      "}",
      "",
      "void get_cutlass_pplx_moe_mm_data(torch::Tensor& expert_offsets,",
      "                                  torch::Tensor& problem_sizes1,",
      "                                  torch::Tensor& problem_sizes2,",
      "                                  const torch::Tensor& expert_num_tokens,",
      "                                  const int64_t num_local_experts,",
      "                                  const int64_t padded_m, const int64_t n,",
      "                                  const int64_t k) {",
      "  // This function currently gets compiled only if we have a valid cutlass moe",
      "  // mm to run it for.",
      "  int32_t version_num = get_sm_version_num();",
      "#if (defined ENABLE_CUTLASS_MOE_SM90 && ENABLE_CUTLASS_MOE_SM90) || \\",
      "    (defined ENABLE_CUTLASS_MOE_SM100 && ENABLE_CUTLASS_MOE_SM100)",
      "  get_cutlass_pplx_moe_mm_data_caller(expert_offsets, problem_sizes1,",
      "                                      problem_sizes2, expert_num_tokens,",
      "                                      num_local_experts, padded_m, n, k);",
      "  return;",
      "#endif",
      "  TORCH_CHECK_NOT_IMPLEMENTED(",
      "      false,",
      "      \"No compiled get_cutlass_pplx_moe_mm_data: no cutlass_scaled_mm kernel \"",
      "      \"for CUDA device capability: \",",
      "      version_num, \". Required capability: 90 or 100\");",
      "}",
      "",
      "void cutlass_scaled_mm_azp(torch::Tensor& c, torch::Tensor const& a,",
      "                           torch::Tensor const& b,",
      "                           torch::Tensor const& a_scales,",
      "                           torch::Tensor const& b_scales,",
      "                           torch::Tensor const& azp_adj,",
      "                           std::optional<torch::Tensor> const& azp,",
      "                           std::optional<torch::Tensor> const& bias) {",
      "  // Checks for conformality",
      "  TORCH_CHECK(a.dim() == 2 && b.dim() == 2 && c.dim() == 2);",
      "  TORCH_CHECK(c.size(0) == a.size(0) && a.size(1) == b.size(0) &&",
      "              b.size(1) == c.size(1));",
      "  TORCH_CHECK(a_scales.numel() == 1 || a_scales.numel() == a.size(0));",
      "  TORCH_CHECK(b_scales.numel() == 1 || b_scales.numel() == b.size(1));",
      "",
      "  // Check for strides and alignment",
      "  TORCH_CHECK(a.stride(1) == 1 && c.stride(1) == 1);  // Row-major",
      "  TORCH_CHECK(b.stride(0) == 1);                      // Column-major",
      "  TORCH_CHECK(c.stride(0) % 16 == 0 &&",
      "              b.stride(1) % 16 == 0);  // 16 Byte Alignment",
      "  TORCH_CHECK(a_scales.is_contiguous() && b_scales.is_contiguous());",
      "",
      "  // bias, azp, azp_adj are all 1d",
      "  // bias and azp_adj have n elements, azp has m elements",
      "  if (bias) {",
      "    TORCH_CHECK(bias->numel() == b.size(1) && bias->is_contiguous());",
      "  }",
      "  if (azp) {",
      "    TORCH_CHECK(azp->numel() == a.size(0) && azp->is_contiguous());",
      "  }",
      "  TORCH_CHECK(azp_adj.numel() == b.size(1) && azp_adj.is_contiguous());",
      "",
      "  // azp & bias types",
      "  TORCH_CHECK(azp_adj.dtype() == torch::kInt32);",
      "  TORCH_CHECK(!azp || azp->dtype() == torch::kInt32);",
      "  TORCH_CHECK(!bias || bias->dtype() == c.dtype(),",
      "              \"currently bias dtype must match output dtype \", c.dtype());",
      "",
      "  at::cuda::OptionalCUDAGuard const device_guard(device_of(a));",
      "",
      "  int32_t version_num = get_sm_version_num();",
      "",
      "#if defined ENABLE_SCALED_MM_SM90 && ENABLE_SCALED_MM_SM90",
      "  if (version_num >= 90) {",
      "    cutlass_scaled_mm_azp_sm90(c, a, b, a_scales, b_scales, azp_adj, azp, bias);",
      "    return;",
      "  }",
      "#endif",
      "",
      "#if defined ENABLE_SCALED_MM_C2X && ENABLE_SCALED_MM_C2X",
      "  if (version_num == 89) {",
      "    // Ada Lovelace",
      "    cutlass_scaled_mm_azp_sm89(c, a, b, a_scales, b_scales, azp_adj, azp, bias);",
      "    return;",
      "  }",
      "",
      "  if (version_num >= 80) {",
      "    // Ampere",
      "    cutlass_scaled_mm_azp_sm80(c, a, b, a_scales, b_scales, azp_adj, azp, bias);",
      "    return;",
      "  }",
      "",
      "  // Turing",
      "  TORCH_CHECK(version_num >= 75);",
      "  cutlass_scaled_mm_azp_sm75(c, a, b, a_scales, b_scales, azp_adj, azp, bias);",
      "  return;",
      "#endif",
      "",
      "  TORCH_CHECK_NOT_IMPLEMENTED(",
      "      false,",
      "      \"No compiled cutlass_scaled_mm_azp for a compute capability less than \"",
      "      \"CUDA device capability: \",",
      "      version_num);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu",
    "source": [
      "#include <stddef.h>",
      "#include <torch/all.h>",
      "#include \"cutlass/cutlass.h\"",
      "",
      "#include \"scaled_mm_c2x.cuh\"",
      "#include \"scaled_mm_c2x_sm75_dispatch.cuh\"",
      "#include \"scaled_mm_c2x_sm80_dispatch.cuh\"",
      "#include \"scaled_mm_c2x_sm89_fp8_dispatch.cuh\"",
      "#include \"scaled_mm_c2x_sm89_int8_dispatch.cuh\"",
      "",
      "#include \"cutlass_extensions/epilogue/scaled_mm_epilogues_c2x.hpp\"",
      "",
      "using namespace vllm;",
      "",
      "/*",
      "   This file defines quantized GEMM operations using the CUTLASS 2.x API, for",
      "   NVIDIA GPUs with SM versions prior to sm90 (Hopper).",
      "*/",
      "",
      "template <template <typename, typename> typename Epilogue,",
      "          typename... EpilogueArgs>",
      "void cutlass_scaled_mm_sm75_epilogue(torch::Tensor& out, torch::Tensor const& a,",
      "                                     torch::Tensor const& b,",
      "                                     EpilogueArgs&&... epilogue_args) {",
      "  TORCH_CHECK(a.dtype() == torch::kInt8);",
      "  TORCH_CHECK(b.dtype() == torch::kInt8);",
      "",
      "  if (out.dtype() == torch::kBFloat16) {",
      "    return cutlass_gemm_sm75_dispatch<int8_t, cutlass::bfloat16_t, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);",
      "  } else {",
      "    TORCH_CHECK(out.dtype() == torch::kFloat16);",
      "    return cutlass_gemm_sm75_dispatch<int8_t, cutlass::half_t, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);",
      "  }",
      "}",
      "",
      "void cutlass_scaled_mm_sm75(torch::Tensor& out, torch::Tensor const& a,",
      "                            torch::Tensor const& b,",
      "                            torch::Tensor const& a_scales,",
      "                            torch::Tensor const& b_scales,",
      "                            std::optional<torch::Tensor> const& bias) {",
      "  TORCH_CHECK(a_scales.dtype() == torch::kFloat32);",
      "  TORCH_CHECK(b_scales.dtype() == torch::kFloat32);",
      "  if (bias) {",
      "    TORCH_CHECK(bias->dtype() == out.dtype(),",
      "                \"currently bias dtype must match output dtype \", out.dtype());",
      "    return cutlass_scaled_mm_sm75_epilogue<c2x::ScaledEpilogueBias>(",
      "        out, a, b, a_scales, b_scales, *bias);",
      "  } else {",
      "    return cutlass_scaled_mm_sm75_epilogue<c2x::ScaledEpilogue>(",
      "        out, a, b, a_scales, b_scales);",
      "  }",
      "}",
      "",
      "void cutlass_scaled_mm_azp_sm75(torch::Tensor& out, torch::Tensor const& a,",
      "                                torch::Tensor const& b,",
      "                                torch::Tensor const& a_scales,",
      "                                torch::Tensor const& b_scales,",
      "                                torch::Tensor const& azp_adj,",
      "                                std::optional<torch::Tensor> const& azp,",
      "                                std::optional<torch::Tensor> const& bias) {",
      "  TORCH_CHECK(a_scales.dtype() == torch::kFloat32);",
      "  TORCH_CHECK(b_scales.dtype() == torch::kFloat32);",
      "",
      "  if (azp) {",
      "    return cutlass_scaled_mm_sm75_epilogue<c2x::ScaledEpilogueBiasAzpToken>(",
      "        out, a, b, a_scales, b_scales, azp_adj, *azp, bias);",
      "  } else {",
      "    return cutlass_scaled_mm_sm75_epilogue<c2x::ScaledEpilogueBiasAzp>(",
      "        out, a, b, a_scales, b_scales, azp_adj, bias);",
      "  }",
      "}",
      "",
      "template <template <typename, typename> typename Epilogue,",
      "          typename... EpilogueArgs>",
      "void cutlass_scaled_mm_sm80_epilogue(torch::Tensor& out, torch::Tensor const& a,",
      "                                     torch::Tensor const& b,",
      "                                     EpilogueArgs&&... epilogue_args) {",
      "  TORCH_CHECK(a.dtype() == torch::kInt8);",
      "  TORCH_CHECK(b.dtype() == torch::kInt8);",
      "",
      "  if (out.dtype() == torch::kBFloat16) {",
      "    return cutlass_gemm_sm80_dispatch<int8_t, cutlass::bfloat16_t, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);",
      "  } else {",
      "    TORCH_CHECK(out.dtype() == torch::kFloat16);",
      "    return cutlass_gemm_sm80_dispatch<int8_t, cutlass::half_t, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);",
      "  }",
      "}",
      "",
      "void cutlass_scaled_mm_sm80(torch::Tensor& out, torch::Tensor const& a,",
      "                            torch::Tensor const& b,",
      "                            torch::Tensor const& a_scales,",
      "                            torch::Tensor const& b_scales,",
      "                            std::optional<torch::Tensor> const& bias) {",
      "  TORCH_CHECK(a_scales.dtype() == torch::kFloat32);",
      "  TORCH_CHECK(b_scales.dtype() == torch::kFloat32);",
      "  if (bias) {",
      "    TORCH_CHECK(bias->dtype() == out.dtype(),",
      "                \"currently bias dtype must match output dtype \", out.dtype());",
      "    return cutlass_scaled_mm_sm80_epilogue<c2x::ScaledEpilogueBias>(",
      "        out, a, b, a_scales, b_scales, *bias);",
      "  } else {",
      "    return cutlass_scaled_mm_sm80_epilogue<c2x::ScaledEpilogue>(",
      "        out, a, b, a_scales, b_scales);",
      "  }",
      "}",
      "",
      "void cutlass_scaled_mm_azp_sm80(torch::Tensor& out, torch::Tensor const& a,",
      "                                torch::Tensor const& b,",
      "                                torch::Tensor const& a_scales,",
      "                                torch::Tensor const& b_scales,",
      "                                torch::Tensor const& azp_adj,",
      "                                std::optional<torch::Tensor> const& azp,",
      "                                std::optional<torch::Tensor> const& bias) {",
      "  TORCH_CHECK(a_scales.dtype() == torch::kFloat32);",
      "  TORCH_CHECK(b_scales.dtype() == torch::kFloat32);",
      "",
      "  if (azp) {",
      "    return cutlass_scaled_mm_sm80_epilogue<c2x::ScaledEpilogueBiasAzpToken>(",
      "        out, a, b, a_scales, b_scales, azp_adj, *azp, bias);",
      "  } else {",
      "    return cutlass_scaled_mm_sm80_epilogue<c2x::ScaledEpilogueBiasAzp>(",
      "        out, a, b, a_scales, b_scales, azp_adj, bias);",
      "  }",
      "}",
      "",
      "template <template <typename, typename> typename Epilogue,",
      "          typename... EpilogueArgs>",
      "void cutlass_scaled_mm_sm89_epilogue(torch::Tensor& out, torch::Tensor const& a,",
      "                                     torch::Tensor const& b,",
      "                                     EpilogueArgs&&... epilogue_args) {",
      "  if (a.dtype() == torch::kInt8) {",
      "    TORCH_CHECK(b.dtype() == torch::kInt8);",
      "",
      "    if (out.dtype() == torch::kBFloat16) {",
      "      return cutlass_gemm_sm89_int8_dispatch<int8_t, cutlass::bfloat16_t,",
      "                                             Epilogue>(",
      "          out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);",
      "    } else {",
      "      assert(out.dtype() == torch::kFloat16);",
      "      return cutlass_gemm_sm89_int8_dispatch<int8_t, cutlass::half_t, Epilogue>(",
      "          out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);",
      "    }",
      "  } else {",
      "    TORCH_CHECK(a.dtype() == torch::kFloat8_e4m3fn);",
      "    TORCH_CHECK(b.dtype() == torch::kFloat8_e4m3fn);",
      "",
      "    if (out.dtype() == torch::kBFloat16) {",
      "      return cutlass_gemm_sm89_fp8_dispatch<cutlass::float_e4m3_t,",
      "                                            cutlass::bfloat16_t, Epilogue>(",
      "          out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);",
      "    } else {",
      "      TORCH_CHECK(out.dtype() == torch::kFloat16);",
      "      return cutlass_gemm_sm89_fp8_dispatch<cutlass::float_e4m3_t,",
      "                                            cutlass::half_t, Epilogue>(",
      "          out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);",
      "    }",
      "  }",
      "}",
      "",
      "void cutlass_scaled_mm_sm89(torch::Tensor& out, torch::Tensor const& a,",
      "                            torch::Tensor const& b,",
      "                            torch::Tensor const& a_scales,",
      "                            torch::Tensor const& b_scales,",
      "                            std::optional<torch::Tensor> const& bias) {",
      "  TORCH_CHECK(a_scales.dtype() == torch::kFloat32);",
      "  TORCH_CHECK(b_scales.dtype() == torch::kFloat32);",
      "  if (bias) {",
      "    TORCH_CHECK(bias->dtype() == out.dtype(),",
      "                \"currently bias dtype must match output dtype \", out.dtype());",
      "    return cutlass_scaled_mm_sm89_epilogue<c2x::ScaledEpilogueBias>(",
      "        out, a, b, a_scales, b_scales, *bias);",
      "  } else {",
      "    return cutlass_scaled_mm_sm89_epilogue<c2x::ScaledEpilogue>(",
      "        out, a, b, a_scales, b_scales);",
      "  }",
      "}",
      "",
      "void cutlass_scaled_mm_azp_sm89(torch::Tensor& out, torch::Tensor const& a,",
      "                                torch::Tensor const& b,",
      "                                torch::Tensor const& a_scales,",
      "                                torch::Tensor const& b_scales,",
      "                                torch::Tensor const& azp_adj,",
      "                                std::optional<torch::Tensor> const& azp,",
      "                                std::optional<torch::Tensor> const& bias) {",
      "  TORCH_CHECK(a_scales.dtype() == torch::kFloat32);",
      "  TORCH_CHECK(b_scales.dtype() == torch::kFloat32);",
      "",
      "  if (azp) {",
      "    return cutlass_scaled_mm_sm89_epilogue<c2x::ScaledEpilogueBiasAzpToken>(",
      "        out, a, b, a_scales, b_scales, azp_adj, *azp, bias);",
      "  } else {",
      "    return cutlass_scaled_mm_sm89_epilogue<c2x::ScaledEpilogueBiasAzp>(",
      "        out, a, b, a_scales, b_scales, azp_adj, bias);",
      "  }",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm89_fp8_dispatch.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"scaled_mm_c2x.cuh\"",
      "#include \"cutlass/float8.h\"",
      "",
      "/**",
      " * This file defines Gemm kernel configurations for SM89 (FP8) based on the Gemm",
      " * shape.",
      " */",
      "",
      "namespace vllm {",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename> typename Epilogue>",
      "struct sm89_fp8_fallback_gemm {",
      "  // Shared Memory required by this Gemm - 61440 bytes",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using TileShape = typename cutlass::gemm::GemmShape<64, 128, 64>;",
      "  using WarpShape = typename cutlass::gemm::GemmShape<32, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "  using FP8MathOperator = typename cutlass::arch::OpMultiplyAdd;",
      "  using Cutlass2xGemm =",
      "      cutlass_2x_gemm<cutlass::arch::Sm89, enable_sm89_to_sm90, InType, OutType,",
      "                      Epilogue, TileShape, WarpShape, InstructionShape, 5,",
      "                      FP8MathOperator>;",
      "};",
      "",
      "struct sm89_fp8_config_default {",
      "  // M in (256, inf)",
      "  using WarpShape = typename cutlass::gemm::GemmShape<64, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "  using FP8MathOperator = typename cutlass::arch::OpMultiplyAddFastAccum;",
      "",
      "  template <typename InType, typename OutType,",
      "            template <typename, typename> typename Epilogue,",
      "            typename... EpilogueArgs>",
      "  static void dispatch(torch::Tensor& out, torch::Tensor const& a,",
      "                       torch::Tensor const& b, EpilogueArgs&&... args) {",
      "    static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "    TORCH_CHECK(a.dtype() == torch::kFloat8_e4m3fn);",
      "",
      "    using FallbackGemm =",
      "        typename sm89_fp8_fallback_gemm<InType, OutType,",
      "                                        Epilogue>::Cutlass2xGemm;",
      "",
      "    uint32_t const n = out.size(1);",
      "    uint32_t const np2 = next_pow_2(n);",
      "",
      "    if (np2 <= 4096) {",
      "      using TileShape = typename cutlass::gemm::GemmShape<128, 128, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5, FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else if (np2 <= 8192) {",
      "      using TileShape = typename cutlass::gemm::GemmShape<256, 128, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 3, FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "",
      "    } else {",
      "      using TileShape = typename cutlass::gemm::GemmShape<128, 128, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5, FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "  }",
      "};",
      "",
      "struct sm89_fp8_config_M256 {",
      "  // M in (128, 256]",
      "  using WarpShape = typename cutlass::gemm::GemmShape<64, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "  using FP8MathOperator = typename cutlass::arch::OpMultiplyAddFastAccum;",
      "",
      "  template <typename InType, typename OutType,",
      "            template <typename, typename> typename Epilogue,",
      "            typename... EpilogueArgs>",
      "  static void dispatch(torch::Tensor& out, torch::Tensor const& a,",
      "                       torch::Tensor const& b, EpilogueArgs&&... args) {",
      "    static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "    TORCH_CHECK(a.dtype() == torch::kFloat8_e4m3fn);",
      "",
      "    using FallbackGemm =",
      "        typename sm89_fp8_fallback_gemm<InType, OutType,",
      "                                        Epilogue>::Cutlass2xGemm;",
      "",
      "    uint32_t const n = out.size(1);",
      "    uint32_t const np2 = next_pow_2(n);",
      "",
      "    if (np2 <= 4096) {",
      "      using TileShape = typename cutlass::gemm::GemmShape<64, 128, 128>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 3, FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else {",
      "      using TileShape = typename cutlass::gemm::GemmShape<128, 128, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5, FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "  }",
      "};",
      "",
      "struct sm89_fp8_config_M128 {",
      "  // M in (64, 128]",
      "  using WarpShape = typename cutlass::gemm::GemmShape<64, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "  using FP8MathOperator = typename cutlass::arch::OpMultiplyAddFastAccum;",
      "",
      "  template <typename InType, typename OutType,",
      "            template <typename, typename> typename Epilogue,",
      "            typename... EpilogueArgs>",
      "  static void dispatch(torch::Tensor& out, torch::Tensor const& a,",
      "                       torch::Tensor const& b, EpilogueArgs&&... args) {",
      "    static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "    TORCH_CHECK(a.dtype() == torch::kFloat8_e4m3fn);",
      "",
      "    using FallbackGemm =",
      "        typename sm89_fp8_fallback_gemm<InType, OutType,",
      "                                        Epilogue>::Cutlass2xGemm;",
      "",
      "    uint32_t const n = out.size(1);",
      "    uint32_t const np2 = next_pow_2(n);",
      "",
      "    if (np2 <= 8192) {",
      "      using TileShape = typename cutlass::gemm::GemmShape<64, 128, 128>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 3, FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "",
      "    } else if (np2 <= 16384) {",
      "      using TileShape = typename cutlass::gemm::GemmShape<128, 128, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5, FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else {",
      "      using TileShape = typename cutlass::gemm::GemmShape<128, 64, 128>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 3, FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "  }",
      "};",
      "",
      "struct sm89_fp8_config_M64 {",
      "  // M in (32, 64]",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "",
      "  template <typename InType, typename OutType,",
      "            template <typename, typename> typename Epilogue,",
      "            typename... EpilogueArgs>",
      "  static void dispatch(torch::Tensor& out, torch::Tensor const& a,",
      "                       torch::Tensor const& b, EpilogueArgs&&... args) {",
      "    static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "    TORCH_CHECK(a.dtype() == torch::kFloat8_e4m3fn);",
      "",
      "    using FallbackGemm =",
      "        typename sm89_fp8_fallback_gemm<InType, OutType,",
      "                                        Epilogue>::Cutlass2xGemm;",
      "",
      "    uint32_t const n = out.size(1);",
      "    uint32_t const np2 = next_pow_2(n);",
      "",
      "    if (np2 <= 8196) {",
      "      using TileShape = typename cutlass::gemm::GemmShape<64, 64, 128>;",
      "      using WarpShape = typename cutlass::gemm::GemmShape<32, 64, 64>;",
      "      using FP8MathOperator = typename cutlass::arch::OpMultiplyAdd;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5, FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else if (np2 <= 16384) {",
      "      using TileShape = typename cutlass::gemm::GemmShape<64, 128, 128>;",
      "      using WarpShape = typename cutlass::gemm::GemmShape<64, 64, 64>;",
      "      using FP8MathOperator = typename cutlass::arch::OpMultiplyAddFastAccum;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 3, FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else {",
      "      using TileShape = typename cutlass::gemm::GemmShape<64, 64, 128>;",
      "      using WarpShape = typename cutlass::gemm::GemmShape<32, 64, 64>;",
      "      using FP8MathOperator = typename cutlass::arch::OpMultiplyAdd;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5, FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "  }",
      "};",
      "",
      "struct sm89_fp8_config_M32 {",
      "  // M in (16, 32]",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "  using FP8MathOperator = typename cutlass::arch::OpMultiplyAddFastAccum;",
      "",
      "  template <typename InType, typename OutType,",
      "            template <typename, typename> typename Epilogue,",
      "            typename... EpilogueArgs>",
      "  static void dispatch(torch::Tensor& out, torch::Tensor const& a,",
      "                       torch::Tensor const& b, EpilogueArgs&&... args) {",
      "    static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "    TORCH_CHECK(a.dtype() == torch::kFloat8_e4m3fn);",
      "",
      "    using FallbackGemm =",
      "        typename sm89_fp8_fallback_gemm<InType, OutType,",
      "                                        Epilogue>::Cutlass2xGemm;",
      "",
      "    uint32_t const n = out.size(1);",
      "    uint32_t const np2 = next_pow_2(n);",
      "",
      "    if (np2 <= 8192) {",
      "      using TileShape = typename cutlass::gemm::GemmShape<32, 64, 128>;",
      "      using WarpShape = typename cutlass::gemm::GemmShape<16, 64, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5, FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else if (np2 <= 16384) {",
      "      using TileShape = typename cutlass::gemm::GemmShape<32, 128, 128>;",
      "      using WarpShape = typename cutlass::gemm::GemmShape<32, 64, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 4, FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else {",
      "      using TileShape = typename cutlass::gemm::GemmShape<32, 64, 128>;",
      "      using WarpShape = typename cutlass::gemm::GemmShape<16, 64, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, 5, FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "  }",
      "};",
      "",
      "struct sm89_fp8_config_M16 {",
      "  // M in [1, 16]",
      "  using WarpShape = typename cutlass::gemm::GemmShape<16, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "  using FP8MathOperator = typename cutlass::arch::OpMultiplyAddFastAccum;",
      "  static const int32_t MainLoopStages = 5;",
      "",
      "  template <typename InType, typename OutType,",
      "            template <typename, typename> typename Epilogue,",
      "            typename... EpilogueArgs>",
      "  static void dispatch(torch::Tensor& out, torch::Tensor const& a,",
      "                       torch::Tensor const& b, EpilogueArgs&&... args) {",
      "    static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "    TORCH_CHECK(a.dtype() == torch::kFloat8_e4m3fn);",
      "",
      "    using FallbackGemm =",
      "        typename sm89_fp8_fallback_gemm<InType, OutType,",
      "                                        Epilogue>::Cutlass2xGemm;",
      "",
      "    uint32_t const n = out.size(1);",
      "    uint32_t const np2 = next_pow_2(n);",
      "",
      "    if (np2 <= 8192) {",
      "      using TileShape = typename cutlass::gemm::GemmShape<16, 64, 128>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, MainLoopStages,",
      "                                FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else if (np2 <= 24576) {",
      "      using TileShape = typename cutlass::gemm::GemmShape<16, 128, 64>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, MainLoopStages,",
      "                                FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else {",
      "      using TileShape = typename cutlass::gemm::GemmShape<32, 64, 128>;",
      "",
      "      return vllm::fallback_cutlass_gemm_caller<",
      "          vllm::cutlass_2x_gemm<cutlass::arch::Sm89, vllm::enable_sm89_to_sm90,",
      "                                InType, OutType, Epilogue, TileShape, WarpShape,",
      "                                InstructionShape, MainLoopStages,",
      "                                FP8MathOperator>,",
      "          FallbackGemm>(out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "  }",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename> typename Epilogue,",
      "          typename... EpilogueArgs>",
      "inline void cutlass_gemm_sm89_fp8_dispatch(torch::Tensor& out,",
      "                                           torch::Tensor const& a,",
      "                                           torch::Tensor const& b,",
      "                                           EpilogueArgs&&... args) {",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  TORCH_CHECK(a.dtype() == torch::kFloat8_e4m3fn);",
      "  TORCH_CHECK(b.dtype() == torch::kFloat8_e4m3fn);",
      "",
      "  uint32_t const m = a.size(0);",
      "  uint32_t const mp2 =",
      "      std::max(static_cast<uint32_t>(16), next_pow_2(m));  // next power of 2",
      "",
      "  if (mp2 <= 16) {",
      "    // M in [1, 16]",
      "    return sm89_fp8_config_M16::dispatch<InType, OutType, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 32) {",
      "    // M in (16, 32]",
      "    return sm89_fp8_config_M32::dispatch<InType, OutType, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 64) {",
      "    // M in (32, 64]",
      "    return sm89_fp8_config_M64::dispatch<InType, OutType, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 128) {",
      "    // M in (64, 128]",
      "    return sm89_fp8_config_M128::dispatch<InType, OutType, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 256) {",
      "    // M in (128, 256]",
      "    return sm89_fp8_config_M256::dispatch<InType, OutType, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else {",
      "    // M in (256, inf)",
      "    return sm89_fp8_config_default::dispatch<InType, OutType, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm100.cu",
    "source": [
      "#include \"c3x/scaled_mm_helper.hpp\"",
      "#include \"c3x/scaled_mm_kernels.hpp\"",
      "",
      "/*",
      "   This file defines quantized GEMM operations using the CUTLASS 3.x API, for",
      "   NVIDIA GPUs with sm100 (Blackwell).",
      "*/",
      "",
      "#if defined ENABLE_SCALED_MM_SM100 && ENABLE_SCALED_MM_SM100",
      "",
      "void cutlass_scaled_mm_sm100(torch::Tensor& c, torch::Tensor const& a,",
      "                             torch::Tensor const& b,",
      "                             torch::Tensor const& a_scales,",
      "                             torch::Tensor const& b_scales,",
      "                             std::optional<torch::Tensor> const& bias) {",
      "  dispatch_scaled_mm(c, a, b, a_scales, b_scales, bias,",
      "                     vllm::cutlass_scaled_mm_sm100_fp8,",
      "                     nullptr,  // int8 not supported on SM100",
      "                     vllm::cutlass_scaled_mm_blockwise_sm100_fp8);",
      "}",
      "",
      "#endif"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm80_dispatch.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"scaled_mm_c2x.cuh\"",
      "",
      "/**",
      " * This file defines Gemm kernel configurations for SM80 based on the Gemm",
      " * shape.",
      " */",
      "",
      "namespace vllm {",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename> typename Epilogue>",
      "struct sm80_config_default {",
      "  // This config is used in 2 cases,",
      "  //  - M in (128, inf)",
      "  //  - M in (64, 128] and N >= 8192",
      "  // Shared Memory required by this Gemm - 81920 bytes",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using TileShape = typename cutlass::gemm::GemmShape<128, 128, 64>;",
      "  using WarpShape = typename cutlass::gemm::GemmShape<64, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "  using Cutlass2xGemm =",
      "      cutlass_2x_gemm<cutlass::arch::Sm80, enable_sm80_to_sm89, InType, OutType,",
      "                      Epilogue, TileShape, WarpShape, InstructionShape, 5>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename> typename Epilogue>",
      "struct sm80_config_M64 {",
      "  // This config is used in 2 cases,",
      "  // - M in (32, 64]",
      "  // - M in (64, 128] and N < 8192",
      "  // Shared Memory required by this Gemm - 122880 bytes",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using TileShape = typename cutlass::gemm::GemmShape<64, 128, 128>;",
      "  using WarpShape = typename cutlass::gemm::GemmShape<64, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "  using Cutlass2xGemm =",
      "      cutlass_2x_gemm<cutlass::arch::Sm80, enable_sm80_to_sm89, InType, OutType,",
      "                      Epilogue, TileShape, WarpShape, InstructionShape, 5>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename> typename Epilogue>",
      "struct sm80_config_M32 {",
      "  // M in (16, 32]",
      "  // Shared Memory required by this Gemm - 61440 bytes",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using TileShape = typename cutlass::gemm::GemmShape<32, 64, 128>;",
      "  using WarpShape = typename cutlass::gemm::GemmShape<32, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "  using Cutlass2xGemm =",
      "      cutlass_2x_gemm<cutlass::arch::Sm80, enable_sm80_to_sm89, InType, OutType,",
      "                      Epilogue, TileShape, WarpShape, InstructionShape, 5>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename> typename Epilogue>",
      "struct sm80_config_M16 {",
      "  // M in [1, 16]",
      "  // Shared Memory required by this Gemm - 51200 bytes",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using TileShape = typename cutlass::gemm::GemmShape<16, 64, 128>;",
      "  using WarpShape = typename cutlass::gemm::GemmShape<16, 64, 64>;",
      "  using InstructionShape = typename cutlass::gemm::GemmShape<16, 8, 32>;",
      "  using Cutlass2xGemm =",
      "      cutlass_2x_gemm<cutlass::arch::Sm80, enable_sm80_to_sm89, InType, OutType,",
      "                      Epilogue, TileShape, WarpShape, InstructionShape, 5>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename> typename Epilogue,",
      "          typename... EpilogueArgs>",
      "inline void cutlass_gemm_sm80_dispatch(torch::Tensor& out,",
      "                                       torch::Tensor const& a,",
      "                                       torch::Tensor const& b,",
      "                                       EpilogueArgs&&... args) {",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  TORCH_CHECK(a.dtype() == torch::kInt8);",
      "  TORCH_CHECK(b.dtype() == torch::kInt8);",
      "",
      "  using Cutlass2xGemmDefault =",
      "      typename sm80_config_default<InType, OutType, Epilogue>::Cutlass2xGemm;",
      "  using Cutlass2xGemmM128BigN =",
      "      typename sm80_config_default<InType, OutType, Epilogue>::Cutlass2xGemm;",
      "  using Cutlass2xGemmM128SmallN =",
      "      typename sm80_config_M64<InType, OutType, Epilogue>::Cutlass2xGemm;",
      "  using Cutlass2xGemmM64 =",
      "      typename sm80_config_M64<InType, OutType, Epilogue>::Cutlass2xGemm;",
      "  using Cutlass2xGemmM32 =",
      "      typename sm80_config_M32<InType, OutType, Epilogue>::Cutlass2xGemm;",
      "  using Cutlass2xGemmM16 =",
      "      typename sm80_config_M16<InType, OutType, Epilogue>::Cutlass2xGemm;",
      "",
      "  // Due to shared memory requirements, some Gemms may fail to run on some",
      "  // GPUs. As the name indicates, the Fallback Gemm is used as an alternative",
      "  // in such cases.",
      "  // sm80_config_M16 has the least shared-memory requirement. However,",
      "  // based on some profiling, we select sm80_config_M32 as a better alternative",
      "  // performance wise.",
      "  using FallbackGemm =",
      "      typename sm80_config_M32<InType, OutType, Epilogue>::Cutlass2xGemm;",
      "",
      "  uint32_t const m = a.size(0);",
      "  uint32_t const mp2 =",
      "      std::max(static_cast<uint32_t>(16), next_pow_2(m));  // next power of 2",
      "  if (mp2 <= 16) {",
      "    // M in [1, 16]",
      "    return fallback_cutlass_gemm_caller<Cutlass2xGemmM16, FallbackGemm>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 32) {",
      "    // M in (16, 32]",
      "    return fallback_cutlass_gemm_caller<Cutlass2xGemmM32, FallbackGemm>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 64) {",
      "    // M in (32, 64]",
      "    return fallback_cutlass_gemm_caller<Cutlass2xGemmM64, FallbackGemm>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 128) {",
      "    // M in (64, 128]",
      "    uint32_t const n = out.size(1);",
      "    bool const small_n = n < 8192;",
      "    if (small_n) {",
      "      return fallback_cutlass_gemm_caller<Cutlass2xGemmM128SmallN,",
      "                                          FallbackGemm>(",
      "          out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else {",
      "      return fallback_cutlass_gemm_caller<Cutlass2xGemmM128BigN, FallbackGemm>(",
      "          out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "  } else {",
      "    // M in (128, inf)",
      "    return fallback_cutlass_gemm_caller<Cutlass2xGemmDefault, FallbackGemm>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm100_fp8.cu",
    "source": [
      "#include \"scaled_mm_kernels.hpp\"",
      "#include \"scaled_mm_blockwise_sm100_fp8_dispatch.cuh\"",
      "#include \"cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp\"",
      "",
      "namespace vllm {",
      "",
      "void cutlass_scaled_mm_blockwise_sm100_fp8(torch::Tensor& out,",
      "                                           torch::Tensor const& a,",
      "                                           torch::Tensor const& b,",
      "                                           torch::Tensor const& a_scales,",
      "                                           torch::Tensor const& b_scales) {",
      "  if (out.dtype() == torch::kBFloat16) {",
      "    cutlass_gemm_blockwise_sm100_fp8_dispatch<cutlass::bfloat16_t>(",
      "        out, a, b, a_scales, b_scales);",
      "",
      "  } else {",
      "    TORCH_CHECK(out.dtype() == torch::kFloat16);",
      "    cutlass_gemm_blockwise_sm100_fp8_dispatch<cutlass::half_t>(",
      "        out, a, b, a_scales, b_scales);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_int8_dispatch.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"scaled_mm.cuh\"",
      "#include \"cutlass_gemm_caller.cuh\"",
      "",
      "/**",
      " * This file defines Gemm kernel configurations for SM90 (int8) based on the",
      " * Gemm shape.",
      " */",
      "",
      "namespace vllm {",
      "",
      "using c3x::cutlass_gemm_caller;",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_int8_config_default {",
      "  // For M > 128 and any N",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using KernelSchedule =",
      "      typename cutlass::gemm::KernelTmaWarpSpecializedPingpong;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_128, _128, _128>;",
      "  using ClusterShape = Shape<_2, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                      KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_int8_config_M128 {",
      "  // For M in (64, 128] and any N",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using KernelSchedule =",
      "      typename cutlass::gemm::KernelTmaWarpSpecializedPingpong;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _128, _128>;",
      "  using ClusterShape = Shape<_2, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                      KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_int8_config_M64 {",
      "  // For M in (32, 64] and any N",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using KernelSchedule = typename cutlass::gemm::KernelTmaWarpSpecialized;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _64, _256>;",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                      KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_int8_config_M32_NBig {",
      "  // For M in [1, 32] and N >= 8192",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using KernelSchedule = typename cutlass::gemm::KernelTmaWarpSpecialized;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _128, _256>;",
      "  using ClusterShape = Shape<_1, _4, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                      KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_int8_config_M32_NSmall {",
      "  // For M in [1, 32] and N < 8192",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  using KernelSchedule = typename cutlass::gemm::KernelTmaWarpSpecialized;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _64, _256>;",
      "  using ClusterShape = Shape<_1, _8, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                      KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue,",
      "          typename... EpilogueArgs>",
      "inline void cutlass_gemm_sm90_int8_dispatch(torch::Tensor& out,",
      "                                            torch::Tensor const& a,",
      "                                            torch::Tensor const& b,",
      "                                            EpilogueArgs&&... args) {",
      "  static_assert(std::is_same<InType, int8_t>());",
      "  TORCH_CHECK(a.dtype() == torch::kInt8);",
      "  TORCH_CHECK(b.dtype() == torch::kInt8);",
      "",
      "  using Cutlass3xGemmDefault =",
      "      typename sm90_int8_config_default<InType, OutType,",
      "                                        Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM128 =",
      "      typename sm90_int8_config_M128<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM64 =",
      "      typename sm90_int8_config_M64<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM32NBig =",
      "      typename sm90_int8_config_M32_NBig<InType, OutType,",
      "                                         Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM32NSmall =",
      "      typename sm90_int8_config_M32_NSmall<InType, OutType,",
      "                                           Epilogue>::Cutlass3xGemm;",
      "",
      "  uint32_t const n = out.size(1);",
      "  bool const is_small_n = n < 8192;",
      "",
      "  uint32_t const m = a.size(0);",
      "  uint32_t const mp2 =",
      "      std::max(static_cast<uint32_t>(32), next_pow_2(m));  // next power of 2",
      "",
      "  if (mp2 <= 32) {",
      "    // m in [1, 32]",
      "    if (is_small_n) {",
      "      return cutlass_gemm_caller<Cutlass3xGemmM32NSmall>(",
      "          out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    } else {",
      "      return cutlass_gemm_caller<Cutlass3xGemmM32NBig>(",
      "          out, a, b, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "  } else if (mp2 <= 64) {",
      "    // m in (32, 64]",
      "    return cutlass_gemm_caller<Cutlass3xGemmM64>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 128) {",
      "    // m in (64, 128]",
      "    return cutlass_gemm_caller<Cutlass3xGemmM128>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else {",
      "    // m in (128, inf)",
      "    return cutlass_gemm_caller<Cutlass3xGemmDefault>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  }",
      "}",
      "",
      "template <template <typename, typename, typename> typename Epilogue,",
      "          typename... EpilogueArgs>",
      "void cutlass_scaled_mm_sm90_int8_epilogue(torch::Tensor& out,",
      "                                          torch::Tensor const& a,",
      "                                          torch::Tensor const& b,",
      "                                          EpilogueArgs&&... epilogue_args) {",
      "  TORCH_CHECK(a.dtype() == torch::kInt8);",
      "  TORCH_CHECK(b.dtype() == torch::kInt8);",
      "",
      "  if (out.dtype() == torch::kBFloat16) {",
      "    return cutlass_gemm_sm90_int8_dispatch<int8_t, cutlass::bfloat16_t,",
      "                                           Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);",
      "  } else {",
      "    TORCH_CHECK(out.dtype() == torch::kFloat16);",
      "    return cutlass_gemm_sm90_int8_dispatch<int8_t, cutlass::half_t, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm120_fp8.cu",
    "source": [
      "#include \"scaled_mm_kernels.hpp\"",
      "#include \"scaled_mm_sm120_fp8_dispatch.cuh\"",
      "#include \"cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp\"",
      "",
      "namespace vllm {",
      "",
      "void cutlass_scaled_mm_sm120_fp8(torch::Tensor& out, torch::Tensor const& a,",
      "                                 torch::Tensor const& b,",
      "                                 torch::Tensor const& a_scales,",
      "                                 torch::Tensor const& b_scales,",
      "                                 std::optional<torch::Tensor> const& bias) {",
      "  TORCH_CHECK(a_scales.is_contiguous() && b_scales.is_contiguous());",
      "  if (bias) {",
      "    TORCH_CHECK(bias->dtype() == out.dtype(),",
      "                \"currently bias dtype must match output dtype \", out.dtype());",
      "    return cutlass_scaled_mm_sm120_fp8_epilogue<c3x::ScaledEpilogueBias>(",
      "        out, a, b, a_scales, b_scales, *bias);",
      "  } else {",
      "    return cutlass_scaled_mm_sm120_fp8_epilogue<c3x::ScaledEpilogue>(",
      "        out, a, b, a_scales, b_scales);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm120_fp8.cu",
    "source": [
      "#include \"scaled_mm_kernels.hpp\"",
      "#include \"scaled_mm_blockwise_sm120_fp8_dispatch.cuh\"",
      "#include \"cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp\"",
      "",
      "namespace vllm {",
      "",
      "void cutlass_scaled_mm_blockwise_sm120_fp8(torch::Tensor& out,",
      "                                           torch::Tensor const& a,",
      "                                           torch::Tensor const& b,",
      "                                           torch::Tensor const& a_scales,",
      "                                           torch::Tensor const& b_scales) {",
      "  if (out.dtype() == torch::kBFloat16) {",
      "    cutlass_gemm_blockwise_sm120_fp8_dispatch<cutlass::bfloat16_t>(",
      "        out, a, b, a_scales, b_scales);",
      "",
      "  } else {",
      "    TORCH_CHECK(out.dtype() == torch::kFloat16);",
      "    cutlass_gemm_blockwise_sm120_fp8_dispatch<cutlass::half_t>(",
      "        out, a, b, a_scales, b_scales);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_fp8_dispatch.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"scaled_mm.cuh\"",
      "#include \"cutlass_gemm_caller.cuh\"",
      "#include \"cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp\"",
      "",
      "/**",
      " * This file defines Gemm kernel configurations for SM90 (fp8) based on the Gemm",
      " * shape.",
      " */",
      "",
      "namespace vllm {",
      "",
      "using c3x::cutlass_gemm_caller;",
      "",
      "template <typename ElementAB_, typename ElementD_,",
      "          template <typename, typename, typename> typename Epilogue_,",
      "          typename TileShape, typename ClusterShape, typename KernelSchedule,",
      "          typename EpilogueSchedule, bool swap_ab_ = false>",
      "struct cutlass_3x_gemm_sm90_fp8 {",
      "  using ElementAB = ElementAB_;",
      "  using ElementC = ElementD_;",
      "  using ElementD = ElementD_;",
      "  using ElementAcc =",
      "      typename std::conditional<std::is_same_v<ElementAB, int8_t>, int32_t,",
      "                                float>::type;",
      "",
      "  using Epilogue = Epilogue_<ElementAcc, ElementD, TileShape>;",
      "",
      "  using EVTCompute = typename Epilogue::EVTCompute;",
      "",
      "  static constexpr int AlignmentAB =",
      "      128 / cutlass::sizeof_bits<ElementAB>::value;",
      "  static constexpr int AlignmentCD =",
      "      128 / cutlass::sizeof_bits<ElementD>::value;",
      "",
      "  // Compile-time swap_ab flag",
      "  static constexpr bool swap_ab = swap_ab_;",
      "",
      "  // -----------------------------------------------------------",
      "  // Layout definitions",
      "  // -----------------------------------------------------------",
      "  using LayoutA = cutlass::layout::RowMajor;",
      "  using LayoutA_T = typename cutlass::layout::LayoutTranspose<LayoutA>::type;",
      "",
      "  using LayoutB = cutlass::layout::ColumnMajor;",
      "  using LayoutB_T = typename cutlass::layout::LayoutTranspose<LayoutB>::type;",
      "",
      "  using LayoutD = cutlass::layout::RowMajor;",
      "  using LayoutD_Transpose =",
      "      typename cutlass::layout::LayoutTranspose<LayoutD>::type;",
      "",
      "  using LayoutC = LayoutD;",
      "  using LayoutC_Transpose = LayoutD_Transpose;",
      "",
      "  // -----------------------------------------------------------",
      "  // Collective epilogue (conditionally swap operands and layouts)",
      "  // -----------------------------------------------------------",
      "  using CollectiveEpilogue =",
      "      typename cutlass::epilogue::collective::CollectiveBuilder<",
      "          cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp, TileShape,",
      "          ClusterShape, cutlass::epilogue::collective::EpilogueTileAuto,",
      "          ElementAcc, float, ElementC,",
      "          conditional_t<swap_ab, LayoutC_Transpose, LayoutC>, AlignmentCD,",
      "          ElementD, conditional_t<swap_ab, LayoutD_Transpose, LayoutD>,",
      "          AlignmentCD, EpilogueSchedule, EVTCompute>::CollectiveOp;",
      "",
      "  static constexpr size_t CEStorageSize =",
      "      sizeof(typename CollectiveEpilogue::SharedStorage);",
      "",
      "  using Stages = typename cutlass::gemm::collective::StageCountAutoCarveout<",
      "      static_cast<int>(CEStorageSize)>;",
      "",
      "  // -----------------------------------------------------------",
      "  // Collective mainloop (conditionally swap operands and layouts)",
      "  // -----------------------------------------------------------",
      "  using CollectiveMainloop = conditional_t<",
      "      swap_ab,",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp, ElementAB,",
      "          LayoutB_T, AlignmentAB,             // Swapped B (as A)",
      "          ElementAB, LayoutA_T, AlignmentAB,  // Swapped A (as B)",
      "          ElementAcc, TileShape, ClusterShape, Stages,",
      "          KernelSchedule>::CollectiveOp,",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp, ElementAB,",
      "          LayoutA, AlignmentAB, ElementAB, LayoutB, AlignmentAB, ElementAcc,",
      "          TileShape, ClusterShape, Stages, KernelSchedule>::CollectiveOp>;",
      "",
      "  // -----------------------------------------------------------",
      "  // Kernel definition",
      "  // -----------------------------------------------------------",
      "  using KernelType = enable_sm90_or_later<cutlass::gemm::kernel::GemmUniversal<",
      "      cute::Shape<int, int, int, int>, CollectiveMainloop, CollectiveEpilogue,",
      "      cutlass::gemm::PersistentScheduler>>;",
      "",
      "  struct GemmKernel : public KernelType {};",
      "};",
      "",
      "template <typename InType, typename OutType, bool EnableBias>",
      "struct sm90_fp8_config_default {",
      "  // M in (128, inf)",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelTmaWarpSpecializedPingpongFP8FastAccum;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_128, _128, _128>;",
      "  using ClusterShape = Shape<_2, _1, _1>;",
      "",
      "  using Cutlass3xGemm = conditional_t<",
      "      EnableBias,",
      "      cutlass_3x_gemm_sm90_fp8<InType, OutType, c3x::ScaledEpilogueBias,",
      "                               TileShape, ClusterShape, KernelSchedule,",
      "                               EpilogueSchedule>,",
      "      cutlass_3x_gemm_sm90_fp8<InType, OutType, c3x::ScaledEpilogue, TileShape,",
      "                               ClusterShape, KernelSchedule, EpilogueSchedule>>;",
      "};",
      "",
      "template <typename InType, typename OutType, bool EnableBias>",
      "struct sm90_fp8_config_M128 {",
      "  // M in (64, 128]",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelTmaWarpSpecializedPingpongFP8FastAccum;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _128, _128>;",
      "  using ClusterShape = Shape<_2, _1, _1>;",
      "  using Cutlass3xGemm = conditional_t<",
      "      EnableBias,",
      "      cutlass_3x_gemm_sm90_fp8<InType, OutType, c3x::ScaledEpilogueBias,",
      "                               TileShape, ClusterShape, KernelSchedule,",
      "                               EpilogueSchedule>,",
      "      cutlass_3x_gemm_sm90_fp8<InType, OutType, c3x::ScaledEpilogue, TileShape,",
      "                               ClusterShape, KernelSchedule, EpilogueSchedule>>;",
      "};",
      "",
      "template <typename InType, typename OutType, bool EnableBias>",
      "struct sm90_fp8_config_M64_N1280 {",
      "  // M in (16, 64], N in [1 1280]",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule = cutlass::gemm::KernelTmaWarpSpecializedFP8FastAccum;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _16, _256>;",
      "  using ClusterShape = Shape<_1, _4, _1>;",
      "",
      "  // enable swap AB for M < 64",
      "  using Cutlass3xGemm = conditional_t<",
      "      EnableBias,",
      "      cutlass_3x_gemm_sm90_fp8<InType, OutType, c3x::ScaledEpilogueColumnBias,",
      "                               TileShape, ClusterShape, KernelSchedule,",
      "                               EpilogueSchedule, true>,",
      "      cutlass_3x_gemm_sm90_fp8<InType, OutType, c3x::ScaledEpilogue, TileShape,",
      "                               ClusterShape, KernelSchedule, EpilogueSchedule,",
      "                               true>>;",
      "};",
      "",
      "template <typename InType, typename OutType, bool EnableBias>",
      "struct sm90_fp8_config_M64_N8192 {",
      "  // M in (16, 64], N > 1280",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule = cutlass::gemm::KernelTmaWarpSpecializedFP8FastAccum;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _64, _256>;",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "",
      "  // enable swap AB for M < 64",
      "  using Cutlass3xGemm = conditional_t<",
      "      EnableBias,",
      "      cutlass_3x_gemm_sm90_fp8<InType, OutType, c3x::ScaledEpilogueColumnBias,",
      "                               TileShape, ClusterShape, KernelSchedule,",
      "                               EpilogueSchedule, true>,",
      "      cutlass_3x_gemm_sm90_fp8<InType, OutType, c3x::ScaledEpilogue, TileShape,",
      "                               ClusterShape, KernelSchedule, EpilogueSchedule,",
      "                               true>>;",
      "};",
      "",
      "template <typename InType, typename OutType, bool EnableBias>",
      "struct sm90_fp8_config_M16_N1280 {",
      "  // M in [1, 16], N in [1, 1280]",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule = cutlass::gemm::KernelTmaWarpSpecializedFP8FastAccum;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _16, _256>;",
      "  using ClusterShape = Shape<_1, _2, _1>;",
      "",
      "  // enable swap AB for M < 64",
      "  using Cutlass3xGemm = conditional_t<",
      "      EnableBias,",
      "      cutlass_3x_gemm_sm90_fp8<InType, OutType, c3x::ScaledEpilogueColumnBias,",
      "                               TileShape, ClusterShape, KernelSchedule,",
      "                               EpilogueSchedule, true>,",
      "      cutlass_3x_gemm_sm90_fp8<InType, OutType, c3x::ScaledEpilogue, TileShape,",
      "                               ClusterShape, KernelSchedule, EpilogueSchedule,",
      "                               true>>;",
      "};",
      "",
      "template <typename InType, typename OutType, bool EnableBias>",
      "struct sm90_fp8_config_M16_N8192 {",
      "  // M in [1, 16], N > 1280",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule = cutlass::gemm::KernelTmaWarpSpecializedFP8FastAccum;",
      "  using EpilogueSchedule = typename cutlass::epilogue::TmaWarpSpecialized;",
      "  using TileShape = Shape<_64, _16, _256>;",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "",
      "  // enable swap AB for M < 64",
      "  using Cutlass3xGemm = conditional_t<",
      "      EnableBias,",
      "      cutlass_3x_gemm_sm90_fp8<InType, OutType, c3x::ScaledEpilogueColumnBias,",
      "                               TileShape, ClusterShape, KernelSchedule,",
      "                               EpilogueSchedule, true>,",
      "      cutlass_3x_gemm_sm90_fp8<InType, OutType, c3x::ScaledEpilogue, TileShape,",
      "                               ClusterShape, KernelSchedule, EpilogueSchedule,",
      "                               true>>;",
      "};",
      "",
      "template <typename Gemm, typename... EpilogueArgs>",
      "void cutlass_gemm_caller_sm90_fp8(torch::Tensor& out, torch::Tensor const& a,",
      "                                  torch::Tensor const& b,",
      "                                  EpilogueArgs&&... epilogue_params) {",
      "  static constexpr bool swap_ab = Gemm::swap_ab;",
      "  using ElementAB = typename Gemm::ElementAB;",
      "  using ElementD = typename Gemm::ElementD;",
      "  using GemmKernel = typename Gemm::GemmKernel;",
      "",
      "  using StrideA = typename Gemm::GemmKernel::StrideA;",
      "  using StrideB = typename Gemm::GemmKernel::StrideB;",
      "  using StrideC = typename Gemm::GemmKernel::StrideC;",
      "",
      "  int32_t m = a.size(0), n = b.size(1), k = a.size(1);",
      "  auto prob_shape =",
      "      swap_ab ? cute::make_shape(n, m, k, 1) : cute::make_shape(m, n, k, 1);",
      "",
      "  StrideA a_stride =",
      "      cutlass::make_cute_packed_stride(StrideA{}, cute::make_shape(m, k, 1));",
      "  StrideB b_stride =",
      "      cutlass::make_cute_packed_stride(StrideB{}, cute::make_shape(n, k, 1));",
      "  StrideC c_stride = cutlass::make_cute_packed_stride(",
      "      StrideC{},",
      "      swap_ab ? cute::make_shape(n, m, 1) : cute::make_shape(m, n, 1));",
      "",
      "  auto a_ptr = static_cast<ElementAB*>(a.data_ptr());",
      "  auto b_ptr = static_cast<ElementAB*>(b.data_ptr());",
      "  auto c_ptr = static_cast<ElementD*>(out.data_ptr());",
      "",
      "  typename GemmKernel::MainloopArguments mainloop_args =",
      "      swap_ab ? typename GemmKernel::MainloopArguments{b_ptr, b_stride, a_ptr,",
      "                                                       a_stride}",
      "              : typename GemmKernel::MainloopArguments{a_ptr, a_stride, b_ptr,",
      "                                                       b_stride};",
      "",
      "  typename GemmKernel::EpilogueArguments epilogue_args{",
      "      Gemm::Epilogue::prepare_args(",
      "          std::forward<EpilogueArgs>(epilogue_params)...),",
      "      c_ptr, c_stride, c_ptr, c_stride};",
      "",
      "  c3x::cutlass_gemm_caller<GemmKernel>(a.device(), prob_shape, mainloop_args,",
      "                                       epilogue_args);",
      "}",
      "",
      "template <typename InType, typename OutType, bool EnableBias,",
      "          typename... EpilogueArgs>",
      "inline void cutlass_gemm_sm90_fp8_dispatch(torch::Tensor& out,",
      "                                           torch::Tensor const& a,",
      "                                           torch::Tensor const& b,",
      "                                           torch::Tensor const& a_scales,",
      "                                           torch::Tensor const& b_scales,",
      "                                           EpilogueArgs&&... args) {",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  TORCH_CHECK(a.dtype() == torch::kFloat8_e4m3fn);",
      "  TORCH_CHECK(b.dtype() == torch::kFloat8_e4m3fn);",
      "",
      "  using Cutlass3xGemmDefault =",
      "      typename sm90_fp8_config_default<InType, OutType,",
      "                                       EnableBias>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM128 =",
      "      typename sm90_fp8_config_M128<InType, OutType, EnableBias>::Cutlass3xGemm;",
      "",
      "  using Cutlass3xGemmM64_N1280 =",
      "      typename sm90_fp8_config_M64_N1280<InType, OutType,",
      "                                         EnableBias>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM64_N8192 =",
      "      typename sm90_fp8_config_M64_N8192<InType, OutType,",
      "                                         EnableBias>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM16_N1280 =",
      "      typename sm90_fp8_config_M16_N1280<InType, OutType,",
      "                                         EnableBias>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM16_N8192 =",
      "      typename sm90_fp8_config_M16_N8192<InType, OutType,",
      "                                         EnableBias>::Cutlass3xGemm;",
      "",
      "  uint32_t const m = a.size(0);",
      "  uint32_t const n = b.size(1);",
      "",
      "  if (m <= 16) {",
      "    // m in [1, 16]",
      "    if (n <= 1280) {",
      "      return cutlass_gemm_caller_sm90_fp8<Cutlass3xGemmM16_N1280>(",
      "          out, a, b, b_scales, a_scales, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "    return cutlass_gemm_caller_sm90_fp8<Cutlass3xGemmM16_N8192>(",
      "        out, a, b, b_scales, a_scales, std::forward<EpilogueArgs>(args)...);",
      "  } else if (m <= 64) {",
      "    // m in (16, 64]",
      "    if (n <= 1280) {",
      "      return cutlass_gemm_caller_sm90_fp8<Cutlass3xGemmM64_N1280>(",
      "          out, a, b, b_scales, a_scales, std::forward<EpilogueArgs>(args)...);",
      "    }",
      "    return cutlass_gemm_caller_sm90_fp8<Cutlass3xGemmM64_N8192>(",
      "        out, a, b, b_scales, a_scales, std::forward<EpilogueArgs>(args)...);",
      "  } else if (m <= 128) {",
      "    // m in (64, 128]",
      "    return cutlass_gemm_caller_sm90_fp8<Cutlass3xGemmM128>(",
      "        out, a, b, a_scales, b_scales, std::forward<EpilogueArgs>(args)...);",
      "  } else {",
      "    // m in (128, inf)",
      "    return cutlass_gemm_caller_sm90_fp8<Cutlass3xGemmDefault>(",
      "        out, a, b, a_scales, b_scales, std::forward<EpilogueArgs>(args)...);",
      "  }",
      "}",
      "",
      "template <bool EnableBias, typename... EpilogueArgs>",
      "void cutlass_scaled_mm_sm90_fp8_epilogue(torch::Tensor& out,",
      "                                         torch::Tensor const& a,",
      "                                         torch::Tensor const& b,",
      "                                         torch::Tensor const& a_scales,",
      "                                         torch::Tensor const& b_scales,",
      "                                         EpilogueArgs&&... epilogue_args) {",
      "  TORCH_CHECK(a.dtype() == torch::kFloat8_e4m3fn);",
      "  TORCH_CHECK(b.dtype() == torch::kFloat8_e4m3fn);",
      "",
      "  if (out.dtype() == torch::kBFloat16) {",
      "    return cutlass_gemm_sm90_fp8_dispatch<cutlass::float_e4m3_t,",
      "                                          cutlass::bfloat16_t, EnableBias>(",
      "        out, a, b, a_scales, b_scales,",
      "        std::forward<EpilogueArgs>(epilogue_args)...);",
      "  } else {",
      "    TORCH_CHECK(out.dtype() == torch::kFloat16);",
      "    return cutlass_gemm_sm90_fp8_dispatch<cutlass::float_e4m3_t,",
      "                                          cutlass::half_t, EnableBias>(",
      "        out, a, b, a_scales, b_scales,",
      "        std::forward<EpilogueArgs>(epilogue_args)...);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_fp8.cu",
    "source": [
      "#include \"scaled_mm_kernels.hpp\"",
      "#include \"scaled_mm_sm90_fp8_dispatch.cuh\"",
      "",
      "namespace vllm {",
      "",
      "void cutlass_scaled_mm_sm90_fp8(torch::Tensor& out, torch::Tensor const& a,",
      "                                torch::Tensor const& b,",
      "                                torch::Tensor const& a_scales,",
      "                                torch::Tensor const& b_scales,",
      "                                std::optional<torch::Tensor> const& bias) {",
      "  TORCH_CHECK(a_scales.is_contiguous() && b_scales.is_contiguous());",
      "  if (bias) {",
      "    TORCH_CHECK(bias->dtype() == out.dtype(),",
      "                \"currently bias dtype must match output dtype \", out.dtype());",
      "    return cutlass_scaled_mm_sm90_fp8_epilogue<true>(out, a, b, a_scales,",
      "                                                     b_scales, *bias);",
      "  } else {",
      "    return cutlass_scaled_mm_sm90_fp8_epilogue<false>(out, a, b, a_scales,",
      "                                                      b_scales);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm120_fp8_dispatch.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"cuda_utils.h\"",
      "#include \"cutlass/cutlass.h\"",
      "#include \"cutlass/numeric_types.h\"",
      "",
      "#include \"cute/tensor.hpp\"",
      "#include \"cutlass/tensor_ref.h\"",
      "#include \"cutlass/gemm/dispatch_policy.hpp\"",
      "#include \"cutlass/gemm/collective/collective_builder.hpp\"",
      "#include \"cutlass/gemm/device/gemm_universal_adapter.h\"",
      "#include \"cutlass/gemm/kernel/gemm_universal.hpp\"",
      "#include \"cutlass/gemm/kernel/tile_scheduler_params.h\"",
      "#include \"cutlass/epilogue/dispatch_policy.hpp\"",
      "#include \"cutlass/epilogue/collective/collective_builder.hpp\"",
      "",
      "#include \"cutlass_extensions/gemm/dispatch_policy.hpp\"",
      "#include \"cutlass_extensions/gemm/collective/collective_builder.hpp\"",
      "",
      "#include \"cutlass_gemm_caller.cuh\"",
      "",
      "namespace vllm {",
      "",
      "using namespace cute;",
      "",
      "// clang-format off",
      "template <class OutType, int ScaleGranularityM,",
      "          int ScaleGranularityN, int ScaleGranularityK,",
      "          class MmaTileShape, class ClusterShape,",
      "          class EpilogueScheduler, class MainloopScheduler>",
      "struct cutlass_3x_gemm_fp8_blockwise {",
      "  using ElementAB = cutlass::float_e4m3_t;",
      "",
      "  using ElementA = ElementAB;",
      "  using LayoutA = cutlass::layout::RowMajor;",
      "  using LayoutA_Transpose = typename cutlass::layout::LayoutTranspose<LayoutA>::type;",
      "  static constexpr int AlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value;",
      "",
      "  using ElementB = ElementAB;",
      "  // ColumnMajor is used for B to match the CUTLASS convention.",
      "  using LayoutB = cutlass::layout::ColumnMajor;",
      "  using LayoutB_Transpose = typename cutlass::layout::LayoutTranspose<LayoutB>::type;",
      "  static constexpr int AlignmentB = 128 / cutlass::sizeof_bits<ElementB>::value;",
      "",
      "  using ElementD = OutType;",
      "  using LayoutD = cutlass::layout::RowMajor;",
      "  using LayoutD_Transpose = typename cutlass::layout::LayoutTranspose<LayoutD>::type;",
      "  static constexpr int AlignmentD = 128 / cutlass::sizeof_bits<ElementD>::value;",
      "",
      "  using ElementC = void; // TODO: support bias",
      "  using LayoutC = LayoutD;",
      "  using LayoutC_Transpose = LayoutD_Transpose;",
      "  static constexpr int AlignmentC = AlignmentD;",
      "",
      "  using ElementAccumulator = float;",
      "  using ElementCompute = float;",
      "  using ElementBlockScale = float; ",
      "",
      "  using ScaleConfig = cutlass::detail::Sm120BlockwiseScaleConfig<",
      "        ScaleGranularityM, ScaleGranularityN, ScaleGranularityK,",
      "        cute::UMMA::Major::MN, cute::UMMA::Major::K>;",
      "",
      "  // layout_SFA and layout_SFB cannot be swapped since they are deduced.",
      "  using LayoutSFA = decltype(ScaleConfig::deduce_layoutSFA());",
      "  using LayoutSFB = decltype(ScaleConfig::deduce_layoutSFB());",
      "",
      "  using ArchTag = cutlass::arch::Sm120;",
      "  using OperatorClass = cutlass::arch::OpClassTensorOp;",
      "",
      "  static constexpr auto RoundStyle = cutlass::FloatRoundStyle::round_to_nearest;",
      "  using ElementScalar = float;",
      "  using DefaultOperation = cutlass::epilogue::fusion::LinearCombination<ElementD, ElementCompute, ElementC, ElementScalar, RoundStyle>;",
      "  using CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveBuilder<",
      "      ArchTag,",
      "      OperatorClass,",
      "      MmaTileShape,",
      "      ClusterShape,",
      "      cutlass::epilogue::collective::EpilogueTileAuto,",
      "      ElementAccumulator,",
      "      ElementCompute,",
      "      ElementC,",
      "      LayoutC,",
      "      AlignmentC,",
      "      ElementD,",
      "      LayoutD,",
      "      AlignmentD,",
      "      EpilogueScheduler,",
      "      DefaultOperation",
      "  >::CollectiveOp;",
      " ",
      "  using StageCountType = cutlass::gemm::collective::StageCountAuto; ",
      "  using CollectiveMainloop = ",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          ArchTag,",
      "          OperatorClass,",
      "          ElementA,",
      "          cute::tuple<LayoutA, LayoutSFA>,",
      "          AlignmentA,",
      "          ElementB,",
      "          cute::tuple<LayoutB, LayoutSFB>,",
      "          AlignmentB,",
      "          ElementAccumulator,",
      "          MmaTileShape,",
      "          ClusterShape,",
      "          cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(sizeof(typename CollectiveEpilogue::SharedStorage))>,",
      "          MainloopScheduler",
      "      >::CollectiveOp;",
      "",
      "  using KernelType = enable_sm120_only<cutlass::gemm::kernel::GemmUniversal<",
      "      Shape<int, int, int, int>, CollectiveMainloop, CollectiveEpilogue>>;",
      "",
      "  struct GemmKernel : public KernelType {};",
      "};",
      "",
      "template <typename Gemm>",
      "void cutlass_gemm_caller_blockwise(torch::Tensor& out, torch::Tensor const& a,",
      "                                   torch::Tensor const& b,",
      "                                   torch::Tensor const& a_scales,",
      "                                   torch::Tensor const& b_scales) {",
      "  using GemmKernel = typename Gemm::GemmKernel;",
      "  using StrideA = typename Gemm::GemmKernel::StrideA;",
      "  using StrideB = typename Gemm::GemmKernel::StrideB;",
      "  using StrideD = typename Gemm::GemmKernel::StrideD;",
      "  using StrideC = typename Gemm::GemmKernel::StrideC;",
      "  using LayoutSFA = typename Gemm::LayoutSFA;",
      "  using LayoutSFB = typename Gemm::LayoutSFB;",
      "  using ScaleConfig = typename Gemm::ScaleConfig;",
      "",
      "  using ElementAB = typename Gemm::ElementAB;",
      "  using ElementD = typename Gemm::ElementD;",
      "",
      "  int32_t m = a.size(0), n = b.size(1), k = a.size(1);",
      "",
      "  StrideA a_stride;",
      "  StrideB b_stride;",
      "  StrideC c_stride;",
      "  a_stride =",
      "      cutlass::make_cute_packed_stride(StrideA{}, cute::make_shape(m, k, 1));",
      "  b_stride =",
      "      cutlass::make_cute_packed_stride(StrideB{}, cute::make_shape(n, k, 1));",
      "  c_stride =",
      "      cutlass::make_cute_packed_stride(StrideC{}, cute::make_shape(m, n, 1));",
      "",
      "  LayoutSFA layout_SFA = ",
      "      ScaleConfig::tile_atom_to_shape_SFA(make_shape(m, n, k, 1));",
      "  LayoutSFB layout_SFB = ",
      "      ScaleConfig::tile_atom_to_shape_SFB(make_shape(m, n, k, 1));",
      "",
      "  auto a_ptr = static_cast<ElementAB*>(a.data_ptr());",
      "  auto b_ptr = static_cast<ElementAB*>(b.data_ptr());",
      "  auto a_scales_ptr = static_cast<float*>(a_scales.data_ptr());",
      "  auto b_scales_ptr = static_cast<float*>(b_scales.data_ptr());",
      "",
      "  auto mainloop_args = [&](){",
      "    return typename GemmKernel::MainloopArguments{",
      "        a_ptr,        a_stride,   b_ptr,        b_stride,",
      "        a_scales_ptr, layout_SFA, b_scales_ptr, layout_SFB",
      "    };",
      "  }();",
      "  auto prob_shape = cute::make_shape(m, n, k, 1);",
      "",
      "  auto c_ptr = static_cast<ElementD*>(out.data_ptr());",
      "  typename GemmKernel::EpilogueArguments epilogue_args{",
      "      {}, c_ptr, c_stride, c_ptr, c_stride};",
      "  c3x::cutlass_gemm_caller<GemmKernel>(a.device(), prob_shape, mainloop_args,",
      "                                       epilogue_args);",
      "}",
      "",
      "template <typename OutType>",
      "void cutlass_gemm_blockwise_sm120_fp8_dispatch(torch::Tensor& out,",
      "                                               torch::Tensor const& a,",
      "                                               torch::Tensor const& b,",
      "                                               torch::Tensor const& a_scales,",
      "                                               torch::Tensor const& b_scales) {",
      "  // TODO: better heuristics",
      "  cutlass_gemm_caller_blockwise<cutlass_3x_gemm_fp8_blockwise<",
      "      OutType, 1, 128, 128, Shape<_128, _128, _128>,",
      "      Shape<_1, _1, _1>, cutlass::epilogue::collective::EpilogueScheduleAuto,",
      "      cutlass::gemm::collective::KernelScheduleAuto>>(",
      "      out, a, b, a_scales, b_scales);",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm100_fp8_dispatch.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"scaled_mm.cuh\"",
      "#include \"cutlass_gemm_caller.cuh\"",
      "",
      "/**",
      " * This file defines Gemm kernel configurations for SM100 (fp8) based on the",
      " * Gemm shape.",
      " */",
      "",
      "namespace vllm {",
      "",
      "using c3x::cutlass_gemm_caller;",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm100_fp8_config_default {",
      "  // M in (256, inf)",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule = cutlass::gemm::collective::KernelScheduleAuto;",
      "  using EpilogueSchedule = cutlass::epilogue::collective::EpilogueScheduleAuto;",
      "  using TileShape = Shape<_256, _128, _128>;",
      "  using ClusterShape = Shape<_2, _2, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_gemm_sm100<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                            KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm100_fp8_config_M256 {",
      "  // M in (64, 256]",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule = cutlass::gemm::collective::KernelScheduleAuto;",
      "  using EpilogueSchedule = cutlass::epilogue::collective::EpilogueScheduleAuto;",
      "  using TileShape = Shape<_128, _128, _128>;",
      "  using ClusterShape = Shape<_2, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_gemm_sm100<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                            KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm100_fp8_config_M64 {",
      "  // M in (16, 64]",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule = cutlass::gemm::collective::KernelScheduleAuto;",
      "  using EpilogueSchedule = cutlass::epilogue::collective::EpilogueScheduleAuto;",
      "  using TileShape = Shape<_64, _64, _128>;",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_gemm_sm100<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                            KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm100_fp8_config_M16 {",
      "  // M in [1, 16]",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule = cutlass::gemm::collective::KernelScheduleAuto;",
      "  using EpilogueSchedule = cutlass::epilogue::collective::EpilogueScheduleAuto;",
      "  using TileShape = Shape<_64, _64, _128>;",
      "  using ClusterShape = Shape<_1, _4, _1>;",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_gemm_sm100<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                            KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue,",
      "          typename... EpilogueArgs>",
      "inline void cutlass_gemm_sm100_fp8_dispatch(torch::Tensor& out,",
      "                                            torch::Tensor const& a,",
      "                                            torch::Tensor const& b,",
      "                                            EpilogueArgs&&... args) {",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  TORCH_CHECK(a.dtype() == torch::kFloat8_e4m3fn);",
      "  TORCH_CHECK(b.dtype() == torch::kFloat8_e4m3fn);",
      "",
      "  using Cutlass3xGemmDefault =",
      "      typename sm100_fp8_config_default<InType, OutType,",
      "                                        Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM16 =",
      "      typename sm100_fp8_config_M16<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM64 =",
      "      typename sm100_fp8_config_M64<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM256 =",
      "      typename sm100_fp8_config_M256<InType, OutType, Epilogue>::Cutlass3xGemm;",
      "",
      "  uint32_t const m = a.size(0);",
      "  uint32_t const mp2 =",
      "      std::max(static_cast<uint32_t>(16), next_pow_2(m));  // next power of 2",
      "",
      "  if (mp2 <= 16) {",
      "    // m in [1, 16]",
      "    return cutlass_gemm_caller<Cutlass3xGemmM16>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 64) {",
      "    // m in (16, 64]",
      "    return cutlass_gemm_caller<Cutlass3xGemmM64>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else if (mp2 <= 256) {",
      "    // m in (64, 256]",
      "    return cutlass_gemm_caller<Cutlass3xGemmM256>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  } else {",
      "    // m in (256, inf)",
      "    return cutlass_gemm_caller<Cutlass3xGemmDefault>(",
      "        out, a, b, std::forward<EpilogueArgs>(args)...);",
      "  }",
      "}",
      "",
      "template <template <typename, typename, typename> typename Epilogue,",
      "          typename... EpilogueArgs>",
      "void cutlass_scaled_mm_sm100_fp8_epilogue(torch::Tensor& out,",
      "                                          torch::Tensor const& a,",
      "                                          torch::Tensor const& b,",
      "                                          EpilogueArgs&&... epilogue_args) {",
      "  TORCH_CHECK(a.dtype() == torch::kFloat8_e4m3fn);",
      "  TORCH_CHECK(b.dtype() == torch::kFloat8_e4m3fn);",
      "",
      "  if (out.dtype() == torch::kBFloat16) {",
      "    return cutlass_gemm_sm100_fp8_dispatch<cutlass::float_e4m3_t,",
      "                                           cutlass::bfloat16_t, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);",
      "  } else {",
      "    TORCH_CHECK(out.dtype() == torch::kFloat16);",
      "    return cutlass_gemm_sm100_fp8_dispatch<cutlass::float_e4m3_t,",
      "                                           cutlass::half_t, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm90_fp8_dispatch.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"cutlass/cutlass.h\"",
      "#include \"cutlass/numeric_types.h\"",
      "",
      "#include \"cute/tensor.hpp\"",
      "#include \"cutlass/tensor_ref.h\"",
      "#include \"cutlass/gemm/dispatch_policy.hpp\"",
      "#include \"cutlass/gemm/collective/collective_builder.hpp\"",
      "#include \"cutlass/gemm/device/gemm_universal_adapter.h\"",
      "#include \"cutlass/gemm/kernel/gemm_universal.hpp\"",
      "#include \"cutlass/gemm/kernel/tile_scheduler_params.h\"",
      "#include \"cutlass/epilogue/dispatch_policy.hpp\"",
      "#include \"cutlass/epilogue/collective/collective_builder.hpp\"",
      "",
      "#include \"cutlass_extensions/gemm/dispatch_policy.hpp\"",
      "#include \"cutlass_extensions/gemm/collective/collective_builder.hpp\"",
      "",
      "#include \"cutlass_gemm_caller.cuh\"",
      "",
      "namespace vllm {",
      "",
      "using namespace cute;",
      "",
      "template <typename SchedulerType, typename OutType, int GroupSizeM_,",
      "          int GroupSizeN_, int GroupSizeK_, int TileSizeM_ = 128,",
      "          class ClusterShape = Shape<_1, _2, _1>>",
      "struct cutlass_3x_gemm_fp8_blockwise {",
      "  using GroupSizeM = Int<GroupSizeM_>;",
      "  using GroupSizeN = Int<GroupSizeN_>;",
      "  using GroupSizeK = Int<GroupSizeK_>;",
      "  using TileSizeM = Int<TileSizeM_>;",
      "",
      "  static_assert(TileSizeM_ % GroupSizeM_ == 0,",
      "                \"TileSizeM must be a multiple of GroupSizeM\");",
      "",
      "  using ElementAB = cutlass::float_e4m3_t;",
      "",
      "  using ElementA = ElementAB;",
      "  using LayoutA = cutlass::layout::RowMajor;",
      "  static constexpr int AlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value;",
      "",
      "  using ElementB = ElementAB;",
      "  using LayoutB = cutlass::layout::ColumnMajor;",
      "  static constexpr int AlignmentB = 128 / cutlass::sizeof_bits<ElementB>::value;",
      "",
      "  using ElementD = OutType;",
      "  using StrideD = Stride<int64_t, Int<1>, Int<0>>;",
      "  static constexpr int AlignmentD = 128 / cutlass::sizeof_bits<ElementD>::value;",
      "",
      "  using ElementC = void;",
      "  using StrideC = StrideD;",
      "  static constexpr int AlignmentC = AlignmentD;",
      "",
      "  using ElementAccumulator = float;",
      "  using ElementBlockScale = float;",
      "  using ElementCompute = float;",
      "  using ArchTag = cutlass::arch::Sm90;",
      "  using OperatorClass = cutlass::arch::OpClassTensorOp;",
      "  using TileShape = Shape<TileSizeM, GroupSizeN, GroupSizeK>;",
      "",
      "  using KernelSchedule = cutlass::gemm::",
      "      KernelTmaWarpSpecializedCooperativeFP8BlockScaledSubGroupMAccum<",
      "          GroupSizeM_>;",
      "  using EpilogueSchedule = cutlass::epilogue::TmaWarpSpecializedCooperative;",
      "  using EpilogueTileType = cutlass::epilogue::collective::EpilogueTileAuto;",
      "",
      "  using StoreEpilogueCompute = typename cutlass::epilogue::fusion::Sm90EVT<",
      "      cutlass::epilogue::fusion::Sm90AccFetch>;",
      "",
      "  using CollectiveEpilogue =",
      "      typename cutlass::epilogue::collective::CollectiveBuilder<",
      "          ArchTag, OperatorClass, TileShape, ClusterShape, EpilogueTileType,",
      "          ElementAccumulator, ElementCompute, ElementC, StrideC, AlignmentC,",
      "          ElementD, StrideD, AlignmentD, EpilogueSchedule,",
      "          StoreEpilogueCompute>::CollectiveOp;",
      "",
      "  using CollectiveMainloop =",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          ArchTag, OperatorClass, ElementA, LayoutA, AlignmentA, ElementB,",
      "          LayoutB, AlignmentB, ElementAccumulator, TileShape, ClusterShape,",
      "          cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(",
      "              sizeof(typename CollectiveEpilogue::SharedStorage))>,",
      "          KernelSchedule>::CollectiveOp;",
      "",
      "  using KernelType = enable_sm90_or_later<cutlass::gemm::kernel::GemmUniversal<",
      "      Shape<int, int, int, int>, CollectiveMainloop, CollectiveEpilogue,",
      "      SchedulerType>>;",
      "",
      "  struct GemmKernel : public KernelType {};",
      "",
      "  using StrideA = typename GemmKernel::StrideA;",
      "  using StrideB = typename GemmKernel::StrideB;",
      "};",
      "",
      "template <typename Gemm>",
      "void cutlass_gemm_caller_blockwise(torch::Tensor& out, torch::Tensor const& a,",
      "                                   torch::Tensor const& b,",
      "                                   torch::Tensor const& a_scales,",
      "                                   torch::Tensor const& b_scales) {",
      "  using GemmKernel = typename Gemm::GemmKernel;",
      "",
      "  using ElementAB = typename Gemm::ElementAB;",
      "  using ElementD = typename Gemm::ElementD;",
      "",
      "  auto prob_shape = c3x::get_problem_shape(a, b);",
      "  int32_t m = get<0>(prob_shape), n = get<1>(prob_shape),",
      "          k = get<2>(prob_shape);",
      "",
      "  int64_t lda = a.stride(0);",
      "  int64_t ldb = b.stride(1);",
      "  int64_t ldc = out.stride(0);",
      "",
      "  using StrideA = Stride<int64_t, Int<1>, int64_t>;",
      "  using StrideB = Stride<int64_t, Int<1>, int64_t>;",
      "  using StrideC = typename Gemm::StrideC;",
      "",
      "  StrideA a_stride{lda, Int<1>{}, 0};",
      "  StrideB b_stride{ldb, Int<1>{}, 0};",
      "  StrideC c_stride{ldc, Int<1>{}, Int<0>{}};",
      "",
      "  auto a_ptr = static_cast<ElementAB*>(a.data_ptr());",
      "  auto b_ptr = static_cast<ElementAB*>(b.data_ptr());",
      "  auto a_scales_ptr = static_cast<float*>(a_scales.data_ptr());",
      "  auto b_scales_ptr = static_cast<float*>(b_scales.data_ptr());",
      "",
      "  // Check is the t is contiguous and is 1D or 2D with one of the dimensions",
      "  // being 1 (i.e. a row or column vector)",
      "  auto is_contiguous_vector = [](const torch::Tensor& t) {",
      "    auto t_sizes = t.sizes();",
      "    return t.is_contiguous() &&",
      "           (t.dim() == 1 ||",
      "            (t.dim() == 2 &&",
      "             *std::min_element(t_sizes.begin(), t_sizes.end()) == 1));",
      "  };",
      "",
      "  // TODO(lucas): lets clean-up the kernel so that we pass in Strides so",
      "  //  we don't have to deal with enforcing implicit layouts",
      "  TORCH_CHECK(a_scales.size(0) == m / Gemm::GroupSizeM::value);",
      "  TORCH_CHECK(a_scales.size(1) == k / Gemm::GroupSizeK::value);",
      "  TORCH_CHECK(a_scales.stride(0) == 1 || is_contiguous_vector(a_scales),",
      "              \"a_scales must be M major\");",
      "  TORCH_CHECK(b_scales.size(0) == k / Gemm::GroupSizeK::value);",
      "  TORCH_CHECK(b_scales.size(1) == n / Gemm::GroupSizeN::value);",
      "  TORCH_CHECK(b_scales.stride(0) == 1 || is_contiguous_vector(b_scales),",
      "              \"b_scales must be K major\");",
      "  typename GemmKernel::MainloopArguments mainloop_args{",
      "      a_ptr, a_stride, b_ptr, b_stride, a_scales_ptr, b_scales_ptr};",
      "",
      "  auto c_ptr = static_cast<ElementD*>(out.data_ptr());",
      "  typename GemmKernel::EpilogueArguments epilogue_args{",
      "      {}, c_ptr, c_stride, c_ptr, c_stride};",
      "",
      "  typename GemmKernel::TileSchedulerArguments scheduler;",
      "",
      "  static constexpr bool UsesStreamKScheduler =",
      "      cute::is_same_v<typename GemmKernel::TileSchedulerTag,",
      "                      cutlass::gemm::StreamKScheduler>;",
      "",
      "  if constexpr (UsesStreamKScheduler) {",
      "    using DecompositionMode = typename cutlass::gemm::kernel::detail::",
      "        PersistentTileSchedulerSm90StreamKParams::DecompositionMode;",
      "    using ReductionMode = typename cutlass::gemm::kernel::detail::",
      "        PersistentTileSchedulerSm90StreamKParams::ReductionMode;",
      "",
      "    scheduler.decomposition_mode = DecompositionMode::StreamK;",
      "    scheduler.reduction_mode = ReductionMode::Nondeterministic;",
      "  }",
      "",
      "  c3x::cutlass_gemm_caller<GemmKernel>(a.device(), prob_shape, mainloop_args,",
      "                                       epilogue_args, scheduler);",
      "}",
      "",
      "template <typename OutType>",
      "void cutlass_gemm_blockwise_sm90_fp8_dispatch(torch::Tensor& out,",
      "                                              torch::Tensor const& a,",
      "                                              torch::Tensor const& b,",
      "                                              torch::Tensor const& a_scales,",
      "                                              torch::Tensor const& b_scales) {",
      "  auto k = a.size(1);",
      "  auto n = b.size(1);",
      "",
      "  if (k > 3 * n) {",
      "    cutlass_gemm_caller_blockwise<cutlass_3x_gemm_fp8_blockwise<",
      "        cutlass::gemm::StreamKScheduler, OutType, 1, 128, 128>>(",
      "        out, a, b, a_scales, b_scales);",
      "  } else {",
      "    cutlass_gemm_caller_blockwise<cutlass_3x_gemm_fp8_blockwise<",
      "        cutlass::gemm::PersistentScheduler, OutType, 1, 128, 128>>(",
      "        out, a, b, a_scales, b_scales);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm.cuh",
    "source": [
      "#pragma once",
      "",
      "// clang-format will break include orders",
      "// clang-format off",
      "",
      "#include \"cutlass/cutlass.h\"",
      "",
      "#include \"cute/tensor.hpp\"",
      "#include \"cute/atom/mma_atom.hpp\"",
      "#include \"cutlass/numeric_types.h\"",
      "",
      "#include \"cutlass/gemm/device/gemm_universal_adapter.h\"",
      "#include \"cutlass/gemm/kernel/gemm_universal.hpp\"",
      "#include \"cutlass/epilogue/collective/collective_builder.hpp\"",
      "#include \"cutlass/gemm/collective/collective_builder.hpp\"",
      "",
      "#include \"core/math.hpp\"",
      "#include \"cutlass_extensions/common.hpp\"",
      "// clang-format on",
      "",
      "/*",
      "  Epilogues defined in,",
      "  csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp,",
      "  must contain a public type named EVTCompute of type Sm90EVT, as well as a",
      "  static prepare_args function that constructs an EVTCompute::Arguments struct.",
      "*/",
      "",
      "using namespace cute;",
      "",
      "namespace vllm {",
      "",
      "template <typename ElementAB_, typename ElementD_,",
      "          template <typename, typename, typename> typename Epilogue_,",
      "          typename TileShape, typename ClusterShape, typename KernelSchedule,",
      "          typename EpilogueSchedule>",
      "struct cutlass_3x_gemm {",
      "  using ElementAB = ElementAB_;",
      "  using ElementD = ElementD_;",
      "  using ElementAcc =",
      "      typename std::conditional<std::is_same_v<ElementAB, int8_t>, int32_t,",
      "                                float>::type;",
      "",
      "  using Epilogue = Epilogue_<ElementAcc, ElementD, TileShape>;",
      "",
      "  using StrideD = Stride<int64_t, Int<1>, Int<0>>;",
      "  using ElementC = void;",
      "  using StrideC = StrideD;",
      "",
      "  using EVTCompute = typename Epilogue::EVTCompute;",
      "",
      "  // These are the minimum alignments needed for the kernels to compile",
      "  static constexpr int AlignmentAB =",
      "      128 / cutlass::sizeof_bits<ElementAB>::value;",
      "  static constexpr int AlignmentCD =",
      "      128 / cutlass::sizeof_bits<ElementD>::value;",
      "",
      "  using CollectiveEpilogue =",
      "      typename cutlass::epilogue::collective::CollectiveBuilder<",
      "          cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp, TileShape,",
      "          ClusterShape, cutlass::epilogue::collective::EpilogueTileAuto,",
      "          ElementAcc, float, ElementC, StrideC, AlignmentCD, ElementD, StrideD,",
      "          AlignmentCD, EpilogueSchedule, EVTCompute>::CollectiveOp;",
      "",
      "  static constexpr size_t CEStorageSize =",
      "      sizeof(typename CollectiveEpilogue::SharedStorage);",
      "  using Stages = typename cutlass::gemm::collective::StageCountAutoCarveout<",
      "      static_cast<int>(CEStorageSize)>;",
      "",
      "  // clang-format off",
      "  using CollectiveMainloop =",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp, ",
      "          ElementAB, cutlass::layout::RowMajor, AlignmentAB, ",
      "          ElementAB, cutlass::layout::ColumnMajor, AlignmentAB, ",
      "          ElementAcc, TileShape, ClusterShape,",
      "          Stages,",
      "          KernelSchedule>::CollectiveOp;",
      "  // clang-format on",
      "",
      "  using KernelType = enable_sm90_or_later<cutlass::gemm::kernel::GemmUniversal<",
      "      cute::Shape<int, int, int, int>, CollectiveMainloop, CollectiveEpilogue,",
      "      cutlass::gemm::PersistentScheduler>>;",
      "",
      "  struct GemmKernel : public KernelType {};",
      "};",
      "",
      "template <typename ElementAB_, typename ElementD_,",
      "          template <typename, typename, typename> typename Epilogue_,",
      "          typename TileShape, typename ClusterShape, typename KernelSchedule,",
      "          typename EpilogueSchedule>",
      "struct cutlass_3x_gemm_sm100 {",
      "  using ElementAB = ElementAB_;",
      "  using LayoutA = cutlass::layout::RowMajor;",
      "  static constexpr int AlignmentA =",
      "      128 / cutlass::sizeof_bits<ElementAB>::value;",
      "",
      "  using LayoutB = cutlass::layout::ColumnMajor;",
      "  static constexpr int AlignmentB =",
      "      128 / cutlass::sizeof_bits<ElementAB>::value;",
      "",
      "  using ElementC = void;",
      "  using LayoutC = cutlass::layout::RowMajor;",
      "  static constexpr int AlignmentC =",
      "      128 / cutlass::sizeof_bits<ElementD_>::value;",
      "",
      "  using ElementD = ElementD_;",
      "  using LayoutD = cutlass::layout::RowMajor;",
      "  static constexpr int AlignmentD = AlignmentC;",
      "",
      "  using ElementAcc =",
      "      typename std::conditional<std::is_same_v<ElementAB, int8_t>, int32_t,",
      "                                float>::type;",
      "  using Epilogue = Epilogue_<ElementAcc, ElementD, TileShape>;",
      "",
      "  // MMA type",
      "  using ElementAccumulator = float;",
      "",
      "  // Epilogue types",
      "  using ElementBias = cutlass::half_t;",
      "  using ElementCompute = float;",
      "  using ElementAux = ElementD;",
      "  using LayoutAux = LayoutD;",
      "  using ElementAmax = float;",
      "",
      "  using EVTCompute = typename Epilogue::EVTCompute;",
      "",
      "  using CollectiveEpilogue =",
      "      typename cutlass::epilogue::collective::CollectiveBuilder<",
      "          cutlass::arch::Sm100, cutlass::arch::OpClassTensorOp, TileShape,",
      "          ClusterShape, cutlass::epilogue::collective::EpilogueTileAuto,",
      "          ElementAccumulator, ElementCompute, ElementC, LayoutC, AlignmentC,",
      "          ElementD, LayoutD, AlignmentD, EpilogueSchedule,",
      "          EVTCompute>::CollectiveOp;",
      "",
      "  using CollectiveMainloop =",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          cutlass::arch::Sm100, cutlass::arch::OpClassTensorOp, ElementAB,",
      "          LayoutA, AlignmentA, ElementAB, LayoutB, AlignmentB,",
      "          ElementAccumulator, TileShape, ClusterShape,",
      "          cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(",
      "              sizeof(typename CollectiveEpilogue::SharedStorage))>,",
      "          KernelSchedule>::CollectiveOp;",
      "",
      "  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<",
      "      Shape<int, int, int, int>, CollectiveMainloop, CollectiveEpilogue, void>;",
      "};",
      "",
      "template <typename ElementAB_, typename ElementD_,",
      "          template <typename, typename, typename> typename Epilogue_,",
      "          typename TileShape, typename ClusterShape, typename KernelSchedule,",
      "          typename EpilogueSchedule>",
      "struct cutlass_3x_gemm_sm120 {",
      "  using ElementAB = ElementAB_;",
      "  using LayoutA = cutlass::layout::RowMajor;",
      "  static constexpr int AlignmentA =",
      "      128 / cutlass::sizeof_bits<ElementAB>::value;",
      "",
      "  using LayoutB = cutlass::layout::ColumnMajor;",
      "  static constexpr int AlignmentB =",
      "      128 / cutlass::sizeof_bits<ElementAB>::value;",
      "",
      "  using ElementC = void;",
      "  using LayoutC = cutlass::layout::RowMajor;",
      "  static constexpr int AlignmentC =",
      "      128 / cutlass::sizeof_bits<ElementD_>::value;",
      "",
      "  using ElementD = ElementD_;",
      "  using LayoutD = cutlass::layout::RowMajor;",
      "  static constexpr int AlignmentD = AlignmentC;",
      "",
      "  using ElementAcc =",
      "      typename std::conditional<std::is_same_v<ElementAB, int8_t>, int32_t,",
      "                                float>::type;",
      "  using Epilogue = Epilogue_<ElementAcc, ElementD, TileShape>;",
      "",
      "  // MMA type",
      "  using ElementAccumulator = float;",
      "",
      "  // Epilogue types",
      "  using ElementBias = cutlass::half_t;",
      "  using ElementCompute = float;",
      "  using ElementAux = ElementD;",
      "  using LayoutAux = LayoutD;",
      "  using ElementAmax = float;",
      "",
      "  using EVTCompute = typename Epilogue::EVTCompute;",
      "",
      "  using CollectiveEpilogue =",
      "      typename cutlass::epilogue::collective::CollectiveBuilder<",
      "          cutlass::arch::Sm120, cutlass::arch::OpClassTensorOp, TileShape,",
      "          ClusterShape, cutlass::epilogue::collective::EpilogueTileAuto,",
      "          ElementAccumulator, ElementCompute, ElementC, LayoutC, AlignmentC,",
      "          ElementD, LayoutD, AlignmentD, EpilogueSchedule,",
      "          EVTCompute>::CollectiveOp;",
      "",
      "  using CollectiveMainloop =",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          cutlass::arch::Sm120, cutlass::arch::OpClassTensorOp, ElementAB,",
      "          LayoutA, AlignmentA, ElementAB, LayoutB, AlignmentB,",
      "          ElementAccumulator, TileShape, ClusterShape,",
      "          cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(",
      "              sizeof(typename CollectiveEpilogue::SharedStorage))>,",
      "          KernelSchedule>::CollectiveOp;",
      "",
      "  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<",
      "      Shape<int, int, int, int>, CollectiveMainloop, CollectiveEpilogue, void>;",
      "};",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_int8.cu",
    "source": [
      "#include \"scaled_mm_kernels.hpp\"",
      "#include \"scaled_mm_sm90_int8_dispatch.cuh\"",
      "#include \"cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp\"",
      "",
      "namespace vllm {",
      "",
      "void cutlass_scaled_mm_sm90_int8(torch::Tensor& out, torch::Tensor const& a,",
      "                                 torch::Tensor const& b,",
      "                                 torch::Tensor const& a_scales,",
      "                                 torch::Tensor const& b_scales,",
      "                                 std::optional<torch::Tensor> const& bias) {",
      "  TORCH_CHECK(a_scales.is_contiguous() && b_scales.is_contiguous());",
      "  if (bias) {",
      "    TORCH_CHECK(bias->dtype() == out.dtype(),",
      "                \"currently bias dtype must match output dtype \", out.dtype());",
      "    return cutlass_scaled_mm_sm90_int8_epilogue<c3x::ScaledEpilogueBias>(",
      "        out, a, b, a_scales, b_scales, *bias);",
      "  } else {",
      "    return cutlass_scaled_mm_sm90_int8_epilogue<c3x::ScaledEpilogue>(",
      "        out, a, b, a_scales, b_scales);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_azp_sm90_int8.cu",
    "source": [
      "#include \"scaled_mm_kernels.hpp\"",
      "#include \"scaled_mm_sm90_int8_dispatch.cuh\"",
      "#include \"cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp\"",
      "",
      "namespace vllm {",
      "",
      "void cutlass_scaled_mm_azp_sm90_int8(torch::Tensor& out, torch::Tensor const& a,",
      "                                     torch::Tensor const& b,",
      "                                     torch::Tensor const& a_scales,",
      "                                     torch::Tensor const& b_scales,",
      "                                     torch::Tensor const& azp_adj,",
      "                                     std::optional<torch::Tensor> const& azp,",
      "                                     std::optional<torch::Tensor> const& bias) {",
      "  if (azp) {",
      "    return cutlass_scaled_mm_sm90_int8_epilogue<",
      "        c3x::ScaledEpilogueBiasAzpToken>(out, a, b, a_scales, b_scales, azp_adj,",
      "                                         *azp, bias);",
      "  } else {",
      "    return cutlass_scaled_mm_sm90_int8_epilogue<c3x::ScaledEpilogueBiasAzp>(",
      "        out, a, b, a_scales, b_scales, azp_adj, bias);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm100_fp8_dispatch.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"cuda_utils.h\"",
      "#include \"cutlass/cutlass.h\"",
      "#include \"cutlass/numeric_types.h\"",
      "",
      "#include \"cute/tensor.hpp\"",
      "#include \"cutlass/tensor_ref.h\"",
      "#include \"cutlass/gemm/dispatch_policy.hpp\"",
      "#include \"cutlass/gemm/collective/collective_builder.hpp\"",
      "#include \"cutlass/gemm/device/gemm_universal_adapter.h\"",
      "#include \"cutlass/gemm/kernel/gemm_universal.hpp\"",
      "#include \"cutlass/gemm/kernel/tile_scheduler_params.h\"",
      "#include \"cutlass/epilogue/dispatch_policy.hpp\"",
      "#include \"cutlass/epilogue/collective/collective_builder.hpp\"",
      "",
      "#include \"cutlass_extensions/gemm/dispatch_policy.hpp\"",
      "#include \"cutlass_extensions/gemm/collective/collective_builder.hpp\"",
      "",
      "#include \"cutlass_gemm_caller.cuh\"",
      "",
      "namespace vllm {",
      "",
      "using namespace cute;",
      "",
      "// clang-format off",
      "template <class OutType, int ScaleGranularityM,",
      "          int ScaleGranularityN, int ScaleGranularityK,",
      "          class MmaTileShape, class ClusterShape,",
      "          class EpilogueScheduler, class MainloopScheduler,",
      "          bool swap_ab_ = false>",
      "struct cutlass_3x_gemm_fp8_blockwise {",
      "  static constexpr bool swap_ab = swap_ab_;",
      "  using ElementAB = cutlass::float_e4m3_t;",
      "",
      "  using ElementA = ElementAB;",
      "  using LayoutA = cutlass::layout::RowMajor;",
      "  using LayoutA_Transpose = typename cutlass::layout::LayoutTranspose<LayoutA>::type;",
      "  static constexpr int AlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value;",
      "",
      "  using ElementB = ElementAB;",
      "  using LayoutB = cutlass::layout::ColumnMajor;",
      "  using LayoutB_Transpose = typename cutlass::layout::LayoutTranspose<LayoutB>::type;",
      "  static constexpr int AlignmentB = 128 / cutlass::sizeof_bits<ElementB>::value;",
      "",
      "  using ElementD = OutType;",
      "  using LayoutD = cutlass::layout::RowMajor;",
      "  using LayoutD_Transpose = typename cutlass::layout::LayoutTranspose<LayoutD>::type;",
      "  static constexpr int AlignmentD = 128 / cutlass::sizeof_bits<ElementD>::value;",
      "",
      "  using ElementC = void; // TODO: support bias",
      "  using LayoutC = LayoutD;",
      "  using LayoutC_Transpose = LayoutD_Transpose;",
      "  static constexpr int AlignmentC = AlignmentD;",
      "",
      "  using ElementAccumulator = float;",
      "  using ElementCompute = float;",
      "  using ElementBlockScale = float;",
      "",
      "  using ScaleConfig = conditional_t<swap_ab,",
      "      cutlass::detail::Sm100BlockwiseScaleConfig<",
      "        ScaleGranularityM, ScaleGranularityN, ScaleGranularityK,",
      "        cute::UMMA::Major::K, cute::UMMA::Major::MN>,",
      "      cutlass::detail::Sm100BlockwiseScaleConfig<",
      "        ScaleGranularityM, ScaleGranularityN, ScaleGranularityK,",
      "        cute::UMMA::Major::MN, cute::UMMA::Major::K>>;",
      "",
      "  // layout_SFA and layout_SFB cannot be swapped since they are deduced.",
      "  using LayoutSFA = decltype(ScaleConfig::deduce_layoutSFA());",
      "  using LayoutSFB = decltype(ScaleConfig::deduce_layoutSFB());",
      "",
      "  using ArchTag = cutlass::arch::Sm100;",
      "  using OperatorClass = cutlass::arch::OpClassTensorOp;",
      "",
      "  static constexpr auto RoundStyle = cutlass::FloatRoundStyle::round_to_nearest;",
      "  using ElementScalar = float;",
      "  using DefaultOperation = cutlass::epilogue::fusion::LinearCombination<ElementD, ElementCompute, ElementC, ElementScalar, RoundStyle>;",
      "  using CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveBuilder<",
      "      ArchTag,",
      "      OperatorClass,",
      "      MmaTileShape,",
      "      ClusterShape,",
      "      cutlass::epilogue::collective::EpilogueTileAuto,",
      "      ElementAccumulator,",
      "      ElementCompute,",
      "      ElementC,",
      "      conditional_t<swap_ab, LayoutC_Transpose, LayoutC>,",
      "      AlignmentC,",
      "      ElementD,",
      "      conditional_t<swap_ab, LayoutD_Transpose, LayoutD>,",
      "      AlignmentD,",
      "      EpilogueScheduler,",
      "      DefaultOperation",
      "  >::CollectiveOp;",
      " ",
      "  using StageCountType = cutlass::gemm::collective::StageCountAuto; ",
      "  using CollectiveMainloop = conditional_t<swap_ab,",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          ArchTag,",
      "          OperatorClass,",
      "          ElementB,",
      "          cute::tuple<LayoutB_Transpose, LayoutSFA>,",
      "          AlignmentB,",
      "          ElementA,",
      "          cute::tuple<LayoutA_Transpose, LayoutSFB>,",
      "          AlignmentA,",
      "          ElementAccumulator,",
      "          MmaTileShape,",
      "          ClusterShape,",
      "          cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(sizeof(typename CollectiveEpilogue::SharedStorage))>,",
      "          MainloopScheduler",
      "      >::CollectiveOp,",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          ArchTag,",
      "          OperatorClass,",
      "          ElementA,",
      "          cute::tuple<LayoutA, LayoutSFA>,",
      "          AlignmentA,",
      "          ElementB,",
      "          cute::tuple<LayoutB, LayoutSFB>,",
      "          AlignmentB,",
      "          ElementAccumulator,",
      "          MmaTileShape,",
      "          ClusterShape,",
      "          cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(sizeof(typename CollectiveEpilogue::SharedStorage))>,",
      "          MainloopScheduler",
      "      >::CollectiveOp>;",
      "",
      "  using KernelType = enable_sm100_only<cutlass::gemm::kernel::GemmUniversal<",
      "      Shape<int, int, int, int>, CollectiveMainloop, CollectiveEpilogue>>;",
      "",
      "  struct GemmKernel : public KernelType {};",
      "};",
      "",
      "template <typename Gemm>",
      "void cutlass_gemm_caller_blockwise(torch::Tensor& out, torch::Tensor const& a,",
      "                                   torch::Tensor const& b,",
      "                                   torch::Tensor const& a_scales,",
      "                                   torch::Tensor const& b_scales) {",
      "  static constexpr bool swap_ab = Gemm::swap_ab;",
      "  using GemmKernel = typename Gemm::GemmKernel;",
      "  using StrideA = typename Gemm::GemmKernel::StrideA;",
      "  using StrideB = typename Gemm::GemmKernel::StrideB;",
      "  using StrideD = typename Gemm::GemmKernel::StrideD;",
      "  using StrideC = typename Gemm::GemmKernel::StrideC;",
      "  using LayoutSFA = typename Gemm::LayoutSFA;",
      "  using LayoutSFB = typename Gemm::LayoutSFB;",
      "  using ScaleConfig = typename Gemm::ScaleConfig;",
      "",
      "  using ElementAB = typename Gemm::ElementAB;",
      "  using ElementD = typename Gemm::ElementD;",
      "",
      "  int32_t m = a.size(0), n = b.size(1), k = a.size(1);",
      "",
      "  StrideA a_stride;",
      "  StrideB b_stride;",
      "  StrideC c_stride;",
      "  a_stride =",
      "      cutlass::make_cute_packed_stride(StrideA{}, cute::make_shape(m, k, 1));",
      "  b_stride =",
      "      cutlass::make_cute_packed_stride(StrideB{}, cute::make_shape(n, k, 1));",
      "  c_stride =",
      "      cutlass::make_cute_packed_stride(StrideC{}, swap_ab ? cute::make_shape(n, m, 1) : cute::make_shape(m, n, 1));",
      "",
      "  LayoutSFA layout_SFA = swap_ab ? ",
      "      ScaleConfig::tile_atom_to_shape_SFA(make_shape(n, m, k, 1)) :",
      "      ScaleConfig::tile_atom_to_shape_SFA(make_shape(m, n, k, 1));",
      "  LayoutSFB layout_SFB = swap_ab ?",
      "      ScaleConfig::tile_atom_to_shape_SFB(make_shape(n, m, k, 1)) :",
      "      ScaleConfig::tile_atom_to_shape_SFB(make_shape(m, n, k, 1));",
      "",
      "  auto a_ptr = static_cast<ElementAB*>(a.data_ptr());",
      "  auto b_ptr = static_cast<ElementAB*>(b.data_ptr());",
      "  auto a_scales_ptr = static_cast<float*>(a_scales.data_ptr());",
      "  auto b_scales_ptr = static_cast<float*>(b_scales.data_ptr());",
      "",
      "  auto mainloop_args = [&](){",
      "    // layout_SFA and layout_SFB cannot be swapped since they are deduced.",
      "    if (swap_ab) {",
      "      return typename GemmKernel::MainloopArguments{",
      "          b_ptr,        b_stride,   a_ptr,        a_stride,",
      "          b_scales_ptr, layout_SFA, a_scales_ptr, layout_SFB",
      "      };",
      "    }",
      "    else {",
      "      return typename GemmKernel::MainloopArguments{",
      "          a_ptr,        a_stride,   b_ptr,        b_stride,",
      "          a_scales_ptr, layout_SFA, b_scales_ptr, layout_SFB",
      "      };",
      "    }",
      "  }();",
      "  auto prob_shape = swap_ab ? cute::make_shape(n, m, k, 1) : cute::make_shape(m, n, k, 1);",
      "",
      "  auto c_ptr = static_cast<ElementD*>(out.data_ptr());",
      "  typename GemmKernel::EpilogueArguments epilogue_args{",
      "      {}, c_ptr, c_stride, c_ptr, c_stride};",
      "  c3x::cutlass_gemm_caller<GemmKernel>(a.device(), prob_shape, mainloop_args,",
      "                                       epilogue_args);",
      "}",
      "",
      "template <typename OutType>",
      "void cutlass_gemm_blockwise_sm100_fp8_dispatch(torch::Tensor& out,",
      "                                               torch::Tensor const& a,",
      "                                               torch::Tensor const& b,",
      "                                               torch::Tensor const& a_scales,",
      "                                               torch::Tensor const& b_scales) {",
      "  int32_t m = a.size(0), n = b.size(1), k = a.size(1), sms;",
      "  cudaDeviceGetAttribute(&sms, cudaDevAttrMultiProcessorCount, a.get_device());",
      "",
      "  constexpr int TILE_K = 128;",
      "  // TODO: better heuristics",
      "  bool swap_ab = (m < 16) || (m % 4 != 0);",
      "  bool use_tma_epilogue = (m * n) % 4 == 0;",
      "  if (!swap_ab) {",
      "    constexpr int TILE_N = 128;",
      "    int tile_m = 256;",
      "    if (cuda_utils::ceil_div(n, TILE_N) * cuda_utils::ceil_div(m, 64) <= sms) {",
      "      tile_m = 64;",
      "    }",
      "    else if (cuda_utils::ceil_div(n, TILE_N) * cuda_utils::ceil_div(m, 128) <= sms) {",
      "      tile_m = 128;",
      "    }",
      "    if (tile_m == 64) {",
      "      if (use_tma_epilogue) {",
      "        cutlass_gemm_caller_blockwise<cutlass_3x_gemm_fp8_blockwise<",
      "            OutType, 1, TILE_N, TILE_K, Shape<_64, Int<TILE_N>, Int<TILE_K>>,",
      "            Shape<_1, _1, _1>, cutlass::epilogue::TmaWarpSpecialized1Sm,",
      "            cutlass::gemm::KernelTmaWarpSpecializedBlockwise1SmSm100>>(",
      "            out, a, b, a_scales, b_scales);",
      "      } else {",
      "        cutlass_gemm_caller_blockwise<cutlass_3x_gemm_fp8_blockwise<",
      "            OutType, 1, TILE_N, TILE_K, Shape<_64, Int<TILE_N>, Int<TILE_K>>,",
      "            Shape<_1, _1, _1>, cutlass::epilogue::NoSmemWarpSpecialized1Sm,",
      "            cutlass::gemm::KernelTmaWarpSpecializedBlockwise1SmSm100>>(",
      "            out, a, b, a_scales, b_scales);",
      "      }",
      "    } else if (tile_m == 128) {",
      "      if (use_tma_epilogue) {",
      "        cutlass_gemm_caller_blockwise<cutlass_3x_gemm_fp8_blockwise<",
      "            OutType, 1, TILE_N, TILE_K, Shape<_128, Int<TILE_N>, Int<TILE_K>>,",
      "            Shape<_1, _1, _1>, cutlass::epilogue::TmaWarpSpecialized1Sm,",
      "            cutlass::gemm::KernelTmaWarpSpecializedBlockwise1SmSm100>>(",
      "            out, a, b, a_scales, b_scales);",
      "      } else {",
      "        cutlass_gemm_caller_blockwise<cutlass_3x_gemm_fp8_blockwise<",
      "            OutType, 1, TILE_N, TILE_K, Shape<_128, Int<TILE_N>, Int<TILE_K>>,",
      "            Shape<_1, _1, _1>, cutlass::epilogue::NoSmemWarpSpecialized1Sm,",
      "            cutlass::gemm::KernelTmaWarpSpecializedBlockwise1SmSm100>>(",
      "            out, a, b, a_scales, b_scales);",
      "      }",
      "    } else { // tile_m == 256",
      "      if (use_tma_epilogue) {",
      "          cutlass_gemm_caller_blockwise<cutlass_3x_gemm_fp8_blockwise<",
      "              OutType, 1, TILE_N, TILE_K, Shape<_256, Int<TILE_N>, Int<TILE_K>>,",
      "            Shape<_2, _1, _1>, cutlass::epilogue::TmaWarpSpecialized2Sm,",
      "            cutlass::gemm::KernelTmaWarpSpecializedBlockwise2SmSm100>>(",
      "            out, a, b, a_scales, b_scales);",
      "      } else {",
      "          cutlass_gemm_caller_blockwise<cutlass_3x_gemm_fp8_blockwise<",
      "              OutType, 1, TILE_N, TILE_K, Shape<_256, Int<TILE_N>, Int<TILE_K>>,",
      "            Shape<_2, _1, _1>, cutlass::epilogue::NoSmemWarpSpecialized2Sm,",
      "            cutlass::gemm::KernelTmaWarpSpecializedBlockwise2SmSm100>>(",
      "            out, a, b, a_scales, b_scales);",
      "      }",
      "    }",
      "  } else {",
      "    // TODO: Test more tile N configs",
      "    constexpr int TILE_M = 128;",
      "    constexpr int TILE_N = 16;",
      "    // TMA epilogue isn't compatible with Swap A/B",
      "    cutlass_gemm_caller_blockwise<cutlass_3x_gemm_fp8_blockwise<",
      "        OutType, TILE_M, 1, TILE_K, Shape<Int<TILE_M>, Int<TILE_N>, Int<TILE_K>>,",
      "        Shape<_1, _1, _1>, cutlass::epilogue::NoSmemWarpSpecialized1Sm,",
      "        cutlass::gemm::KernelTmaWarpSpecializedBlockwise1SmSm100, true>>(",
      "        out, a, b, a_scales, b_scales);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm100_fp8.cu",
    "source": [
      "#include \"scaled_mm_kernels.hpp\"",
      "#include \"scaled_mm_sm100_fp8_dispatch.cuh\"",
      "#include \"cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp\"",
      "",
      "namespace vllm {",
      "",
      "void cutlass_scaled_mm_sm100_fp8(torch::Tensor& out, torch::Tensor const& a,",
      "                                 torch::Tensor const& b,",
      "                                 torch::Tensor const& a_scales,",
      "                                 torch::Tensor const& b_scales,",
      "                                 std::optional<torch::Tensor> const& bias) {",
      "  TORCH_CHECK(a_scales.is_contiguous() && b_scales.is_contiguous());",
      "  if (bias) {",
      "    TORCH_CHECK(bias->dtype() == out.dtype(),",
      "                \"currently bias dtype must match output dtype \", out.dtype());",
      "    return cutlass_scaled_mm_sm100_fp8_epilogue<c3x::ScaledEpilogueBias>(",
      "        out, a, b, a_scales, b_scales, *bias);",
      "  } else {",
      "    return cutlass_scaled_mm_sm100_fp8_epilogue<c3x::ScaledEpilogue>(",
      "        out, a, b, a_scales, b_scales);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm90_fp8.cu",
    "source": [
      "",
      "#include \"scaled_mm_kernels.hpp\"",
      "#include \"scaled_mm_blockwise_sm90_fp8_dispatch.cuh\"",
      "#include \"cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp\"",
      "",
      "namespace vllm {",
      "",
      "void cutlass_scaled_mm_blockwise_sm90_fp8(torch::Tensor& out,",
      "                                          torch::Tensor const& a,",
      "                                          torch::Tensor const& b,",
      "                                          torch::Tensor const& a_scales,",
      "                                          torch::Tensor const& b_scales) {",
      "  if (out.dtype() == torch::kBFloat16) {",
      "    cutlass_gemm_blockwise_sm90_fp8_dispatch<cutlass::bfloat16_t>(",
      "        out, a, b, a_scales, b_scales);",
      "",
      "  } else {",
      "    TORCH_CHECK(out.dtype() == torch::kFloat16);",
      "    cutlass_gemm_blockwise_sm90_fp8_dispatch<cutlass::half_t>(",
      "        out, a, b, a_scales, b_scales);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/cutlass_gemm_caller.cuh",
    "source": [
      "#pragma once",
      "",
      "// clang-format will break include orders",
      "// clang-format off",
      "#include <torch/all.h>",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "",
      "#include \"cutlass/cutlass.h\"",
      "",
      "#include \"cute/tensor.hpp\"",
      "#include \"cute/atom/mma_atom.hpp\"",
      "#include \"cutlass/numeric_types.h\"",
      "",
      "#include \"cutlass/gemm/device/gemm_universal_adapter.h\"",
      "#include \"cutlass/gemm/kernel/gemm_universal.hpp\"",
      "#include \"cutlass/epilogue/collective/collective_builder.hpp\"",
      "#include \"cutlass/gemm/collective/collective_builder.hpp\"",
      "#include \"cutlass/util/packed_stride.hpp\"",
      "",
      "#include \"core/math.hpp\"",
      "#include \"cutlass_extensions/common.hpp\"",
      "// clang-format on",
      "",
      "namespace vllm::c3x {",
      "",
      "static inline cute::Shape<int, int, int, int> get_problem_shape(",
      "    torch::Tensor const& a, torch::Tensor const& b) {",
      "  int32_t m = a.size(0), n = b.size(1), k = a.size(1);",
      "  return {m, n, k, 1};",
      "}",
      "",
      "template <typename GemmKernel>",
      "void cutlass_gemm_caller(",
      "    torch::Device device, cute::Shape<int, int, int, int> prob_shape,",
      "    typename GemmKernel::MainloopArguments mainloop_args,",
      "    typename GemmKernel::EpilogueArguments epilogue_args,",
      "    typename GemmKernel::TileSchedulerArguments scheduler = {}) {",
      "  cutlass::KernelHardwareInfo hw_info;",
      "  typename GemmKernel::Arguments args{cutlass::gemm::GemmUniversalMode::kGemm,",
      "                                      prob_shape,",
      "                                      mainloop_args,",
      "                                      epilogue_args,",
      "                                      hw_info,",
      "                                      scheduler};",
      "",
      "  // Launch the CUTLASS GEMM kernel.",
      "  using GemmOp = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;",
      "  GemmOp gemm_op;",
      "  CUTLASS_CHECK(gemm_op.can_implement(args));",
      "",
      "  size_t workspace_size = gemm_op.get_workspace_size(args);",
      "  auto const workspace_options =",
      "      torch::TensorOptions().dtype(torch::kUInt8).device(device);",
      "  auto workspace = torch::empty(workspace_size, workspace_options);",
      "",
      "  auto stream = at::cuda::getCurrentCUDAStream(device.index());",
      "",
      "  cutlass::Status status = gemm_op.run(args, workspace.data_ptr(), stream);",
      "  CUTLASS_CHECK(status);",
      "}",
      "",
      "template <typename Gemm, typename... EpilogueArgs>",
      "void cutlass_gemm_caller(torch::Tensor& out, torch::Tensor const& a,",
      "                         torch::Tensor const& b,",
      "                         EpilogueArgs&&... epilogue_params) {",
      "  using ElementAB = typename Gemm::ElementAB;",
      "  using ElementC = typename Gemm::ElementC;",
      "  using ElementD = typename Gemm::ElementD;",
      "  using GemmKernel = typename Gemm::GemmKernel;",
      "",
      "  using StrideA = typename Gemm::GemmKernel::StrideA;",
      "  using StrideB = typename Gemm::GemmKernel::StrideB;",
      "  using StrideC = typename Gemm::GemmKernel::StrideC;",
      "  using StrideD = StrideC;",
      "  using StrideAux = StrideC;",
      "",
      "  typename GemmKernel::ProblemShape prob_shape = get_problem_shape(a, b);",
      "  auto [M, N, K, L] = prob_shape;",
      "",
      "  StrideA a_stride =",
      "      cutlass::make_cute_packed_stride(StrideA{}, cute::make_shape(M, K, L));",
      "  StrideB b_stride =",
      "      cutlass::make_cute_packed_stride(StrideB{}, cute::make_shape(N, K, L));",
      "  StrideC c_stride =",
      "      cutlass::make_cute_packed_stride(StrideC{}, cute::make_shape(M, N, L));",
      "  StrideD d_stride =",
      "      cutlass::make_cute_packed_stride(StrideD{}, cute::make_shape(M, N, L));",
      "  StrideAux aux_stride = d_stride;",
      "",
      "  auto a_ptr = static_cast<ElementAB*>(a.data_ptr());",
      "  auto b_ptr = static_cast<ElementAB*>(b.data_ptr());",
      "  typename GemmKernel::MainloopArguments mainloop_args{a_ptr, a_stride, b_ptr,",
      "                                                       b_stride};",
      "",
      "  auto c_ptr = static_cast<ElementD*>(out.data_ptr());",
      "  // auto d_ptr = static_cast<ElementC*>(out.data_ptr());",
      "  typename GemmKernel::EpilogueArguments epilogue_args{",
      "      Gemm::Epilogue::prepare_args(",
      "          std::forward<EpilogueArgs>(epilogue_params)...),",
      "      c_ptr, c_stride, c_ptr, d_stride};",
      "",
      "  cutlass_gemm_caller<GemmKernel>(a.device(), prob_shape, mainloop_args,",
      "                                  epilogue_args);",
      "}",
      "",
      "}  // namespace vllm::c3x"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm120_fp8_dispatch.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"scaled_mm.cuh\"",
      "#include \"cutlass_gemm_caller.cuh\"",
      "",
      "/**",
      " * This file defines Gemm kernel configurations for SM120 (fp8) based on the",
      " * Gemm shape.",
      " */",
      "",
      "namespace vllm {",
      "",
      "using c3x::cutlass_gemm_caller;",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm120_fp8_config_default {",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule = cutlass::gemm::collective::KernelScheduleAuto;",
      "  using EpilogueSchedule = cutlass::epilogue::collective::EpilogueScheduleAuto;",
      "  using TileShape = Shape<_128, _128, _128>;",
      "  using ClusterShape = Shape<_1, _1, _1>;  // Only work with Shape<_1, _1, _1>",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_gemm_sm120<InType, OutType, Epilogue, TileShape, ClusterShape,",
      "                            KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue,",
      "          typename... EpilogueArgs>",
      "inline void cutlass_gemm_sm120_fp8_dispatch(torch::Tensor& out,",
      "                                            torch::Tensor const& a,",
      "                                            torch::Tensor const& b,",
      "                                            EpilogueArgs&&... args) {",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  TORCH_CHECK(a.dtype() == torch::kFloat8_e4m3fn);",
      "  TORCH_CHECK(b.dtype() == torch::kFloat8_e4m3fn);",
      "",
      "  using Cutlass3xGemmDefault =",
      "      typename sm120_fp8_config_default<InType, OutType,",
      "                                        Epilogue>::Cutlass3xGemm;",
      "  return cutlass_gemm_caller<Cutlass3xGemmDefault>(",
      "      out, a, b, std::forward<EpilogueArgs>(args)...);",
      "}",
      "",
      "template <template <typename, typename, typename> typename Epilogue,",
      "          typename... EpilogueArgs>",
      "void cutlass_scaled_mm_sm120_fp8_epilogue(torch::Tensor& out,",
      "                                          torch::Tensor const& a,",
      "                                          torch::Tensor const& b,",
      "                                          EpilogueArgs&&... epilogue_args) {",
      "  TORCH_CHECK(a.dtype() == torch::kFloat8_e4m3fn);",
      "  TORCH_CHECK(b.dtype() == torch::kFloat8_e4m3fn);",
      "",
      "  if (out.dtype() == torch::kBFloat16) {",
      "    return cutlass_gemm_sm120_fp8_dispatch<cutlass::float_e4m3_t,",
      "                                           cutlass::bfloat16_t, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);",
      "  } else {",
      "    TORCH_CHECK(out.dtype() == torch::kFloat16);",
      "    return cutlass_gemm_sm120_fp8_dispatch<cutlass::float_e4m3_t,",
      "                                           cutlass::half_t, Epilogue>(",
      "        out, a, b, std::forward<EpilogueArgs>(epilogue_args)...);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/moe/blockwise_scaled_group_mm_sm100.cu",
    "source": [
      "#include \"core/registration.h\"",
      "",
      "#include <torch/all.h>",
      "#include <cutlass/arch/arch.h>",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <c10/cuda/CUDAStream.h>",
      "",
      "#include \"cute/tensor.hpp\"",
      "#include \"cutlass/tensor_ref.h\"",
      "#include \"cutlass/epilogue/collective/default_epilogue.hpp\"",
      "#include \"cutlass/epilogue/thread/linear_combination.h\"",
      "#include \"cutlass/gemm/dispatch_policy.hpp\"",
      "#include \"cutlass/gemm/group_array_problem_shape.hpp\"",
      "#include \"cutlass/gemm/collective/collective_builder.hpp\"",
      "#include \"cutlass/epilogue/collective/collective_builder.hpp\"",
      "#include \"cutlass/gemm/device/gemm_universal_adapter.h\"",
      "#include \"cutlass/gemm/kernel/gemm_universal.hpp\"",
      "",
      "#include \"cutlass/util/command_line.h\"",
      "#include \"cutlass/util/distribution.h\"",
      "#include \"cutlass/util/host_tensor.h\"",
      "#include \"cutlass/util/packed_stride.hpp\"",
      "#include \"cutlass/util/tensor_view_io.h\"",
      "#include \"cutlass/util/reference/device/gemm.h\"",
      "#include \"cutlass/util/reference/device/tensor_compare.h\"",
      "#include \"cutlass/util/reference/host/tensor_fill.h\"",
      "#include \"cutlass/util/reference/host/gett.hpp\"",
      "#include \"cutlass/util/reference/host/tensor_norm.h\"",
      "#include \"cutlass/util/reference/host/tensor_compare.h\"",
      "#include <cassert>",
      "",
      "using namespace cute;",
      "",
      "template <typename ElementAB, typename ElementC, typename ElementAccumulator,",
      "          typename LayoutSFA, typename LayoutSFB, typename ScaleConfig>",
      "__global__ void get_ggemm_starts(",
      "    int32_t* expert_offsets, ElementAB** a_offsets, ElementAB** b_offsets,",
      "    ElementC** out_offsets, ElementAccumulator** a_scale_offsets,",
      "    ElementAccumulator** b_scale_offsets, ElementAB* a_base_as_int,",
      "    ElementAB* b_base_as_int, ElementC* out_base_as_int,",
      "    ElementAccumulator* a_scale_base_as_int,",
      "    ElementAccumulator* b_scale_base_as_int, LayoutSFA* layout_sfa_base_as_int,",
      "    LayoutSFB* layout_sfb_base_as_int, int* problem_sizes) {",
      "  int expert_id = threadIdx.x;",
      "",
      "  if (expert_id >= gridDim.x * blockDim.x) {",
      "    return;",
      "  }",
      "",
      "  int m = problem_sizes[expert_id * 3];",
      "  int n = problem_sizes[expert_id * 3 + 1];",
      "  int k = problem_sizes[expert_id * 3 + 2];",
      "",
      "  int32_t expert_offset = expert_offsets[expert_id];",
      "  int a_stride = expert_offset * k;",
      "  int b_stride = expert_id * k * n;",
      "  int a_scale_stride = expert_offset * k / 128;",
      "  int b_scale_stride = expert_id * k * n / 128 / 128;",
      "",
      "  a_offsets[expert_id] = a_base_as_int + a_stride;",
      "  b_offsets[expert_id] = b_base_as_int + b_stride;",
      "  out_offsets[expert_id] = out_base_as_int + expert_offset * n;",
      "  a_scale_offsets[expert_id] = a_scale_base_as_int + a_scale_stride;",
      "  b_scale_offsets[expert_id] = b_scale_base_as_int + b_scale_stride;",
      "",
      "  LayoutSFA* layout_sfa_ptr = layout_sfa_base_as_int + expert_id;",
      "  LayoutSFB* layout_sfb_ptr = layout_sfb_base_as_int + expert_id;",
      "",
      "  *layout_sfa_ptr =",
      "      ScaleConfig::tile_atom_to_shape_SFA(cute::make_shape(m, n, k, 1));",
      "  *layout_sfb_ptr =",
      "      ScaleConfig::tile_atom_to_shape_SFB(cute::make_shape(m, n, k, 1));",
      "}",
      "",
      "#define __CALL_GET_STARTS_KERNEL(TENSOR_C_TYPE, C_TYPE, LayoutSFA, LayoutSFB, \\",
      "                                 ScaleConfig)                                 \\",
      "  else if (out_tensors.dtype() == TENSOR_C_TYPE) {                            \\",
      "    get_ggemm_starts<cutlass::float_e4m3_t, C_TYPE, float, LayoutSFA,         \\",
      "                     LayoutSFB, ScaleConfig><<<1, num_experts, 0, stream>>>(  \\",
      "        static_cast<int32_t*>(expert_offsets.data_ptr()),                     \\",
      "        static_cast<cutlass::float_e4m3_t**>(a_ptrs.data_ptr()),              \\",
      "        static_cast<cutlass::float_e4m3_t**>(b_ptrs.data_ptr()),              \\",
      "        static_cast<C_TYPE**>(out_ptrs.data_ptr()),                           \\",
      "        static_cast<float**>(a_scales_ptrs.data_ptr()),                       \\",
      "        static_cast<float**>(b_scales_ptrs.data_ptr()),                       \\",
      "        static_cast<cutlass::float_e4m3_t*>(a_tensors.data_ptr()),            \\",
      "        static_cast<cutlass::float_e4m3_t*>(b_tensors.data_ptr()),            \\",
      "        static_cast<C_TYPE*>(out_tensors.data_ptr()),                         \\",
      "        static_cast<float*>(a_scales.data_ptr()),                             \\",
      "        static_cast<float*>(b_scales.data_ptr()),                             \\",
      "        reinterpret_cast<LayoutSFA*>(layout_sfa.data_ptr()),                  \\",
      "        reinterpret_cast<LayoutSFB*>(layout_sfb.data_ptr()),                  \\",
      "        static_cast<int*>(problem_sizes.data_ptr()));                         \\",
      "  }",
      "",
      "template <typename LayoutSFA, typename LayoutSFB, typename ScaleConfig>",
      "void run_get_ggemm_starts(",
      "    torch::Tensor const& expert_offsets, torch::Tensor& a_ptrs,",
      "    torch::Tensor& b_ptrs, torch::Tensor& out_ptrs,",
      "    torch::Tensor& a_scales_ptrs, torch::Tensor& b_scales_ptrs,",
      "    torch::Tensor const& a_tensors, torch::Tensor const& b_tensors,",
      "    torch::Tensor out_tensors, torch::Tensor const& a_scales,",
      "    torch::Tensor const& b_scales, torch::Tensor const& layout_sfa,",
      "    torch::Tensor const& layout_sfb, torch::Tensor const& problem_sizes) {",
      "  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn);",
      "  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn);",
      "  TORCH_CHECK(a_scales.dtype() == torch::kFloat32);",
      "  TORCH_CHECK(b_scales.dtype() == torch::kFloat32);",
      "  TORCH_CHECK(out_tensors.size(1) % 128 == 0 or out_tensors.size(0) % 128 == 0);",
      "  TORCH_CHECK(a_tensors.size(1) % 128 == 0 or a_tensors.size(0) % 128 == 0);",
      "",
      "  int num_experts = (int)expert_offsets.size(0);",
      "  auto stream = at::cuda::getCurrentCUDAStream(a_tensors.device().index());",
      "",
      "  if (false) {",
      "  }",
      "  __CALL_GET_STARTS_KERNEL(torch::kBFloat16, cutlass::bfloat16_t, LayoutSFA,",
      "                           LayoutSFB, ScaleConfig)",
      "  __CALL_GET_STARTS_KERNEL(torch::kFloat16, cutlass::half_t, LayoutSFA,",
      "                           LayoutSFB, ScaleConfig)",
      "  else {",
      "    TORCH_CHECK(false, \"Unsupported output tensor type\");",
      "  }",
      "}",
      "",
      "template <typename OutType, typename ScheduleConfig, typename LayoutD>",
      "void run_blockwise_scaled_group_mm(",
      "    torch::Tensor& out_ptrs, const torch::Tensor& a_ptrs,",
      "    const torch::Tensor& b_ptrs, const torch::Tensor& a_scales_ptrs,",
      "    const torch::Tensor& b_scales_ptrs, const torch::Tensor& stride_a,",
      "    const torch::Tensor& stride_b, const torch::Tensor& stride_c,",
      "    const torch::Tensor& layout_sfa, const torch::Tensor& layout_sfb,",
      "    const torch::Tensor& problem_sizes, const torch::Tensor& expert_offsets) {",
      "  using ProblemShape = cutlass::gemm::GroupProblemShape<Shape<int, int, int>>;",
      "",
      "  // Types",
      "  using ElementA = cutlass::float_e4m3_t;",
      "  using ElementB = cutlass::float_e4m3_t;",
      "  using ElementC = OutType;",
      "  using ElementD = ElementC;",
      "  using ElementAccumulator = float;",
      "  using LayoutA = cutlass::layout::RowMajor;",
      "  using LayoutB = cutlass::layout::ColumnMajor;",
      "  using LayoutC = LayoutD;",
      "",
      "  // Alignments",
      "  static constexpr int AlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value;",
      "  static constexpr int AlignmentB = 128 / cutlass::sizeof_bits<ElementB>::value;",
      "  static constexpr int AlignmentC = 128 / cutlass::sizeof_bits<ElementC>::value;",
      "",
      "  using ArchTag = cutlass::arch::Sm100;",
      "  using OperatorClass = cutlass::arch::OpClassTensorOp;",
      "",
      "  using CollectiveEpilogue =",
      "      typename cutlass::epilogue::collective::CollectiveBuilder<",
      "          ArchTag, OperatorClass, typename ScheduleConfig::MmaTileShape,",
      "          typename ScheduleConfig::ClusterShape,",
      "          cutlass::epilogue::collective::EpilogueTileAuto, ElementAccumulator,",
      "          ElementAccumulator, void, LayoutC*, AlignmentC, ElementD, LayoutC*,",
      "          AlignmentC, typename ScheduleConfig::EpilogueSchedule>::CollectiveOp;",
      "",
      "  using CollectiveMainloop =",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          ArchTag, OperatorClass, ElementA,",
      "          cute::tuple<LayoutA*, typename ScheduleConfig::LayoutSFA*>,",
      "          AlignmentA, ElementB,",
      "          cute::tuple<LayoutB*, typename ScheduleConfig::LayoutSFB*>,",
      "          AlignmentB, ElementAccumulator, typename ScheduleConfig::MmaTileShape,",
      "          typename ScheduleConfig::ClusterShape,",
      "          cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(",
      "              sizeof(typename CollectiveEpilogue::SharedStorage))>,",
      "          typename ScheduleConfig::KernelSchedule>::CollectiveOp;",
      "",
      "  using GemmKernel =",
      "      cutlass::gemm::kernel::GemmUniversal<ProblemShape, CollectiveMainloop,",
      "                                           CollectiveEpilogue, void>;",
      "",
      "  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;",
      "  using StrideA = typename Gemm::GemmKernel::InternalStrideA;",
      "  using StrideB = typename Gemm::GemmKernel::InternalStrideB;",
      "  using StrideC = typename Gemm::GemmKernel::InternalStrideC;",
      "  using StrideD = typename Gemm::GemmKernel::InternalStrideD;",
      "",
      "  using UnderlyingProblemShape = ProblemShape::UnderlyingProblemShape;",
      "  int num_experts = (int)expert_offsets.size(0);",
      "",
      "  Gemm gemm_op;",
      "",
      "  // Mainloop Arguments",
      "  typename GemmKernel::MainloopArguments mainloop_args{",
      "      static_cast<const ElementA**>(a_ptrs.data_ptr()),",
      "      static_cast<StrideA*>(stride_a.data_ptr()),",
      "      static_cast<const ElementB**>(b_ptrs.data_ptr()),",
      "      static_cast<StrideB*>(stride_b.data_ptr()),",
      "      static_cast<const ElementAccumulator**>(a_scales_ptrs.data_ptr()),",
      "      reinterpret_cast<typename ScheduleConfig::LayoutSFA*>(",
      "          layout_sfa.data_ptr()),",
      "      static_cast<const ElementAccumulator**>(b_scales_ptrs.data_ptr()),",
      "      reinterpret_cast<typename ScheduleConfig::LayoutSFB*>(",
      "          layout_sfb.data_ptr())};",
      "",
      "  int device_id = a_ptrs.device().index();",
      "  static const cutlass::KernelHardwareInfo hw_info{",
      "      device_id, cutlass::KernelHardwareInfo::query_device_multiprocessor_count(",
      "                     device_id)};",
      "",
      "  // Epilogue Arguments",
      "  typename GemmKernel::EpilogueArguments epilogue_args{",
      "      {},  // epilogue.thread",
      "      nullptr,",
      "      static_cast<StrideC*>(stride_c.data_ptr()),",
      "      static_cast<ElementD**>(out_ptrs.data_ptr()),",
      "      static_cast<StrideC*>(stride_c.data_ptr())};",
      "",
      "  UnderlyingProblemShape* problem_sizes_as_shapes =",
      "      static_cast<UnderlyingProblemShape*>(problem_sizes.data_ptr());",
      "",
      "  // Gemm Arguments",
      "  typename GemmKernel::Arguments args{",
      "      cutlass::gemm::GemmUniversalMode::kGrouped,",
      "      {num_experts, problem_sizes_as_shapes, nullptr},",
      "      mainloop_args,",
      "      epilogue_args,",
      "      hw_info};",
      "",
      "  at::cuda::CUDAGuard device_guard{(char)a_ptrs.device().index()};",
      "  const cudaStream_t stream =",
      "      at::cuda::getCurrentCUDAStream(a_ptrs.get_device());",
      "",
      "  auto can_implement_status = gemm_op.can_implement(args);",
      "  TORCH_CHECK(can_implement_status == cutlass::Status::kSuccess,",
      "              \"Failed to implement GEMM\");",
      "",
      "  size_t workspace_size = gemm_op.get_workspace_size(args);",
      "  auto const workspace_options =",
      "      torch::TensorOptions().dtype(torch::kUInt8).device(a_ptrs.device());",
      "  auto workspace = torch::empty(workspace_size, workspace_options);",
      "",
      "  auto status = gemm_op.initialize(args, workspace.data_ptr(), stream);",
      "  TORCH_CHECK(status == cutlass::Status::kSuccess, \"Failed to initialize GEMM\");",
      "",
      "  status = gemm_op.run(stream);",
      "  TORCH_CHECK(status == cutlass::Status::kSuccess, \"Failed to run GEMM\");",
      "}",
      "",
      "template <typename OutType>",
      "void blockwise_scaled_group_mm_dispatch_shape(",
      "    torch::Tensor& output, const torch::Tensor& a, const torch::Tensor& b,",
      "    const torch::Tensor& scales_a, const torch::Tensor& scales_b,",
      "    const torch::Tensor& problem_sizes, const torch::Tensor& expert_offsets) {",
      "  struct MmaConfig {",
      "    using ElementA = cutlass::float_e4m3_t;",
      "    using KernelSchedule =",
      "        cutlass::gemm::KernelPtrArrayTmaWarpSpecializedBlockwise1SmSm100;",
      "    using EpilogueSchedule = cutlass::epilogue::PtrArrayTmaWarpSpecialized1Sm;",
      "    using ScaleConfig = cutlass::detail::Sm100BlockwiseScaleConfig<",
      "        1, 128, 128, cute::UMMA::Major::K, cute::UMMA::Major::K>;",
      "    using LayoutSFA = decltype(ScaleConfig::deduce_layoutSFA());",
      "    using LayoutSFB = decltype(ScaleConfig::deduce_layoutSFB());",
      "    using LayoutC = cutlass::layout::RowMajor;",
      "    using MmaTileShape = Shape<_128, _128, _128>;",
      "    using ClusterShape = Shape<_1, _1, _1>;",
      "  };",
      "",
      "  int num_experts = (int)expert_offsets.size(0);",
      "",
      "  auto a_ptrs = torch::empty(",
      "      {num_experts},",
      "      torch::TensorOptions().dtype(torch::kInt64).device(a.device()));",
      "  auto b_ptrs = torch::empty(",
      "      {num_experts},",
      "      torch::TensorOptions().dtype(torch::kInt64).device(a.device()));",
      "  auto out_ptrs = torch::empty(",
      "      {num_experts},",
      "      torch::TensorOptions().dtype(torch::kInt64).device(a.device()));",
      "  auto a_scales_ptrs = torch::empty(",
      "      {num_experts},",
      "      torch::TensorOptions().dtype(torch::kInt64).device(a.device()));",
      "  auto b_scales_ptrs = torch::empty(",
      "      {num_experts},",
      "      torch::TensorOptions().dtype(torch::kInt64).device(a.device()));",
      "",
      "  auto layout_sfa = torch::empty(",
      "      {num_experts, 5},",
      "      torch::TensorOptions().dtype(torch::kInt32).device(a.device()));",
      "  auto layout_sfb = torch::empty(",
      "      {num_experts, 5},",
      "      torch::TensorOptions().dtype(torch::kInt32).device(a.device()));",
      "",
      "  auto stride_a = torch::full(",
      "      {num_experts}, a.size(1),",
      "      torch::TensorOptions().dtype(torch::kInt64).device(a.device()));",
      "  auto stride_b = torch::full(",
      "      {num_experts}, a.size(1),",
      "      torch::TensorOptions().dtype(torch::kInt64).device(a.device()));",
      "  auto stride_c = torch::full(",
      "      {num_experts}, output.size(1),",
      "      torch::TensorOptions().dtype(torch::kInt64).device(a.device()));",
      "",
      "  torch::TensorOptions options_int =",
      "      torch::TensorOptions().dtype(torch::kInt64).device(a.device());",
      "",
      "  run_get_ggemm_starts<typename MmaConfig::LayoutSFA,",
      "                       typename MmaConfig::LayoutSFB,",
      "                       typename MmaConfig::ScaleConfig>(",
      "      expert_offsets, a_ptrs, b_ptrs, out_ptrs, a_scales_ptrs, b_scales_ptrs, a,",
      "      b, output, scales_a, scales_b, layout_sfa, layout_sfb, problem_sizes);",
      "",
      "  run_blockwise_scaled_group_mm<OutType, MmaConfig,",
      "                                typename MmaConfig::LayoutC>(",
      "      out_ptrs, a_ptrs, b_ptrs, a_scales_ptrs, b_scales_ptrs, stride_a,",
      "      stride_b, stride_c, layout_sfa, layout_sfb, problem_sizes,",
      "      expert_offsets);",
      "}",
      "",
      "void cutlass_blockwise_scaled_grouped_mm(",
      "    torch::Tensor& output, const torch::Tensor& a, const torch::Tensor& b,",
      "    const torch::Tensor& scales_a, const torch::Tensor& scales_b,",
      "    const torch::Tensor& problem_sizes, const torch::Tensor& expert_offsets) {",
      "  TORCH_CHECK(problem_sizes.dim() == 2, \"problem_sizes must be 2D tensor\");",
      "  TORCH_CHECK(problem_sizes.size(1) == 3,",
      "              \"problem_sizes must have shape (num_experts, 3)\");",
      "  TORCH_CHECK(problem_sizes.size(0) == expert_offsets.size(0),",
      "              \"Number of experts in problem_sizes must match expert_offsets\");",
      "  TORCH_CHECK(problem_sizes.dtype() == torch::kInt32,",
      "              \"problem_sizes must be int32\");",
      "  TORCH_CHECK(a.scalar_type() == torch::kFloat8_e4m3fn,",
      "              \"a must be kFloat8_e4m3fn\");",
      "  TORCH_CHECK(b.scalar_type() == torch::kFloat8_e4m3fn,",
      "              \"b must be kFloat8_e4m3fn\");",
      "  TORCH_CHECK(output.scalar_type() == torch::kBFloat16 ||",
      "                  output.scalar_type() == torch::kHalf,",
      "              \"output must be bfloat16 or half\");",
      "  TORCH_CHECK(scales_a.scalar_type() == torch::kFloat32,",
      "              \"scales_a must be float32\");",
      "  TORCH_CHECK(scales_b.scalar_type() == torch::kFloat32,",
      "              \"scales_b must be float32\");",
      "  TORCH_CHECK(expert_offsets.scalar_type() == torch::kInt32,",
      "              \"expert_offsets must be int32\");",
      "",
      "  TORCH_CHECK(output.dim() == 2, \"output must be 2D tensor\");",
      "  TORCH_CHECK(a.dim() == 2, \"a must be 2D tensor\");",
      "  TORCH_CHECK(b.dim() == 3, \"b must be 3D tensor\");",
      "  TORCH_CHECK(scales_a.dim() == 2, \"scales_a must be 2D tensor\");",
      "  TORCH_CHECK(scales_b.dim() == 3, \"scales_b must be 3D tensor\");",
      "  TORCH_CHECK(problem_sizes.dim() == 2, \"problem_sizes must be 2D tensor\");",
      "  TORCH_CHECK(problem_sizes.size(1) == 3,",
      "              \"problem_sizes must have shape (num_experts, 3)\");",
      "  TORCH_CHECK(problem_sizes.size(0) == expert_offsets.size(0),",
      "              \"Number of experts in problem_sizes must match expert_offsets\");",
      "  TORCH_CHECK(problem_sizes.dtype() == torch::kInt32,",
      "              \"problem_sizes must be int32\");",
      "  TORCH_CHECK(expert_offsets.dim() == 1, \"expert_offsets must be 1D tensor\");",
      "",
      "#if defined(ENABLE_CUTLASS_MOE_SM100) && ENABLE_CUTLASS_MOE_SM100",
      "  if (output.scalar_type() == torch::kBFloat16) {",
      "    blockwise_scaled_group_mm_dispatch_shape<cutlass::bfloat16_t>(",
      "        output, a, b, scales_a, scales_b, problem_sizes, expert_offsets);",
      "  } else if (output.scalar_type() == torch::kFloat16) {",
      "    blockwise_scaled_group_mm_dispatch_shape<cutlass::half_t>(",
      "        output, a, b, scales_a, scales_b, problem_sizes, expert_offsets);",
      "  } else {",
      "    TORCH_CHECK(false, \"Unsupported output tensor type\");",
      "  }",
      "#endif",
      "}",
      "",
      "TORCH_LIBRARY_IMPL_EXPAND(TORCH_EXTENSION_NAME, CUDA, m) {",
      "  m.impl(\"cutlass_blockwise_scaled_grouped_mm\",",
      "         &cutlass_blockwise_scaled_grouped_mm);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x_sm100.cu",
    "source": [
      "#include <cudaTypedefs.h>",
      "",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <torch/all.h>",
      "",
      "#include \"cutlass/cutlass.h\"",
      "#include \"grouped_mm_c3x.cuh\"",
      "",
      "using namespace cute;",
      "",
      "namespace {",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm100_fp8_config_default {",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelPtrArrayTmaWarpSpecialized1SmSm100;",
      "  using EpilogueSchedule = cutlass::epilogue::PtrArrayTmaWarpSpecialized1Sm;",
      "  using TileShape = cute::Shape<cute::_128, cute::_256, cute::_128>;",
      "  using ClusterShape = cute::Shape<cute::_1, cute::_1, cute::_1>;",
      "  using ArchTag = cutlass::arch::Sm100;",
      "",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_group_gemm<InType, OutType, ArchTag, Epilogue, TileShape,",
      "                            ClusterShape, KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm100_fp8_config_M64 {",
      "  // M in [1,64]",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelPtrArrayTmaWarpSpecialized1SmSm100;",
      "  using EpilogueSchedule = cutlass::epilogue::PtrArrayTmaWarpSpecialized1Sm;",
      "  using TileShape = cute::Shape<cute::_128, cute::_16, cute::_128>;",
      "  using ClusterShape = cute::Shape<cute::_1, cute::_1, cute::_1>;",
      "  using ArchTag = cutlass::arch::Sm100;",
      "",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_group_gemm<InType, OutType, ArchTag, Epilogue, TileShape,",
      "                            ClusterShape, KernelSchedule, EpilogueSchedule,",
      "                            true>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm100_fp8_config_N8192 {",
      "  // N in [8192, inf)",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelPtrArrayTmaWarpSpecialized2SmSm100;",
      "  using EpilogueSchedule = cutlass::epilogue::PtrArrayTmaWarpSpecialized2Sm;",
      "  using TileShape = cute::Shape<cute::_128, cute::_256, cute::_128>;",
      "  using ClusterShape = cute::Shape<cute::_2, cute::_1, cute::_1>;",
      "  using ArchTag = cutlass::arch::Sm100;",
      "",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_group_gemm<InType, OutType, ArchTag, Epilogue, TileShape,",
      "                            ClusterShape, KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType>",
      "void run_cutlass_moe_mm_sm100(",
      "    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,",
      "    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,",
      "    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,",
      "    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,",
      "    torch::Tensor const& b_strides, torch::Tensor const& c_strides,",
      "    bool per_act_token, bool per_out_ch) {",
      "  TORCH_CHECK(a_tensors.size(0) > 0, \"No input A tensors provided.\");",
      "  TORCH_CHECK(b_tensors.size(0) > 0, \"No input B tensors provided.\");",
      "  TORCH_CHECK(out_tensors.size(0) > 0, \"No output tensors provided.\");",
      "",
      "  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn,",
      "              \"A tensors must be of type float8_e4m3fn.\");",
      "  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn,",
      "              \"B tensors must be of type float8_e4m3fn.\");",
      "",
      "  using Cutlass3xGemmDefault = typename sm100_fp8_config_default<",
      "      InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;",
      "  using Cutlass3xGemmN8192 = typename sm100_fp8_config_N8192<",
      "      InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM64 = typename sm100_fp8_config_M64<",
      "      InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;",
      "",
      "  uint32_t const m = a_tensors.size(0);",
      "  uint32_t const n = out_tensors.size(1);",
      "",
      "  if (m <= 64) {",
      "    cutlass_group_gemm_caller<Cutlass3xGemmM64>(",
      "        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,",
      "        problem_sizes, a_strides, b_strides, c_strides, per_act_token,",
      "        per_out_ch);",
      "  } else if (n >= 8192) {",
      "    cutlass_group_gemm_caller<Cutlass3xGemmN8192>(",
      "        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,",
      "        problem_sizes, a_strides, b_strides, c_strides, per_act_token,",
      "        per_out_ch);",
      "  } else {",
      "    cutlass_group_gemm_caller<Cutlass3xGemmDefault>(",
      "        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,",
      "        problem_sizes, a_strides, b_strides, c_strides, per_act_token,",
      "        per_out_ch);",
      "  }",
      "}",
      "}  // namespace",
      "",
      "void dispatch_moe_mm_sm100(",
      "    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,",
      "    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,",
      "    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,",
      "    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,",
      "    torch::Tensor const& b_strides, torch::Tensor const& c_strides,",
      "    bool per_act_token, bool per_out_ch) {",
      "  if (out_tensors.dtype() == torch::kBFloat16) {",
      "    run_cutlass_moe_mm_sm100<cutlass::float_e4m3_t, cutlass::bfloat16_t>(",
      "        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,",
      "        problem_sizes, a_strides, b_strides, c_strides, per_act_token,",
      "        per_out_ch);",
      "  } else {",
      "    run_cutlass_moe_mm_sm100<cutlass::float_e4m3_t, cutlass::half_t>(",
      "        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,",
      "        problem_sizes, a_strides, b_strides, c_strides, per_act_token,",
      "        per_out_ch);",
      "  }",
      "}",
      "",
      "void cutlass_moe_mm_sm100(",
      "    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,",
      "    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,",
      "    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,",
      "    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,",
      "    torch::Tensor const& b_strides, torch::Tensor const& c_strides,",
      "    bool per_act_token, bool per_out_ch) {",
      "  dispatch_moe_mm_sm100(out_tensors, a_tensors, b_tensors, a_scales, b_scales,",
      "                        expert_offsets, problem_sizes, a_strides, b_strides,",
      "                        c_strides, per_act_token, per_out_ch);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x_sm90.cu",
    "source": [
      "#include <cudaTypedefs.h>",
      "",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <torch/all.h>",
      "",
      "#include \"cutlass/cutlass.h\"",
      "#include \"grouped_mm_c3x.cuh\"",
      "",
      "using namespace cute;",
      "",
      "namespace {",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_default {",
      "  // M in (16, inf)",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;",
      "  using EpilogueSchedule =",
      "      cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;",
      "  using TileShape = cute::Shape<cute::_64, cute::_256, cute::_128>;",
      "  using ClusterShape = cute::Shape<cute::_1, cute::_2, cute::_1>;",
      "  using ArchTag = cutlass::arch::Sm90;",
      "",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_group_gemm<InType, OutType, ArchTag, Epilogue, TileShape,",
      "                            ClusterShape, KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_M4 {",
      "  // M in [1, 4]",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;",
      "  using EpilogueSchedule =",
      "      cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;",
      "  using TileShape = cute::Shape<cute::_128, cute::_16, cute::_128>;",
      "  using ClusterShape = cute::Shape<cute::_1, cute::_1, cute::_1>;",
      "  using ArchTag = cutlass::arch::Sm90;",
      "",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_group_gemm<InType, OutType, ArchTag, Epilogue, TileShape,",
      "                            ClusterShape, KernelSchedule, EpilogueSchedule,",
      "                            true>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_M64 {",
      "  // M in (4, 64]",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;",
      "  using EpilogueSchedule =",
      "      cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;",
      "  using TileShape = cute::Shape<cute::_128, cute::_16, cute::_256>;",
      "  using ClusterShape = cute::Shape<cute::_2, cute::_1, cute::_1>;",
      "  using ArchTag = cutlass::arch::Sm90;",
      "",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_group_gemm<InType, OutType, ArchTag, Epilogue, TileShape,",
      "                            ClusterShape, KernelSchedule, EpilogueSchedule,",
      "                            true>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_K8192 {",
      "  // K in [8192, inf)",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;",
      "  using EpilogueSchedule =",
      "      cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;",
      "  using TileShape = cute::Shape<cute::_128, cute::_128, cute::_128>;",
      "  using ClusterShape = cute::Shape<cute::_1, cute::_8, cute::_1>;",
      "  using ArchTag = cutlass::arch::Sm90;",
      "",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_group_gemm<InType, OutType, ArchTag, Epilogue, TileShape,",
      "                            ClusterShape, KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType,",
      "          template <typename, typename, typename> typename Epilogue>",
      "struct sm90_fp8_config_N8192 {",
      "  // N in [8192, inf)",
      "  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());",
      "  using KernelSchedule =",
      "      cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;",
      "  using EpilogueSchedule =",
      "      cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;",
      "  using TileShape = cute::Shape<cute::_64, cute::_128, cute::_256>;",
      "  using ClusterShape = cute::Shape<cute::_1, cute::_8, cute::_1>;",
      "  using ArchTag = cutlass::arch::Sm90;",
      "",
      "  using Cutlass3xGemm =",
      "      cutlass_3x_group_gemm<InType, OutType, ArchTag, Epilogue, TileShape,",
      "                            ClusterShape, KernelSchedule, EpilogueSchedule>;",
      "};",
      "",
      "template <typename InType, typename OutType>",
      "void run_cutlass_moe_mm_sm90(",
      "    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,",
      "    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,",
      "    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,",
      "    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,",
      "    torch::Tensor const& b_strides, torch::Tensor const& c_strides,",
      "    bool per_act_token, bool per_out_ch) {",
      "  TORCH_CHECK(a_tensors.size(0) > 0, \"No input A tensors provided.\");",
      "  TORCH_CHECK(b_tensors.size(0) > 0, \"No input B tensors provided.\");",
      "  TORCH_CHECK(out_tensors.size(0) > 0, \"No output tensors provided.\");",
      "",
      "  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn,",
      "              \"A tensors must be of type float8_e4m3fn.\");",
      "  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn,",
      "              \"B tensors must be of type float8_e4m3fn.\");",
      "",
      "  using Cutlass3xGemmN8192 = typename sm90_fp8_config_N8192<",
      "      InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;",
      "  using Cutlass3xGemmK8192 = typename sm90_fp8_config_K8192<",
      "      InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM4 = typename sm90_fp8_config_M4<",
      "      InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;",
      "  using Cutlass3xGemmM64 = typename sm90_fp8_config_M64<",
      "      InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;",
      "  using Cutlass3xGemmDefault = typename sm90_fp8_config_default<",
      "      InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;",
      "",
      "  uint32_t const m = a_tensors.size(0);",
      "  uint32_t const n = out_tensors.size(1);",
      "  uint32_t const k = a_tensors.size(1);",
      "",
      "  // Use swap_ab for M <= 64 by default to reduce padding",
      "  if (m <= 4) {",
      "    cutlass_group_gemm_caller<Cutlass3xGemmM4>(",
      "        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,",
      "        problem_sizes, a_strides, b_strides, c_strides, per_act_token,",
      "        per_out_ch);",
      "  } else if (m <= 64) {",
      "    cutlass_group_gemm_caller<Cutlass3xGemmM64>(",
      "        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,",
      "        problem_sizes, a_strides, b_strides, c_strides, per_act_token,",
      "        per_out_ch);",
      "  } else if (n >= 8192) {",
      "    cutlass_group_gemm_caller<Cutlass3xGemmN8192>(",
      "        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,",
      "        problem_sizes, a_strides, b_strides, c_strides, per_act_token,",
      "        per_out_ch);",
      "  } else if (k >= 8192) {",
      "    cutlass_group_gemm_caller<Cutlass3xGemmK8192>(",
      "        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,",
      "        problem_sizes, a_strides, b_strides, c_strides, per_act_token,",
      "        per_out_ch);",
      "  } else {",
      "    cutlass_group_gemm_caller<Cutlass3xGemmDefault>(",
      "        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,",
      "        problem_sizes, a_strides, b_strides, c_strides, per_act_token,",
      "        per_out_ch);",
      "  }",
      "}",
      "",
      "void dispatch_moe_mm_sm90(",
      "    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,",
      "    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,",
      "    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,",
      "    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,",
      "    torch::Tensor const& b_strides, torch::Tensor const& c_strides,",
      "    bool per_act_token, bool per_out_ch) {",
      "  if (out_tensors.dtype() == torch::kBFloat16) {",
      "    run_cutlass_moe_mm_sm90<cutlass::float_e4m3_t, cutlass::bfloat16_t>(",
      "        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,",
      "        problem_sizes, a_strides, b_strides, c_strides, per_act_token,",
      "        per_out_ch);",
      "  } else {",
      "    run_cutlass_moe_mm_sm90<cutlass::float_e4m3_t, cutlass::half_t>(",
      "        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,",
      "        problem_sizes, a_strides, b_strides, c_strides, per_act_token,",
      "        per_out_ch);",
      "  }",
      "}",
      "",
      "}  // namespace",
      "",
      "void cutlass_moe_mm_sm90(",
      "    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,",
      "    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,",
      "    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,",
      "    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,",
      "    torch::Tensor const& b_strides, torch::Tensor const& c_strides,",
      "    bool per_act_token, bool per_out_ch) {",
      "  dispatch_moe_mm_sm90(out_tensors, a_tensors, b_tensors, a_scales, b_scales,",
      "                       expert_offsets, problem_sizes, a_strides, b_strides,",
      "                       c_strides, per_act_token, per_out_ch);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cuh",
    "source": [
      "#pragma once",
      "",
      "#include \"cutlass/cutlass.h\"",
      "",
      "#include \"cutlass/gemm/collective/collective_builder.hpp\"",
      "#include \"cutlass/epilogue/collective/collective_builder.hpp\"",
      "#include \"cutlass/gemm/device/gemm_universal_adapter.h\"",
      "",
      "#include \"cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp\"",
      "#include \"cutlass_extensions/common.hpp\"",
      "#include \"get_group_starts.cuh\"",
      "",
      "using namespace cute;",
      "",
      "namespace {",
      "",
      "using ProblemShape =",
      "    cutlass::gemm::GroupProblemShape<cute::Shape<int, int, int>>;",
      "",
      "using ElementAccumulator = float;",
      "using OperatorClass = cutlass::arch::OpClassTensorOp;",
      "",
      "using LayoutA = cutlass::layout::RowMajor;",
      "using LayoutA_Transpose =",
      "    typename cutlass::layout::LayoutTranspose<LayoutA>::type;",
      "using LayoutB = cutlass::layout::ColumnMajor;",
      "using LayoutB_Transpose =",
      "    typename cutlass::layout::LayoutTranspose<LayoutB>::type;",
      "using LayoutD = cutlass::layout::RowMajor;",
      "using LayoutD_Transpose =",
      "    typename cutlass::layout::LayoutTranspose<LayoutD>::type;",
      "using LayoutC = LayoutD;",
      "using LayoutC_Transpose = LayoutD_Transpose;",
      "",
      "template <typename ElementAB_, typename ElementC_, typename ArchTag_,",
      "          template <typename, typename, typename> typename Epilogue_,",
      "          typename TileShape, typename ClusterShape, typename KernelSchedule,",
      "          typename EpilogueSchedule, bool swap_ab_ = false>",
      "struct cutlass_3x_group_gemm {",
      "  static constexpr bool swap_ab = swap_ab_;",
      "  using ElementAB = ElementAB_;",
      "  using ElementC = void;",
      "  using ElementD = ElementC_;",
      "  using ElementAccumulator = float;",
      "  using ArchTag = ArchTag_;",
      "",
      "  using Epilogue = Epilogue_<ElementAccumulator, ElementD, TileShape>;",
      "",
      "  static constexpr int AlignmentAB =",
      "      128 / cutlass::sizeof_bits<ElementAB>::value;",
      "  static constexpr int AlignmentC = 128 / cutlass::sizeof_bits<ElementD>::value;",
      "",
      "  using EVTCompute = typename Epilogue::EVTCompute;",
      "",
      "  using CollectiveEpilogue =",
      "      typename cutlass::epilogue::collective::CollectiveBuilder<",
      "          ArchTag, OperatorClass, TileShape, ClusterShape,",
      "          cutlass::epilogue::collective::EpilogueTileAuto, ElementAccumulator,",
      "          ElementAccumulator, ElementC,",
      "          conditional_t<swap_ab, LayoutC_Transpose*, LayoutC*>, AlignmentC,",
      "          ElementD, conditional_t<swap_ab, LayoutD_Transpose*, LayoutD*>,",
      "          AlignmentC, EpilogueSchedule, EVTCompute>::CollectiveOp;",
      "",
      "  static constexpr size_t CEStorageSize =",
      "      sizeof(typename CollectiveEpilogue::SharedStorage);",
      "  using Stages = typename cutlass::gemm::collective::StageCountAutoCarveout<",
      "      static_cast<int>(CEStorageSize)>;",
      "",
      "  using CollectiveMainloop = conditional_t<",
      "      swap_ab,",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          ArchTag, OperatorClass, ElementAB, LayoutB_Transpose*, AlignmentAB,",
      "          ElementAB, LayoutA_Transpose*, AlignmentAB, ElementAccumulator,",
      "          TileShape, ClusterShape, Stages, KernelSchedule>::CollectiveOp,",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          ArchTag, OperatorClass, ElementAB, LayoutA*, AlignmentAB, ElementAB,",
      "          LayoutB*, AlignmentAB, ElementAccumulator, TileShape, ClusterShape,",
      "          Stages, KernelSchedule>::CollectiveOp>;",
      "",
      "  using KernelType = enable_sm90_or_later<cutlass::gemm::kernel::GemmUniversal<",
      "      ProblemShape, CollectiveMainloop, CollectiveEpilogue>>;",
      "",
      "  struct GemmKernel : public KernelType {};",
      "};",
      "",
      "template <typename Gemm>",
      "void cutlass_group_gemm_caller(",
      "    torch::Tensor& out_tensors, torch::Tensor const& a_tensors,",
      "    torch::Tensor const& b_tensors, torch::Tensor const& a_scales,",
      "    torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,",
      "    torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,",
      "    torch::Tensor const& b_strides, torch::Tensor const& c_strides,",
      "    bool per_act_token, bool per_out_ch) {",
      "  static constexpr bool swap_ab = Gemm::swap_ab;",
      "",
      "  using ElementAB = typename Gemm::ElementAB;",
      "  using ElementD = typename Gemm::ElementD;",
      "",
      "  int num_experts = static_cast<int>(expert_offsets.size(0));",
      "",
      "  auto stream = at::cuda::getCurrentCUDAStream(a_tensors.device().index());",
      "",
      "  auto options_int =",
      "      torch::TensorOptions().dtype(torch::kInt64).device(a_tensors.device());",
      "",
      "  torch::Tensor a_ptrs = torch::empty(num_experts, options_int);",
      "  torch::Tensor b_ptrs = torch::empty(num_experts, options_int);",
      "  torch::Tensor out_ptrs = torch::empty(num_experts, options_int);",
      "  torch::Tensor a_scales_ptrs = torch::empty(num_experts, options_int);",
      "  torch::Tensor b_scales_ptrs = torch::empty(num_experts, options_int);",
      "",
      "  run_get_group_gemm_starts(expert_offsets, a_ptrs, b_ptrs, out_ptrs,",
      "                            a_scales_ptrs, b_scales_ptrs, a_tensors, b_tensors,",
      "                            out_tensors, a_scales, b_scales);",
      "",
      "  using GemmKernel = typename Gemm::GemmKernel;",
      "  using StrideA = Stride<int64_t, Int<1>, Int<0>>;",
      "  using StrideB = Stride<int64_t, Int<1>, Int<0>>;",
      "  using StrideC = typename GemmKernel::InternalStrideC;",
      "",
      "  ProblemShape::UnderlyingProblemShape* problem_sizes_as_shapes =",
      "      static_cast<ProblemShape::UnderlyingProblemShape*>(",
      "          problem_sizes.data_ptr());",
      "  ProblemShape prob_shape{num_experts, problem_sizes_as_shapes, nullptr};",
      "",
      "  typename GemmKernel::MainloopArguments mainloop_args;",
      "  if constexpr (swap_ab) {",
      "    mainloop_args = typename GemmKernel::MainloopArguments{",
      "        static_cast<const ElementAB**>(b_ptrs.data_ptr()),",
      "        static_cast<StrideB*>(b_strides.data_ptr()),",
      "        static_cast<const ElementAB**>(a_ptrs.data_ptr()),",
      "        static_cast<StrideA*>(a_strides.data_ptr())};",
      "  } else {",
      "    mainloop_args = typename GemmKernel::MainloopArguments{",
      "        static_cast<const ElementAB**>(a_ptrs.data_ptr()),",
      "        static_cast<StrideA*>(a_strides.data_ptr()),",
      "        static_cast<const ElementAB**>(b_ptrs.data_ptr()),",
      "        static_cast<StrideB*>(b_strides.data_ptr())};",
      "  }",
      "",
      "  // Currently, we are only able to do broadcast on either all or none a_scales",
      "  // and on either all or none b_scales",
      "  typename GemmKernel::EpilogueArguments epilogue_args{",
      "      Gemm::Epilogue::prepare_args(",
      "          swap_ab ? static_cast<const ElementAccumulator**>(",
      "                        b_scales_ptrs.data_ptr())",
      "                  : static_cast<const ElementAccumulator**>(",
      "                        a_scales_ptrs.data_ptr()),",
      "          swap_ab ? static_cast<const ElementAccumulator**>(",
      "                        a_scales_ptrs.data_ptr())",
      "                  : static_cast<const ElementAccumulator**>(",
      "                        b_scales_ptrs.data_ptr()),",
      "          swap_ab ? per_out_ch : per_act_token,",
      "          swap_ab ? per_act_token : per_out_ch),",
      "      nullptr, static_cast<StrideC*>(c_strides.data_ptr()),",
      "      static_cast<ElementD**>(out_ptrs.data_ptr()),",
      "      static_cast<StrideC*>(c_strides.data_ptr())};",
      "",
      "  int device_id = a_tensors.device().index();",
      "  static const cutlass::KernelHardwareInfo hw_info{",
      "      device_id, cutlass::KernelHardwareInfo::query_device_multiprocessor_count(",
      "                     device_id)};",
      "",
      "  typename GemmKernel::Arguments args{",
      "      cutlass::gemm::GemmUniversalMode::kGrouped, prob_shape, mainloop_args,",
      "      epilogue_args, hw_info};",
      "",
      "  using GemmOp = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;",
      "  GemmOp gemm_op;",
      "  CUTLASS_CHECK(gemm_op.can_implement(args));",
      "",
      "  size_t workspace_size = gemm_op.get_workspace_size(args);",
      "  auto const workspace_options =",
      "      torch::TensorOptions().dtype(torch::kUInt8).device(a_tensors.device());",
      "  auto workspace = torch::empty(workspace_size, workspace_options);",
      "",
      "  cutlass::Status status = gemm_op.run(args, workspace.data_ptr(), stream);",
      "  CUTLASS_CHECK(status);",
      "}",
      "",
      "}  // namespace"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/moe/moe_data.cu",
    "source": [
      "#include <cudaTypedefs.h>",
      "",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <torch/all.h>",
      "",
      "#include <iostream>",
      "",
      "constexpr uint64_t THREADS_PER_EXPERT = 512;",
      "// threshold must match the dispatch logic in run_cutlass_moe_mm_sm90()",
      "constexpr int SWAP_AB_THRESHOLD = 64;",
      "",
      "template <bool SWAP_AB>",
      "__global__ void compute_problem_sizes(const int32_t* __restrict__ topk_ids,",
      "                                      int32_t* problem_sizes1,",
      "                                      int32_t* problem_sizes2,",
      "                                      int32_t* atomic_buffer,",
      "                                      const int topk_length, const int n,",
      "                                      const int k) {",
      "  int expert_id = blockIdx.x;",
      "",
      "  int occurrences = 0;",
      "  for (int i = threadIdx.x; i < topk_length; i += THREADS_PER_EXPERT) {",
      "    occurrences += (topk_ids[i] == expert_id);",
      "  }",
      "  atomicAdd(&atomic_buffer[expert_id], occurrences);",
      "  __syncthreads();",
      "",
      "  if (threadIdx.x == 0) {",
      "    int final_occurrences = atomic_buffer[expert_id];",
      "    if constexpr (!SWAP_AB) {",
      "      problem_sizes1[expert_id * 3] = final_occurrences;",
      "      problem_sizes1[expert_id * 3 + 1] = 2 * n;",
      "      problem_sizes1[expert_id * 3 + 2] = k;",
      "      problem_sizes2[expert_id * 3] = final_occurrences;",
      "      problem_sizes2[expert_id * 3 + 1] = k;",
      "      problem_sizes2[expert_id * 3 + 2] = n;",
      "    } else {",
      "      problem_sizes1[expert_id * 3] = 2 * n;",
      "      problem_sizes1[expert_id * 3 + 1] = final_occurrences;",
      "      problem_sizes1[expert_id * 3 + 2] = k;",
      "      problem_sizes2[expert_id * 3] = k;",
      "      problem_sizes2[expert_id * 3 + 1] = final_occurrences;",
      "      problem_sizes2[expert_id * 3 + 2] = n;",
      "    }",
      "  }",
      "}",
      "",
      "__global__ void compute_expert_offsets(",
      "    const int32_t* __restrict__ problem_sizes1, int32_t* expert_offsets,",
      "    int32_t* atomic_buffer, const int num_experts, const bool swap_ab) {",
      "  int32_t tot_offset = 0;",
      "  expert_offsets[0] = 0;",
      "  for (int i = 0; i < num_experts; ++i) {",
      "    atomic_buffer[i] = tot_offset;",
      "    tot_offset += swap_ab ? problem_sizes1[i * 3 + 1] : problem_sizes1[i * 3];",
      "    expert_offsets[i + 1] = tot_offset;",
      "  }",
      "}",
      "",
      "__global__ void compute_expert_blockscale_offsets(",
      "    const int32_t* __restrict__ problem_sizes1, int32_t* expert_offsets,",
      "    int32_t* blockscale_offsets, int32_t* atomic_buffer, const int num_experts,",
      "    const bool swap_ab) {",
      "  int32_t tot_offset = 0;",
      "  int32_t tot_offset_round = 0;",
      "  expert_offsets[0] = 0;",
      "  blockscale_offsets[0] = 0;",
      "  for (int i = 0; i < num_experts; ++i) {",
      "    int32_t cur_offset =",
      "        swap_ab ? problem_sizes1[i * 3 + 1] : problem_sizes1[i * 3];",
      "    atomic_buffer[i] = tot_offset;",
      "    tot_offset += cur_offset;",
      "    expert_offsets[i + 1] = tot_offset;",
      "    tot_offset_round += (cur_offset + (128 - 1)) / 128 * 128;",
      "    blockscale_offsets[i + 1] = tot_offset_round;",
      "  }",
      "}",
      "",
      "__global__ void compute_arg_sorts(const int32_t* __restrict__ topk_ids,",
      "                                  const int32_t* __restrict__ expert_offsets,",
      "                                  int32_t* input_permutation,",
      "                                  int32_t* output_permutation,",
      "                                  int32_t* atomic_buffer, const int topk_length,",
      "                                  const int topk) {",
      "  int const blk_expert_id = blockIdx.x;",
      "  int const num_experts = gridDim.x;",
      "  int32_t const num_tokens = expert_offsets[num_experts];",
      "",
      "  for (int i = threadIdx.x; i < topk_length; i += THREADS_PER_EXPERT) {",
      "    int const expert_id = topk_ids[i];",
      "    if (expert_id == -1 && blockIdx.x == 0) {",
      "      // output_permutation is used to re-order the moe outputs. It is",
      "      // used as c2 = c2[c_map], where c2 is a torch.tensor that is the",
      "      // output of the cutlass kernels and c_map is the output_permutation.",
      "      // c2 is initialized to zeros, therefore by setting the output_permutation",
      "      // to num_tokens, we are guaranteed to fill the moe outputs to zero",
      "      // for \"invalid\" topk_ids.",
      "      output_permutation[i] = num_tokens;",
      "    } else if (expert_id == blk_expert_id) {",
      "      int start = atomicAdd(&atomic_buffer[expert_id], 1);",
      "      input_permutation[start] = i / topk;",
      "      output_permutation[i] = start;",
      "    }",
      "  }",
      "}",
      "",
      "namespace {",
      "inline void launch_compute_problem_sizes(const torch::Tensor& topk_ids,",
      "                                         torch::Tensor& problem_sizes1,",
      "                                         torch::Tensor& problem_sizes2,",
      "                                         torch::Tensor& atomic_buffer,",
      "                                         int64_t num_experts, int64_t n,",
      "                                         int64_t k, cudaStream_t stream,",
      "                                         const bool swap_ab) {",
      "  int num_threads = min(THREADS_PER_EXPERT, topk_ids.numel());",
      "",
      "  const int32_t* topk_ptr = static_cast<const int32_t*>(topk_ids.data_ptr());",
      "  int32_t* ps1_ptr = static_cast<int32_t*>(problem_sizes1.data_ptr());",
      "  int32_t* ps2_ptr = static_cast<int32_t*>(problem_sizes2.data_ptr());",
      "  int32_t* atomic_ptr = static_cast<int32_t*>(atomic_buffer.data_ptr());",
      "",
      "  if (swap_ab) {",
      "    compute_problem_sizes<true><<<num_experts, num_threads, 0, stream>>>(",
      "        topk_ptr, ps1_ptr, ps2_ptr, atomic_ptr,",
      "        static_cast<int>(topk_ids.numel()), static_cast<int>(n),",
      "        static_cast<int>(k));",
      "  } else {",
      "    compute_problem_sizes<false><<<num_experts, num_threads, 0, stream>>>(",
      "        topk_ptr, ps1_ptr, ps2_ptr, atomic_ptr,",
      "        static_cast<int>(topk_ids.numel()), static_cast<int>(n),",
      "        static_cast<int>(k));",
      "  }",
      "}",
      "}  // namespace",
      "",
      "void get_cutlass_moe_mm_problem_sizes_caller(",
      "    const torch::Tensor& topk_ids, torch::Tensor& problem_sizes1,",
      "    torch::Tensor& problem_sizes2, const int64_t num_experts, const int64_t n,",
      "    const int64_t k, const std::optional<torch::Tensor>& blockscale_offsets) {",
      "  auto stream = at::cuda::getCurrentCUDAStream(topk_ids.device().index());",
      "  auto options_int32 =",
      "      torch::TensorOptions().dtype(torch::kInt32).device(topk_ids.device());",
      "  torch::Tensor atomic_buffer = torch::zeros(num_experts, options_int32);",
      "",
      "  // Swap-AB should be disabled for FP4 path",
      "  bool may_swap_ab = (!blockscale_offsets.has_value()) &&",
      "                     (topk_ids.numel() <= SWAP_AB_THRESHOLD);",
      "",
      "  launch_compute_problem_sizes(topk_ids, problem_sizes1, problem_sizes2,",
      "                               atomic_buffer, num_experts, n, k, stream,",
      "                               may_swap_ab);",
      "}",
      "",
      "void get_cutlass_moe_mm_data_caller(",
      "    const torch::Tensor& topk_ids, torch::Tensor& expert_offsets,",
      "    torch::Tensor& problem_sizes1, torch::Tensor& problem_sizes2,",
      "    torch::Tensor& input_permutation, torch::Tensor& output_permutation,",
      "    const int64_t num_experts, const int64_t n, const int64_t k,",
      "    const std::optional<torch::Tensor>& blockscale_offsets) {",
      "  auto stream = at::cuda::getCurrentCUDAStream(topk_ids.device().index());",
      "  auto options_int32 =",
      "      torch::TensorOptions().dtype(torch::kInt32).device(topk_ids.device());",
      "  torch::Tensor atomic_buffer = torch::zeros(num_experts, options_int32);",
      "",
      "  int num_threads = min(THREADS_PER_EXPERT, topk_ids.numel());",
      "",
      "  // Swap-AB should be disabled for FP4 path",
      "  bool may_swap_ab = (!blockscale_offsets.has_value()) &&",
      "                     (topk_ids.numel() <= SWAP_AB_THRESHOLD);",
      "",
      "  launch_compute_problem_sizes(topk_ids, problem_sizes1, problem_sizes2,",
      "                               atomic_buffer, num_experts, n, k, stream,",
      "                               may_swap_ab);",
      "",
      "  if (blockscale_offsets.has_value()) {",
      "    // fp4 path",
      "    compute_expert_blockscale_offsets<<<1, 1, 0, stream>>>(",
      "        static_cast<const int32_t*>(problem_sizes1.data_ptr()),",
      "        static_cast<int32_t*>(expert_offsets.data_ptr()),",
      "        static_cast<int32_t*>(blockscale_offsets.value().data_ptr()),",
      "        static_cast<int32_t*>(atomic_buffer.data_ptr()), num_experts,",
      "        may_swap_ab);",
      "  } else {",
      "    compute_expert_offsets<<<1, 1, 0, stream>>>(",
      "        static_cast<const int32_t*>(problem_sizes1.data_ptr()),",
      "        static_cast<int32_t*>(expert_offsets.data_ptr()),",
      "        static_cast<int32_t*>(atomic_buffer.data_ptr()), num_experts,",
      "        may_swap_ab);",
      "  }",
      "  compute_arg_sorts<<<num_experts, num_threads, 0, stream>>>(",
      "      static_cast<const int32_t*>(topk_ids.data_ptr()),",
      "      static_cast<const int32_t*>(expert_offsets.data_ptr()),",
      "      static_cast<int32_t*>(input_permutation.data_ptr()),",
      "      static_cast<int32_t*>(output_permutation.data_ptr()),",
      "      static_cast<int32_t*>(atomic_buffer.data_ptr()), topk_ids.numel(),",
      "      topk_ids.size(1));",
      "}",
      "",
      "template <bool SWAP_AB>",
      "__global__ void compute_pplx_data(int32_t* expert_offsets,",
      "                                  int32_t* problem_sizes1,",
      "                                  int32_t* problem_sizes2,",
      "                                  const int32_t* __restrict__ expert_num_tokens,",
      "                                  const int padded_m, const int n,",
      "                                  const int k) {",
      "  int expert_idx = threadIdx.x;",
      "  expert_offsets[expert_idx] = expert_idx * padded_m;",
      "",
      "  if constexpr (!SWAP_AB) {",
      "    problem_sizes1[expert_idx * 3] = expert_num_tokens[expert_idx];",
      "    problem_sizes1[expert_idx * 3 + 1] = 2 * n;",
      "    problem_sizes1[expert_idx * 3 + 2] = k;",
      "    problem_sizes2[expert_idx * 3] = expert_num_tokens[expert_idx];",
      "    problem_sizes2[expert_idx * 3 + 1] = k;",
      "    problem_sizes2[expert_idx * 3 + 2] = n;",
      "  } else {",
      "    problem_sizes1[expert_idx * 3] = 2 * n;",
      "    problem_sizes1[expert_idx * 3 + 1] = expert_num_tokens[expert_idx];",
      "    problem_sizes1[expert_idx * 3 + 2] = k;",
      "    problem_sizes2[expert_idx * 3] = k;",
      "    problem_sizes2[expert_idx * 3 + 1] = expert_num_tokens[expert_idx];",
      "    problem_sizes2[expert_idx * 3 + 2] = n;",
      "  }",
      "}",
      "",
      "void get_cutlass_pplx_moe_mm_data_caller(torch::Tensor& expert_offsets,",
      "                                         torch::Tensor& problem_sizes1,",
      "                                         torch::Tensor& problem_sizes2,",
      "                                         const torch::Tensor& expert_num_tokens,",
      "                                         const int64_t num_local_experts,",
      "                                         const int64_t padded_m,",
      "                                         const int64_t n, const int64_t k) {",
      "  auto stream = at::cuda::getCurrentCUDAStream(expert_offsets.device().index());",
      "",
      "  if (num_local_experts * padded_m > SWAP_AB_THRESHOLD) {",
      "    compute_pplx_data<false><<<1, num_local_experts, 0, stream>>>(",
      "        static_cast<int32_t*>(expert_offsets.data_ptr()),",
      "        static_cast<int32_t*>(problem_sizes1.data_ptr()),",
      "        static_cast<int32_t*>(problem_sizes2.data_ptr()),",
      "        static_cast<const int32_t*>(expert_num_tokens.data_ptr()), padded_m, n,",
      "        k);",
      "  } else {",
      "    compute_pplx_data<true><<<1, num_local_experts, 0, stream>>>(",
      "        static_cast<int32_t*>(expert_offsets.data_ptr()),",
      "        static_cast<int32_t*>(problem_sizes1.data_ptr()),",
      "        static_cast<int32_t*>(problem_sizes2.data_ptr()),",
      "        static_cast<const int32_t*>(expert_num_tokens.data_ptr()), padded_m, n,",
      "        k);",
      "  }",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/cutlass_w8a8/moe/get_group_starts.cuh",
    "source": [
      "#pragma once",
      "",
      "#include <cuda.h>",
      "#include <torch/all.h>",
      "#include <c10/cuda/CUDAStream.h>",
      "",
      "#include \"core/scalar_type.hpp\"",
      "#include \"cutlass/bfloat16.h\"",
      "#include \"cutlass/float8.h\"",
      "",
      "template <typename ElementAB, typename ElementC, typename ElementAccumulator>",
      "__global__ void get_group_gemm_starts(",
      "    int64_t* expert_offsets, ElementAB** a_offsets, ElementAB** b_offsets,",
      "    ElementC** out_offsets, ElementAccumulator** a_scales_offsets,",
      "    ElementAccumulator** b_scales_offsets, ElementAB* a_base_as_int,",
      "    ElementAB* b_base_as_int, ElementC* out_base_as_int,",
      "    ElementAccumulator* a_scales_base_as_int,",
      "    ElementAccumulator* b_scales_base_as_int, int64_t n, int64_t k,",
      "    bool per_act_token, bool per_out_ch) {",
      "  int expert_id = threadIdx.x;",
      "",
      "  int64_t expert_offset = expert_offsets[expert_id];",
      "",
      "  a_offsets[expert_id] = a_base_as_int + expert_offset * k;",
      "  b_offsets[expert_id] = b_base_as_int + expert_id * k * n;",
      "  out_offsets[expert_id] = out_base_as_int + expert_offset * n;",
      "  a_scales_offsets[expert_id] =",
      "      a_scales_base_as_int + (per_act_token ? expert_offset : 0);",
      "  b_scales_offsets[expert_id] =",
      "      b_scales_base_as_int + (per_out_ch ? n * expert_id : expert_id);",
      "}",
      "",
      "#define __CALL_GET_STARTS_KERNEL(TENSOR_C_TYPE, C_TYPE)                    \\",
      "  else if (out_tensors.dtype() == TENSOR_C_TYPE) {                         \\",
      "    get_group_gemm_starts<cutlass::float_e4m3_t, C_TYPE, float>            \\",
      "        <<<1, num_experts, 0, stream>>>(                                   \\",
      "            static_cast<int64_t*>(expert_offsets.data_ptr()),              \\",
      "            static_cast<cutlass::float_e4m3_t**>(a_ptrs.data_ptr()),       \\",
      "            static_cast<cutlass::float_e4m3_t**>(b_ptrs.data_ptr()),       \\",
      "            static_cast<C_TYPE**>(out_ptrs.data_ptr()),                    \\",
      "            static_cast<float**>(a_scales_ptrs.data_ptr()),                \\",
      "            static_cast<float**>(b_scales_ptrs.data_ptr()),                \\",
      "            static_cast<cutlass::float_e4m3_t*>(a_tensors.data_ptr()),     \\",
      "            static_cast<cutlass::float_e4m3_t*>(b_tensors.data_ptr()),     \\",
      "            static_cast<C_TYPE*>(out_tensors.data_ptr()),                  \\",
      "            static_cast<float*>(a_scales.data_ptr()),                      \\",
      "            static_cast<float*>(b_scales.data_ptr()), out_tensors.size(1), \\",
      "            a_tensors.size(1), per_act_token, per_out_ch);                 \\",
      "  }",
      "",
      "namespace {",
      "",
      "void run_get_group_gemm_starts(",
      "    torch::Tensor const& expert_offsets, torch::Tensor& a_ptrs,",
      "    torch::Tensor& b_ptrs, torch::Tensor& out_ptrs,",
      "    torch::Tensor& a_scales_ptrs, torch::Tensor& b_scales_ptrs,",
      "    torch::Tensor const& a_tensors, torch::Tensor const& b_tensors,",
      "    torch::Tensor& out_tensors, torch::Tensor const& a_scales,",
      "    torch::Tensor const& b_scales) {",
      "  TORCH_CHECK(a_tensors.dtype() == torch::kFloat8_e4m3fn);",
      "  TORCH_CHECK(b_tensors.dtype() == torch::kFloat8_e4m3fn);",
      "  TORCH_CHECK(a_scales.dtype() == torch::kFloat32);",
      "  TORCH_CHECK(b_scales.dtype() == torch::kFloat32);",
      "  // expect int64_t to avoid overflow during offset calculations",
      "  TORCH_CHECK(expert_offsets.dtype() == torch::kInt64);",
      "",
      "  int num_experts = static_cast<int>(expert_offsets.size(0));",
      "  bool per_act_token = a_scales.numel() != 1;",
      "  bool per_out_ch = b_scales.numel() != num_experts;",
      "",
      "  auto stream = at::cuda::getCurrentCUDAStream(a_tensors.device().index());",
      "",
      "  if (false) {",
      "  }",
      "  __CALL_GET_STARTS_KERNEL(torch::kBFloat16, cutlass::bfloat16_t)",
      "  __CALL_GET_STARTS_KERNEL(torch::kFloat16, half)",
      "  else {",
      "    TORCH_CHECK(false, \"Invalid output type (must be float16 or bfloat16)\");",
      "  }",
      "}",
      "",
      "}  // namespace"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fp4/nvfp4_scaled_mm_kernels.cu",
    "source": [
      "/*",
      " * Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "",
      "#include <torch/all.h>",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#include \"cutlass_extensions/common.hpp\"",
      "",
      "#include \"cutlass/cutlass.h\"",
      "",
      "#include \"cutlass/gemm/collective/collective_builder.hpp\"",
      "#include \"cutlass/epilogue/collective/collective_builder.hpp\"",
      "#include \"cutlass/gemm/device/gemm_universal_adapter.h\"",
      "#include \"cutlass/gemm/kernel/gemm_universal.hpp\"",
      "",
      "#include \"cutlass/util/packed_stride.hpp\"",
      "",
      "#include \"core/math.hpp\"",
      "",
      "using namespace cute;",
      "",
      "#if defined(CUTLASS_ARCH_MMA_SM100_SUPPORTED)",
      "",
      "// Configuration for M in (256, inf)",
      "struct sm100_fp4_config_default {",
      "  using KernelSchedule = cutlass::gemm::collective::KernelScheduleAuto;",
      "  using EpilogueSchedule = cutlass::epilogue::collective::EpilogueScheduleAuto;",
      "  using TileShape = Shape<_256, _256, _256>;",
      "  using ClusterShape = Shape<_2, _1, _1>;",
      "  using PerSmTileShape_MNK = Shape<_128, _256, _256>;",
      "};",
      "",
      "// Configuration for M in (16, 256]",
      "struct sm100_fp4_config_M256 {",
      "  using KernelSchedule = cutlass::gemm::collective::KernelScheduleAuto;",
      "  using EpilogueSchedule = cutlass::epilogue::collective::EpilogueScheduleAuto;",
      "  using TileShape = Shape<_256, _128, _256>;",
      "  using ClusterShape = Shape<_2, _1, _1>;",
      "  using PerSmTileShape_MNK = Shape<_128, _128, _256>;",
      "};",
      "",
      "// Configuration for M in [1, 16]",
      "struct sm100_fp4_config_M16 {",
      "  using KernelSchedule = cutlass::gemm::collective::KernelScheduleAuto;",
      "  using EpilogueSchedule = cutlass::epilogue::collective::EpilogueScheduleAuto;",
      "  using TileShape = Shape<_128, _128, _256>;",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "  using PerSmTileShape_MNK = Shape<_128, _128, _256>;",
      "};",
      "",
      "template <typename Config, typename OutType>",
      "struct Fp4GemmSm100 {",
      "  // A matrix configuration",
      "  using ElementA = cutlass::nv_float4_t<cutlass::float_e2m1_t>;",
      "  using LayoutATag = cutlass::layout::RowMajor;",
      "  static constexpr int AlignmentA = 32;",
      "",
      "  // B matrix configuration",
      "  using ElementB = cutlass::nv_float4_t<cutlass::float_e2m1_t>;",
      "  using LayoutBTag = cutlass::layout::ColumnMajor;",
      "  static constexpr int AlignmentB = 32;",
      "",
      "  // C/D matrix configuration",
      "  using ElementD = OutType;",
      "  using ElementC = OutType;",
      "  using LayoutCTag = cutlass::layout::RowMajor;",
      "  using LayoutDTag = cutlass::layout::RowMajor;",
      "  static constexpr int AlignmentD = 128 / cutlass::sizeof_bits<ElementD>::value;",
      "  static constexpr int AlignmentC = 128 / cutlass::sizeof_bits<ElementC>::value;",
      "",
      "  // Kernel functional config",
      "  using ElementAccumulator = float;",
      "  using ArchTag = cutlass::arch::Sm100;",
      "  using OperatorClass = cutlass::arch::OpClassBlockScaledTensorOp;",
      "",
      "  // Use config's tile shapes",
      "  using MmaTileShape = typename Config::TileShape;",
      "  using ClusterShape = typename Config::ClusterShape;",
      "  using PerSmTileShape_MNK = typename Config::PerSmTileShape_MNK;",
      "",
      "  using CollectiveEpilogue =",
      "      typename cutlass::epilogue::collective::CollectiveBuilder<",
      "          ArchTag, OperatorClass, PerSmTileShape_MNK, ClusterShape,",
      "          cutlass::epilogue::collective::EpilogueTileAuto, ElementAccumulator,",
      "          ElementAccumulator, ElementC, LayoutCTag, AlignmentC, ElementD,",
      "          LayoutDTag, AlignmentD,",
      "          cutlass::epilogue::collective::EpilogueScheduleAuto>::CollectiveOp;",
      "",
      "  using CollectiveMainloop =",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          ArchTag, OperatorClass, ElementA, LayoutATag, AlignmentA, ElementB,",
      "          LayoutBTag, AlignmentB, ElementAccumulator, MmaTileShape,",
      "          ClusterShape,",
      "          cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(",
      "              sizeof(typename CollectiveEpilogue::SharedStorage))>,",
      "          cutlass::gemm::collective::KernelScheduleAuto>::CollectiveOp;",
      "",
      "  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<",
      "      Shape<int, int, int, int>, CollectiveMainloop, CollectiveEpilogue, void>;",
      "  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;",
      "  using StrideA = typename Gemm::GemmKernel::StrideA;",
      "  using LayoutA = decltype(cute::make_layout(make_shape(0, 0, 0), StrideA{}));",
      "  using LayoutSFA = typename Gemm::GemmKernel::CollectiveMainloop::LayoutSFA;",
      "  using StrideB = typename Gemm::GemmKernel::StrideB;",
      "  using LayoutB = decltype(cute::make_layout(make_shape(0, 0, 0), StrideB{}));",
      "  using LayoutSFB = typename Gemm::GemmKernel::CollectiveMainloop::LayoutSFB;",
      "  using StrideC = typename Gemm::GemmKernel::StrideC;",
      "  using LayoutC = decltype(cute::make_layout(make_shape(0, 0, 0), StrideC{}));",
      "  using StrideD = typename Gemm::GemmKernel::StrideD;",
      "  using LayoutD = decltype(cute::make_layout(make_shape(0, 0, 0), StrideD{}));",
      "};",
      "",
      "template <typename Config>",
      "typename Config::Gemm::Arguments args_from_options(",
      "    at::Tensor& D, at::Tensor const& A, at::Tensor const& B,",
      "    at::Tensor const& A_sf, at::Tensor const& B_sf, at::Tensor const& alpha,",
      "    int64_t M, int64_t N, int64_t K) {",
      "  using ElementA = typename Config::Gemm::ElementA;",
      "  using ElementB = typename Config::Gemm::ElementB;",
      "  using ElementSFA = cutlass::float_ue4m3_t;",
      "  using ElementSFB = cutlass::float_ue4m3_t;",
      "  using ElementD = typename Config::Gemm::ElementD;",
      "  using ElementCompute = float;",
      "  using StrideA = typename Config::StrideA;",
      "  using StrideB = typename Config::StrideB;",
      "  using StrideD = typename Config::StrideD;",
      "  using Sm100BlkScaledConfig = typename Config::Gemm::GemmKernel::",
      "      CollectiveMainloop::Sm1xxBlkScaledConfig;",
      "",
      "  int m = static_cast<int>(M);",
      "  int n = static_cast<int>(N);",
      "  int k = static_cast<int>(K);",
      "  auto stride_A = cutlass::make_cute_packed_stride(StrideA{}, {m, k, 1});",
      "  auto stride_B = cutlass::make_cute_packed_stride(StrideB{}, {n, k, 1});",
      "  auto stride_D = cutlass::make_cute_packed_stride(StrideD{}, {m, n, 1});",
      "",
      "  auto layout_SFA = Sm100BlkScaledConfig::tile_atom_to_shape_SFA(",
      "      cute::make_shape(m, n, k, 1));",
      "  auto layout_SFB = Sm100BlkScaledConfig::tile_atom_to_shape_SFB(",
      "      cute::make_shape(m, n, k, 1));",
      "",
      "  typename Config::Gemm::Arguments arguments{",
      "      cutlass::gemm::GemmUniversalMode::kGemm,",
      "      {m, n, k, 1},",
      "      {// Mainloop arguments",
      "       static_cast<ElementA const*>(A.data_ptr()), stride_A,",
      "       static_cast<ElementB const*>(B.data_ptr()), stride_B,",
      "       static_cast<ElementSFA const*>(A_sf.data_ptr()), layout_SFA,",
      "       static_cast<ElementSFB const*>(B_sf.data_ptr()), layout_SFB},",
      "      {     // Epilogue arguments",
      "       {},  // epilogue.thread",
      "       static_cast<ElementD const*>(D.data_ptr()),",
      "       stride_D,",
      "       static_cast<ElementD*>(D.data_ptr()),",
      "       stride_D}};",
      "  auto& fusion_args = arguments.epilogue.thread;",
      "  fusion_args.alpha_ptr = static_cast<ElementCompute const*>(alpha.data_ptr());",
      "  return arguments;",
      "}",
      "",
      "template <typename Config>",
      "void runGemm(at::Tensor& D, at::Tensor const& A, at::Tensor const& B,",
      "             at::Tensor const& A_sf, at::Tensor const& B_sf,",
      "             at::Tensor const& alpha, int64_t m, int64_t n, int64_t k,",
      "             cudaStream_t stream) {",
      "  typename Config::Gemm gemm;",
      "",
      "  auto arguments =",
      "      args_from_options<Config>(D, A, B, A_sf, B_sf, alpha, m, n, k);",
      "",
      "  size_t workspace_size = Config::Gemm::get_workspace_size(arguments);",
      "  auto const workspace_options =",
      "      torch::TensorOptions().dtype(torch::kUInt8).device(A.device());",
      "  auto workspace = torch::empty(workspace_size, workspace_options);",
      "",
      "  CUTLASS_CHECK(gemm.can_implement(arguments));",
      "",
      "  CUTLASS_CHECK(gemm.initialize(arguments, workspace.data_ptr(), stream));",
      "",
      "  CUTLASS_CHECK(gemm.run(arguments, workspace.data_ptr(), stream));",
      "}",
      "",
      "// Dispatch function to select appropriate config based on M",
      "template <typename OutType>",
      "void cutlass_fp4_gemm_dispatch(torch::Tensor& D, torch::Tensor const& A,",
      "                               torch::Tensor const& B,",
      "                               torch::Tensor const& A_sf,",
      "                               torch::Tensor const& B_sf,",
      "                               torch::Tensor const& alpha, int64_t m, int64_t n,",
      "                               int64_t k, cudaStream_t stream) {",
      "  uint32_t const mp2 = std::max(static_cast<uint32_t>(16), next_pow_2(m));",
      "",
      "  if (mp2 <= 16) {",
      "    // m in [1, 16]",
      "    runGemm<Fp4GemmSm100<sm100_fp4_config_M16, OutType>>(",
      "        D, A, B, A_sf, B_sf, alpha, m, n, k, stream);",
      "  } else if (mp2 <= 256) {",
      "    // m in (16, 256]",
      "    runGemm<Fp4GemmSm100<sm100_fp4_config_M256, OutType>>(",
      "        D, A, B, A_sf, B_sf, alpha, m, n, k, stream);",
      "  } else {",
      "    // m in (256, inf)",
      "    runGemm<Fp4GemmSm100<sm100_fp4_config_default, OutType>>(",
      "        D, A, B, A_sf, B_sf, alpha, m, n, k, stream);",
      "  }",
      "}",
      "",
      "#else",
      "template <typename OutType>",
      "void cutlass_fp4_gemm_dispatch(torch::Tensor& D, torch::Tensor const& A,",
      "                               torch::Tensor const& B,",
      "                               torch::Tensor const& A_sf,",
      "                               torch::Tensor const& B_sf,",
      "                               torch::Tensor const& alpha, int64_t m, int64_t n,",
      "                               int64_t k, cudaStream_t stream) {",
      "  TORCH_CHECK(false,",
      "              \"Unsupported CUTLASS version. Set VLLM_CUTLASS_SRC_DIR to \"",
      "              \"a CUTLASS 3.8 source directory to enable support.\");",
      "}",
      "#endif  // defined(CUTLASS_ARCH_MMA_SM100_SUPPORTED)",
      "",
      "#define CHECK_TYPE(x, st, m) \\",
      "  TORCH_CHECK(x.scalar_type() == st, \": Inconsistency of Tensor type:\", m)",
      "#define CHECK_TH_CUDA(x, m) \\",
      "  TORCH_CHECK(x.is_cuda(), m, \": must be a CUDA tensor\")",
      "#define CHECK_CONTIGUOUS(x, m) \\",
      "  TORCH_CHECK(x.is_contiguous(), m, \": must be contiguous\")",
      "#define CHECK_INPUT(x, st, m) \\",
      "  CHECK_TH_CUDA(x, m);        \\",
      "  CHECK_CONTIGUOUS(x, m);     \\",
      "  CHECK_TYPE(x, st, m)",
      "",
      "constexpr auto FLOAT4_E2M1X2 = at::ScalarType::Byte;",
      "constexpr auto SF_DTYPE = at::ScalarType::Float8_e4m3fn;",
      "",
      "void cutlass_scaled_fp4_mm_sm100a(torch::Tensor& D, torch::Tensor const& A,",
      "                                  torch::Tensor const& B,",
      "                                  torch::Tensor const& A_sf,",
      "                                  torch::Tensor const& B_sf,",
      "                                  torch::Tensor const& alpha) {",
      "  CHECK_INPUT(A, FLOAT4_E2M1X2, \"a\");",
      "  CHECK_INPUT(B, FLOAT4_E2M1X2, \"b\");",
      "",
      "  CHECK_INPUT(A_sf, SF_DTYPE, \"scale_a\");",
      "  CHECK_INPUT(B_sf, SF_DTYPE, \"scale_b\");",
      "",
      "  CHECK_INPUT(alpha, at::ScalarType::Float, \"alpha\");",
      "",
      "  TORCH_CHECK(A.dim() == 2, \"a must be a matrix\");",
      "  TORCH_CHECK(B.dim() == 2, \"b must be a matrix\");",
      "  TORCH_CHECK(A.sizes()[1] == B.sizes()[1],",
      "              \"a and b shapes cannot be multiplied (\", A.sizes()[0], \"x\",",
      "              A.sizes()[1], \" and \", B.sizes()[0], \"x\", B.sizes()[1], \")\");",
      "",
      "  auto const m = A.sizes()[0];",
      "  auto const n = B.sizes()[0];",
      "  auto const k = A.sizes()[1] * 2;",
      "",
      "  constexpr int alignment = 32;",
      "  TORCH_CHECK(k % alignment == 0, \"Expected k to be divisible by \", alignment,",
      "              \", but got a shape: (\", A.sizes()[0], \"x\", A.sizes()[1],",
      "              \"), k: \", k, \".\");",
      "  TORCH_CHECK(n % alignment == 0, \"Expected n to be divisible by \", alignment,",
      "              \", but got b shape: (\", B.sizes()[0], \"x\", B.sizes()[1], \").\");",
      "",
      "  auto round_up = [](int x, int y) { return (x + y - 1) / y * y; };",
      "  int rounded_m = round_up(m, 128);",
      "  int rounded_n = round_up(n, 128);",
      "  // Since k is divisible by 32 (alignment), k / 16 is guaranteed to be an",
      "  // integer.",
      "  int rounded_k = round_up(k / 16, 4);",
      "",
      "  TORCH_CHECK(A_sf.dim() == 2, \"scale_a must be a matrix\");",
      "  TORCH_CHECK(B_sf.dim() == 2, \"scale_b must be a matrix\");",
      "  TORCH_CHECK(A_sf.sizes()[1] == B_sf.sizes()[1],",
      "              \"scale_a and scale_b shapes cannot be multiplied (\",",
      "              A_sf.sizes()[0], \"x\", A_sf.sizes()[1], \" and \", B_sf.sizes()[0],",
      "              \"x\", B_sf.sizes()[1], \")\");",
      "  TORCH_CHECK(A_sf.sizes()[0] == rounded_m && A_sf.sizes()[1] == rounded_k,",
      "              \"scale_a must be padded and swizzled to a shape (\", rounded_m,",
      "              \"x\", rounded_k, \"), but got a shape (\", A_sf.sizes()[0], \"x\",",
      "              A_sf.sizes()[1], \")\");",
      "  TORCH_CHECK(B_sf.sizes()[0] == rounded_n && B_sf.sizes()[1] == rounded_k,",
      "              \"scale_b must be padded and swizzled to a shape (\", rounded_n,",
      "              \"x\", rounded_k, \"), but got a shape (\", B_sf.sizes()[0], \"x\",",
      "              B_sf.sizes()[1], \")\");",
      "",
      "  auto out_dtype = D.dtype();",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(A));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream(A.get_device());",
      "",
      "  if (out_dtype == at::ScalarType::Half) {",
      "    cutlass_fp4_gemm_dispatch<cutlass::half_t>(D, A, B, A_sf, B_sf, alpha, m, n,",
      "                                               k, stream);",
      "  } else if (out_dtype == at::ScalarType::BFloat16) {",
      "    cutlass_fp4_gemm_dispatch<cutlass::bfloat16_t>(D, A, B, A_sf, B_sf, alpha,",
      "                                                   m, n, k, stream);",
      "  } else {",
      "    TORCH_CHECK(false, \"Unsupported output data type of nvfp4 mm (\", out_dtype,",
      "                \")\");",
      "  }",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fp4/nvfp4_blockwise_moe_kernel.cu",
    "source": [
      "/*",
      " * Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "",
      "#include <torch/all.h>",
      "#include <cutlass/arch/arch.h>",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <c10/cuda/CUDAStream.h>",
      "",
      "#include \"cute/tensor.hpp\"",
      "#include \"cutlass/tensor_ref.h\"",
      "#include \"cutlass/epilogue/collective/default_epilogue.hpp\"",
      "#include \"cutlass/epilogue/thread/linear_combination.h\"",
      "#include \"cutlass/gemm/dispatch_policy.hpp\"",
      "#include \"cutlass/gemm/group_array_problem_shape.hpp\"",
      "#include \"cutlass/gemm/collective/collective_builder.hpp\"",
      "#include \"cutlass/epilogue/collective/collective_builder.hpp\"",
      "#include \"cutlass/gemm/device/gemm_universal_adapter.h\"",
      "#include \"cutlass/gemm/kernel/gemm_universal.hpp\"",
      "",
      "#include \"cutlass/util/command_line.h\"",
      "#include \"cutlass/util/distribution.h\"",
      "#include \"cutlass/util/host_tensor.h\"",
      "#include \"cutlass/util/packed_stride.hpp\"",
      "#include \"cutlass/util/tensor_view_io.h\"",
      "#include \"cutlass/util/reference/device/gemm.h\"",
      "#include \"cutlass/util/reference/device/tensor_compare.h\"",
      "#include \"cutlass/util/reference/host/tensor_fill.h\"",
      "#include \"cutlass/util/reference/host/gett.hpp\"",
      "#include \"cutlass/util/reference/host/tensor_norm.h\"",
      "#include \"cutlass/util/reference/host/tensor_compare.h\"",
      "#include <cassert>",
      "",
      "using namespace cute;",
      "",
      "template <typename ElementAB, typename ElementC, typename ElementSF,",
      "          typename ElementAccumulator, typename LayoutSFA, typename LayoutSFB,",
      "          typename ScaleConfig>",
      "__global__ void __get_group_gemm_starts(",
      "    ElementAB** a_offsets, ElementAB** b_offsets, ElementC** out_offsets,",
      "    ElementSF** a_scales_offsets, ElementSF** b_scales_offsets,",
      "    ElementAccumulator** alpha_offsets, LayoutSFA* layout_sfa_base_as_int,",
      "    LayoutSFB* layout_sfb_base_as_int, ElementAB* a_base_as_int,",
      "    ElementAB* b_base_as_int, ElementC* out_base_as_int,",
      "    ElementSF* a_scales_base_as_int, ElementSF* b_scales_base_as_int,",
      "    ElementAccumulator* alphas_base_as_int, const int32_t* expert_offsets,",
      "    const int32_t* sf_offsets, const int32_t* problem_sizes_as_shapes,",
      "    const int K, const int N) {",
      "  int64_t expert_id = threadIdx.x;",
      "  if (expert_id >= gridDim.x * blockDim.x) {",
      "    return;",
      "  }",
      "  // Originally int32_t but upcasting to int64_t to avoid overflow",
      "  // during offset calculations",
      "  int64_t expert_offset = static_cast<int64_t>(expert_offsets[expert_id]);",
      "  int64_t sf_offset = static_cast<int64_t>(sf_offsets[expert_id]);",
      "  // size for block in block scale.",
      "  int64_t group_size = 16;",
      "  int64_t m = static_cast<int64_t>(problem_sizes_as_shapes[expert_id * 3]);",
      "  int64_t n = static_cast<int64_t>(problem_sizes_as_shapes[expert_id * 3 + 1]);",
      "  int64_t k = static_cast<int64_t>(problem_sizes_as_shapes[expert_id * 3 + 2]);",
      "  assert((m >= 0 && n == N && k == K && k % 2 == 0) &&",
      "         \"unexpected problem sizes\");",
      "",
      "  int64_t half_k = static_cast<int64_t>(k / 2);",
      "  int64_t group_k = static_cast<int64_t>(k / group_size);",
      "  // Shape of A as uint8/byte = [M, K // 2]",
      "  // Shape of B as uint8/byte = [E, N, K // 2]",
      "  a_offsets[expert_id] = a_base_as_int + expert_offset * half_k;",
      "",
      "  b_offsets[expert_id] = b_base_as_int + expert_id * n * half_k;",
      "  // Shape of C = [M, N]",
      "  out_offsets[expert_id] = out_base_as_int + expert_offset * n;",
      "  // Shape of a_scale = [sum(sf_sizes), K // group_size]",
      "  a_scales_offsets[expert_id] = a_scales_base_as_int + sf_offset * group_k;",
      "",
      "  assert((reinterpret_cast<uintptr_t>(a_scales_offsets[expert_id]) % 128) ==",
      "             0 &&",
      "         \"TMA requires 128-byte alignment\");",
      "",
      "  // Shape of B scale = [E, N, K // group_size]",
      "  b_scales_offsets[expert_id] = b_scales_base_as_int + expert_id * n * group_k;",
      "  assert((reinterpret_cast<uintptr_t>(b_scales_offsets[expert_id]) % 128) ==",
      "             0 &&",
      "         \"TMA requires 128-byte alignment\");",
      "  // Shape of alpha = [E]",
      "  alpha_offsets[expert_id] = alphas_base_as_int + expert_id;",
      "",
      "  LayoutSFA* layout_sfa_ptr = layout_sfa_base_as_int + expert_id;",
      "  LayoutSFB* layout_sfb_ptr = layout_sfb_base_as_int + expert_id;",
      "",
      "  *layout_sfa_ptr = ScaleConfig::tile_atom_to_shape_SFA(cute::make_shape(",
      "      static_cast<int>(m), static_cast<int>(n), static_cast<int>(k), 1));",
      "  *layout_sfb_ptr = ScaleConfig::tile_atom_to_shape_SFB(cute::make_shape(",
      "      static_cast<int>(m), static_cast<int>(n), static_cast<int>(k), 1));",
      "}",
      "",
      "#define __CALL_GET_STARTS_KERNEL_BLOCKSCALE(ELEMENT_AB_TYPE, SF_TYPE,         \\",
      "                                            TENSOR_C_TYPE, C_TYPE, LayoutSFA, \\",
      "                                            LayoutSFB, ScaleConfig)           \\",
      "  else if (out_tensors.dtype() == TENSOR_C_TYPE) {                            \\",
      "    __get_group_gemm_starts<ELEMENT_AB_TYPE, C_TYPE, SF_TYPE, float,          \\",
      "                            LayoutSFA, LayoutSFB, ScaleConfig>                \\",
      "        <<<1, num_experts, 0, stream>>>(                                      \\",
      "            static_cast<ELEMENT_AB_TYPE**>(a_starts.data_ptr()),              \\",
      "            static_cast<ELEMENT_AB_TYPE**>(b_starts.data_ptr()),              \\",
      "            static_cast<C_TYPE**>(out_starts.data_ptr()),                     \\",
      "            static_cast<SF_TYPE**>(a_scales_starts.data_ptr()),               \\",
      "            static_cast<SF_TYPE**>(b_scales_starts.data_ptr()),               \\",
      "            static_cast<float**>(alpha_starts.data_ptr()),                    \\",
      "            reinterpret_cast<LayoutSFA*>(layout_sfa.data_ptr()),              \\",
      "            reinterpret_cast<LayoutSFB*>(layout_sfb.data_ptr()),              \\",
      "            static_cast<ELEMENT_AB_TYPE*>(a_tensors.data_ptr()),              \\",
      "            static_cast<ELEMENT_AB_TYPE*>(b_tensors.data_ptr()),              \\",
      "            static_cast<C_TYPE*>(out_tensors.data_ptr()),                     \\",
      "            static_cast<SF_TYPE*>(a_scales.data_ptr()),                       \\",
      "            static_cast<SF_TYPE*>(b_scales.data_ptr()),                       \\",
      "            static_cast<float*>(alphas.data_ptr()),                           \\",
      "            static_cast<int32_t*>(expert_offsets.data_ptr()),                 \\",
      "            static_cast<int32_t*>(sf_offsets.data_ptr()),                     \\",
      "            static_cast<int32_t*>(problem_sizes.data_ptr()), K, N);           \\",
      "  }",
      "",
      "template <typename LayoutSFA, typename LayoutSFB, typename ScaleConfig>",
      "void run_get_group_gemm_starts(",
      "    const torch::Tensor& a_starts, const torch::Tensor& b_starts,",
      "    const torch::Tensor& out_starts, const torch::Tensor& a_scales_starts,",
      "    const torch::Tensor& b_scales_starts, const torch::Tensor& alpha_starts,",
      "    const torch::Tensor& layout_sfa, const torch::Tensor& layout_sfb,",
      "    /*these are used for their base addresses*/",
      "    torch::Tensor const& a_tensors, torch::Tensor const& b_tensors,",
      "    torch::Tensor const& out_tensors, torch::Tensor const& a_scales,",
      "    torch::Tensor const& b_scales, torch::Tensor const& alphas,",
      "    torch::Tensor const& expert_offsets, torch::Tensor const& sf_offsets,",
      "    torch::Tensor const& problem_sizes, int M, int N, int K) {",
      "  int num_experts = (int)expert_offsets.size(0);",
      "  auto stream = at::cuda::getCurrentCUDAStream(a_tensors.device().index());",
      "",
      "  TORCH_CHECK(out_tensors.size(1) == N,",
      "              \"Output tensor shape doesn't match expected shape\");",
      "  TORCH_CHECK(K / 2 == b_tensors.size(2),",
      "              \"b_tensors(dim = 2) and a_tensors(dim = 1) trailing\"",
      "              \" dimension must match\");",
      "  if (false) {",
      "  }",
      "  //(ELEMENT_AB_TYPE, BS_TYPE, TENSOR_C_TYPE, C_TYPE, LayoutSFA, LayoutSFB,",
      "  // ScaleConfig)",
      "  __CALL_GET_STARTS_KERNEL_BLOCKSCALE(",
      "      cutlass::float_e2m1_t, cutlass::float_ue4m3_t, torch::kBFloat16,",
      "      cutlass::bfloat16_t, LayoutSFA, LayoutSFB, ScaleConfig)",
      "  __CALL_GET_STARTS_KERNEL_BLOCKSCALE(cutlass::float_e2m1_t,",
      "                                      cutlass::float_ue4m3_t, torch::kFloat16,",
      "                                      half, LayoutSFA, LayoutSFB, ScaleConfig)",
      "  else {",
      "    TORCH_CHECK(false, \"Invalid output type (must be float16 or bfloat16)\");",
      "  }",
      "}",
      "",
      "template <typename OutType>",
      "void run_fp4_blockwise_scaled_group_mm(",
      "    torch::Tensor& output, const torch::Tensor& a, const torch::Tensor& b,",
      "    const torch::Tensor& a_blockscale, const torch::Tensor& b_blockscales,",
      "    const torch::Tensor& alphas, const torch::Tensor& problem_sizes,",
      "    const torch::Tensor& expert_offsets, const torch::Tensor& sf_offsets, int M,",
      "    int N, int K) {",
      "  using ProblemShape =",
      "      cutlass::gemm::GroupProblemShape<Shape<int32_t, int32_t, int32_t>>;",
      "  using ElementType = cutlass::float_e2m1_t;",
      "  using ElementSFType = cutlass::float_ue4m3_t;",
      "  using ElementA = cutlass::nv_float4_t<cutlass::float_e2m1_t>;",
      "  using ElementB = cutlass::nv_float4_t<cutlass::float_e2m1_t>;",
      "",
      "  using ElementC = OutType;",
      "  using ElementD = ElementC;",
      "  using ElementAccumulator = float;",
      "  // Layout definitions",
      "  using LayoutA = cutlass::layout::RowMajor;",
      "  using LayoutB = cutlass::layout::ColumnMajor;",
      "  using LayoutC = cutlass::layout::RowMajor;",
      "  using LayoutD = LayoutC;",
      "",
      "  // Alignment constraints",
      "  static constexpr int AlignmentA = 32;",
      "  static constexpr int AlignmentB = 32;",
      "  static constexpr int AlignmentC = 128 / cutlass::sizeof_bits<ElementC>::value;",
      "  static constexpr int AlignmentD = 128 / cutlass::sizeof_bits<ElementD>::value;",
      "",
      "  // Architecture definitions",
      "  using ArchTag = cutlass::arch::Sm100;",
      "  using EpilogueOperatorClass =",
      "      cutlass::arch::OpClassTensorOp;  // Epilogue Operator class tag",
      "  using MainloopOperatorClass =",
      "      cutlass::arch::OpClassBlockScaledTensorOp;  // Mainloop Operator class tag",
      "  using StageCountType =",
      "      cutlass::gemm::collective::StageCountAuto;  // Stage count maximized based",
      "                                                  // on the tile size",
      "",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "  struct MMA1SMConfig {",
      "    using MmaTileShape = Shape<_128, _128, _128>;",
      "    using KernelSchedule = cutlass::gemm::",
      "        KernelPtrArrayTmaWarpSpecialized1SmNvf4Sm100;  // Kernel to launch",
      "    using EpilogueSchedule =",
      "        cutlass::epilogue::PtrArrayTmaWarpSpecialized1Sm;  // Epilogue to launch",
      "  };",
      "",
      "  using CollectiveEpilogue =",
      "      typename cutlass::epilogue::collective::CollectiveBuilder<",
      "          ArchTag, EpilogueOperatorClass, typename MMA1SMConfig::MmaTileShape,",
      "          ClusterShape, Shape<_128, _64>, ElementAccumulator,",
      "          ElementAccumulator, ElementC, LayoutC*, AlignmentC, ElementD,",
      "          LayoutC*, AlignmentD,",
      "          typename MMA1SMConfig::EpilogueSchedule>::CollectiveOp;",
      "",
      "  using CollectiveMainloop =",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          ArchTag, MainloopOperatorClass, ElementA, LayoutA*, AlignmentA,",
      "          ElementB, LayoutB*, AlignmentB, ElementAccumulator,",
      "          typename MMA1SMConfig::MmaTileShape, ClusterShape,",
      "          cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(",
      "              sizeof(typename CollectiveEpilogue::SharedStorage))>,",
      "          typename MMA1SMConfig::KernelSchedule>::CollectiveOp;",
      "",
      "  using GemmKernel =",
      "      cutlass::gemm::kernel::GemmUniversal<ProblemShape, CollectiveMainloop,",
      "                                           CollectiveEpilogue>;",
      "",
      "  using Gemm1SM = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;",
      "  using Gemm = Gemm1SM;",
      "  using StrideA = typename Gemm::GemmKernel::InternalStrideA;",
      "  using StrideB = typename Gemm::GemmKernel::InternalStrideB;",
      "  using StrideC = typename Gemm::GemmKernel::InternalStrideC;",
      "  using StrideD = typename Gemm::GemmKernel::InternalStrideD;",
      "",
      "  using LayoutSFA =",
      "      typename Gemm::GemmKernel::CollectiveMainloop::InternalLayoutSFA;",
      "  using LayoutSFB =",
      "      typename Gemm::GemmKernel::CollectiveMainloop::InternalLayoutSFB;",
      "  using ScaleConfig =",
      "      typename Gemm::GemmKernel::CollectiveMainloop::Sm1xxBlkScaledConfig;",
      "",
      "  using UnderlyingProblemShape = ProblemShape::UnderlyingProblemShape;",
      "  int num_experts = static_cast<int>(expert_offsets.size(0));",
      "  auto options_int =",
      "      torch::TensorOptions().dtype(torch::kInt64).device(a.device());",
      "",
      "  torch::Tensor a_ptrs = torch::empty(num_experts, options_int);",
      "  torch::Tensor b_ptrs = torch::empty(num_experts, options_int);",
      "  torch::Tensor out_ptrs = torch::empty(num_experts, options_int);",
      "  torch::Tensor a_scales_ptrs = torch::empty(num_experts, options_int);",
      "  torch::Tensor b_scales_ptrs = torch::empty(num_experts, options_int);",
      "  torch::Tensor alpha_ptrs = torch::empty(num_experts, options_int);",
      "  torch::Tensor layout_sfa = torch::empty({num_experts, 5}, options_int);",
      "  torch::Tensor layout_sfb = torch::empty({num_experts, 5}, options_int);",
      "  torch::Tensor c_strides1 =",
      "      torch::full({num_experts}, output.stride(0), options_int);",
      "  torch::Tensor a_strides1 =",
      "      torch::full({num_experts}, a.stride(0) * 2, options_int);",
      "  torch::Tensor b_strides1 =",
      "      torch::full({num_experts}, b.stride(1) * 2, options_int);",
      "",
      "  run_get_group_gemm_starts<LayoutSFA, LayoutSFB, ScaleConfig>(",
      "      a_ptrs, b_ptrs, out_ptrs, a_scales_ptrs, b_scales_ptrs, alpha_ptrs,",
      "      layout_sfa, layout_sfb, a, b, output, a_blockscale, b_blockscales, alphas,",
      "      expert_offsets, sf_offsets, problem_sizes, M, N, K);",
      "",
      "  // Create an instance of the GEMM",
      "  Gemm gemm_op;",
      "",
      "  // Initialize problem_sizes_as_shapes correctly",
      "  UnderlyingProblemShape* problem_sizes_as_shapes =",
      "      static_cast<UnderlyingProblemShape*>(problem_sizes.data_ptr());",
      "",
      "  // Set the Scheduler info",
      "  cutlass::KernelHardwareInfo hw_info;",
      "  using RasterOrderOptions = typename cutlass::gemm::kernel::detail::",
      "      PersistentTileSchedulerSm100GroupParams<",
      "          typename ProblemShape::UnderlyingProblemShape>::RasterOrderOptions;",
      "  typename Gemm::GemmKernel::TileSchedulerArguments scheduler;",
      "  scheduler.raster_order = RasterOrderOptions::AlongM;",
      "  hw_info.device_id = a.get_device();",
      "  static std::unordered_map<int, int> cached_sm_counts;",
      "  if (cached_sm_counts.find(hw_info.device_id) == cached_sm_counts.end()) {",
      "    cached_sm_counts[hw_info.device_id] =",
      "        cutlass::KernelHardwareInfo::query_device_multiprocessor_count(",
      "            hw_info.device_id);",
      "  }",
      "  hw_info.sm_count = min(cached_sm_counts[hw_info.device_id], INT_MAX);",
      "",
      "  // Mainloop Arguments",
      "  typename GemmKernel::MainloopArguments mainloop_args{",
      "      static_cast<const ElementType**>(a_ptrs.data_ptr()),",
      "      static_cast<StrideA*>(a_strides1.data_ptr()),",
      "      static_cast<const ElementType**>(b_ptrs.data_ptr()),",
      "      static_cast<StrideB*>(b_strides1.data_ptr()),",
      "      static_cast<const ElementSFType**>(a_scales_ptrs.data_ptr()),",
      "      reinterpret_cast<LayoutSFA*>(layout_sfa.data_ptr()),",
      "      static_cast<const ElementSFType**>(b_scales_ptrs.data_ptr()),",
      "      reinterpret_cast<LayoutSFB*>(layout_sfb.data_ptr())};",
      "",
      "  // Epilogue Arguments",
      "  typename GemmKernel::EpilogueArguments epilogue_args{",
      "      {},  // epilogue.thread",
      "      nullptr,",
      "      static_cast<StrideC*>(c_strides1.data_ptr()),",
      "      static_cast<ElementD**>(out_ptrs.data_ptr()),",
      "      static_cast<StrideC*>(c_strides1.data_ptr())};",
      "  auto& fusion_args = epilogue_args.thread;",
      "  fusion_args.alpha_ptr_array =",
      "      reinterpret_cast<float**>(alpha_ptrs.data_ptr());",
      "  fusion_args.dAlpha = {_0{}, _0{}, 1};",
      "",
      "  // Gemm Arguments",
      "  typename GemmKernel::Arguments args{",
      "      cutlass::gemm::GemmUniversalMode::kGrouped,",
      "      {num_experts, problem_sizes_as_shapes, nullptr},",
      "      mainloop_args,",
      "      epilogue_args,",
      "      hw_info,",
      "      scheduler};",
      "",
      "  size_t workspace_size = Gemm::get_workspace_size(args);",
      "  auto const workspace_options =",
      "      torch::TensorOptions().dtype(torch::kUInt8).device(a.device());",
      "  auto workspace = torch::empty(workspace_size, workspace_options);",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream(a.get_device());",
      "",
      "  auto can_implement_status = gemm_op.can_implement(args);",
      "  TORCH_CHECK(can_implement_status == cutlass::Status::kSuccess,",
      "              \"Failed to implement GEMM\");",
      "",
      "  // Run the GEMM",
      "  auto status = gemm_op.initialize(args, workspace.data_ptr());",
      "  TORCH_CHECK(status == cutlass::Status::kSuccess, \"Failed to initialize GEMM\");",
      "",
      "  status = gemm_op.run(args, workspace.data_ptr(), stream);",
      "  TORCH_CHECK(status == cutlass::Status::kSuccess, \"Failed to run GEMM\");",
      "}",
      "",
      "#if defined ENABLE_NVFP4_SM100 && ENABLE_NVFP4_SM100",
      "constexpr auto FLOAT4_E2M1X2 = at::ScalarType::Byte;",
      "constexpr auto SF_DTYPE = at::ScalarType::Float8_e4m3fn;",
      "#endif",
      "",
      "#define CHECK_TYPE(x, st, m) \\",
      "  TORCH_CHECK(x.scalar_type() == st, \": Inconsistency of Tensor type:\", m)",
      "#define CHECK_TH_CUDA(x, m) \\",
      "  TORCH_CHECK(x.is_cuda(), m, \": must be a CUDA tensor.\")",
      "#define CHECK_CONTIGUOUS(x, m) \\",
      "  TORCH_CHECK(x.is_contiguous(), m, \": must be contiguous.\")",
      "#define CHECK_INPUT(x, st, m) \\",
      "  CHECK_TH_CUDA(x, m);        \\",
      "  CHECK_CONTIGUOUS(x, m);     \\",
      "  CHECK_TYPE(x, st, m)",
      "",
      "void cutlass_fp4_group_mm(",
      "    torch::Tensor& output, const torch::Tensor& a, const torch::Tensor& b,",
      "    const torch::Tensor& a_blockscale, const torch::Tensor& b_blockscales,",
      "    const torch::Tensor& alphas, const torch::Tensor& problem_sizes,",
      "    const torch::Tensor& expert_offsets, const torch::Tensor& sf_offsets) {",
      "#if defined ENABLE_NVFP4_SM100 && ENABLE_NVFP4_SM100",
      "  // Input validation",
      "  CHECK_INPUT(a, FLOAT4_E2M1X2, \"a\");",
      "  CHECK_INPUT(b, FLOAT4_E2M1X2, \"b\");",
      "  CHECK_INPUT(a_blockscale, SF_DTYPE, \"a_blockscale\");",
      "  CHECK_INPUT(b_blockscales, SF_DTYPE, \"b_blockscales\");",
      "  CHECK_INPUT(alphas, at::ScalarType::Float, \"alphas\");",
      "",
      "  TORCH_CHECK(a_blockscale.dim() == 2,",
      "              \"expected a_blockscale to be of shape [num_experts, rounded_m,\"",
      "              \" k // group_size], observed rank: \",",
      "              a_blockscale.dim())",
      "  TORCH_CHECK(b_blockscales.dim() == 3,",
      "              \"expected b_blockscale to be of shape: \"",
      "              \" [num_experts, n, k // group_size], observed rank: \",",
      "              b_blockscales.dim())",
      "  TORCH_CHECK(problem_sizes.dim() == 2, \"problem_sizes must be  a 2D tensor\");",
      "  TORCH_CHECK(problem_sizes.size(1) == 3,",
      "              \"problem_sizes must have the shape (num_experts, 3)\");",
      "  TORCH_CHECK(problem_sizes.size(0) == expert_offsets.size(0),",
      "              \"Number of experts in problem_sizes must match expert_offsets\");",
      "  TORCH_CHECK(problem_sizes.dtype() == torch::kInt32,",
      "              \"problem_sizes must be int32.\");",
      "",
      "  int M = static_cast<int>(a.size(0));",
      "  int N = static_cast<int>(b.size(1));",
      "  int E = static_cast<int>(b.size(0));",
      "  int K = static_cast<int>(2 * b.size(2));",
      "",
      "  if (output.scalar_type() == torch::kBFloat16) {",
      "    run_fp4_blockwise_scaled_group_mm<cutlass::bfloat16_t>(",
      "        output, a, b, a_blockscale, b_blockscales, alphas, problem_sizes,",
      "        expert_offsets, sf_offsets, M, N, K);",
      "  } else {",
      "    run_fp4_blockwise_scaled_group_mm<cutlass::half_t>(",
      "        output, a, b, a_blockscale, b_blockscales, alphas, problem_sizes,",
      "        expert_offsets, sf_offsets, M, N, K);",
      "  }",
      "#else",
      "  TORCH_CHECK_NOT_IMPLEMENTED(",
      "      false,",
      "      \"No compiled cutlass_fp4_group_mm kernel, vLLM must \"",
      "      \"be compiled with ENABLE_NVFP4_SM100 for SM100+ and CUDA \"",
      "      \"12.8 or above.\");",
      "#endif",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fp4/nvfp4_scaled_mm_entry.cu",
    "source": [
      "/*",
      " * Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "",
      "#include <torch/all.h>",
      "",
      "#if defined ENABLE_NVFP4_SM100 && ENABLE_NVFP4_SM100",
      "void cutlass_scaled_fp4_mm_sm100a(torch::Tensor& D, torch::Tensor const& A,",
      "                                  torch::Tensor const& B,",
      "                                  torch::Tensor const& A_sf,",
      "                                  torch::Tensor const& B_sf,",
      "                                  torch::Tensor const& alpha);",
      "#endif",
      "",
      "#if defined ENABLE_NVFP4_SM120 && ENABLE_NVFP4_SM120",
      "void cutlass_scaled_fp4_mm_sm120a(torch::Tensor& D, torch::Tensor const& A,",
      "                                  torch::Tensor const& B,",
      "                                  torch::Tensor const& A_sf,",
      "                                  torch::Tensor const& B_sf,",
      "                                  torch::Tensor const& alpha);",
      "#endif",
      "",
      "void cutlass_scaled_fp4_mm(torch::Tensor& D, torch::Tensor const& A,",
      "                           torch::Tensor const& B, torch::Tensor const& A_sf,",
      "                           torch::Tensor const& B_sf,",
      "                           torch::Tensor const& alpha) {",
      "#if defined ENABLE_NVFP4_SM100 && ENABLE_NVFP4_SM100",
      "  return cutlass_scaled_fp4_mm_sm100a(D, A, B, A_sf, B_sf, alpha);",
      "#elif defined ENABLE_NVFP4_SM120 && ENABLE_NVFP4_SM120",
      "  return cutlass_scaled_fp4_mm_sm120a(D, A, B, A_sf, B_sf, alpha);",
      "#endif",
      "  TORCH_CHECK_NOT_IMPLEMENTED(false,",
      "                              \"No compiled nvfp4 mm kernel, vLLM should \"",
      "                              \"be compiled using CUDA 12.8 and target \"",
      "                              \"compute capability 100 or above.\");",
      "}",
      "",
      "bool cutlass_scaled_mm_supports_fp4(int64_t cuda_device_capability) {",
      "  int runtimeVersion;",
      "  cudaRuntimeGetVersion(&runtimeVersion);",
      "  return cuda_device_capability >= 100 && runtimeVersion >= 12080;",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fp4/nvfp4_quant_kernels.cu",
    "source": [
      "/*",
      " * Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "",
      "#include <torch/all.h>",
      "",
      "#include <cuda_runtime_api.h>",
      "#include <cuda_runtime.h>",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#include <cuda_fp8.h>",
      "#include \"dispatch_utils.h\"",
      "",
      "#include \"cuda_utils.h\"",
      "#include \"nvfp4_utils.cuh\"",
      "",
      "namespace vllm {",
      "",
      "// Use UE4M3 by default.",
      "template <class Type, bool UE8M0_SF = false>",
      "__global__ void __launch_bounds__(512, 4)",
      "    cvt_fp16_to_fp4(int32_t numRows, int32_t numCols, Type const* in,",
      "                    float const* SFScale, uint32_t* out, uint32_t* SFout) {",
      "  using PackedVec = PackedVec<Type>;",
      "  static constexpr int CVT_FP4_NUM_THREADS_PER_SF =",
      "      (CVT_FP4_SF_VEC_SIZE / CVT_FP4_ELTS_PER_THREAD);",
      "  static_assert(sizeof(PackedVec) == sizeof(Type) * CVT_FP4_ELTS_PER_THREAD,",
      "                \"Vec size is not matched.\");",
      "",
      "  // Get the global scaling factor, which will be applied to the SF.",
      "  // Note SFScale is the same as next GEMM's alpha, which is",
      "  // (448.f / (Alpha_A / 6.f)).",
      "  float const SFScaleVal = SFScale == nullptr ? 1.0f : SFScale[0];",
      "",
      "  // Input tensor row/col loops.",
      "  for (int rowIdx = blockIdx.x; rowIdx < numRows; rowIdx += gridDim.x) {",
      "    for (int colIdx = threadIdx.x; colIdx < numCols / CVT_FP4_ELTS_PER_THREAD;",
      "         colIdx += blockDim.x) {",
      "      int64_t inOffset = rowIdx * (numCols / CVT_FP4_ELTS_PER_THREAD) + colIdx;",
      "      PackedVec in_vec = reinterpret_cast<PackedVec const*>(in)[inOffset];",
      "      // Get the output tensor offset.",
      "      // Same as inOffset because 8 elements are packed into one uint32_t.",
      "      int64_t outOffset = inOffset;",
      "      auto& out_pos = out[outOffset];",
      "",
      "      auto sf_out =",
      "          cvt_quant_to_fp4_get_sf_out_offset<uint32_t,",
      "                                             CVT_FP4_NUM_THREADS_PER_SF>(",
      "              rowIdx, colIdx, numCols, SFout);",
      "",
      "      out_pos =",
      "          cvt_warp_fp16_to_fp4<Type, UE8M0_SF>(in_vec, SFScaleVal, sf_out);",
      "    }",
      "  }",
      "}",
      "",
      "template <typename T>",
      "void invokeFP4Quantization(int m, int n, T const* input, float const* SFScale,",
      "                           int64_t* output, int32_t* SFOuput, bool useUE8M0,",
      "                           int multiProcessorCount, cudaStream_t stream) {",
      "  // Grid, Block size.",
      "  // Each thread converts 8 values.",
      "  dim3 block(std::min(int(n / ELTS_PER_THREAD), 512));",
      "  // Get number of blocks per SM (assume we can fully utilize the SM).",
      "  int const numBlocksPerSM = 2048 / block.x;",
      "  dim3 grid(std::min(int(m), multiProcessorCount * numBlocksPerSM));",
      "",
      "  // Launch the cvt kernel.",
      "  if (useUE8M0) {",
      "    cvt_fp16_to_fp4<T, true><<<grid, block, 0, stream>>>(",
      "        m, n, input, SFScale, reinterpret_cast<uint32_t*>(output),",
      "        reinterpret_cast<uint32_t*>(SFOuput));",
      "  } else {",
      "    cvt_fp16_to_fp4<T, false><<<grid, block, 0, stream>>>(",
      "        m, n, input, SFScale, reinterpret_cast<uint32_t*>(output),",
      "        reinterpret_cast<uint32_t*>(SFOuput));",
      "  }",
      "}",
      "",
      "// Instantiate the function.",
      "template void invokeFP4Quantization(int m, int n, half const* input,",
      "                                    float const* SFScale, int64_t* output,",
      "                                    int32_t* SFOuput, bool useUE8M0,",
      "                                    int multiProcessorCount,",
      "                                    cudaStream_t stream);",
      "",
      "template void invokeFP4Quantization(int m, int n, __nv_bfloat16 const* input,",
      "                                    float const* SFScale, int64_t* output,",
      "                                    int32_t* SFOuput, bool useUE8M0,",
      "                                    int multiProcessorCount,",
      "                                    cudaStream_t stream);",
      "",
      "}  // namespace vllm",
      "",
      "void scaled_fp4_quant_sm1xxa(torch::Tensor const& output,",
      "                             torch::Tensor const& input,",
      "                             torch::Tensor const& output_sf,",
      "                             torch::Tensor const& input_sf) {",
      "  int32_t m = input.size(0);",
      "  int32_t n = input.size(1);",
      "",
      "  TORCH_CHECK(n % 16 == 0, \"The N dimension must be multiple of 16.\");",
      "  TORCH_CHECK(input.scalar_type() == at::ScalarType::Half ||",
      "                  input.scalar_type() == at::ScalarType::BFloat16,",
      "              \"Unsupported input data type for quantize_to_fp4.\");",
      "",
      "  int multiProcessorCount =",
      "      get_device_attribute(cudaDevAttrMultiProcessorCount, -1);",
      "",
      "  auto input_sf_ptr = static_cast<float const*>(input_sf.data_ptr());",
      "  auto sf_out = static_cast<int32_t*>(output_sf.data_ptr());",
      "  auto output_ptr = static_cast<int64_t*>(output.data_ptr());",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));",
      "  auto stream = at::cuda::getCurrentCUDAStream(input.get_device());",
      "",
      "  // We don't support e8m0 scales at this moment.",
      "  bool useUE8M0 = false;",
      "",
      "  VLLM_DISPATCH_HALF_TYPES(input.scalar_type(), \"nvfp4_quant_kernel\", [&] {",
      "    using cuda_type = vllm::CUDATypeConverter<scalar_t>::Type;",
      "    auto input_ptr = static_cast<cuda_type const*>(input.data_ptr());",
      "    vllm::invokeFP4Quantization(m, n, input_ptr, input_sf_ptr, output_ptr,",
      "                                sf_out, useUE8M0, multiProcessorCount, stream);",
      "  });",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fp4/nvfp4_quant_entry.cu",
    "source": [
      "/*",
      " * Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "",
      "#include <torch/all.h>",
      "",
      "#if (defined(ENABLE_NVFP4_SM100) && ENABLE_NVFP4_SM100) || \\",
      "    (defined(ENABLE_NVFP4_SM120) && ENABLE_NVFP4_SM120)",
      "void scaled_fp4_quant_sm1xxa(torch::Tensor const& output,",
      "                             torch::Tensor const& input,",
      "                             torch::Tensor const& output_sf,",
      "                             torch::Tensor const& input_sf);",
      "#endif",
      "",
      "#if defined ENABLE_NVFP4_SM100 && ENABLE_NVFP4_SM100",
      "void scaled_fp4_experts_quant_sm100a(",
      "    torch::Tensor& output, torch::Tensor& output_scale,",
      "    torch::Tensor const& input, torch::Tensor const& input_global_scale,",
      "    torch::Tensor const& input_offset_by_experts,",
      "    torch::Tensor const& output_scale_offset_by_experts);",
      "#endif",
      "",
      "#if (defined(ENABLE_NVFP4_SM100) && ENABLE_NVFP4_SM100) || \\",
      "    (defined(ENABLE_NVFP4_SM120) && ENABLE_NVFP4_SM120)",
      "void silu_and_mul_nvfp4_quant_sm1xxa(torch::Tensor& output,",
      "                                     torch::Tensor& output_sf,",
      "                                     torch::Tensor& input,",
      "                                     torch::Tensor& input_sf);",
      "#endif",
      "",
      "void scaled_fp4_quant(torch::Tensor& output, torch::Tensor const& input,",
      "                      torch::Tensor& output_sf, torch::Tensor const& input_sf) {",
      "#if (defined(ENABLE_NVFP4_SM100) && ENABLE_NVFP4_SM100) || \\",
      "    (defined(ENABLE_NVFP4_SM120) && ENABLE_NVFP4_SM120)",
      "  return scaled_fp4_quant_sm1xxa(output, input, output_sf, input_sf);",
      "#endif",
      "  TORCH_CHECK_NOT_IMPLEMENTED(false, \"No compiled nvfp4 quantization kernel\");",
      "}",
      "",
      "void scaled_fp4_experts_quant(",
      "    torch::Tensor& output, torch::Tensor& output_scale,",
      "    torch::Tensor const& input, torch::Tensor const& input_global_scale,",
      "    torch::Tensor const& input_offset_by_experts,",
      "    torch::Tensor const& output_scale_offset_by_experts) {",
      "#if defined ENABLE_NVFP4_SM100 && ENABLE_NVFP4_SM100",
      "  return scaled_fp4_experts_quant_sm100a(",
      "      output, output_scale, input, input_global_scale, input_offset_by_experts,",
      "      output_scale_offset_by_experts);",
      "#endif",
      "  TORCH_CHECK_NOT_IMPLEMENTED(false,",
      "                              \"No compiled nvfp4 experts quantization kernel\");",
      "}",
      "",
      "void silu_and_mul_nvfp4_quant(torch::Tensor& output, torch::Tensor& output_sf,",
      "                              torch::Tensor& input, torch::Tensor& input_sf) {",
      "#if (defined(ENABLE_NVFP4_SM100) && ENABLE_NVFP4_SM100) || \\",
      "    (defined(ENABLE_NVFP4_SM120) && ENABLE_NVFP4_SM120)",
      "  return silu_and_mul_nvfp4_quant_sm1xxa(output, output_sf, input, input_sf);",
      "#endif",
      "  TORCH_CHECK_NOT_IMPLEMENTED(",
      "      false, \"No compiled silu_and_mul nvfp4 quantization kernel\");",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fp4/nvfp4_scaled_mm_sm120_kernels.cu",
    "source": [
      "/*",
      " * Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "",
      "#include <torch/all.h>",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#include \"cutlass_extensions/common.hpp\"",
      "",
      "#include \"cutlass/cutlass.h\"",
      "",
      "#include \"cutlass/gemm/collective/collective_builder.hpp\"",
      "#include \"cutlass/epilogue/collective/collective_builder.hpp\"",
      "#include \"cutlass/gemm/device/gemm_universal_adapter.h\"",
      "#include \"cutlass/gemm/kernel/gemm_universal.hpp\"",
      "",
      "#include \"cutlass/util/packed_stride.hpp\"",
      "",
      "#include \"core/math.hpp\"",
      "",
      "using namespace cute;",
      "",
      "#define CHECK_TYPE(x, st, m) \\",
      "  TORCH_CHECK(x.scalar_type() == st, \": Inconsistency of Tensor type:\", m)",
      "#define CHECK_TH_CUDA(x, m) \\",
      "  TORCH_CHECK(x.is_cuda(), m, \": must be a CUDA tensor\")",
      "#define CHECK_CONTIGUOUS(x, m) \\",
      "  TORCH_CHECK(x.is_contiguous(), m, \": must be contiguous\")",
      "#define CHECK_INPUT(x, st, m) \\",
      "  CHECK_TH_CUDA(x, m);        \\",
      "  CHECK_CONTIGUOUS(x, m);     \\",
      "  CHECK_TYPE(x, st, m)",
      "",
      "constexpr auto FLOAT4_E2M1X2 = at::ScalarType::Byte;",
      "constexpr auto SF_DTYPE = at::ScalarType::Float8_e4m3fn;",
      "",
      "struct sm120_fp4_config_M256 {",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "  using MmaTileShape = Shape<_128, _128, _128>;",
      "  using PerSmTileShape_MNK = Shape<_128, _128, _128>;",
      "};",
      "",
      "struct sm120_fp4_config_default {",
      "  using ClusterShape = Shape<_1, _1, _1>;",
      "  using MmaTileShape = Shape<_256, _128, _128>;",
      "  using PerSmTileShape_MNK = Shape<_256, _128, _128>;",
      "};",
      "",
      "template <typename Config, typename OutType>",
      "struct Fp4GemmSm120 {",
      "  using ElementA = cutlass::nv_float4_t<cutlass::float_e2m1_t>;",
      "  using LayoutATag = cutlass::layout::RowMajor;",
      "  static constexpr int AlignmentA = 32;",
      "",
      "  using ElementB = cutlass::nv_float4_t<cutlass::float_e2m1_t>;",
      "  using LayoutBTag = cutlass::layout::ColumnMajor;",
      "  static constexpr int AlignmentB = 32;",
      "",
      "  using ElementD = OutType;",
      "  using ElementC = OutType;",
      "  using LayoutCTag = cutlass::layout::RowMajor;",
      "  using LayoutDTag = cutlass::layout::RowMajor;",
      "  static constexpr int AlignmentD = 128 / cutlass::sizeof_bits<ElementD>::value;",
      "  static constexpr int AlignmentC = 128 / cutlass::sizeof_bits<ElementC>::value;",
      "",
      "  using ElementAccumulator = float;",
      "  using ArchTag = cutlass::arch::Sm120;",
      "  using OperatorClass = cutlass::arch::OpClassBlockScaledTensorOp;",
      "",
      "  using MmaTileShape = typename Config::MmaTileShape;",
      "  using ClusterShape = typename Config::ClusterShape;",
      "  using PerSmTileShape_MNK = typename Config::PerSmTileShape_MNK;",
      "",
      "  using CollectiveEpilogue =",
      "      typename cutlass::epilogue::collective::CollectiveBuilder<",
      "          ArchTag, OperatorClass, PerSmTileShape_MNK, ClusterShape,",
      "          cutlass::epilogue::collective::EpilogueTileAuto, ElementAccumulator,",
      "          ElementAccumulator, ElementC, LayoutCTag, AlignmentC, ElementD,",
      "          LayoutDTag, AlignmentD,",
      "          cutlass::epilogue::collective::EpilogueScheduleAuto>::CollectiveOp;",
      "",
      "  using CollectiveMainloop =",
      "      typename cutlass::gemm::collective::CollectiveBuilder<",
      "          ArchTag, OperatorClass, ElementA, LayoutATag, AlignmentA, ElementB,",
      "          LayoutBTag, AlignmentB, ElementAccumulator, MmaTileShape,",
      "          ClusterShape,",
      "          cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(",
      "              sizeof(typename CollectiveEpilogue::SharedStorage))>,",
      "          cutlass::gemm::collective::KernelScheduleAuto>::CollectiveOp;",
      "",
      "  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<",
      "      Shape<int, int, int, int>, CollectiveMainloop, CollectiveEpilogue, void>;",
      "",
      "  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;",
      "};",
      "",
      "template <typename Gemm>",
      "typename Gemm::Arguments args_from_options(at::Tensor& D, at::Tensor const& A,",
      "                                           at::Tensor const& B,",
      "                                           at::Tensor const& A_sf,",
      "                                           at::Tensor const& B_sf,",
      "                                           torch::Tensor const& alpha, int M,",
      "                                           int N, int K) {",
      "  using ElementA = typename Gemm::ElementA;",
      "  using ElementB = typename Gemm::ElementB;",
      "  using ElementD = typename Gemm::ElementD;",
      "  using ElementSFA = cutlass::float_ue4m3_t;",
      "  using ElementSFB = cutlass::float_ue4m3_t;",
      "  using ElementCompute = float;",
      "",
      "  using StrideA = typename Gemm::GemmKernel::StrideA;",
      "  using StrideB = typename Gemm::GemmKernel::StrideB;",
      "  using StrideC = typename Gemm::GemmKernel::StrideC;",
      "  using StrideD = typename Gemm::GemmKernel::StrideD;",
      "",
      "  using Sm1xxBlkScaledConfig =",
      "      typename Gemm::GemmKernel::CollectiveMainloop::Sm1xxBlkScaledConfig;",
      "",
      "  auto stride_A = cutlass::make_cute_packed_stride(StrideA{}, {M, K, 1});",
      "  auto stride_B = cutlass::make_cute_packed_stride(StrideB{}, {N, K, 1});",
      "  auto stride_D = cutlass::make_cute_packed_stride(StrideD{}, {M, N, 1});",
      "",
      "  auto layout_SFA = Sm1xxBlkScaledConfig::tile_atom_to_shape_SFA(",
      "      cute::make_shape(M, N, K, 1));",
      "  auto layout_SFB = Sm1xxBlkScaledConfig::tile_atom_to_shape_SFB(",
      "      cute::make_shape(M, N, K, 1));",
      "",
      "  typename Gemm::Arguments arguments{",
      "      cutlass::gemm::GemmUniversalMode::kGemm,",
      "      {M, N, K, 1},",
      "      {static_cast<ElementA const*>(A.data_ptr()), stride_A,",
      "       static_cast<ElementB const*>(B.data_ptr()), stride_B,",
      "       static_cast<ElementSFA const*>(A_sf.data_ptr()), layout_SFA,",
      "       static_cast<ElementSFB const*>(B_sf.data_ptr()), layout_SFB},",
      "      {{},",
      "       static_cast<ElementD const*>(D.data_ptr()),",
      "       stride_D,",
      "       static_cast<ElementD*>(D.data_ptr()),",
      "       stride_D}};",
      "  auto& fusion_args = arguments.epilogue.thread;",
      "  fusion_args.alpha_ptr = static_cast<ElementCompute const*>(alpha.data_ptr());",
      "",
      "  return arguments;",
      "}",
      "",
      "template <typename Gemm>",
      "void runGemm(at::Tensor& D, at::Tensor const& A, at::Tensor const& B,",
      "             at::Tensor const& A_sf, at::Tensor const& B_sf,",
      "             torch::Tensor const& alpha, int M, int N, int K,",
      "             cudaStream_t stream) {",
      "  Gemm gemm;",
      "",
      "  auto arguments = args_from_options<Gemm>(D, A, B, A_sf, B_sf, alpha, M, N, K);",
      "",
      "  size_t workspace_size = Gemm::get_workspace_size(arguments);",
      "  auto const workspace_options =",
      "      torch::TensorOptions().dtype(torch::kUInt8).device(A.device());",
      "  auto workspace = torch::empty(workspace_size, workspace_options);",
      "",
      "  CUTLASS_CHECK(gemm.can_implement(arguments));",
      "",
      "  CUTLASS_CHECK(gemm.initialize(arguments, workspace.data_ptr(), stream));",
      "",
      "  CUTLASS_CHECK(gemm.run(arguments, workspace.data_ptr(), stream));",
      "}",
      "",
      "void cutlass_fp4_bf16_gemm_dispatch(torch::Tensor& D, torch::Tensor const& A,",
      "                                    torch::Tensor const& B,",
      "                                    torch::Tensor const& A_sf,",
      "                                    torch::Tensor const& B_sf,",
      "                                    torch::Tensor const& alpha, int m, int n,",
      "                                    int k, cudaStream_t stream) {",
      "  uint32_t const mp2 = std::max(static_cast<uint32_t>(16), next_pow_2(m));",
      "  if (mp2 <= 256) {",
      "    runGemm<Fp4GemmSm120<sm120_fp4_config_M256, cutlass::bfloat16_t>::Gemm>(",
      "        D, A, B, A_sf, B_sf, alpha, m, n, k, stream);",
      "  } else {",
      "    runGemm<Fp4GemmSm120<sm120_fp4_config_default, cutlass::bfloat16_t>::Gemm>(",
      "        D, A, B, A_sf, B_sf, alpha, m, n, k, stream);",
      "  }",
      "}",
      "",
      "void cutlass_fp4_f16_gemm_dispatch(torch::Tensor& D, torch::Tensor const& A,",
      "                                   torch::Tensor const& B,",
      "                                   torch::Tensor const& A_sf,",
      "                                   torch::Tensor const& B_sf,",
      "                                   torch::Tensor const& alpha, int m, int n,",
      "                                   int k, cudaStream_t stream) {",
      "  uint32_t const mp2 = std::max(static_cast<uint32_t>(16), next_pow_2(m));",
      "  if (mp2 <= 256) {",
      "    runGemm<Fp4GemmSm120<sm120_fp4_config_M256, cutlass::half_t>::Gemm>(",
      "        D, A, B, A_sf, B_sf, alpha, m, n, k, stream);",
      "  } else {",
      "    runGemm<Fp4GemmSm120<sm120_fp4_config_default, cutlass::half_t>::Gemm>(",
      "        D, A, B, A_sf, B_sf, alpha, m, n, k, stream);",
      "  }",
      "}",
      "",
      "void cutlass_scaled_fp4_mm_sm120a(torch::Tensor& D, torch::Tensor const& A,",
      "                                  torch::Tensor const& B,",
      "                                  torch::Tensor const& A_sf,",
      "                                  torch::Tensor const& B_sf,",
      "                                  torch::Tensor const& alpha) {",
      "#if defined(CUTLASS_ARCH_MMA_SM120_SUPPORTED)",
      "  CHECK_INPUT(A, FLOAT4_E2M1X2, \"a\");",
      "  CHECK_INPUT(B, FLOAT4_E2M1X2, \"b\");",
      "",
      "  CHECK_INPUT(A_sf, SF_DTYPE, \"scale_a\");",
      "  CHECK_INPUT(B_sf, SF_DTYPE, \"scale_b\");",
      "",
      "  CHECK_INPUT(alpha, at::ScalarType::Float, \"alpha\");",
      "",
      "  TORCH_CHECK(A.dim() == 2, \"a must be a matrix\");",
      "  TORCH_CHECK(B.dim() == 2, \"b must be a matrix\");",
      "  TORCH_CHECK(A.sizes()[1] == B.sizes()[1],",
      "              \"a and b shapes cannot be multiplied (\", A.sizes()[0], \"x\",",
      "              A.sizes()[1], \" and \", B.sizes()[0], \"x\", B.sizes()[1], \")\");",
      "",
      "  auto const m = A.sizes()[0];",
      "  auto const n = B.sizes()[0];",
      "  auto const k = A.sizes()[1] * 2;",
      "",
      "  constexpr int alignment = 32;",
      "  TORCH_CHECK(k % alignment == 0, \"Expected k to be divisible by \", alignment,",
      "              \", but got a shape: (\", A.sizes()[0], \"x\", A.sizes()[1],",
      "              \"), k: \", k, \".\");",
      "  TORCH_CHECK(n % alignment == 0, \"Expected n to be divisible by \", alignment,",
      "              \", but got b shape: (\", B.sizes()[0], \"x\", B.sizes()[1], \").\");",
      "",
      "  auto round_up = [](int x, int y) { return (x + y - 1) / y * y; };",
      "  int rounded_m = round_up(m, 128);",
      "  int rounded_n = round_up(n, 128);",
      "  // Since k is divisible by 32 (alignment), k / 16 is guaranteed to be an",
      "  // integer.",
      "  int rounded_k = round_up(k / 16, 4);",
      "",
      "  TORCH_CHECK(A_sf.dim() == 2, \"scale_a must be a matrix\");",
      "  TORCH_CHECK(B_sf.dim() == 2, \"scale_b must be a matrix\");",
      "  TORCH_CHECK(A_sf.sizes()[1] == B_sf.sizes()[1],",
      "              \"scale_a and scale_b shapes cannot be multiplied (\",",
      "              A_sf.sizes()[0], \"x\", A_sf.sizes()[1], \" and \", B_sf.sizes()[0],",
      "              \"x\", B_sf.sizes()[1], \")\");",
      "  TORCH_CHECK(A_sf.sizes()[0] == rounded_m && A_sf.sizes()[1] == rounded_k,",
      "              \"scale_a must be padded and swizzled to a shape (\", rounded_m,",
      "              \"x\", rounded_k, \"), but got a shape (\", A_sf.sizes()[0], \"x\",",
      "              A_sf.sizes()[1], \")\");",
      "  TORCH_CHECK(B_sf.sizes()[0] == rounded_n && B_sf.sizes()[1] == rounded_k,",
      "              \"scale_b must be padded and swizzled to a shape (\", rounded_n,",
      "              \"x\", rounded_k, \"), but got a shape (\", B_sf.sizes()[0], \"x\",",
      "              B_sf.sizes()[1], \")\");",
      "",
      "  auto out_dtype = D.dtype();",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(A));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream(A.get_device());",
      "",
      "  if (out_dtype == at::ScalarType::BFloat16) {",
      "    return cutlass_fp4_bf16_gemm_dispatch(D, A, B, A_sf, B_sf, alpha, m, n, k,",
      "                                          stream);",
      "  } else if (out_dtype == at::ScalarType::Half) {",
      "    return cutlass_fp4_f16_gemm_dispatch(D, A, B, A_sf, B_sf, alpha, m, n, k,",
      "                                         stream);",
      "  } else {",
      "    TORCH_CHECK(false, \"Unsupported output data type of nvfp4 mm sm120 (\",",
      "                out_dtype, \")\");",
      "  }",
      "#else",
      "  TORCH_CHECK(false,",
      "              \"Unsupported CUTLASS version. Set VLLM_CUTLASS_SRC_DIR to \"",
      "              \"a CUTLASS 3.8 source directory to enable support.\");",
      "#endif  // defined(CUTLASS_ARCH_MMA_SM120_SUPPORTED)",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fp4/nvfp4_utils.cuh",
    "source": [
      "/*",
      " * Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "",
      "#pragma once",
      "",
      "#include <cuda_runtime.h>",
      "#include <cuda_fp8.h>",
      "",
      "#define ELTS_PER_THREAD 8",
      "",
      "constexpr int CVT_FP4_ELTS_PER_THREAD = 8;",
      "constexpr int CVT_FP4_SF_VEC_SIZE = 16;",
      "",
      "namespace vllm {",
      "",
      "// Convert PyTorch cpp type to CUDA type",
      "template <typename T>",
      "struct CUDATypeConverter {",
      "  using Type = T;",
      "};",
      "",
      "template <>",
      "struct CUDATypeConverter<at::Half> {",
      "  using Type = half;",
      "};",
      "",
      "template <>",
      "struct CUDATypeConverter<at::BFloat16> {",
      "  using Type = __nv_bfloat16;",
      "};",
      "",
      "// Get type2 from type or vice versa (applied to half and bfloat16)",
      "template <typename T>",
      "struct TypeConverter {",
      "  using Type = half2;",
      "};  // keep for generality",
      "",
      "template <>",
      "struct TypeConverter<half2> {",
      "  using Type = half;",
      "};",
      "",
      "template <>",
      "struct TypeConverter<half> {",
      "  using Type = half2;",
      "};",
      "",
      "template <>",
      "struct TypeConverter<__nv_bfloat162> {",
      "  using Type = __nv_bfloat16;",
      "};",
      "",
      "template <>",
      "struct TypeConverter<__nv_bfloat16> {",
      "  using Type = __nv_bfloat162;",
      "};",
      "",
      "// Define a 16 bytes packed data type.",
      "template <class Type>",
      "struct PackedVec {",
      "  typename TypeConverter<Type>::Type elts[4];",
      "};",
      "",
      "template <>",
      "struct PackedVec<__nv_fp8_e4m3> {",
      "  __nv_fp8x2_e4m3 elts[8];",
      "};",
      "",
      "// Convert 8 float32 values into 8 e2m1 values (represented as one uint32_t).",
      "inline __device__ uint32_t fp32_vec_to_e2m1(float (&array)[8]) {",
      "  uint32_t val;",
      "  asm volatile(",
      "      \"{\\n\"",
      "      \".reg .b8 byte0;\\n\"",
      "      \".reg .b8 byte1;\\n\"",
      "      \".reg .b8 byte2;\\n\"",
      "      \".reg .b8 byte3;\\n\"",
      "      \"cvt.rn.satfinite.e2m1x2.f32   byte0, %2, %1;\\n\"",
      "      \"cvt.rn.satfinite.e2m1x2.f32   byte1, %4, %3;\\n\"",
      "      \"cvt.rn.satfinite.e2m1x2.f32   byte2, %6, %5;\\n\"",
      "      \"cvt.rn.satfinite.e2m1x2.f32   byte3, %8, %7;\\n\"",
      "      \"mov.b32 %0, {byte0, byte1, byte2, byte3};\\n\"",
      "      \"}\"",
      "      : \"=r\"(val)",
      "      : \"f\"(array[0]), \"f\"(array[1]), \"f\"(array[2]), \"f\"(array[3]),",
      "        \"f\"(array[4]), \"f\"(array[5]), \"f\"(array[6]), \"f\"(array[7]));",
      "  return val;",
      "}",
      "",
      "// Convert 4 float2 values into 8 e2m1 values (represented as one uint32_t).",
      "inline __device__ uint32_t fp32_vec_to_e2m1(float2 (&array)[4]) {",
      "  uint32_t val;",
      "  asm volatile(",
      "      \"{\\n\"",
      "      \".reg .b8 byte0;\\n\"",
      "      \".reg .b8 byte1;\\n\"",
      "      \".reg .b8 byte2;\\n\"",
      "      \".reg .b8 byte3;\\n\"",
      "      \"cvt.rn.satfinite.e2m1x2.f32   byte0, %2, %1;\\n\"",
      "      \"cvt.rn.satfinite.e2m1x2.f32   byte1, %4, %3;\\n\"",
      "      \"cvt.rn.satfinite.e2m1x2.f32   byte2, %6, %5;\\n\"",
      "      \"cvt.rn.satfinite.e2m1x2.f32   byte3, %8, %7;\\n\"",
      "      \"mov.b32 %0, {byte0, byte1, byte2, byte3};\\n\"",
      "      \"}\"",
      "      : \"=r\"(val)",
      "      : \"f\"(array[0].x), \"f\"(array[0].y), \"f\"(array[1].x), \"f\"(array[1].y),",
      "        \"f\"(array[2].x), \"f\"(array[2].y), \"f\"(array[3].x), \"f\"(array[3].y));",
      "  return val;",
      "}",
      "",
      "// Fast reciprocal.",
      "inline __device__ float reciprocal_approximate_ftz(float a) {",
      "  float b;",
      "  asm volatile(\"rcp.approx.ftz.f32 %0, %1;\\n\" : \"=f\"(b) : \"f\"(a));",
      "  return b;",
      "}",
      "",
      "template <class SFType, int CVT_FP4_NUM_THREADS_PER_SF>",
      "__device__ uint8_t* cvt_quant_to_fp4_get_sf_out_offset(int rowIdx, int colIdx,",
      "                                                       int numCols,",
      "                                                       SFType* SFout) {",
      "  static_assert(CVT_FP4_NUM_THREADS_PER_SF == 1 ||",
      "                CVT_FP4_NUM_THREADS_PER_SF == 2);",
      "",
      "  // One pair of threads write one SF to global memory.",
      "  // TODO: stage through smem for packed STG.32",
      "  // is it better than STG.8 from 4 threads ?",
      "  if (threadIdx.x % CVT_FP4_NUM_THREADS_PER_SF == 0) {",
      "    // SF vector index (16 elements share one SF in the K dimension).",
      "    int32_t kIdx = colIdx / CVT_FP4_NUM_THREADS_PER_SF;",
      "    int32_t mIdx = rowIdx;",
      "",
      "    // SF layout [numMTiles, numKTiles, 32 (mTile), 4 (mTile), 4(kTile)]",
      "    // --> index [mTileIdx, kTileIdx, outerMIdx, innerMIdx, innerKIdx]",
      "",
      "    int32_t mTileIdx = mIdx / (32 * 4);",
      "    // SF vector size 16.",
      "    int factor = CVT_FP4_SF_VEC_SIZE * 4;",
      "    int32_t numKTiles = (numCols + factor - 1) / factor;",
      "    int64_t mTileStride = numKTiles * 32 * 4 * 4;",
      "",
      "    int32_t kTileIdx = (kIdx / 4);",
      "    int64_t kTileStride = 32 * 4 * 4;",
      "",
      "    // M tile layout [32, 4] is column-major.",
      "    int32_t outerMIdx = (mIdx % 32);",
      "    int64_t outerMStride = 4 * 4;",
      "",
      "    int32_t innerMIdx = (mIdx % (32 * 4)) / 32;",
      "    int64_t innerMStride = 4;",
      "",
      "    int32_t innerKIdx = (kIdx % 4);",
      "    int64_t innerKStride = 1;",
      "",
      "    // Compute the global offset.",
      "    int64_t SFOffset = mTileIdx * mTileStride + kTileIdx * kTileStride +",
      "                       outerMIdx * outerMStride + innerMIdx * innerMStride +",
      "                       innerKIdx * innerKStride;",
      "",
      "    return reinterpret_cast<uint8_t*>(SFout) + SFOffset;",
      "  }",
      "  return nullptr;",
      "}",
      "",
      "// Quantizes the provided PackedVec into the uint32_t output",
      "template <class Type, bool UE8M0_SF = false>",
      "__device__ uint32_t cvt_warp_fp16_to_fp4(PackedVec<Type>& vec, float SFScaleVal,",
      "                                         uint8_t* SFout) {",
      "  // Get absolute maximum values among the local 8 values.",
      "  auto localMax = __habs2(vec.elts[0]);",
      "",
      "// Local maximum value.",
      "#pragma unroll",
      "  for (int i = 1; i < CVT_FP4_ELTS_PER_THREAD / 2; i++) {",
      "    localMax = __hmax2(localMax, __habs2(vec.elts[i]));",
      "  }",
      "",
      "  // Get the absolute maximum among all 16 values (two threads).",
      "  localMax = __hmax2(__shfl_xor_sync(uint32_t(-1), localMax, 1), localMax);",
      "  // Get the final absolute maximum values.",
      "  float vecMax = float(__hmax(localMax.x, localMax.y));",
      "",
      "  // Get the SF (max value of the vector / max value of e2m1).",
      "  // maximum value of e2m1 = 6.0.",
      "  // TODO: use half as compute data type.",
      "  float SFValue = SFScaleVal * (vecMax * reciprocal_approximate_ftz(6.0f));",
      "  // 8 bits representation of the SF.",
      "  uint8_t fp8SFVal;",
      "  // Write the SF to global memory (STG.8).",
      "  if constexpr (UE8M0_SF) {",
      "    // Extract the 8 exponent bits from float32.",
      "    // float 32bits = 1 sign bit + 8 exponent bits + 23 mantissa bits.",
      "    uint32_t tmp = reinterpret_cast<uint32_t&>(SFValue) >> 23;",
      "    fp8SFVal = tmp & 0xff;",
      "    // Convert back to fp32.",
      "    reinterpret_cast<uint32_t&>(SFValue) = tmp << 23;",
      "  } else {",
      "    // Here SFValue is always positive, so E4M3 is the same as UE4M3.",
      "    __nv_fp8_e4m3 tmp = __nv_fp8_e4m3(SFValue);",
      "    reinterpret_cast<__nv_fp8_e4m3&>(fp8SFVal) = tmp;",
      "    // Convert back to fp32.",
      "    SFValue = float(tmp);",
      "  }",
      "  // Get the output scale.",
      "  // Recipe: final_scale = reciprocal(fp32(fp8(SFValue * SFScaleVal))) *",
      "  //                       reciprocal(SFScaleVal))",
      "  float outputScale =",
      "      SFValue != 0 ? reciprocal_approximate_ftz(",
      "                         SFValue * reciprocal_approximate_ftz(SFScaleVal))",
      "                   : 0.0f;",
      "",
      "  if (SFout) {",
      "    // Write the SF to global memory (STG.8).",
      "    *SFout = fp8SFVal;",
      "  }",
      "",
      "  // Convert the input to float.",
      "  float2 fp2Vals[CVT_FP4_ELTS_PER_THREAD / 2];",
      "",
      "#pragma unroll",
      "  for (int i = 0; i < CVT_FP4_ELTS_PER_THREAD / 2; i++) {",
      "    if constexpr (std::is_same_v<Type, half>) {",
      "      fp2Vals[i] = __half22float2(vec.elts[i]);",
      "    } else {",
      "      fp2Vals[i] = __bfloat1622float2(vec.elts[i]);",
      "    }",
      "    fp2Vals[i].x *= outputScale;",
      "    fp2Vals[i].y *= outputScale;",
      "  }",
      "",
      "  // Convert to e2m1 values.",
      "  uint32_t e2m1Vec = fp32_vec_to_e2m1(fp2Vals);",
      "",
      "  // Write the e2m1 values to global memory.",
      "  return e2m1Vec;",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fp4/activation_nvfp4_quant_fusion_kernels.cu",
    "source": [
      "/*",
      " * Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "",
      "#include <torch/all.h>",
      "",
      "#include <cuda_runtime_api.h>",
      "#include <cuda_runtime.h>",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#include <cuda_fp8.h>",
      "#include \"dispatch_utils.h\"",
      "",
      "#include \"cuda_utils.h\"",
      "#include \"nvfp4_utils.cuh\"",
      "",
      "namespace vllm {",
      "",
      "template <class Type>",
      "__inline__ __device__ PackedVec<Type> compute_silu(PackedVec<Type>& vec,",
      "                                                   PackedVec<Type>& vec2) {",
      "  PackedVec<Type> result;",
      "#pragma unroll",
      "  for (int i = 0; i < CVT_FP4_ELTS_PER_THREAD / 2; ++i) {",
      "    if constexpr (std::is_same_v<Type, half>) {",
      "      half2 val(0.5f, 0.5f);",
      "      half2 t0 = __hmul2(vec.elts[i], val);",
      "      half2 t1 = __hfma2(h2tanh(t0), val, val);",
      "      half2 t2 = __hmul2(vec.elts[i], t1);",
      "      result.elts[i] = __hmul2(t2, vec2.elts[i]);",
      "    } else {",
      "      __nv_bfloat162 val(0.5f, 0.5f);",
      "      __nv_bfloat162 t0 = __hmul2(vec.elts[i], val);",
      "      __nv_bfloat162 t1 = __hfma2(h2tanh(t0), val, val);",
      "      __nv_bfloat162 t2 = __hmul2(vec.elts[i], t1);",
      "      result.elts[i] = __hmul2(t2, vec2.elts[i]);",
      "    }",
      "  }",
      "  return result;",
      "}",
      "",
      "// Quantizes the provided PackedVec into the uint32_t output",
      "template <class Type, bool UE8M0_SF = false>",
      "__device__ uint32_t silu_and_cvt_warp_fp16_to_fp4(PackedVec<Type>& vec,",
      "                                                  PackedVec<Type>& vec2,",
      "                                                  float SFScaleVal,",
      "                                                  uint8_t* SFout) {",
      "  PackedVec<Type> out_silu = compute_silu(vec, vec2);",
      "  // Get absolute maximum values among the local 8 values.",
      "  auto localMax = __habs2(out_silu.elts[0]);",
      "",
      "// Local maximum value.",
      "#pragma unroll",
      "  for (int i = 1; i < CVT_FP4_ELTS_PER_THREAD / 2; i++) {",
      "    localMax = __hmax2(localMax, __habs2(out_silu.elts[i]));",
      "  }",
      "",
      "  // Get the absolute maximum among all 16 values (two threads).",
      "  localMax = __hmax2(__shfl_xor_sync(uint32_t(-1), localMax, 1), localMax);",
      "  // Get the final absolute maximum values.",
      "  float vecMax = float(__hmax(localMax.x, localMax.y));",
      "",
      "  // Get the SF (max value of the vector / max value of e2m1).",
      "  // maximum value of e2m1 = 6.0.",
      "  // TODO: use half as compute data type.",
      "  float SFValue = SFScaleVal * (vecMax * reciprocal_approximate_ftz(6.0f));",
      "  // 8 bits representation of the SF.",
      "  uint8_t fp8SFVal;",
      "  // Write the SF to global memory (STG.8).",
      "  if constexpr (UE8M0_SF) {",
      "    // Extract the 8 exponent bits from float32.",
      "    // float 32bits = 1 sign bit + 8 exponent bits + 23 mantissa bits.",
      "    uint32_t tmp = reinterpret_cast<uint32_t&>(SFValue) >> 23;",
      "    fp8SFVal = tmp & 0xff;",
      "    // Convert back to fp32.",
      "    reinterpret_cast<uint32_t&>(SFValue) = tmp << 23;",
      "  } else {",
      "    // Here SFValue is always positive, so E4M3 is the same as UE4M3.",
      "    __nv_fp8_e4m3 tmp = __nv_fp8_e4m3(SFValue);",
      "    reinterpret_cast<__nv_fp8_e4m3&>(fp8SFVal) = tmp;",
      "    // Convert back to fp32.",
      "    SFValue = float(tmp);",
      "  }",
      "  // Get the output scale.",
      "  // Recipe: final_scale = reciprocal(fp32(fp8(SFValue * SFScaleVal))) *",
      "  //                       reciprocal(SFScaleVal))",
      "  float outputScale =",
      "      SFValue != 0 ? reciprocal_approximate_ftz(",
      "                         SFValue * reciprocal_approximate_ftz(SFScaleVal))",
      "                   : 0.0f;",
      "",
      "  if (SFout) {",
      "    // Write the SF to global memory (STG.8).",
      "    *SFout = fp8SFVal;",
      "  }",
      "",
      "  // Convert the input to float.",
      "  float2 fp2Vals[CVT_FP4_ELTS_PER_THREAD / 2];",
      "",
      "#pragma unroll",
      "  for (int i = 0; i < CVT_FP4_ELTS_PER_THREAD / 2; i++) {",
      "    if constexpr (std::is_same_v<Type, half>) {",
      "      fp2Vals[i] = __half22float2(out_silu.elts[i]);",
      "    } else {",
      "      fp2Vals[i] = __bfloat1622float2(out_silu.elts[i]);",
      "    }",
      "    fp2Vals[i].x *= outputScale;",
      "    fp2Vals[i].y *= outputScale;",
      "  }",
      "",
      "  // Convert to e2m1 values.",
      "  uint32_t e2m1Vec = fp32_vec_to_e2m1(fp2Vals);",
      "",
      "  // Write the e2m1 values to global memory.",
      "  return e2m1Vec;",
      "}",
      "",
      "// Use UE4M3 by default.",
      "template <class Type, bool UE8M0_SF = false>",
      "__global__ void __launch_bounds__(1024, 4)",
      "    silu_and_cvt_fp16_to_fp4(int32_t numRows, int32_t numCols, Type const* in,",
      "                             float const* SFScale, uint32_t* out,",
      "                             uint32_t* SFout) {",
      "  using PackedVec = PackedVec<Type>;",
      "  static constexpr int CVT_FP4_NUM_THREADS_PER_SF =",
      "      (CVT_FP4_SF_VEC_SIZE / CVT_FP4_ELTS_PER_THREAD);",
      "  static_assert(sizeof(PackedVec) == sizeof(Type) * CVT_FP4_ELTS_PER_THREAD,",
      "                \"Vec size is not matched.\");",
      "",
      "  // Get the global scaling factor, which will be applied to the SF.",
      "  // Note SFScale is the same as next GEMM's alpha, which is",
      "  // (448.f / (Alpha_A / 6.f)).",
      "  float const SFScaleVal = SFScale == nullptr ? 1.0f : SFScale[0];",
      "",
      "  // Input tensor row/col loops.",
      "  for (int rowIdx = blockIdx.x; rowIdx < numRows; rowIdx += gridDim.x) {",
      "    for (int colIdx = threadIdx.x; colIdx < numCols / CVT_FP4_ELTS_PER_THREAD;",
      "         colIdx += blockDim.x) {",
      "      int64_t inOffset =",
      "          rowIdx * (numCols * 2 / CVT_FP4_ELTS_PER_THREAD) + colIdx;",
      "      int64_t inOffset2 = rowIdx * (numCols * 2 / CVT_FP4_ELTS_PER_THREAD) +",
      "                          numCols / CVT_FP4_ELTS_PER_THREAD + colIdx;",
      "      PackedVec in_vec = reinterpret_cast<PackedVec const*>(in)[inOffset];",
      "      PackedVec in_vec2 = reinterpret_cast<PackedVec const*>(in)[inOffset2];",
      "",
      "      // Get the output tensor offset.",
      "      // Same as inOffset because 8 elements are packed into one uint32_t.",
      "      int64_t outOffset = rowIdx * (numCols / CVT_FP4_ELTS_PER_THREAD) + colIdx;",
      "      ;",
      "      auto& out_pos = out[outOffset];",
      "",
      "      auto sf_out =",
      "          cvt_quant_to_fp4_get_sf_out_offset<uint32_t,",
      "                                             CVT_FP4_NUM_THREADS_PER_SF>(",
      "              rowIdx, colIdx, numCols, SFout);",
      "",
      "      out_pos = silu_and_cvt_warp_fp16_to_fp4<Type, UE8M0_SF>(",
      "          in_vec, in_vec2, SFScaleVal, sf_out);",
      "    }",
      "  }",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "void silu_and_mul_nvfp4_quant_sm1xxa(torch::Tensor& output,  // [..., d]",
      "                                     torch::Tensor& output_sf,",
      "                                     torch::Tensor& input,  // [..., 2 * d]",
      "                                     torch::Tensor& input_sf) {",
      "  int32_t m = input.size(0);",
      "  int32_t n = input.size(1) / 2;",
      "",
      "  TORCH_CHECK(n % 16 == 0, \"The N dimension must be multiple of 16.\");",
      "  TORCH_CHECK(input.scalar_type() == at::ScalarType::Half ||",
      "                  input.scalar_type() == at::ScalarType::BFloat16,",
      "              \"Unsupported input data type for quantize_to_fp4.\");",
      "",
      "  int multiProcessorCount =",
      "      get_device_attribute(cudaDevAttrMultiProcessorCount, -1);",
      "",
      "  auto input_sf_ptr = static_cast<float const*>(input_sf.data_ptr());",
      "  auto sf_out = static_cast<int32_t*>(output_sf.data_ptr());",
      "  auto output_ptr = static_cast<int64_t*>(output.data_ptr());",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));",
      "  auto stream = at::cuda::getCurrentCUDAStream(input.get_device());",
      "  dim3 block(std::min(int(n / ELTS_PER_THREAD), 1024));",
      "  int const numBlocksPerSM = 2048 / block.x;",
      "  dim3 grid(std::min(int(m), multiProcessorCount * numBlocksPerSM));",
      "",
      "  VLLM_DISPATCH_HALF_TYPES(",
      "      input.scalar_type(), \"silu_and_mul_nvfp4_quant_kernel\", [&] {",
      "        using cuda_type = vllm::CUDATypeConverter<scalar_t>::Type;",
      "        auto input_ptr = static_cast<cuda_type const*>(input.data_ptr());",
      "        vllm::silu_and_cvt_fp16_to_fp4<cuda_type><<<grid, block, 0, stream>>>(",
      "            m, n, input_ptr, input_sf_ptr,",
      "            reinterpret_cast<uint32_t*>(output_ptr),",
      "            reinterpret_cast<uint32_t*>(sf_out));",
      "      });",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fp4/nvfp4_experts_quant.cu",
    "source": [
      "/*",
      " * Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.",
      " *",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");",
      " * you may not use this file except in compliance with the License.",
      " * You may obtain a copy of the License at",
      " *",
      " *     http://www.apache.org/licenses/LICENSE-2.0",
      " *",
      " * Unless required by applicable law or agreed to in writing, software",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      " * See the License for the specific language governing permissions and",
      " * limitations under the License.",
      " */",
      "",
      "#include <torch/all.h>",
      "",
      "#include <cuda_runtime_api.h>",
      "#include <cuda_runtime.h>",
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#include <cuda_fp8.h>",
      "#include \"dispatch_utils.h\"",
      "",
      "#include \"nvfp4_utils.cuh\"",
      "",
      "namespace vllm {",
      "",
      "// Use UE4M3 by default.",
      "template <class Type, bool UE8M0_SF = false, bool SMALL_NUM_EXPERTS = false>",
      "__global__ void __launch_bounds__(512, 4)",
      "    cvt_fp16_to_fp4(int32_t numRows, int32_t numCols, Type const* in,",
      "                    float const* SFScale, uint32_t* out, uint32_t* SFout,",
      "                    uint32_t* input_offset_by_experts,",
      "                    uint32_t* output_scale_offset_by_experts, int n_experts,",
      "                    bool low_latency) {",
      "  using PackedVec = PackedVec<Type>;",
      "  static constexpr int CVT_FP4_NUM_THREADS_PER_SF =",
      "      (CVT_FP4_SF_VEC_SIZE / CVT_FP4_ELTS_PER_THREAD);",
      "  static_assert(sizeof(PackedVec) == sizeof(Type) * CVT_FP4_ELTS_PER_THREAD,",
      "                \"Vec size is not matched.\");",
      "",
      "  int tid = blockIdx.x * blockDim.x + threadIdx.x;",
      "  int colsPerRow = numCols / CVT_FP4_ELTS_PER_THREAD;",
      "",
      "  // Each global thread processes one element",
      "  for (int globalIdx = tid; globalIdx < numRows * colsPerRow;",
      "       globalIdx += gridDim.x * blockDim.x) {",
      "    // Calculate which row and column this global thread should process",
      "    int rowIdx = globalIdx / colsPerRow;",
      "    int colIdx = globalIdx % colsPerRow;",
      "",
      "    int64_t inOffset = rowIdx * colsPerRow + colIdx;",
      "    PackedVec in_vec = reinterpret_cast<PackedVec const*>(in)[inOffset];",
      "    // Get the output tensor offset.",
      "    // Same as inOffset because 8 elements are packed into one uint32_t.",
      "    int64_t outOffset = inOffset;",
      "    auto& out_pos = out[outOffset];",
      "",
      "    // Find index within the experts using different strategies based on expert",
      "    // count",
      "    int rowIdx_in_expert = 0;",
      "    int expert_idx = 0;",
      "",
      "    if constexpr (SMALL_NUM_EXPERTS) {",
      "      for (int i = 0; i < n_experts; i++) {",
      "        uint32_t current_offset = __ldca(&input_offset_by_experts[i]);",
      "        uint32_t next_offset = __ldca(&input_offset_by_experts[i + 1]);",
      "        if (rowIdx >= current_offset && rowIdx < next_offset) {",
      "          rowIdx_in_expert = rowIdx - current_offset;",
      "          expert_idx = i;",
      "          break;",
      "        }",
      "      }",
      "    } else {",
      "      // Load input offsets into registers first, then do the computation.",
      "      // Local array size set to 17 because of register limit.",
      "      uint32_t local_offsets[17];",
      "      for (int chunk_start = 0; chunk_start < n_experts; chunk_start += 16) {",
      "        *reinterpret_cast<int4*>(local_offsets) =",
      "            __ldca(reinterpret_cast<const int4*>(",
      "                &input_offset_by_experts[chunk_start]));",
      "        *reinterpret_cast<int4*>(local_offsets + 4) =",
      "            __ldca(reinterpret_cast<const int4*>(",
      "                &input_offset_by_experts[chunk_start + 4]));",
      "        *reinterpret_cast<int4*>(local_offsets + 8) =",
      "            __ldca(reinterpret_cast<const int4*>(",
      "                &input_offset_by_experts[chunk_start + 8]));",
      "        *reinterpret_cast<int4*>(local_offsets + 12) =",
      "            __ldca(reinterpret_cast<const int4*>(",
      "                &input_offset_by_experts[chunk_start + 12]));",
      "        local_offsets[16] = __ldca(&input_offset_by_experts[chunk_start + 16]);",
      "",
      "// Check against the 16 loaded offsets",
      "#pragma unroll",
      "        for (int i = 0; i < 16; i++) {",
      "          if (rowIdx >= local_offsets[i] && rowIdx < local_offsets[i + 1]) {",
      "            rowIdx_in_expert = rowIdx - local_offsets[i];",
      "            expert_idx = chunk_start + i;",
      "            break;",
      "          }",
      "        }",
      "      }",
      "    }",
      "",
      "    // Get the global scaling factor, which will be applied to the SF.",
      "    // Note SFScale is the same as next GEMM's alpha, which is",
      "    // (448.f / (Alpha_A / 6.f)).",
      "    float const SFScaleVal = SFScale == nullptr ? 1.0f : SFScale[expert_idx];",
      "",
      "    int factor = CVT_FP4_SF_VEC_SIZE * 4;",
      "    // The actual output_scales dim is computed from the padded numCols.",
      "    int32_t numCols_padded = (numCols + factor - 1) / factor * factor;",
      "    int numCols_SFout = numCols_padded / CVT_FP4_SF_VEC_SIZE / 4;",
      "    uint32_t* SFout_in_expert =",
      "        SFout + output_scale_offset_by_experts[expert_idx] * numCols_SFout;",
      "",
      "    auto sf_out =",
      "        cvt_quant_to_fp4_get_sf_out_offset<uint32_t,",
      "                                           CVT_FP4_NUM_THREADS_PER_SF>(",
      "            rowIdx_in_expert, colIdx, numCols, SFout_in_expert);",
      "",
      "    out_pos = cvt_warp_fp16_to_fp4<Type, UE8M0_SF>(in_vec, SFScaleVal, sf_out);",
      "  }",
      "}",
      "",
      "// Kernel for LARGE_M_TOPK = true (large m_topk optimized version)",
      "template <class Type, bool UE8M0_SF = false, bool SMALL_NUM_EXPERTS = false>",
      "__global__ void __launch_bounds__(1024, 4)",
      "    cvt_fp16_to_fp4(int32_t numRows, int32_t numCols, Type const* in,",
      "                    float const* SFScale, uint32_t* out, uint32_t* SFout,",
      "                    uint32_t* input_offset_by_experts,",
      "                    uint32_t* output_scale_offset_by_experts, int n_experts) {",
      "  using PackedVec = PackedVec<Type>;",
      "  static constexpr int CVT_FP4_NUM_THREADS_PER_SF =",
      "      (CVT_FP4_SF_VEC_SIZE / CVT_FP4_ELTS_PER_THREAD);",
      "  static_assert(sizeof(PackedVec) == sizeof(Type) * CVT_FP4_ELTS_PER_THREAD,",
      "                \"Vec size is not matched.\");",
      "  extern __shared__ uint32_t shared_input_offsets[];",
      "",
      "  // Load input offsets into shared memory.",
      "  // If n_experts is larger than 4, use vectorized int4 to save instructions.",
      "  // If n_experts is smaller than 4, read directly.",
      "  if constexpr (SMALL_NUM_EXPERTS) {",
      "    for (int i = threadIdx.x; i < n_experts + 1; i += blockDim.x) {",
      "      shared_input_offsets[i] = input_offset_by_experts[i];",
      "    }",
      "  } else {",
      "    for (int i = threadIdx.x * 4; i < n_experts; i += blockDim.x * 4) {",
      "      *reinterpret_cast<int4*>(&shared_input_offsets[i]) =",
      "          *reinterpret_cast<const int4*>(&input_offset_by_experts[i]);",
      "    }",
      "    if (threadIdx.x == 0) {",
      "      shared_input_offsets[n_experts] = input_offset_by_experts[n_experts];",
      "    }",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  int tid = blockIdx.x * blockDim.x + threadIdx.x;",
      "  int colsPerRow = numCols / CVT_FP4_ELTS_PER_THREAD;",
      "",
      "  // Each global thread processes one element",
      "  for (int globalIdx = tid; globalIdx < numRows * colsPerRow;",
      "       globalIdx += gridDim.x * blockDim.x) {",
      "    // Calculate which row and column this global thread should process",
      "    int rowIdx = globalIdx / colsPerRow;",
      "    int colIdx = globalIdx % colsPerRow;",
      "",
      "    int64_t inOffset = rowIdx * colsPerRow + colIdx;",
      "    PackedVec in_vec = reinterpret_cast<PackedVec const*>(in)[inOffset];",
      "    int64_t outOffset = inOffset;",
      "    auto& out_pos = out[outOffset];",
      "",
      "    // Find expert using binary search for better performance with large m_topk",
      "    int rowIdx_in_expert = 0;",
      "    int expert_idx = 0;",
      "",
      "    // Binary search through experts using shared memory",
      "    int left = 0, right = n_experts - 1;",
      "    while (left <= right) {",
      "      int mid = (left + right) / 2;",
      "      // Get offsets: shared_input_offsets[i] corresponds to",
      "      // input_offset_by_experts[i]",
      "      uint32_t mid_offset = shared_input_offsets[mid];",
      "      uint32_t next_offset = shared_input_offsets[mid + 1];",
      "",
      "      if (rowIdx >= mid_offset && rowIdx < next_offset) {",
      "        rowIdx_in_expert = rowIdx - mid_offset;",
      "        expert_idx = mid;",
      "        break;",
      "      } else if (rowIdx < mid_offset) {",
      "        right = mid - 1;",
      "      } else {",
      "        left = mid + 1;",
      "      }",
      "    }",
      "",
      "    float const SFScaleVal = SFScale == nullptr ? 1.0f : SFScale[expert_idx];",
      "",
      "    int factor = CVT_FP4_SF_VEC_SIZE * 4;",
      "    int32_t numCols_padded = (numCols + factor - 1) / factor * factor;",
      "    int numCols_SFout = numCols_padded / CVT_FP4_SF_VEC_SIZE / 4;",
      "    uint32_t* SFout_in_expert =",
      "        SFout + output_scale_offset_by_experts[expert_idx] * numCols_SFout;",
      "",
      "    auto sf_out =",
      "        cvt_quant_to_fp4_get_sf_out_offset<uint32_t,",
      "                                           CVT_FP4_NUM_THREADS_PER_SF>(",
      "            rowIdx_in_expert, colIdx, numCols, SFout_in_expert);",
      "",
      "    out_pos = cvt_warp_fp16_to_fp4<Type, UE8M0_SF>(in_vec, SFScaleVal, sf_out);",
      "  }",
      "}",
      "",
      "template <typename T>",
      "void quant_impl(void* output, void* output_scale, void* input,",
      "                void* input_global_scale, void* input_offset_by_experts,",
      "                void* output_scale_offset_by_experts, int m_topk, int k,",
      "                int n_experts, cudaStream_t stream) {",
      "  // TODO: this multiProcessorCount should be cached.",
      "  int device;",
      "  cudaGetDevice(&device);",
      "  int multiProcessorCount;",
      "  cudaDeviceGetAttribute(&multiProcessorCount, cudaDevAttrMultiProcessorCount,",
      "                         device);",
      "",
      "  // Grid, Block size.",
      "  // Each thread converts 8 values.",
      "  int const workSizePerRow = k / ELTS_PER_THREAD;",
      "  int const totalWorkSize = m_topk * workSizePerRow;",
      "  dim3 block(std::min(workSizePerRow, 512));",
      "  // Get number of blocks per SM (assume we can fully utilize the SM).",
      "  int const numBlocksPerSM = 2048 / block.x;",
      "  dim3 grid(std::min(static_cast<int>((totalWorkSize + block.x - 1) / block.x),",
      "                     multiProcessorCount * numBlocksPerSM));",
      "  while (grid.x <= multiProcessorCount && block.x > 64) {",
      "    grid.x *= 2;",
      "    block.x = (block.x + 1) / 2;",
      "  }",
      "",
      "  int const blockRepeat =",
      "      (totalWorkSize + block.x * grid.x - 1) / (block.x * grid.x);",
      "  if (blockRepeat > 1) {",
      "    size_t shared_mem_size = (n_experts + 1) * sizeof(uint32_t);",
      "    if (n_experts >= 4) {",
      "      cvt_fp16_to_fp4<T, false, false>",
      "          <<<grid, block, shared_mem_size, stream>>>(",
      "              m_topk, k, reinterpret_cast<T*>(input),",
      "              reinterpret_cast<float*>(input_global_scale),",
      "              reinterpret_cast<uint32_t*>(output),",
      "              reinterpret_cast<uint32_t*>(output_scale),",
      "              reinterpret_cast<uint32_t*>(input_offset_by_experts),",
      "              reinterpret_cast<uint32_t*>(output_scale_offset_by_experts),",
      "              n_experts);",
      "    } else {",
      "      cvt_fp16_to_fp4<T, false, true><<<grid, block, shared_mem_size, stream>>>(",
      "          m_topk, k, reinterpret_cast<T*>(input),",
      "          reinterpret_cast<float*>(input_global_scale),",
      "          reinterpret_cast<uint32_t*>(output),",
      "          reinterpret_cast<uint32_t*>(output_scale),",
      "          reinterpret_cast<uint32_t*>(input_offset_by_experts),",
      "          reinterpret_cast<uint32_t*>(output_scale_offset_by_experts),",
      "          n_experts);",
      "    }",
      "  } else {",
      "    if (n_experts >= 16) {",
      "      cvt_fp16_to_fp4<T, false, false><<<grid, block, 0, stream>>>(",
      "          m_topk, k, reinterpret_cast<T*>(input),",
      "          reinterpret_cast<float*>(input_global_scale),",
      "          reinterpret_cast<uint32_t*>(output),",
      "          reinterpret_cast<uint32_t*>(output_scale),",
      "          reinterpret_cast<uint32_t*>(input_offset_by_experts),",
      "          reinterpret_cast<uint32_t*>(output_scale_offset_by_experts),",
      "          n_experts, /* bool low_latency */ true);",
      "    } else {",
      "      cvt_fp16_to_fp4<T, false, true><<<grid, block, 0, stream>>>(",
      "          m_topk, k, reinterpret_cast<T*>(input),",
      "          reinterpret_cast<float*>(input_global_scale),",
      "          reinterpret_cast<uint32_t*>(output),",
      "          reinterpret_cast<uint32_t*>(output_scale),",
      "          reinterpret_cast<uint32_t*>(input_offset_by_experts),",
      "          reinterpret_cast<uint32_t*>(output_scale_offset_by_experts),",
      "          n_experts, /* bool low_latency */ true);",
      "    }",
      "  }",
      "}",
      "",
      "}  // namespace vllm",
      "",
      "/*Quantization entry for fp4 experts quantization*/",
      "#define CHECK_TH_CUDA(x, m) TORCH_CHECK(x.is_cuda(), m, \"must be a CUDA tensor\")",
      "#define CHECK_CONTIGUOUS(x, m) \\",
      "  TORCH_CHECK(x.is_contiguous(), m, \"must be contiguous\")",
      "#define CHECK_INPUT(x, m) \\",
      "  CHECK_TH_CUDA(x, m);    \\",
      "  CHECK_CONTIGUOUS(x, m);",
      "",
      "constexpr auto HALF = at::ScalarType::Half;",
      "constexpr auto BF16 = at::ScalarType::BFloat16;",
      "constexpr auto FLOAT = at::ScalarType::Float;",
      "constexpr auto INT = at::ScalarType::Int;",
      "constexpr auto UINT8 = at::ScalarType::Byte;",
      "",
      "void scaled_fp4_experts_quant_sm100a(",
      "    torch::Tensor& output, torch::Tensor& output_scale,",
      "    torch::Tensor const& input, torch::Tensor const& input_global_scale,",
      "    torch::Tensor const& input_offset_by_experts,",
      "    torch::Tensor const& output_scale_offset_by_experts) {",
      "  CHECK_INPUT(output, \"output must be a CUDA tensor\");",
      "  CHECK_INPUT(output_scale, \"output_scale must be a CUDA tensor\");",
      "  CHECK_INPUT(input, \"input must be a CUDA tensor\");",
      "  CHECK_INPUT(input_global_scale, \"input_global_scale must be a CUDA tensor\");",
      "  CHECK_INPUT(input_offset_by_experts,",
      "              \"input_offset_by_experts must be a CUDA tensor\");",
      "  CHECK_INPUT(output_scale_offset_by_experts,",
      "              \"output_scale_offset_by_experts must be a CUDA tensor\");",
      "",
      "  TORCH_CHECK(output.dim() == 2);",
      "  TORCH_CHECK(output_scale.dim() == 2);",
      "  TORCH_CHECK(input.dim() == 2);",
      "  TORCH_CHECK(input_global_scale.dim() == 1);",
      "  TORCH_CHECK(input_offset_by_experts.dim() == 1);",
      "  TORCH_CHECK(output_scale_offset_by_experts.dim() == 1);",
      "",
      "  TORCH_CHECK(input.scalar_type() == HALF || input.scalar_type() == BF16);",
      "  TORCH_CHECK(input_global_scale.scalar_type() == FLOAT);",
      "  TORCH_CHECK(input_offset_by_experts.scalar_type() == INT);",
      "  TORCH_CHECK(output_scale_offset_by_experts.scalar_type() == INT);",
      "  // output is uint8 (two nvfp4 values are packed into one uint8)",
      "  // output_scale is int32 (four fp8 values are packed into one int32)",
      "  TORCH_CHECK(output.scalar_type() == UINT8);",
      "  TORCH_CHECK(output_scale.scalar_type() == INT);",
      "",
      "  const int BLOCK_SIZE = 16;",
      "  auto m_topk = input.size(0);",
      "  auto k = input.size(1);",
      "  TORCH_CHECK(k % BLOCK_SIZE == 0, \"k must be a multiple of 16\");",
      "  auto n_experts = input_global_scale.size(0);",
      "  TORCH_CHECK(input_offset_by_experts.size(0) == n_experts + 1);",
      "  TORCH_CHECK(output_scale_offset_by_experts.size(0) == n_experts + 1);",
      "  TORCH_CHECK(output.size(0) == m_topk);",
      "  TORCH_CHECK(output.size(1) == k / 2);",
      "  int scales_k = k / BLOCK_SIZE;",
      "  // 4 means the swizzle requirement by nvidia nvfp4.",
      "  int padded_k = (scales_k + (4 - 1)) / 4 * 4;",
      "  // 4 means 4 fp8 values are packed into one int32",
      "  TORCH_CHECK(output_scale.size(1) * 4 == padded_k);",
      "",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));",
      "  const cudaStream_t stream =",
      "      at::cuda::getCurrentCUDAStream(input.get_device());",
      "",
      "  VLLM_DISPATCH_HALF_TYPES(",
      "      input.scalar_type(), \"nvfp4_experts_quant_kernel\", [&] {",
      "        using cuda_type = vllm::CUDATypeConverter<scalar_t>::Type;",
      "        vllm::quant_impl<cuda_type>(",
      "            output.data_ptr(), output_scale.data_ptr(), input.data_ptr(),",
      "            input_global_scale.data_ptr(), input_offset_by_experts.data_ptr(),",
      "            output_scale_offset_by_experts.data_ptr(), m_topk, k, n_experts,",
      "            stream);",
      "      });",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fused_kernels/quant_conversions.cuh",
    "source": [
      "#pragma once",
      "",
      "/**",
      " * __device__ helper functions to deal with float -> quant datatype conversion",
      " */",
      "",
      "#include \"quantization/vectorization.cuh\"",
      "// TODO(luka/varun):refactor common.cuh to use this file instead",
      "#include \"quantization/fp8/common.cuh\"",
      "",
      "namespace vllm {",
      "",
      "// TODO(luka/varun): combine into common utilities for int8",
      "//  (with int8_quant_kernels.cu)",
      "static __device__ __forceinline__ int8_t float_to_int8_rn(float const x) {",
      "#ifdef USE_ROCM",
      "  static const float i8_min =",
      "      static_cast<float>(std::numeric_limits<int8_t>::min());",
      "  static const float i8_max =",
      "      static_cast<float>(std::numeric_limits<int8_t>::max());",
      "  // round",
      "  float dst = std::nearbyint(x);",
      "  // saturate",
      "",
      "  // See https://github.com/pytorch/pytorch/issues/127666",
      "  // See https://github.com/llvm/llvm-project/issues/95183",
      "  // hip-clang std::clamp __glibcxx_assert_fail host function when building on",
      "  // Arch/gcc14. The following replaces std::clamp usage with similar logic",
      "  // dst = std::clamp(dst, i8_min, i8_max);",
      "  dst = (dst < i8_min) ? i8_min : (dst > i8_max) ? i8_max : dst;",
      "  return static_cast<int8_t>(dst);",
      "#else",
      "  // CUDA path",
      "  uint32_t dst;",
      "  asm volatile(\"cvt.rni.sat.s8.f32 %0, %1;\" : \"=r\"(dst) : \"f\"(x));",
      "  return reinterpret_cast<const int8_t&>(dst);",
      "#endif",
      "}",
      "",
      "template <typename fp8_type>",
      "static __device__ __forceinline__ fp8_type float_to_fp8(float const x) {",
      "  float const r =",
      "      fmax(-quant_type_max_v<fp8_type>, fmin(x, quant_type_max_v<fp8_type>));",
      "  return static_cast<fp8_type>(r);",
      "}",
      "",
      "template <typename quant_type_t, bool is_scale_inverted, typename enable = void>",
      "struct ScaledQuant;",
      "",
      "template <typename quant_type_t, bool is_scale_inverted>",
      "struct ScaledQuant<",
      "    quant_type_t, is_scale_inverted,",
      "    typename std::enable_if_t<std::is_same_v<quant_type_t, int8_t>>> {",
      "  static __device__ __forceinline__ quant_type_t quant_fn(float const x,",
      "                                                          float const scale) {",
      "    if constexpr (is_scale_inverted) {",
      "      return float_to_int8_rn(x * scale);",
      "    } else {",
      "      return float_to_int8_rn(x / scale);",
      "    }",
      "  }",
      "};",
      "",
      "template <typename quant_type_t, bool is_scale_inverted>",
      "struct ScaledQuant<quant_type_t, is_scale_inverted,",
      "                   typename std::enable_if_t<",
      "                       std::is_same_v<quant_type_t, c10::Float8_e4m3fn> ||",
      "                       std::is_same_v<quant_type_t, c10::Float8_e4m3fnuz>>> {",
      "  static __device__ __forceinline__ quant_type_t quant_fn(float const x,",
      "                                                          float const scale) {",
      "    if constexpr (is_scale_inverted) {",
      "      return float_to_fp8<quant_type_t>(x * scale);",
      "    } else {",
      "      return float_to_fp8<quant_type_t>(x / scale);",
      "    }",
      "  }",
      "};",
      "",
      "template <typename scalar_t, typename quant_type_t, bool is_scale_inverted>",
      "__device__ void scaled_quant_conversion(quant_type_t* __restrict__ output,",
      "                                        scalar_t const* __restrict__ input,",
      "                                        float const scale, int const tid,",
      "                                        int const num_elements,",
      "                                        int const step) {",
      "  for (int i = tid; i < num_elements; i += step) {",
      "    output[i] = ScaledQuant<quant_type_t, is_scale_inverted>(input[i], scale);",
      "  }",
      "}",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu",
    "source": [
      "",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "",
      "#include \"../../dispatch_utils.h\"",
      "#include \"layernorm_utils.cuh\"",
      "#include \"quant_conversions.cuh\"",
      "",
      "namespace vllm {",
      "",
      "template <typename scalar_t, typename scalar_out_t, bool has_residual = false>",
      "__device__ void rms_norm_dynamic_per_token_quant_vec(",
      "    scalar_out_t* __restrict__ out,       // [..., hidden_size]",
      "    float* __restrict__ scales,           // [num_tokens]",
      "    scalar_t const* __restrict__ input,   // [..., hidden_size]",
      "    scalar_t const* __restrict__ weight,  // [hidden_size]",
      "    float const* scale_ub, float const var_epsilon, int32_t const hidden_size,",
      "    scalar_t* __restrict__ residual = nullptr) {",
      "  float rms = 0.0f;",
      "  float token_scale = 0.0f;",
      "",
      "  // Compute rms",
      "  vllm::vectorized::compute_rms<scalar_t, has_residual>(",
      "      &rms, input, hidden_size, var_epsilon, residual);",
      "",
      "  // Compute scale",
      "  vllm::vectorized::compute_dynamic_per_token_scales<scalar_t, scalar_out_t,",
      "                                                     has_residual>(",
      "      &token_scale, scales, input, weight, rms, scale_ub, hidden_size,",
      "      residual);",
      "",
      "  // RMS Norm + Quant",
      "  if constexpr (std::is_same_v<scalar_out_t, int8_t>) {",
      "    vllm::vectorized::norm_and_quant<scalar_t, scalar_out_t, true,",
      "                                     has_residual>(",
      "        out, input, weight, rms, 1.0f / token_scale, hidden_size, residual);",
      "  } else {",
      "    // FP8 - Do not invert token_scale for exact match with FBGemm",
      "    vllm::vectorized::norm_and_quant<scalar_t, scalar_out_t, false,",
      "                                     has_residual>(",
      "        out, input, weight, rms, token_scale, hidden_size, residual);",
      "  }",
      "}",
      "",
      "// RMS norm + quant kernel",
      "template <typename scalar_t, typename scalar_out_t, bool has_residual = false>",
      "__global__ void rms_norm_dynamic_per_token_quant_kernel(",
      "    scalar_out_t* __restrict__ out,       // [..., hidden_size]",
      "    float* __restrict__ scales,           // [num_tokens]",
      "    scalar_t const* __restrict__ input,   // [..., hidden_size]",
      "    scalar_t const* __restrict__ weight,  // [hidden_size]",
      "    float const* scale_ub, float const var_epsilon, int32_t const hidden_size,",
      "    scalar_t* __restrict__ residual = nullptr) {",
      "  // For vectorization, token_input and token_output pointers need to be",
      "  // aligned at 8-byte and 4-byte addresses respectively.",
      "  bool const can_vectorize = hidden_size % 4 == 0;",
      "",
      "  if (can_vectorize) {",
      "    return rms_norm_dynamic_per_token_quant_vec<scalar_t, scalar_out_t,",
      "                                                has_residual>(",
      "        out, scales, input, weight, scale_ub, var_epsilon, hidden_size,",
      "        residual);",
      "  }",
      "",
      "  float rms = 0.0f;",
      "  float token_scale = 0.0f;",
      "",
      "  // Compute RMS",
      "  vllm::compute_rms<scalar_t, has_residual>(&rms, input, hidden_size,",
      "                                            var_epsilon, residual);",
      "  // Compute Scale",
      "  vllm::compute_dynamic_per_token_scales<scalar_t, scalar_out_t, has_residual>(",
      "      &token_scale, scales, input, weight, rms, scale_ub, hidden_size,",
      "      residual);",
      "",
      "  // RMS Norm + Quant",
      "  if constexpr (std::is_same_v<scalar_out_t, int8_t>) {",
      "    vllm::norm_and_quant<scalar_t, scalar_out_t, true, has_residual>(",
      "        out, input, weight, rms, 1.0f / token_scale, hidden_size, residual);",
      "  } else {",
      "    // FP8 - Do not invert s_token_scale for exact match with FBGemm",
      "    vllm::norm_and_quant<scalar_t, scalar_out_t, false, has_residual>(",
      "        out, input, weight, rms, token_scale, hidden_size, residual);",
      "  }",
      "}",
      "}  // namespace vllm",
      "",
      "// Residual add + RMS norm + dynamic per token",
      "template <typename scalar_in_t>",
      "void rms_norm_dynamic_per_token_quant_dispatch(",
      "    torch::Tensor& out,           // [..., hidden_size]",
      "    torch::Tensor const& input,   // [..., hidden_size]",
      "    torch::Tensor const& weight,  // [hidden_size]",
      "    torch::Tensor& scales,        // [num_tokens]",
      "    double const var_epsilon,     // Variance epsilon used in norm calculation",
      "    std::optional<at::Tensor> const& scale_ub,",
      "    std::optional<at::Tensor>& residual) {",
      "  int32_t hidden_size = input.size(-1);",
      "  auto num_tokens = input.numel() / hidden_size;",
      "",
      "  dim3 grid(num_tokens);",
      "  dim3 block(std::min(hidden_size, 1024));",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "",
      "  if (residual.has_value()) {",
      "    VLLM_DISPATCH_QUANT_TYPES(",
      "        out.scalar_type(), \"rms_norm_dynamic_per_token_quant_kernel\", [&] {",
      "          vllm::rms_norm_dynamic_per_token_quant_kernel<scalar_in_t, scalar_t,",
      "                                                        true>",
      "              <<<grid, block, 0, stream>>>(",
      "                  out.data_ptr<scalar_t>(), scales.data_ptr<float>(),",
      "                  input.data_ptr<scalar_in_t>(), weight.data_ptr<scalar_in_t>(),",
      "                  scale_ub.has_value() ? scale_ub->data_ptr<float>() : nullptr,",
      "                  var_epsilon, hidden_size, residual->data_ptr<scalar_in_t>());",
      "        });",
      "",
      "  } else {",
      "    VLLM_DISPATCH_QUANT_TYPES(",
      "        out.scalar_type(), \"rms_norm_dynamic_per_token_quant_kernel\", [&] {",
      "          vllm::rms_norm_dynamic_per_token_quant_kernel<scalar_in_t, scalar_t,",
      "                                                        false>",
      "              <<<grid, block, 0, stream>>>(",
      "                  out.data_ptr<scalar_t>(), scales.data_ptr<float>(),",
      "                  input.data_ptr<scalar_in_t>(), weight.data_ptr<scalar_in_t>(),",
      "                  scale_ub.has_value() ? scale_ub->data_ptr<float>() : nullptr,",
      "                  var_epsilon, hidden_size, nullptr);",
      "        });",
      "  }",
      "}",
      "",
      "void rms_norm_dynamic_per_token_quant(",
      "    torch::Tensor& out,           // [..., hidden_size]",
      "    torch::Tensor const& input,   // [..., hidden_size]",
      "    torch::Tensor const& weight,  // [hidden_size]",
      "    torch::Tensor& scales,        // [num_tokens]",
      "    double const var_epsilon,     // Variance epsilon used in norm calculation",
      "    std::optional<at::Tensor> scale_ub, std::optional<at::Tensor> residual) {",
      "  static c10::ScalarType kFp8Type = is_fp8_ocp()",
      "                                        ? c10::ScalarType::Float8_e4m3fn",
      "                                        : c10::ScalarType::Float8_e4m3fnuz;",
      "  TORCH_CHECK(out.dtype() == kFp8Type || out.dtype() == torch::kInt8);",
      "  TORCH_CHECK(out.is_contiguous() && input.is_contiguous());",
      "",
      "  if (scale_ub.has_value()) {",
      "    TORCH_CHECK(out.dtype() == kFp8Type);",
      "  }",
      "  TORCH_CHECK(scales.dtype() == torch::kFloat32);",
      "",
      "  VLLM_DISPATCH_FLOATING_TYPES(",
      "      input.scalar_type(), \"rms_norm_dynamic_per_token_quant_dispatch\", [&] {",
      "        rms_norm_dynamic_per_token_quant_dispatch<scalar_t>(",
      "            out, input, weight, scales, var_epsilon, scale_ub, residual);",
      "      });",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/fused_kernels/layernorm_utils.cuh",
    "source": [
      "#pragma once",
      "",
      "/**",
      " * __device__ layernorm utilities.",
      " */",
      "",
      "#include \"quantization/vectorization.cuh\"",
      "#include \"quantization/utils.cuh\"",
      "#include \"quant_conversions.cuh\"",
      "",
      "#ifndef USE_ROCM",
      "  #include <cub/cub.cuh>",
      "#else",
      "  #include <hipcub/hipcub.hpp>",
      "#endif",
      "",
      "namespace vllm {",
      "",
      "// has_residual must be true, if residual is not a nullptr",
      "template <typename scalar_t, bool has_residual = false>",
      "__device__ void compute_rms(float* rms, scalar_t const* __restrict__ input,",
      "                            int32_t const hidden_size, float const epsilon,",
      "                            scalar_t const* __restrict__ residual = nullptr) {",
      "  int64_t const token_offset = blockIdx.x * static_cast<int64_t>(hidden_size);",
      "  // sum of squares",
      "  float ss = 0.0f;",
      "",
      "  for (auto i = threadIdx.x; i < hidden_size; i += blockDim.x) {",
      "    float x = static_cast<float>(input[token_offset + i]);",
      "    if constexpr (has_residual) {",
      "      x += static_cast<float>(residual[token_offset + i]);",
      "    }",
      "",
      "    ss += x * x;",
      "  }",
      "",
      "  using BlockReduce = cub::BlockReduce<float, 1024>;",
      "  __shared__ typename BlockReduce::TempStorage reduceStore;",
      "  ss = BlockReduce(reduceStore).Reduce(ss, cub::Sum{}, blockDim.x);",
      "",
      "  __shared__ float s_rms;",
      "  if (threadIdx.x == 0) {",
      "    s_rms = rsqrtf(ss / hidden_size + epsilon);",
      "  }",
      "  __syncthreads();",
      "",
      "  *rms = s_rms;",
      "}",
      "",
      "template <typename scalar_t, typename scalar_out_t, bool has_residual = false>",
      "__device__ void compute_dynamic_per_token_scales(",
      "    float* __restrict__ token_scale, float* __restrict__ all_token_scales,",
      "    scalar_t const* __restrict__ input, scalar_t const* __restrict__ weight,",
      "    float const rms, float const* __restrict__ scale_ub,",
      "    int32_t const hidden_size,",
      "    scalar_t const* __restrict__ residual = nullptr) {",
      "  int64_t const token_offset = blockIdx.x * static_cast<int64_t>(hidden_size);",
      "  ;",
      "  constexpr scalar_out_t qmax{quant_type_max_v<scalar_out_t>};",
      "",
      "  float block_absmax_val_maybe = 0.0f;",
      "  for (auto i = threadIdx.x; i < hidden_size; i += blockDim.x) {",
      "    float x = static_cast<float>(input[token_offset + i]);",
      "    if constexpr (has_residual) {",
      "      x += static_cast<float>(residual[token_offset + i]);",
      "    }",
      "",
      "    x = static_cast<float>(static_cast<scalar_t>(x * rms) * weight[i]);",
      "    block_absmax_val_maybe = fmaxf(block_absmax_val_maybe, fabsf(x));",
      "  }",
      "",
      "  using BlockReduce = cub::BlockReduce<float, 1024>;",
      "  __shared__ typename BlockReduce::TempStorage reduceStore;",
      "  block_absmax_val_maybe =",
      "      BlockReduce(reduceStore)",
      "          .Reduce(block_absmax_val_maybe, cub::Max{}, blockDim.x);",
      "",
      "  __shared__ float s_token_scale;",
      "  if (threadIdx.x == 0) {",
      "    float scale = 0.0f;",
      "    if (scale_ub) {",
      "      scale = min(block_absmax_val_maybe, *scale_ub);",
      "    } else {",
      "      scale = block_absmax_val_maybe;",
      "    }",
      "    // token scale computation",
      "    scale = max(scale / qmax, min_scaling_factor<scalar_out_t>::val());",
      "    s_token_scale = scale;                 // Shared memory store",
      "    all_token_scales[blockIdx.x] = scale;  // Global output store",
      "  }",
      "  __syncthreads();",
      "",
      "  *token_scale = s_token_scale;",
      "}",
      "",
      "template <typename scalar_t, typename scalar_out_t, bool is_scale_inverted,",
      "          bool has_residual = false>",
      "__device__ void norm_and_quant(scalar_out_t* __restrict__ output,",
      "                               scalar_t const* __restrict__ input,",
      "                               scalar_t const* __restrict__ weight,",
      "                               float const rms, float const scale,",
      "                               int32_t const hidden_size,",
      "                               scalar_t* __restrict__ residual = nullptr) {",
      "  int64_t const token_offset = blockIdx.x * static_cast<int64_t>(hidden_size);",
      "  ;",
      "",
      "  for (auto i = threadIdx.x; i < hidden_size; i += blockDim.x) {",
      "    float x = static_cast<float>(input[token_offset + i]);",
      "    if constexpr (has_residual) {",
      "      x += static_cast<float>(residual[token_offset + i]);",
      "      residual[token_offset + i] = static_cast<scalar_t>(x);",
      "    }",
      "    // Norm",
      "    x = static_cast<float>(static_cast<scalar_t>(x * rms) * weight[i]);",
      "    // Quant",
      "    output[token_offset + i] =",
      "        ScaledQuant<scalar_out_t, is_scale_inverted>::quant_fn(x, scale);",
      "  }",
      "}",
      "",
      "namespace vectorized {",
      "",
      "// Compute 1.0/rms(input)",
      "// hidden_size must be a multiple of 4",
      "template <typename scalar_t, bool has_residual = false>",
      "__device__ void compute_rms(float* rms, scalar_t const* __restrict__ input,",
      "                            int32_t const hidden_size, float const epsilon,",
      "                            scalar_t const* __restrict__ residual = nullptr) {",
      "  int64_t const token_offset = blockIdx.x * static_cast<int64_t>(hidden_size);",
      "",
      "  // Vectorized input/output to better utilize memory bandwidth.",
      "  vec4_t<scalar_t> const* vec_input =",
      "      reinterpret_cast<vec4_t<scalar_t> const*>(&input[token_offset]);",
      "  vec4_t<scalar_t> const* vec_residual = nullptr;",
      "  if constexpr (has_residual) {",
      "    vec_residual =",
      "        reinterpret_cast<vec4_t<scalar_t> const*>(&residual[token_offset]);",
      "  }",
      "",
      "  // sum of squares",
      "  float ss = 0.0f;",
      "",
      "  const int VEC_SIZE = 4;",
      "  int32_t const num_vec_elems = hidden_size >> 2;",
      "",
      "#pragma unroll 4",
      "  for (auto i = threadIdx.x; i < num_vec_elems; i += blockDim.x) {",
      "    vec4_t<scalar_t> in = vec_input[i];",
      "",
      "    vec4_t<float> x;",
      "#pragma unroll",
      "    for (int j = 0; j < VEC_SIZE; ++j) {",
      "      x.val[j] = static_cast<float>(in.val[j]);",
      "    }",
      "",
      "    if constexpr (has_residual) {",
      "      vec4_t<scalar_t> r = vec_residual[i];",
      "#pragma unroll",
      "      for (int j = 0; j < VEC_SIZE; ++j) {",
      "        x.val[j] += static_cast<float>(r.val[j]);",
      "      }",
      "    }",
      "",
      "#pragma unroll",
      "    for (int j = 0; j < VEC_SIZE; ++j) {",
      "      ss += x.val[j] * x.val[j];",
      "    }",
      "  }",
      "",
      "  using BlockReduce = cub::BlockReduce<float, 1024>;",
      "  __shared__ typename BlockReduce::TempStorage reduceStore;",
      "  ss = BlockReduce(reduceStore).Reduce(ss, cub::Sum{}, blockDim.x);",
      "",
      "  __shared__ float s_rms;",
      "  if (threadIdx.x == 0) {",
      "    s_rms = rsqrtf(ss / hidden_size + epsilon);",
      "  }",
      "  __syncthreads();",
      "",
      "  *rms = s_rms;",
      "}",
      "",
      "// Vectorized version of vllm::compute_dynamic_per_token_scales",
      "// hidden_size must be a multiple of 4",
      "template <typename scalar_t, typename scalar_out_t, bool has_residual = false>",
      "__device__ void compute_dynamic_per_token_scales(",
      "    float* __restrict__ token_scale, float* __restrict__ all_token_scales,",
      "    scalar_t const* __restrict__ input, scalar_t const* __restrict__ weight,",
      "    float const rms, float const* __restrict__ scale_ub,",
      "    int32_t const hidden_size,",
      "    scalar_t const* __restrict__ residual = nullptr) {",
      "  int64_t const token_offset = blockIdx.x * static_cast<int64_t>(hidden_size);",
      "  ;",
      "",
      "  // Vectorized input/weight/residual to better utilize memory bandwidth.",
      "  vec4_t<scalar_t> const* vec_input =",
      "      reinterpret_cast<vec4_t<scalar_t> const*>(&input[token_offset]);",
      "  vec4_t<scalar_t> const* vec_weight =",
      "      reinterpret_cast<vec4_t<scalar_t> const*>(weight);",
      "  vec4_t<scalar_t> const* vec_residual = nullptr;",
      "  if constexpr (has_residual) {",
      "    vec_residual =",
      "        reinterpret_cast<vec4_t<scalar_t> const*>(&residual[token_offset]);",
      "  }",
      "",
      "  constexpr scalar_out_t qmax{quant_type_max_v<scalar_out_t>};",
      "",
      "  const int VEC_SIZE = 4;",
      "  int32_t const num_vec_elems = hidden_size >> 2;",
      "  float block_absmax_val_maybe = 0.0f;",
      "",
      "#pragma unroll 4",
      "  for (auto i = threadIdx.x; i < num_vec_elems; i += blockDim.x) {",
      "    vec4_t<scalar_t> in = vec_input[i];",
      "    vec4_t<scalar_t> const w = vec_weight[i];",
      "",
      "    vec4_t<float> x;",
      "#pragma unroll",
      "    for (int j = 0; j < VEC_SIZE; ++j) {",
      "      x.val[j] = static_cast<float>(in.val[j]);",
      "    }",
      "",
      "    if constexpr (has_residual) {",
      "      vec4_t<scalar_t> r = vec_residual[i];",
      "#pragma unroll",
      "      for (int j = 0; j < VEC_SIZE; ++j) {",
      "        x.val[j] += static_cast<float>(r.val[j]);",
      "      }",
      "    }",
      "",
      "#pragma unroll",
      "    for (int j = 0; j < VEC_SIZE; ++j) {",
      "      block_absmax_val_maybe =",
      "          fmaxf(block_absmax_val_maybe,",
      "                fabs(static_cast<scalar_t>(x.val[j] * rms) * w.val[j]));",
      "    }",
      "  }",
      "",
      "  using BlockReduce = cub::BlockReduce<float, 1024>;",
      "  __shared__ typename BlockReduce::TempStorage reduceStore;",
      "  block_absmax_val_maybe =",
      "      BlockReduce(reduceStore)",
      "          .Reduce(block_absmax_val_maybe, cub::Max{}, blockDim.x);",
      "",
      "  __shared__ float s_token_scale;",
      "  if (threadIdx.x == 0) {",
      "    float scale = 0.0f;",
      "    if (scale_ub) {",
      "      scale = min(block_absmax_val_maybe, *scale_ub);",
      "    } else {",
      "      scale = block_absmax_val_maybe;",
      "    }",
      "    // token scale computation",
      "    scale = max(scale / qmax, min_scaling_factor<scalar_out_t>::val());",
      "    s_token_scale = scale;                 // shared memory store",
      "    all_token_scales[blockIdx.x] = scale;  // global output store",
      "  }",
      "  __syncthreads();",
      "",
      "  *token_scale = s_token_scale;",
      "}",
      "",
      "// hidden_size must be a multiple of 4",
      "template <typename scalar_t, typename scalar_out_t, bool is_scale_inverted,",
      "          bool has_residual = false>",
      "__device__ void norm_and_quant(scalar_out_t* __restrict__ output,",
      "                               scalar_t const* __restrict__ input,",
      "                               scalar_t const* __restrict__ weight,",
      "                               float const rms, float const scale,",
      "                               int32_t const hidden_size,",
      "                               scalar_t* __restrict__ residual = nullptr) {",
      "  int64_t const token_offset = blockIdx.x * static_cast<int64_t>(hidden_size);",
      "  ;",
      "",
      "  // Vectorized input/output/weight/residual to better utilize memory bandwidth.",
      "  vec4_t<scalar_t> const* vec_input =",
      "      reinterpret_cast<vec4_t<scalar_t> const*>(&input[token_offset]);",
      "  vec4_t<scalar_t> const* vec_weight =",
      "      reinterpret_cast<vec4_t<scalar_t> const*>(weight);",
      "  q8x4_t<scalar_out_t>* vec_output =",
      "      reinterpret_cast<q8x4_t<scalar_out_t>*>(&output[token_offset]);",
      "  vec4_t<scalar_t>* vec_residual = nullptr;",
      "  if constexpr (has_residual) {",
      "    vec_residual = reinterpret_cast<vec4_t<scalar_t>*>(&residual[token_offset]);",
      "  }",
      "",
      "  const int VEC_SIZE = 4;",
      "  int32_t const num_vec_elems = hidden_size >> 2;",
      "",
      "// TODO(luka/varun) extract into type-agnostic vectorized quant function to",
      "//  replace scaled_fp8_conversion_vec",
      "#pragma unroll 4",
      "  for (auto i = threadIdx.x; i < num_vec_elems; i += blockDim.x) {",
      "    vec4_t<scalar_t> const in = vec_input[i];",
      "    vec4_t<scalar_t> const w = vec_weight[i];",
      "",
      "    vec4_t<float> x;",
      "#pragma unroll",
      "    for (int j = 0; j < VEC_SIZE; ++j) {",
      "      x.val[j] = static_cast<float>(in.val[j]);",
      "    }",
      "",
      "    if constexpr (has_residual) {",
      "      vec4_t<scalar_t> r = vec_residual[i];",
      "#pragma unroll",
      "      for (int j = 0; j < VEC_SIZE; ++j) {",
      "        x.val[j] += static_cast<float>(r.val[j]);",
      "      }",
      "// Update residual",
      "#pragma unroll",
      "      for (int j = 0; j < VEC_SIZE; ++j) {",
      "        r.val[j] = static_cast<scalar_t>(x.val[j]);",
      "      }",
      "      vec_residual[i] = r;",
      "    }",
      "",
      "    q8x4_t<scalar_out_t> out;",
      "#pragma unroll",
      "    for (int j = 0; j < VEC_SIZE; ++j) {",
      "      out.val[j] = ScaledQuant<scalar_out_t, is_scale_inverted>::quant_fn(",
      "          static_cast<scalar_t>(x.val[j] * rms) * w.val[j], scale);",
      "    }",
      "    vec_output[i] = out;",
      "  }",
      "}",
      "",
      "}  // namespace vectorized",
      "",
      "}  // namespace vllm"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq/qdq_2.cuh",
    "source": [
      "/*",
      "Copied from https://github.com/turboderp/exllamav2",
      "*/",
      "",
      "#ifndef _qdq_2_cuh",
      "#define _qdq_2_cuh",
      "",
      "#include \"qdq_util.cuh\"",
      "",
      "namespace vllm {",
      "namespace gptq {",
      "",
      "// Permutation:",
      "//",
      "// ffddbb99 77553311  eeccaa88 66442200",
      "",
      "__forceinline__ __device__ void shuffle_2bit_16(uint32_t* q, int stride) {",
      "  uint32_t qa = q[0];",
      "  uint32_t qb = 0;",
      "",
      "#pragma unroll",
      "  for (int i = 0; i < 8; i++) {",
      "    uint32_t qa0 = qa & 0x03;",
      "    uint32_t qa1 = (qa & 0x0c) >> 2;",
      "    qa >>= 4;",
      "    qb |= (qa1 << (i * 2 + 16));",
      "    qb |= (qa0 << (i * 2));",
      "  }",
      "  q[0] = qb;",
      "}",
      "",
      "__forceinline__ __device__ void dequant_2bit_16(const uint32_t q_0,",
      "                                                half2 (&dq)[8], int stride,",
      "                                                const uint32_t zero) {",
      "  const uint32_t c0 = 0x64006400;",
      "  const half y4_ = __float2half_rn(1.0f / 4.0f);",
      "  const half y16_ = __float2half_rn(1.0f / 16.0f);",
      "  const half y64_ = __float2half_rn(1.0f / 64.0f);",
      "  const half2 y4 = __halves2half2(y4_, y4_);",
      "  const half2 y16 = __halves2half2(y16_, y16_);",
      "  const half2 y64 = __halves2half2(y64_, y64_);",
      "",
      "  const half_uint16 z1_(0xe400 | zero);  // half(-1024.0f - zero);",
      "  const half z4_ = __hsub(__int2half_rn(-256), __int2half_rn(zero));",
      "  const half z16_ = __hsub(__int2half_rn(-64), __int2half_rn(zero));",
      "  const half z64_ = __hsub(__int2half_rn(-16), __int2half_rn(zero));",
      "  const half2 z1 = __half2half2(z1_.as_half);",
      "  const half2 z4 = __half2half2(z4_);",
      "  const half2 z16 = __half2half2(z16_);",
      "  const half2 z64 = __half2half2(z64_);",
      "",
      "  uint32_t qa = q_0;",
      "  half2_uint32 q0((qa & 0x00030003) | c0);  // half2(q[ 0], q[ 1])      + 1024",
      "  half2_uint32 q1((qa & 0x000c000c) | c0);  // half2(q[ 2], q[ 3]) *  4 + 1024",
      "  half2_uint32 q2((qa & 0x00300030) | c0);  // half2(q[ 4], q[ 5]) * 16 + 1024",
      "  half2_uint32 q3((qa & 0x00c000c0) | c0);  // half2(q[ 6], q[ 7]) * 64 + 1024",
      "  qa >>= 8;",
      "  half2_uint32 q4((qa & 0x00030003) | c0);  // half2(q[ 8], q[ 8])      + 1024",
      "  half2_uint32 q5((qa & 0x000c000c) | c0);  // half2(q[10], q[11]) *  4 + 1024",
      "  half2_uint32 q6((qa & 0x00300030) | c0);  // half2(q[12], q[13]) * 16 + 1024",
      "  half2_uint32 q7((qa & 0x00c000c0) | c0);  // half2(q[14], q[15]) * 64 + 1024",
      "",
      "  dq[0] = __hadd2(q0.as_half2, z1);",
      "  dq[1] = __hfma2(q1.as_half2, y4, z4);",
      "  dq[2] = __hfma2(q2.as_half2, y16, z16);",
      "  dq[3] = __hfma2(q3.as_half2, y64, z64);",
      "  dq[4] = __hadd2(q4.as_half2, z1);",
      "  dq[5] = __hfma2(q5.as_half2, y4, z4);",
      "  dq[6] = __hfma2(q6.as_half2, y16, z16);",
      "  dq[7] = __hfma2(q7.as_half2, y64, z64);",
      "}",
      "",
      "}  // namespace gptq",
      "}  // namespace vllm",
      "",
      "#endif"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq/qdq_8.cuh",
    "source": [
      "/*",
      "Copied from https://github.com/turboderp/exllamav2",
      "*/",
      "",
      "#ifndef _qdq_8_cuh",
      "#define _qdq_8_cuh",
      "",
      "#include \"qdq_util.cuh\"",
      "",
      "namespace vllm {",
      "namespace gptq {",
      "",
      "__forceinline__ __device__ void shuffle_8bit_4(uint32_t* q, int stride) {}",
      "",
      "__forceinline__ __device__ void dequant_8bit_8(const uint32_t q_0,",
      "                                               const uint32_t q_1,",
      "                                               half2 (&dq)[4], int stride,",
      "                                               const uint32_t zero) {",
      "  half dqh[8];",
      "  for (int i = 0; i < 4; i++) dqh[i] = dq_ns(exb(q_0, i * 8, 0xff), zero);",
      "  for (int i = 0; i < 4; i++) dqh[i + 4] = dq_ns(exb(q_1, i * 8, 0xff), zero);",
      "",
      "  for (int i = 0; i < 4; i++)",
      "    dq[i] = __halves2half2(dqh[i * 2], dqh[i * 2 + 1]);",
      "}",
      "",
      "}  // namespace gptq",
      "}  // namespace vllm",
      "",
      "#endif"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq/q_gemm.cu",
    "source": [
      "/*",
      "Adapted from https://github.com/turboderp/exllamav2 and",
      "https://github.com/qwopqwop200/GPTQ-for-LLaMa",
      "*/",
      "",
      "#include <cstdint>",
      "#include <cstdio>",
      "",
      "#include <torch/all.h>",
      "#include <c10/cuda/CUDAGuard.h>",
      "#include <ATen/cuda/CUDAContext.h>",
      "#include <cuda_runtime.h>",
      "#include <cuda_fp16.h>",
      "",
      "#include \"compat.cuh\"",
      "#include \"matrix_view.cuh\"",
      "#include \"qdq_2.cuh\"",
      "#include \"qdq_3.cuh\"",
      "#include \"qdq_4.cuh\"",
      "#include \"qdq_8.cuh\"",
      "",
      "namespace vllm {",
      "namespace gptq {",
      "",
      "#define BLOCK_KN_SIZE 128",
      "#define BLOCK_M_SIZE_MAX 8",
      "#define MAX_GROUPS_IN_BLOCK (BLOCK_KN_SIZE / 32)",
      "#define MAX_Q_GEMM_ROWS 50",
      "#define MAX_Q_GEMM_ROWS_8BIT 24",
      "#define MAX_ALT_GEMM_ROWS 8",
      "#define THREADS_X 32",
      "#define THREADS_Y 32",
      "#define DIVIDE(x, size) (((x) + (size) - 1) / (size))",
      "",
      "#if defined(USE_ROCM)",
      "  #include <hipblas/hipblas.h>",
      "__host__ __forceinline__ hipblasStatus_t __compat_hipblasHgemm(",
      "    hipblasHandle_t handle, hipblasOperation_t transA,",
      "    hipblasOperation_t transB, int m, int n, int k, const half* alpha,",
      "    const half* AP, int lda, const half* BP, int ldb, const half* beta,",
      "    half* CP, int ldc) {",
      "  return hipblasHgemm(handle, transA, transB, m, n, k,",
      "                      reinterpret_cast<const hipblasHalf*>(alpha),",
      "                      reinterpret_cast<const hipblasHalf*>(AP), lda,",
      "                      reinterpret_cast<const hipblasHalf*>(BP), ldb,",
      "                      reinterpret_cast<const hipblasHalf*>(beta),",
      "                      reinterpret_cast<hipblasHalf*>(CP), ldc);",
      "}",
      "  #define hipblasHgemm __compat_hipblasHgemm",
      "",
      "  // Previous version of PyTorch were converting to rocBLAS instead of hipBLAS.",
      "  #define rocblas_operation_none HIPBLAS_OP_N",
      "  #define rocblas_hgemm __compat_hipblasHgemm",
      "#endif",
      "",
      "__forceinline__ __device__ half2 dot22_8(half2 (&dq)[4], const half* a_ptr,",
      "                                         const half2 g_result) {",
      "  half2 result = {};",
      "  const half2* a2_ptr = (const half2*)a_ptr;",
      "#pragma unroll",
      "  for (int i = 0; i < 4; i++) result = __hfma2(dq[i], *a2_ptr++, result);",
      "  return __hadd2(result, g_result);",
      "}",
      "",
      "__forceinline__ __device__ float dot22_8_f(half2 (&dq)[4], const half* a_ptr) {",
      "  half2 result = {};",
      "  const half2* a2_ptr = (const half2*)a_ptr;",
      "#pragma unroll",
      "  for (int i = 0; i < 4; i++) result = __hfma2(dq[i], *a2_ptr++, result);",
      "  return __half2float(__low2half(result)) + __half2float(__high2half(result));",
      "}",
      "",
      "__forceinline__ __device__ half2 dot22_8(half2 (&dq)[4], const half* a_ptr,",
      "                                         const half2 g_result,",
      "                                         const half qs_h) {",
      "  half2 result = {};",
      "  const half2* a2_ptr = (const half2*)a_ptr;",
      "#pragma unroll",
      "  for (int i = 0; i < 4; i++) result = __hfma2(dq[i], *a2_ptr++, result);",
      "  return __hfma2(result, __halves2half2(qs_h, qs_h), g_result);",
      "}",
      "",
      "__forceinline__ __device__ half2 dot22_16(half2 (&dq)[8], const half* a_ptr,",
      "                                          const half2 g_result,",
      "                                          const half qs_h) {",
      "  half2 result = {};",
      "  const half2* a2_ptr = (const half2*)a_ptr;",
      "#pragma unroll",
      "  for (int i = 0; i < 8; i++) result = __hfma2(dq[i], *a2_ptr++, result);",
      "  return __hfma2(result, __halves2half2(qs_h, qs_h), g_result);",
      "}",
      "",
      "__forceinline__ __device__ half2 dot22_32(half2 (&dq)[16], const half* a_ptr,",
      "                                          const half2 g_result,",
      "                                          const half qs_h) {",
      "  half2 result = {};",
      "  const half2* a2_ptr = (const half2*)a_ptr;",
      "#pragma unroll",
      "  for (int i = 0; i < 16; i += 1) result = __hfma2(dq[i], *a2_ptr++, result);",
      "  return __hfma2(result, __halves2half2(qs_h, qs_h), g_result);",
      "}",
      "",
      "__forceinline__ __device__ float dot22_8_f(half2 (&dq)[4], const half* a_ptr,",
      "                                           const float g_result,",
      "                                           const float qs_f) {",
      "  half2 result = {};",
      "  const half2* a2_ptr = (const half2*)a_ptr;",
      "#pragma unroll",
      "  for (int i = 0; i < 4; i++) result = __hfma2(dq[i], *a2_ptr++, result);",
      "  float result_f =",
      "      __half2float(__low2half(result)) + __half2float(__high2half(result));",
      "  return fma(result_f, qs_f, g_result);",
      "}",
      "",
      "__forceinline__ __device__ float dot22_16_f(half2 (&dq)[8], const half* a_ptr,",
      "                                            const float g_result,",
      "                                            const float qs_f) {",
      "  half2 result = {};",
      "  const half2* a2_ptr = (const half2*)a_ptr;",
      "#pragma unroll",
      "  for (int i = 0; i < 8; i++) result = __hfma2(dq[i], *a2_ptr++, result);",
      "  float result_f =",
      "      __half2float(__low2half(result)) + __half2float(__high2half(result));",
      "  return fma(result_f, qs_f, g_result);",
      "}",
      "",
      "__forceinline__ __device__ float dot22_32_f(half2 (&dq)[16], const half* a_ptr,",
      "                                            const float g_result,",
      "                                            const float qs_f) {",
      "  half2 result = {};",
      "  const half2* a2_ptr = (const half2*)a_ptr;",
      "#pragma unroll",
      "  for (int i = 0; i < 16; i += 1) result = __hfma2(dq[i], *a2_ptr++, result);",
      "  float result_f =",
      "      __half2float(__low2half(result)) + __half2float(__high2half(result));",
      "  return fma(result_f, qs_f, g_result);",
      "}",
      "",
      "__forceinline__ __device__ half dot22_8_h(half2 (&dq)[4], const half* a_ptr,",
      "                                          const half g_result,",
      "                                          const half qs_h) {",
      "  // Use FP32 accumulator to avoid potential overflow since unscaled weights are",
      "  // in the range -128..127",
      "",
      "  float result = {};",
      "#pragma unroll",
      "  for (int i = 0; i < 4; i++) {",
      "    half2 w01 = dq[i];",
      "    float w0 = __low2float(w01);",
      "    float w1 = __high2float(w01);",
      "    float x0 = __half2float(*a_ptr++);",
      "    float x1 = __half2float(*a_ptr++);",
      "    result = fma(w0, x0, result);",
      "    result = fma(w1, x1, result);",
      "  }",
      "  float qs = __half2float(qs_h);",
      "  result *= qs;",
      "  half result_h = __float2half_rn(result);",
      "  return __hadd(result_h, g_result);",
      "}",
      "",
      "__forceinline__ __device__ half dot22_16_h(half2 (&dq)[8], const half* a_ptr,",
      "                                           const half g_result,",
      "                                           const half qs_h) {",
      "  half2 result = {};",
      "  const half2* a2_ptr = (const half2*)a_ptr;",
      "#pragma unroll",
      "  for (int i = 0; i < 8; i++) result = __hfma2(dq[i], *a2_ptr++, result);",
      "  half result_h = __hadd(__low2half(result), __high2half(result));",
      "  return __hfma(result_h, qs_h, g_result);",
      "}",
      "",
      "__forceinline__ __device__ half dot22_32_h(half2 (&dq)[16], const half* a_ptr,",
      "                                           const half g_result,",
      "                                           const half qs_h) {",
      "  half2 result = {};",
      "  const half2* a2_ptr = (const half2*)a_ptr;",
      "#pragma unroll",
      "  for (int i = 0; i < 16; i += 1) result = __hfma2(dq[i], *a2_ptr++, result);",
      "  half result_h = __hadd(__low2half(result), __high2half(result));",
      "  return __hfma(result_h, qs_h, g_result);",
      "}",
      "",
      "typedef void (*fp_gemm_half_q_half_gptq_kernel)(const half*, const uint32_t*,",
      "                                                const uint32_t*, const half*,",
      "                                                half*, const int, const int,",
      "                                                const int, const int,",
      "                                                const int*);",
      "",
      "template <bool first_block, int m_count>",
      "__global__ void gemm_half_q_half_gptq_4bit_kernel(",
      "    const half* __restrict__ a, const uint32_t* __restrict__ b_q_weight,",
      "    const uint32_t* __restrict__ b_gptq_qzeros,",
      "    const half* __restrict__ b_gptq_scales, half* __restrict__ c,",
      "    const int size_m, const int size_n, const int size_k, const int groups,",
      "    const int* __restrict__ b_q_perm) {",
      "  MatrixView_half a_(a, size_m, size_k);",
      "  MatrixView_half_rw c_(c, size_m, size_n);",
      "  MatrixView_q4_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);",
      "  MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);",
      "",
      "  auto t = threadIdx.x;",
      "",
      "  // Block",
      "  auto offset_n = blockIdx.x * BLOCK_KN_SIZE * 4;",
      "  auto offset_m = blockIdx.y * m_count;",
      "  auto offset_k = blockIdx.z * BLOCK_KN_SIZE;",
      "",
      "  int end_k = min(offset_k + BLOCK_KN_SIZE, size_k);",
      "",
      "  int n = offset_n + t * 4;",
      "",
      "  // Preload block_a",
      "  __shared__ half block_a[m_count][BLOCK_KN_SIZE];",
      "",
      "  if (offset_k + t < end_k) {",
      "    for (int m = 0; m < m_count; ++m) {",
      "      const half* a_ptr = a_.item_ptr(offset_m + m, 0);",
      "      half* block_a_ptr = block_a[m];",
      "",
      "      half a0;",
      "      if (b_q_perm)",
      "        a0 = a_ptr[b_q_perm[offset_k + t]];",
      "      else",
      "        a0 = a_ptr[offset_k + t];",
      "      block_a_ptr[t] = a0;",
      "    }",
      "  }",
      "",
      "  // Zero output",
      "  if (n >= size_n) return;",
      "",
      "  if (blockIdx.z == 0) {",
      "    for (int m = 0; m < m_count; m++)",
      "      *((uint64_t*)c_.item_ptr(offset_m + m, n)) = 0;",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  // Find initial group",
      "  int groupsize = size_k / groups;",
      "  int group = offset_k / groupsize;",
      "  int nextgroup = offset_k + groupsize;",
      "",
      "  // a, b offset",
      "  int qk = offset_k / (32 / 4);",
      "",
      "  const uint32_t* b_ptr = b_q_weight + qk * size_n + n;",
      "  const half* a_ptr = &block_a[0][0];",
      "  int a_stride = BLOCK_KN_SIZE;",
      "",
      "  // Initial group",
      "  int zeros[4];",
      "  float scales[4];",
      "  half2 z1z16[4][2];",
      "  half2 y1y16[4][2];",
      "  b_gptq_qzeros_.item4(zeros, group, n);",
      "  b_gptq_scales_.item4_f(scales, group, n);",
      "  dequant_4bit_8_prep_zero(zeros[0] + 1, z1z16[0], y1y16[0]);",
      "  dequant_4bit_8_prep_zero(zeros[1] + 1, z1z16[1], y1y16[1]);",
      "  dequant_4bit_8_prep_zero(zeros[2] + 1, z1z16[2], y1y16[2]);",
      "  dequant_4bit_8_prep_zero(zeros[3] + 1, z1z16[3], y1y16[3]);",
      "",
      "  // Column result",
      "  float block_c[m_count][4] = {};",
      "",
      "  // Dequantize and multiply",
      "  int k = offset_k;",
      "  while (k < end_k) {",
      "    if (k == nextgroup) {",
      "      group++;",
      "      nextgroup += groupsize;",
      "      b_gptq_qzeros_.item4(zeros, group, n);",
      "      b_gptq_scales_.item4_f(scales, group, n);",
      "      dequant_4bit_8_prep_zero(zeros[0] + 1, z1z16[0], y1y16[0]);",
      "      dequant_4bit_8_prep_zero(zeros[1] + 1, z1z16[1], y1y16[1]);",
      "      dequant_4bit_8_prep_zero(zeros[2] + 1, z1z16[2], y1y16[2]);",
      "      dequant_4bit_8_prep_zero(zeros[3] + 1, z1z16[3], y1y16[3]);",
      "    }",
      "",
      "#pragma unroll",
      "    for (int j = 0; j < 4; j++) {",
      "      const int4* b_ptr4 = (int4*)b_ptr;",
      "      int4 load_int4 = *b_ptr4;",
      "",
      "      half2 dq[4][4];",
      "      dequant_4bit_8_gptq(load_int4.x, dq[0], z1z16[0], y1y16[0], size_n,",
      "                          false);",
      "      dequant_4bit_8_gptq(load_int4.y, dq[1], z1z16[1], y1y16[1], size_n,",
      "                          false);",
      "      dequant_4bit_8_gptq(load_int4.z, dq[2], z1z16[2], y1y16[2], size_n,",
      "                          false);",
      "      dequant_4bit_8_gptq(load_int4.w, dq[3], z1z16[3], y1y16[3], size_n,",
      "                          false);",
      "",
      "#pragma unroll",
      "      for (int m = 0; m < m_count; m++) {",
      "        block_c[m][0] = fma(dot22_8_f(dq[0], a_ptr + m * a_stride), scales[0],",
      "                            block_c[m][0]);",
      "        block_c[m][1] = fma(dot22_8_f(dq[1], a_ptr + m * a_stride), scales[1],",
      "                            block_c[m][1]);",
      "        block_c[m][2] = fma(dot22_8_f(dq[2], a_ptr + m * a_stride), scales[2],",
      "                            block_c[m][2]);",
      "        block_c[m][3] = fma(dot22_8_f(dq[3], a_ptr + m * a_stride), scales[3],",
      "                            block_c[m][3]);",
      "      }",
      "",
      "      b_ptr += size_n;",
      "      a_ptr += 8;",
      "    }",
      "",
      "    k += 32;",
      "  }",
      "",
      "  for (int m = 0; m < m_count; m++) {",
      "    half2* out = (half2*)c_.item_ptr(offset_m + m, n);",
      "    half2 result01 = __halves2half2(__float2half_rn(block_c[m][0]),",
      "                                    __float2half_rn(block_c[m][1]));",
      "    half2 result23 = __halves2half2(__float2half_rn(block_c[m][2]),",
      "                                    __float2half_rn(block_c[m][3]));",
      "    atomicAdd(out, result01);",
      "    atomicAdd(out + 1, result23);",
      "  }",
      "}",
      "",
      "template <bool first_block, int m_count>",
      "__global__ void gemm_half_q_half_gptq_2bit_kernel(",
      "    const half* __restrict__ a, const uint32_t* __restrict__ b_q_weight,",
      "    const uint32_t* __restrict__ b_gptq_qzeros,",
      "    const half* __restrict__ b_gptq_scales, half* __restrict__ c,",
      "    const int size_m, const int size_n, const int size_k, const int groups,",
      "    const int* __restrict__ b_q_perm) {",
      "  MatrixView_half a_(a, size_m, size_k);",
      "  MatrixView_half_rw c_(c, size_m, size_n);",
      "  MatrixView_q2_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);",
      "  MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);",
      "",
      "  auto t = threadIdx.x;",
      "",
      "  // Block",
      "  auto offset_n = blockIdx.x * BLOCK_KN_SIZE * 4;",
      "  auto offset_m = blockIdx.y * m_count;",
      "  auto offset_k = blockIdx.z * BLOCK_KN_SIZE;",
      "",
      "  int end_k = min(offset_k + BLOCK_KN_SIZE, size_k);",
      "",
      "  int n = offset_n + t * 4;",
      "",
      "  // Preload block_a",
      "  __shared__ half block_a[m_count][BLOCK_KN_SIZE];",
      "",
      "  if (offset_k + t < end_k) {",
      "    for (int m = 0; m < m_count; ++m) {",
      "      const half* a_ptr = a_.item_ptr(offset_m + m, 0);",
      "      half* block_a_ptr = block_a[m];",
      "",
      "      half a0;",
      "      if (b_q_perm)",
      "        a0 = a_ptr[b_q_perm[offset_k + t]];",
      "      else",
      "        a0 = a_ptr[offset_k + t];",
      "      block_a_ptr[t] = a0;",
      "    }",
      "  }",
      "",
      "  // Zero output",
      "  if (n >= size_n) return;",
      "",
      "  if (blockIdx.z == 0) {",
      "    for (int m = 0; m < m_count; m++)",
      "      *((uint64_t*)c_.item_ptr(offset_m + m, n)) = 0;",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  // Find initial group",
      "  int groupsize = size_k / groups;",
      "  int group = offset_k / groupsize;",
      "  int nextgroup = offset_k + groupsize;",
      "",
      "  // a, b offset",
      "  int qk = offset_k / (32 / 2);",
      "",
      "  const uint32_t* b_ptr = b_q_weight + qk * size_n + n;",
      "  const half* a_ptr = &block_a[0][0];",
      "  int a_stride = BLOCK_KN_SIZE;",
      "",
      "  // Initial group",
      "  int zeros[4];",
      "  half scales[4];",
      "  b_gptq_qzeros_.item4(zeros, group, n);",
      "  b_gptq_scales_.item4(scales, group, n);",
      "  // Column result",
      "  half block_c[m_count][4] = {};",
      "",
      "  // Dequantize and multiply",
      "  int k = offset_k;",
      "  while (k < end_k) {",
      "    if (k == nextgroup) {",
      "      group++;",
      "      nextgroup += groupsize;",
      "      b_gptq_qzeros_.item4(zeros, group, n);",
      "      b_gptq_scales_.item4(scales, group, n);",
      "    }",
      "",
      "#pragma unroll",
      "    for (int j = 0; j < 1; j++) {",
      "      const int4* b_ptr4 = (int4*)b_ptr;",
      "      int4 load_int4 = *b_ptr4;",
      "",
      "      half2 dq[4][8];",
      "      dequant_2bit_16(load_int4.x, dq[0], size_n, zeros[0] + 1);",
      "      dequant_2bit_16(load_int4.y, dq[1], size_n, zeros[1] + 1);",
      "      dequant_2bit_16(load_int4.z, dq[2], size_n, zeros[2] + 1);",
      "      dequant_2bit_16(load_int4.w, dq[3], size_n, zeros[3] + 1);",
      "",
      "#pragma unroll",
      "      for (int m = 0; m < m_count; m++) {",
      "        block_c[m][0] =",
      "            dot22_16_h(dq[0], a_ptr + m * a_stride, block_c[m][0], scales[0]);",
      "        block_c[m][1] =",
      "            dot22_16_h(dq[1], a_ptr + m * a_stride, block_c[m][1], scales[1]);",
      "        block_c[m][2] =",
      "            dot22_16_h(dq[2], a_ptr + m * a_stride, block_c[m][2], scales[2]);",
      "        block_c[m][3] =",
      "            dot22_16_h(dq[3], a_ptr + m * a_stride, block_c[m][3], scales[3]);",
      "      }",
      "",
      "      b_ptr += size_n;",
      "      a_ptr += 16;",
      "    }",
      "",
      "    k += 16;",
      "  }",
      "",
      "  for (int m = 0; m < m_count; m++) {",
      "    half2* out = (half2*)c_.item_ptr(offset_m + m, n);",
      "    half2 result01 = __halves2half2(block_c[m][0], block_c[m][1]);",
      "    half2 result23 = __halves2half2(block_c[m][2], block_c[m][3]);",
      "    atomicAdd(out, result01);",
      "    atomicAdd(out + 1, result23);",
      "  }",
      "}",
      "",
      "template <bool first_block, int m_count>",
      "__global__ void gemm_half_q_half_gptq_3bit_kernel(",
      "    const half* __restrict__ a, const uint32_t* __restrict__ b_q_weight,",
      "    const uint32_t* __restrict__ b_gptq_qzeros,",
      "    const half* __restrict__ b_gptq_scales, half* __restrict__ c,",
      "    const int size_m, const int size_n, const int size_k, const int groups,",
      "    const int* __restrict__ b_q_perm) {",
      "  MatrixView_half a_(a, size_m, size_k);",
      "  MatrixView_half_rw c_(c, size_m, size_n);",
      "  MatrixView_q3_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);",
      "  MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);",
      "",
      "  auto t = threadIdx.x;",
      "",
      "  // Block",
      "  auto offset_n = blockIdx.x * BLOCK_KN_SIZE * 4;",
      "  auto offset_m = blockIdx.y * m_count;",
      "  auto offset_k = blockIdx.z * BLOCK_KN_SIZE;",
      "",
      "  int end_k = min(offset_k + BLOCK_KN_SIZE, size_k);",
      "",
      "  int n = offset_n + t * 4;",
      "",
      "  // Preload block_a",
      "  __shared__ half block_a[m_count][BLOCK_KN_SIZE];",
      "",
      "  if (offset_k + t < end_k) {",
      "    for (int m = 0; m < m_count; ++m) {",
      "      const half* a_ptr = a_.item_ptr(offset_m + m, 0);",
      "      half* block_a_ptr = block_a[m];",
      "",
      "      half a0;",
      "      if (b_q_perm)",
      "        a0 = a_ptr[b_q_perm[offset_k + t]];",
      "      else",
      "        a0 = a_ptr[offset_k + t];",
      "      block_a_ptr[t] = a0;",
      "    }",
      "  }",
      "",
      "  // Zero output",
      "  if (n >= size_n) return;",
      "",
      "  if (blockIdx.z == 0) {",
      "    for (int m = 0; m < m_count; m++)",
      "      *((uint64_t*)c_.item_ptr(offset_m + m, n)) = 0;",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  // Find initial group",
      "  int groupsize = size_k / groups;",
      "  int group = offset_k / groupsize;",
      "  int nextgroup = offset_k + groupsize;",
      "",
      "  // a, b offset",
      "  int qk = offset_k / 32 * 3;",
      "",
      "  const uint32_t* b_ptr = b_q_weight + qk * size_n + n;",
      "  const half* a_ptr = &block_a[0][0];",
      "  int a_stride = BLOCK_KN_SIZE;",
      "",
      "  // Initial group",
      "  int zeros[4];",
      "  half scales[4];",
      "  b_gptq_qzeros_.item4(zeros, group, n);",
      "  b_gptq_scales_.item4(scales, group, n);",
      "  // Column result",
      "  half block_c[m_count][4] = {};",
      "",
      "  // Dequantize and multiply",
      "  int k = offset_k;",
      "  while (k < end_k) {",
      "    if (k == nextgroup) {",
      "      group++;",
      "      nextgroup += groupsize;",
      "      b_gptq_qzeros_.item4(zeros, group, n);",
      "      b_gptq_scales_.item4(scales, group, n);",
      "    }",
      "",
      "#pragma unroll",
      "    for (int j = 0; j < 1; j++) {",
      "      int4 load_int4[3];",
      "      load_int4[0] = *((int4*)b_ptr);",
      "      b_ptr += size_n;",
      "      load_int4[1] = *((int4*)b_ptr);",
      "      b_ptr += size_n;",
      "      load_int4[2] = *((int4*)b_ptr);",
      "      b_ptr += size_n;",
      "",
      "      half2 dq[4][16];",
      "      dequant_3bit_32(load_int4[0].x, load_int4[1].x, load_int4[2].x, dq[0],",
      "                      size_n, zeros[0] + 1);",
      "      dequant_3bit_32(load_int4[0].y, load_int4[1].y, load_int4[2].y, dq[1],",
      "                      size_n, zeros[1] + 1);",
      "      dequant_3bit_32(load_int4[0].z, load_int4[1].z, load_int4[2].z, dq[2],",
      "                      size_n, zeros[2] + 1);",
      "      dequant_3bit_32(load_int4[0].w, load_int4[1].w, load_int4[2].w, dq[3],",
      "                      size_n, zeros[3] + 1);",
      "",
      "#pragma unroll",
      "      for (int m = 0; m < m_count; m++) {",
      "        block_c[m][0] =",
      "            dot22_32_h(dq[0], a_ptr + m * a_stride, block_c[m][0], scales[0]);",
      "        block_c[m][1] =",
      "            dot22_32_h(dq[1], a_ptr + m * a_stride, block_c[m][1], scales[1]);",
      "        block_c[m][2] =",
      "            dot22_32_h(dq[2], a_ptr + m * a_stride, block_c[m][2], scales[2]);",
      "        block_c[m][3] =",
      "            dot22_32_h(dq[3], a_ptr + m * a_stride, block_c[m][3], scales[3]);",
      "      }",
      "      a_ptr += 32;",
      "    }",
      "",
      "    k += 32;",
      "  }",
      "",
      "  for (int m = 0; m < m_count; m++) {",
      "    half2* out = (half2*)c_.item_ptr(offset_m + m, n);",
      "    half2 result01 = __halves2half2(block_c[m][0], block_c[m][1]);",
      "    half2 result23 = __halves2half2(block_c[m][2], block_c[m][3]);",
      "    atomicAdd(out, result01);",
      "    atomicAdd(out + 1, result23);",
      "  }",
      "}",
      "",
      "template <bool first_block, int m_count>",
      "__global__ void gemm_half_q_half_gptq_8bit_kernel(",
      "    const half* __restrict__ a, const uint32_t* __restrict__ b_q_weight,",
      "    const uint32_t* __restrict__ b_gptq_qzeros,",
      "    const half* __restrict__ b_gptq_scales, half* __restrict__ c,",
      "    const int size_m, const int size_n, const int size_k, const int groups,",
      "    const int* __restrict__ b_q_perm) {",
      "  MatrixView_half a_(a, size_m, size_k);",
      "  MatrixView_half_rw c_(c, size_m, size_n);",
      "  MatrixView_q8_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);",
      "  MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);",
      "",
      "  auto t = threadIdx.x;",
      "",
      "  // Block",
      "  auto offset_n = blockIdx.x * BLOCK_KN_SIZE * 4;",
      "  auto offset_m = blockIdx.y * m_count;",
      "  auto offset_k = blockIdx.z * BLOCK_KN_SIZE;",
      "",
      "  int end_k = min(offset_k + BLOCK_KN_SIZE, size_k);",
      "",
      "  int n = offset_n + t * 4;",
      "",
      "  // Preload block_a",
      "  __shared__ half block_a[m_count][BLOCK_KN_SIZE];",
      "",
      "  if (offset_k + t < end_k) {",
      "    for (int m = 0; m < m_count; ++m) {",
      "      const half* a_ptr = a_.item_ptr(offset_m + m, 0);",
      "      half* block_a_ptr = block_a[m];",
      "",
      "      half a0;",
      "      if (b_q_perm)",
      "        a0 = a_ptr[b_q_perm[offset_k + t]];",
      "      else",
      "        a0 = a_ptr[offset_k + t];",
      "      block_a_ptr[t] = a0;",
      "    }",
      "  }",
      "",
      "  // Zero output",
      "  if (n >= size_n) return;",
      "",
      "  if (blockIdx.z == 0) {",
      "    for (int m = 0; m < m_count; m++)",
      "      *((uint64_t*)c_.item_ptr(offset_m + m, n)) = 0;",
      "  }",
      "",
      "  __syncthreads();",
      "",
      "  // Find initial group",
      "  int groupsize = size_k / groups;",
      "  int group = offset_k / groupsize;",
      "  int nextgroup = offset_k + groupsize;",
      "",
      "  // a, b offset",
      "  int qk = offset_k / (32 / 8);",
      "",
      "  const uint32_t* b_ptr = b_q_weight + qk * size_n + n;",
      "  const half* a_ptr = &block_a[0][0];",
      "  int a_stride = BLOCK_KN_SIZE;",
      "",
      "  // Initial group",
      "  int zeros[4];",
      "  half scales[4];",
      "  b_gptq_qzeros_.item4(zeros, group, n);",
      "  b_gptq_scales_.item4(scales, group, n);",
      "  // Column result",
      "  half block_c[m_count][4] = {};",
      "",
      "  // Dequantize and multiply",
      "  int k = offset_k;",
      "  while (k < end_k) {",
      "    if (k == nextgroup) {",
      "      group++;",
      "      nextgroup += groupsize;",
      "      b_gptq_qzeros_.item4(zeros, group, n);",
      "      b_gptq_scales_.item4(scales, group, n);",
      "    }",
      "",
      "#pragma unroll",
      "    for (int j = 0; j < 4; j++) {",
      "      int4 load_int4[2];",
      "      load_int4[0] = *((int4*)b_ptr);",
      "      b_ptr += size_n;",
      "      load_int4[1] = *((int4*)b_ptr);",
      "      b_ptr += size_n;",
      "",
      "      half2 dq[4][4];",
      "      dequant_8bit_8(load_int4[0].x, load_int4[1].x, dq[0], size_n,",
      "                     zeros[0] + 1);",
      "      dequant_8bit_8(load_int4[0].y, load_int4[1].y, dq[1], size_n,",
      "                     zeros[1] + 1);",
      "      dequant_8bit_8(load_int4[0].z, load_int4[1].z, dq[2], size_n,",
      "                     zeros[2] + 1);",
      "      dequant_8bit_8(load_int4[0].w, load_int4[1].w, dq[3], size_n,",
      "                     zeros[3] + 1);",
      "",
      "      for (int m = 0; m < m_count; m++) {",
      "        block_c[m][0] =",
      "            dot22_8_h(dq[0], a_ptr + m * a_stride, block_c[m][0], scales[0]);",
      "        block_c[m][1] =",
      "            dot22_8_h(dq[1], a_ptr + m * a_stride, block_c[m][1], scales[1]);",
      "        block_c[m][2] =",
      "            dot22_8_h(dq[2], a_ptr + m * a_stride, block_c[m][2], scales[2]);",
      "        block_c[m][3] =",
      "            dot22_8_h(dq[3], a_ptr + m * a_stride, block_c[m][3], scales[3]);",
      "      }",
      "      a_ptr += 8;",
      "    }",
      "    k += 32;",
      "  }",
      "",
      "  for (int m = 0; m < m_count; m++) {",
      "    half2* out = (half2*)c_.item_ptr(offset_m + m, n);",
      "    half2 result01 = __halves2half2(block_c[m][0], block_c[m][1]);",
      "    half2 result23 = __halves2half2(block_c[m][2], block_c[m][3]);",
      "    atomicAdd(out, result01);",
      "    atomicAdd(out + 1, result23);",
      "  }",
      "}",
      "",
      "fp_gemm_half_q_half_gptq_kernel pick_gemm_half_q_half_gptq_kernel(",
      "    bool first_block, const int m_count, const int bit) {",
      "#define SELECT_KERNEL(M_COUNT)                                             \\",
      "  if (m_count == M_COUNT) {                                                \\",
      "    if (bit == 2) return gemm_half_q_half_gptq_2bit_kernel<true, M_COUNT>; \\",
      "    if (bit == 3) return gemm_half_q_half_gptq_3bit_kernel<true, M_COUNT>; \\",
      "    if (bit == 4) return gemm_half_q_half_gptq_4bit_kernel<true, M_COUNT>; \\",
      "    if (bit == 8) return gemm_half_q_half_gptq_8bit_kernel<true, M_COUNT>; \\",
      "  }",
      "#if BLOCK_M_SIZE_MAX >= 1",
      "  SELECT_KERNEL(1);",
      "#endif",
      "#if BLOCK_M_SIZE_MAX >= 2",
      "  SELECT_KERNEL(2);",
      "#endif",
      "#if BLOCK_M_SIZE_MAX >= 3",
      "  SELECT_KERNEL(3);",
      "#endif",
      "#if BLOCK_M_SIZE_MAX >= 4",
      "  SELECT_KERNEL(4);",
      "#endif",
      "#if BLOCK_M_SIZE_MAX >= 5",
      "  SELECT_KERNEL(5);",
      "#endif",
      "#if BLOCK_M_SIZE_MAX >= 6",
      "  SELECT_KERNEL(6);",
      "#endif",
      "#if BLOCK_M_SIZE_MAX >= 7",
      "  SELECT_KERNEL(7);",
      "#endif",
      "#if BLOCK_M_SIZE_MAX >= 8",
      "  SELECT_KERNEL(8);",
      "#endif",
      "  return NULL;",
      "}",
      "",
      "void gemm_half_q_half_cuda_part(const half* a, const uint32_t* b_q_weight,",
      "                                const uint32_t* b_gptq_qzeros,",
      "                                const half* b_gptq_scales, const int* b_q_perm,",
      "                                half* c, int size_m, int size_n, int size_k,",
      "                                int m_count, int groups, int bit) {",
      "  dim3 blockDim, gridDim;",
      "  blockDim.x = BLOCK_KN_SIZE;",
      "  blockDim.y = 1;",
      "  blockDim.z = 1;",
      "  gridDim.x = DIVIDE(size_n, BLOCK_KN_SIZE * 4);",
      "  gridDim.y = DIVIDE(size_m, m_count);",
      "  gridDim.z = DIVIDE(size_k, BLOCK_KN_SIZE);",
      "",
      "  fp_gemm_half_q_half_gptq_kernel kernel =",
      "      pick_gemm_half_q_half_gptq_kernel(true, m_count, bit);",
      "",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  kernel<<<gridDim, blockDim, 0, stream>>>(a, b_q_weight, b_gptq_qzeros,",
      "                                           b_gptq_scales, c, size_m, size_n,",
      "                                           size_k, groups, b_q_perm);",
      "}",
      "",
      "__global__ void reconstruct_exllama_8bit_kernel(",
      "    const uint32_t* __restrict__ b_q_weight, const int* __restrict__ b_q_perm,",
      "    const uint32_t* __restrict__ b_gptq_qzeros,",
      "    const half* __restrict__ b_gptq_scales, const int size_k, const int size_n,",
      "    const int groups, half* __restrict__ b) {",
      "  MatrixView_half_rw b_(b, size_k, size_n);",
      "  MatrixView_q8_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);",
      "  MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);",
      "",
      "  auto offset_k = BLOCK_KN_SIZE * blockIdx.y;",
      "  auto offset_n = BLOCK_KN_SIZE * blockIdx.x * 4;",
      "",
      "  int end_k = min(offset_k + BLOCK_KN_SIZE, size_k);",
      "",
      "  // Preload remapping table",
      "  __shared__ int perm[BLOCK_KN_SIZE];",
      "  auto t = threadIdx.x;",
      "",
      "  if (b_q_perm) {",
      "    if (offset_k + t < size_k) perm[t] = b_q_perm[offset_k + t];",
      "  }",
      "",
      "  // Column",
      "  int n = offset_n + t * 4;",
      "  if (n >= size_n) return;",
      "",
      "  // Find initial group",
      "  int groupsize = size_k / groups;",
      "  int group = offset_k / groupsize;",
      "  int nextgroup = offset_k + groupsize;",
      "",
      "  // b offset",
      "  int qk = offset_k / (32 / 8);",
      "",
      "  const uint32_t* b_ptr = b_q_weight + qk * size_n + n;",
      "",
      "  // Initial zeros/scale",
      "  int zeros[4];",
      "  half2 scales[4];",
      "  b_gptq_qzeros_.item4(zeros, group, n);",
      "  b_gptq_scales_.item4_h2(scales, group, n);",
      "",
      "  __syncthreads();",
      "",
      "  int k = offset_k;",
      "  int lk = 0;",
      "",
      "  while (k < end_k) {",
      "    if (k == nextgroup) {",
      "      group++;",
      "      nextgroup += groupsize;",
      "      b_gptq_qzeros_.item4(zeros, group, n);",
      "      b_gptq_scales_.item4_h2(scales, group, n);",
      "    }",
      "",
      "    for (int p = 0; p < 4; p++) {",
      "      int4 load_int4[2];",
      "      load_int4[0] = *((int4*)b_ptr);",
      "      b_ptr += size_n;",
      "      load_int4[1] = *((int4*)b_ptr);",
      "      b_ptr += size_n;",
      "",
      "      half2 dq[4][4];",
      "      dequant_8bit_8(load_int4[0].x, load_int4[1].x, dq[0], size_n,",
      "                     zeros[0] + 1);",
      "      dequant_8bit_8(load_int4[0].y, load_int4[1].y, dq[1], size_n,",
      "                     zeros[1] + 1);",
      "      dequant_8bit_8(load_int4[0].z, load_int4[1].z, dq[2], size_n,",
      "                     zeros[2] + 1);",
      "      dequant_8bit_8(load_int4[0].w, load_int4[1].w, dq[3], size_n,",
      "                     zeros[3] + 1);",
      "",
      "      // half* dqh = (half*)dq;",
      "      if (b_q_perm) {",
      "        for (int j = 0; j < 4; j++) {",
      "          for (int v = 0; v < 4; v++) dq[v][j] = __hmul2(scales[v], dq[v][j]);",
      "          b_.set4(perm[lk++], n, __low2half(dq[0][j]), __low2half(dq[1][j]),",
      "                  __low2half(dq[2][j]), __low2half(dq[3][j]));",
      "          b_.set4(perm[lk++], n, __high2half(dq[0][j]), __high2half(dq[1][j]),",
      "                  __high2half(dq[2][j]), __high2half(dq[3][j]));",
      "        }",
      "      } else {",
      "        for (int j = 0; j < 4; j++) {",
      "          for (int v = 0; v < 4; v++) dq[v][j] = __hmul2(scales[v], dq[v][j]);",
      "          b_.set4(offset_k + lk++, n, __low2half(dq[0][j]),",
      "                  __low2half(dq[1][j]), __low2half(dq[2][j]),",
      "                  __low2half(dq[3][j]));",
      "          b_.set4(offset_k + lk++, n, __high2half(dq[0][j]),",
      "                  __high2half(dq[1][j]), __high2half(dq[2][j]),",
      "                  __high2half(dq[3][j]));",
      "        }",
      "      }",
      "    }",
      "    k += 32;",
      "  }",
      "}",
      "",
      "__global__ void reconstruct_exllama_4bit_kernel(",
      "    const uint32_t* __restrict__ b_q_weight, const int* __restrict__ b_q_perm,",
      "    const uint32_t* __restrict__ b_gptq_qzeros,",
      "    const half* __restrict__ b_gptq_scales, const int size_k, const int size_n,",
      "    const int groups, half* __restrict__ b) {",
      "  MatrixView_half_rw b_(b, size_k, size_n);",
      "  MatrixView_q4_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);",
      "  MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);",
      "",
      "  auto offset_k = BLOCK_KN_SIZE * blockIdx.y;",
      "  auto offset_n = BLOCK_KN_SIZE * blockIdx.x * 4;",
      "",
      "  int end_k = min(offset_k + BLOCK_KN_SIZE, size_k);",
      "",
      "  // Preload remapping table",
      "  __shared__ int perm[BLOCK_KN_SIZE];",
      "  auto t = threadIdx.x;",
      "",
      "  if (b_q_perm) {",
      "    if (offset_k + t < size_k) perm[t] = b_q_perm[offset_k + t];",
      "  }",
      "",
      "  // Column",
      "  int n = offset_n + t * 4;",
      "  if (n >= size_n) return;",
      "",
      "  // Find initial group",
      "  int groupsize = size_k / groups;",
      "  int group = offset_k / groupsize;",
      "  int nextgroup = offset_k + groupsize;",
      "",
      "  // b offset",
      "  int qk = offset_k / (32 / 4);",
      "",
      "  const uint32_t* b_ptr = b_q_weight + qk * size_n + n;",
      "",
      "  // Initial zeros/scale",
      "  int zeros[4];",
      "  half2 scales[4];",
      "  half2 z1z16[4][2];",
      "  half2 y1y16[4][2];",
      "  b_gptq_qzeros_.item4(zeros, group, n);",
      "  b_gptq_scales_.item4_h2(scales, group, n);",
      "  dequant_4bit_8_prep_zero(zeros[0] + 1, z1z16[0], y1y16[0]);",
      "  dequant_4bit_8_prep_zero(zeros[1] + 1, z1z16[1], y1y16[1]);",
      "  dequant_4bit_8_prep_zero(zeros[2] + 1, z1z16[2], y1y16[2]);",
      "  dequant_4bit_8_prep_zero(zeros[3] + 1, z1z16[3], y1y16[3]);",
      "",
      "  __syncthreads();",
      "",
      "  int k = offset_k;",
      "  int lk = 0;",
      "",
      "  while (k < end_k) {",
      "    if (k == nextgroup) {",
      "      group++;",
      "      nextgroup += groupsize;",
      "      b_gptq_qzeros_.item4(zeros, group, n);",
      "      b_gptq_scales_.item4_h2(scales, group, n);",
      "      dequant_4bit_8_prep_zero(zeros[0] + 1, z1z16[0], y1y16[0]);",
      "      dequant_4bit_8_prep_zero(zeros[1] + 1, z1z16[1], y1y16[1]);",
      "      dequant_4bit_8_prep_zero(zeros[2] + 1, z1z16[2], y1y16[2]);",
      "      dequant_4bit_8_prep_zero(zeros[3] + 1, z1z16[3], y1y16[3]);",
      "    }",
      "",
      "    for (int p = 0; p < 4; p++) {",
      "      half2 dq[4][4];",
      "      const int4* b_ptr4 = (int4*)b_ptr;",
      "      int4 load_int4 = *b_ptr4;",
      "",
      "      dequant_4bit_8_gptq(load_int4.x, dq[0], z1z16[0], y1y16[0], size_n,",
      "                          false);",
      "      dequant_4bit_8_gptq(load_int4.y, dq[1], z1z16[1], y1y16[1], size_n,",
      "                          false);",
      "      dequant_4bit_8_gptq(load_int4.z, dq[2], z1z16[2], y1y16[2], size_n,",
      "                          false);",
      "      dequant_4bit_8_gptq(load_int4.w, dq[3], z1z16[3], y1y16[3], size_n,",
      "                          false);",
      "",
      "      b_ptr += size_n;",
      "      // half* dqh = (half*)dq;",
      "      if (b_q_perm) {",
      "        for (int j = 0; j < 4; j++) {",
      "          for (int v = 0; v < 4; v++) dq[v][j] = __hmul2(scales[v], dq[v][j]);",
      "          b_.set4(perm[lk++], n, __low2half(dq[0][j]), __low2half(dq[1][j]),",
      "                  __low2half(dq[2][j]), __low2half(dq[3][j]));",
      "          b_.set4(perm[lk++], n, __high2half(dq[0][j]), __high2half(dq[1][j]),",
      "                  __high2half(dq[2][j]), __high2half(dq[3][j]));",
      "        }",
      "      } else {",
      "        for (int j = 0; j < 4; j++) {",
      "          for (int v = 0; v < 4; v++) dq[v][j] = __hmul2(scales[v], dq[v][j]);",
      "          b_.set4(offset_k + lk++, n, __low2half(dq[0][j]),",
      "                  __low2half(dq[1][j]), __low2half(dq[2][j]),",
      "                  __low2half(dq[3][j]));",
      "          b_.set4(offset_k + lk++, n, __high2half(dq[0][j]),",
      "                  __high2half(dq[1][j]), __high2half(dq[2][j]),",
      "                  __high2half(dq[3][j]));",
      "        }",
      "      }",
      "    }",
      "    k += 32;",
      "  }",
      "}",
      "",
      "__global__ void reconstruct_exllama_3bit_kernel(",
      "    const uint32_t* __restrict__ b_q_weight, const int* __restrict__ b_q_perm,",
      "    const uint32_t* __restrict__ b_gptq_qzeros,",
      "    const half* __restrict__ b_gptq_scales, const int size_k, const int size_n,",
      "    const int groups, half* __restrict__ b) {",
      "  MatrixView_half_rw b_(b, size_k, size_n);",
      "  MatrixView_q3_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);",
      "  MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);",
      "",
      "  auto offset_k = BLOCK_KN_SIZE * blockIdx.y;",
      "  auto offset_n = BLOCK_KN_SIZE * blockIdx.x * 4;",
      "",
      "  int end_k = min(offset_k + BLOCK_KN_SIZE, size_k);",
      "",
      "  // Preload remapping table",
      "  __shared__ int perm[BLOCK_KN_SIZE];",
      "  auto t = threadIdx.x;",
      "",
      "  if (b_q_perm) {",
      "    if (offset_k + t < size_k) perm[t] = b_q_perm[offset_k + t];",
      "  }",
      "",
      "  // Column",
      "  int n = offset_n + t * 4;",
      "  if (n >= size_n) return;",
      "",
      "  // Find initial group",
      "  int groupsize = size_k / groups;",
      "  int group = offset_k / groupsize;",
      "  int nextgroup = offset_k + groupsize;",
      "",
      "  // b offset",
      "  int qk = offset_k / 32 * 3;",
      "",
      "  const uint32_t* b_ptr = b_q_weight + qk * size_n + n;",
      "",
      "  // Initial zeros/scale",
      "  int zeros[4];",
      "  half2 scales[4];",
      "  b_gptq_qzeros_.item4(zeros, group, n);",
      "  b_gptq_scales_.item4_h2(scales, group, n);",
      "",
      "  __syncthreads();",
      "",
      "  int k = offset_k;",
      "  int lk = 0;",
      "",
      "  while (k < end_k) {",
      "    if (k == nextgroup) {",
      "      group++;",
      "      nextgroup += groupsize;",
      "      b_gptq_qzeros_.item4(zeros, group, n);",
      "      b_gptq_scales_.item4_h2(scales, group, n);",
      "    }",
      "",
      "    for (int p = 0; p < 1; p++) {",
      "      int4 load_int4[3];",
      "      load_int4[0] = *((int4*)b_ptr);",
      "      b_ptr += size_n;",
      "      load_int4[1] = *((int4*)b_ptr);",
      "      b_ptr += size_n;",
      "      load_int4[2] = *((int4*)b_ptr);",
      "      b_ptr += size_n;",
      "",
      "      half2 dq[4][16];",
      "      dequant_3bit_32(load_int4[0].x, load_int4[1].x, load_int4[2].x, dq[0],",
      "                      size_n, zeros[0] + 1);",
      "      dequant_3bit_32(load_int4[0].y, load_int4[1].y, load_int4[2].y, dq[1],",
      "                      size_n, zeros[1] + 1);",
      "      dequant_3bit_32(load_int4[0].z, load_int4[1].z, load_int4[2].z, dq[2],",
      "                      size_n, zeros[2] + 1);",
      "      dequant_3bit_32(load_int4[0].w, load_int4[1].w, load_int4[2].w, dq[3],",
      "                      size_n, zeros[3] + 1);",
      "",
      "      if (b_q_perm) {",
      "        for (int j = 0; j < 16; j++) {",
      "          for (int v = 0; v < 4; v++) dq[v][j] = __hmul2(scales[v], dq[v][j]);",
      "          b_.set4(perm[lk++], n, __low2half(dq[0][j]), __low2half(dq[1][j]),",
      "                  __low2half(dq[2][j]), __low2half(dq[3][j]));",
      "          b_.set4(perm[lk++], n, __high2half(dq[0][j]), __high2half(dq[1][j]),",
      "                  __high2half(dq[2][j]), __high2half(dq[3][j]));",
      "        }",
      "      } else {",
      "        for (int j = 0; j < 16; j++) {",
      "          for (int v = 0; v < 4; v++) dq[v][j] = __hmul2(scales[v], dq[v][j]);",
      "          b_.set4(offset_k + lk++, n, __low2half(dq[0][j]),",
      "                  __low2half(dq[1][j]), __low2half(dq[2][j]),",
      "                  __low2half(dq[3][j]));",
      "          b_.set4(offset_k + lk++, n, __high2half(dq[0][j]),",
      "                  __high2half(dq[1][j]), __high2half(dq[2][j]),",
      "                  __high2half(dq[3][j]));",
      "        }",
      "      }",
      "    }",
      "    k += 32;",
      "  }",
      "}",
      "",
      "__global__ void reconstruct_exllama_2bit_kernel(",
      "    const uint32_t* __restrict__ b_q_weight, const int* __restrict__ b_q_perm,",
      "    const uint32_t* __restrict__ b_gptq_qzeros,",
      "    const half* __restrict__ b_gptq_scales, const int size_k, const int size_n,",
      "    const int groups, half* __restrict__ b) {",
      "  MatrixView_half_rw b_(b, size_k, size_n);",
      "  MatrixView_q2_row b_gptq_qzeros_(b_gptq_qzeros, groups, size_n);",
      "  MatrixView_half b_gptq_scales_(b_gptq_scales, groups, size_n);",
      "",
      "  auto offset_k = BLOCK_KN_SIZE * blockIdx.y;",
      "  auto offset_n = BLOCK_KN_SIZE * blockIdx.x * 4;",
      "",
      "  int end_k = min(offset_k + BLOCK_KN_SIZE, size_k);",
      "",
      "  // Preload remapping table",
      "  __shared__ int perm[BLOCK_KN_SIZE];",
      "  auto t = threadIdx.x;",
      "",
      "  if (b_q_perm) {",
      "    if (offset_k + t < size_k) perm[t] = b_q_perm[offset_k + t];",
      "  }",
      "",
      "  // Column",
      "  int n = offset_n + t * 4;",
      "  if (n >= size_n) return;",
      "",
      "  // Find initial group",
      "  int groupsize = size_k / groups;",
      "  int group = offset_k / groupsize;",
      "  int nextgroup = offset_k + groupsize;",
      "",
      "  // b offset",
      "  int qk = offset_k / (32 / 2);",
      "",
      "  const uint32_t* b_ptr = b_q_weight + qk * size_n + n;",
      "",
      "  // Initial zeros/scale",
      "  int zeros[4];",
      "  half2 scales[4];",
      "  b_gptq_qzeros_.item4(zeros, group, n);",
      "  b_gptq_scales_.item4_h2(scales, group, n);",
      "",
      "  __syncthreads();",
      "",
      "  int k = offset_k;",
      "  int lk = 0;",
      "",
      "  while (k < end_k) {",
      "    if (k == nextgroup) {",
      "      group++;",
      "      nextgroup += groupsize;",
      "      b_gptq_qzeros_.item4(zeros, group, n);",
      "      b_gptq_scales_.item4_h2(scales, group, n);",
      "    }",
      "",
      "    for (int p = 0; p < 2; p++) {",
      "      const int4* b_ptr4 = (int4*)b_ptr;",
      "      int4 load_int4 = *b_ptr4;",
      "",
      "      half2 dq[4][8];",
      "      dequant_2bit_16(load_int4.x, dq[0], size_n, zeros[0] + 1);",
      "      dequant_2bit_16(load_int4.y, dq[1], size_n, zeros[1] + 1);",
      "      dequant_2bit_16(load_int4.z, dq[2], size_n, zeros[2] + 1);",
      "      dequant_2bit_16(load_int4.w, dq[3], size_n, zeros[3] + 1);",
      "",
      "      b_ptr += size_n;",
      "      // half* dqh = (half*)dq;",
      "      if (b_q_perm) {",
      "        for (int j = 0; j < 8; j++) {",
      "          for (int v = 0; v < 4; v++) dq[v][j] = __hmul2(scales[v], dq[v][j]);",
      "          b_.set4(perm[lk++], n, __low2half(dq[0][j]), __low2half(dq[1][j]),",
      "                  __low2half(dq[2][j]), __low2half(dq[3][j]));",
      "          b_.set4(perm[lk++], n, __high2half(dq[0][j]), __high2half(dq[1][j]),",
      "                  __high2half(dq[2][j]), __high2half(dq[3][j]));",
      "        }",
      "      } else {",
      "        for (int j = 0; j < 8; j++) {",
      "          for (int v = 0; v < 4; v++) dq[v][j] = __hmul2(scales[v], dq[v][j]);",
      "          b_.set4(offset_k + lk++, n, __low2half(dq[0][j]),",
      "                  __low2half(dq[1][j]), __low2half(dq[2][j]),",
      "                  __low2half(dq[3][j]));",
      "          b_.set4(offset_k + lk++, n, __high2half(dq[0][j]),",
      "                  __high2half(dq[1][j]), __high2half(dq[2][j]),",
      "                  __high2half(dq[3][j]));",
      "        }",
      "      }",
      "    }",
      "    k += 32;",
      "  }",
      "}",
      "",
      "void reconstruct_exllama(const uint32_t* b_q_weight,",
      "                         const uint32_t* b_gptq_qzeros,",
      "                         const half* b_gptq_scales, const int* b_q_perm,",
      "                         half* out, int height, int width, int groups,",
      "                         int bit) {",
      "  dim3 blockDim, gridDim;",
      "  blockDim.x = BLOCK_KN_SIZE;",
      "  blockDim.y = 1;",
      "  gridDim.y = DIVIDE(height, BLOCK_KN_SIZE);",
      "  gridDim.x = DIVIDE(width, BLOCK_KN_SIZE);",
      "",
      "  auto reconstruct_exllama_kernel = reconstruct_exllama_4bit_kernel;",
      "  if (bit == 2) {",
      "    reconstruct_exllama_kernel = reconstruct_exllama_2bit_kernel;",
      "  } else if (bit == 3) {",
      "    reconstruct_exllama_kernel = reconstruct_exllama_3bit_kernel;",
      "  } else if (bit == 8) {",
      "    reconstruct_exllama_kernel = reconstruct_exllama_8bit_kernel;",
      "  }",
      "",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  reconstruct_exllama_kernel<<<gridDim, blockDim, 0, stream>>>(",
      "      b_q_weight, b_q_perm, b_gptq_qzeros, b_gptq_scales, height, width, groups,",
      "      out);",
      "}",
      "",
      "__global__ void gemm_half_q_half_alt_4bit_kernel(",
      "    const half2* __restrict__ vec, const uint32_t* __restrict__ mat,",
      "    half* __restrict__ mul, const half* __restrict__ scales,",
      "    const uint32_t* __restrict__ zeros, const int* __restrict__ g_idx,",
      "    int batch, int height, int width) {",
      "  int zero_width = width / 8;",
      "  int vec_height = height * 4;",
      "  const int blockwidth2 = BLOCK_KN_SIZE / 2;",
      "  auto b = blockIdx.y * BLOCK_M_SIZE_MAX;",
      "  int b_end = min(BLOCK_M_SIZE_MAX, batch - b);",
      "  auto h = BLOCK_KN_SIZE * blockIdx.z / 8;",
      "  int h_end = min(BLOCK_KN_SIZE / 8, height - h) * 4;",
      "  auto w = BLOCK_KN_SIZE * blockIdx.x + threadIdx.x;",
      "",
      "  __shared__ half2 blockvec[BLOCK_M_SIZE_MAX][blockwidth2];",
      "  if (threadIdx.x < h_end) {",
      "    for (int m = 0; m < b_end; ++m) {",
      "      blockvec[m][threadIdx.x] =",
      "          vec[(m + b) * vec_height + blockIdx.z * BLOCK_KN_SIZE / 2 +",
      "              threadIdx.x];",
      "    }",
      "  }",
      "",
      "  __shared__ half2 deq2[256][8];",
      "  auto val = threadIdx.x / 8;",
      "  auto off = threadIdx.x % 8;",
      "  for (; val < 256; val += BLOCK_KN_SIZE / 8) {",
      "    deq2[val][off] =",
      "        __halves2half2(__int2half_rn(val & 0xF), __int2half_rn(val >> 4));",
      "  }",
      "",
      "  if (blockIdx.z == 0) {",
      "    for (int m = 0; m < b_end; m++) mul[(b + m) * width + w] = __int2half_rn(0);",
      "  }",
      "  __syncthreads();",
      "",
      "  int i = width * h + w;",
      "  int g_h = h * 8;",
      "  int k = 0;",
      "  int z_w = w / 8;",
      "  int z_mod = (w % 8) * 4;",
      "  half2 res2;",
      "  half res[BLOCK_M_SIZE_MAX] = {};",
      "",
      "  unsigned int tmp;",
      "  while (k < h_end) {",
      "    tmp = mat[i];",
      "    half2 scales_tmp[4];",
      "    half2 zeros_tmp[4];",
      "    for (int tmp_k = 0; tmp_k < 4; tmp_k++) {",
      "      int g = g_idx[g_h + (k + tmp_k) * 2];",
      "      int g2 = g_idx[g_h + (k + tmp_k) * 2 + 1];",
      "      half scale_f = scales[g * width + w];",
      "      half scale_f2 = scales[g2 * width + w];",
      "      half2 scale = __halves2half2(scale_f, scale_f2);",
      "      half2 zero = __halves2half2(",
      "          __hmul(scale_f,",
      "                 __int2half_rn(-((zeros[g * zero_width + z_w] >> z_mod) & 0xF) -",
      "                               1)),",
      "          __hmul(scale_f2,",
      "                 __int2half_rn(",
      "                     -((zeros[g2 * zero_width + z_w] >> z_mod) & 0xF) - 1)));",
      "      scales_tmp[tmp_k] = scale;",
      "      zeros_tmp[tmp_k] = zero;",
      "    }",
      "    for (int m = 0; m < b_end; m++) {",
      "#ifndef USE_ROCM",
      "      res2 = {};",
      "#else",
      "      res2.x = __half_as_ushort(__float2half(0));",
      "      res2.y = __half_as_ushort(__float2half(0));",
      "#endif",
      "      res2 = __hfma2(",
      "          __hfma2(deq2[(tmp >> 0) & 0xff][off], scales_tmp[0], zeros_tmp[0]),",
      "          blockvec[m][k + 0], res2);",
      "      res2 = __hfma2(",
      "          __hfma2(deq2[(tmp >> 8) & 0xff][off], scales_tmp[1], zeros_tmp[1]),",
      "          blockvec[m][k + 1], res2);",
      "      res2 = __hfma2(",
      "          __hfma2(deq2[(tmp >> 16) & 0xff][off], scales_tmp[2], zeros_tmp[2]),",
      "          blockvec[m][k + 2], res2);",
      "      res2 = __hfma2(",
      "          __hfma2(deq2[(tmp >> 24) & 0xff][off], scales_tmp[3], zeros_tmp[3]),",
      "          blockvec[m][k + 3], res2);",
      "#ifndef USE_ROCM",
      "      res[m] = __hadd(res[m], __hadd(res2.x, res2.y));",
      "#else",
      "      res[m] = __hadd(",
      "          res[m], __hadd(__ushort_as_half(res2.x), __ushort_as_half(res2.y)));",
      "#endif",
      "    }",
      "    i += width;",
      "    k += 4;",
      "  }",
      "  for (int m = 0; m < b_end; m++) {",
      "    atomicAdd(&mul[(b + m) * width + w], res[m]);",
      "  }",
      "}",
      "",
      "__global__ void gemm_half_q_half_alt_8bit_kernel(",
      "    const half2* __restrict__ vec, const uint32_t* __restrict__ mat,",
      "    half* __restrict__ mul, const half* __restrict__ scales,",
      "    const uint32_t* __restrict__ zeros, const int* __restrict__ g_idx,",
      "    int batch, int height, int width) {",
      "  int zero_width = width / 4;",
      "  int vec_height = height * 2;",
      "  const int blockwidth2 = BLOCK_KN_SIZE / 2;",
      "  auto b = blockIdx.y * BLOCK_M_SIZE_MAX;",
      "  int b_end = min(BLOCK_M_SIZE_MAX, batch - b);",
      "  auto h = BLOCK_KN_SIZE * blockIdx.z / 4;",
      "  int h_end = min(BLOCK_KN_SIZE / 4, height - h) * 2;",
      "  auto w = BLOCK_KN_SIZE * blockIdx.x + threadIdx.x;",
      "",
      "  __shared__ half2 blockvec[BLOCK_M_SIZE_MAX][blockwidth2];",
      "  if (threadIdx.x < h_end) {",
      "    for (int m = 0; m < b_end; ++m) {",
      "      blockvec[m][threadIdx.x] =",
      "          vec[(m + b) * vec_height + blockIdx.z * BLOCK_KN_SIZE / 2 +",
      "              threadIdx.x];",
      "    }",
      "  }",
      "",
      "  if (blockIdx.z == 0) {",
      "    for (int m = 0; m < b_end; m++) mul[(b + m) * width + w] = __int2half_rn(0);",
      "  }",
      "  __syncthreads();",
      "",
      "  int i = width * h + w;",
      "  int g_h = h * 4;",
      "  int k = 0;",
      "  int z_w = w / 4;",
      "  int z_mod = (w % 4) * 8;",
      "  half2 res2;",
      "  half res[BLOCK_M_SIZE_MAX] = {};",
      "",
      "  unsigned int tmp;",
      "  while (k < h_end) {",
      "    tmp = mat[i];",
      "    half2 scales_tmp[2];",
      "    half2 zeros_tmp[2];",
      "    for (int tmp_k = 0; tmp_k < 2; tmp_k++) {",
      "      int g = g_idx[g_h + (k + tmp_k) * 2];",
      "      int g2 = g_idx[g_h + (k + tmp_k) * 2 + 1];",
      "      half scale_f = scales[g * width + w];",
      "      half scale_f2 = scales[g2 * width + w];",
      "      half2 scale = __halves2half2(scale_f, scale_f2);",
      "      half2 zero = __halves2half2(",
      "          __hmul(scale_f,",
      "                 __int2half_rn(",
      "                     -((zeros[g * zero_width + z_w] >> z_mod) & 0xff) - 1)),",
      "          __hmul(scale_f2,",
      "                 __int2half_rn(",
      "                     -((zeros[g2 * zero_width + z_w] >> z_mod) & 0xff) - 1)));",
      "      scales_tmp[tmp_k] = scale;",
      "      zeros_tmp[tmp_k] = zero;",
      "    }",
      "    for (int m = 0; m < b_end; m++) {",
      "#ifndef USE_ROCM",
      "      res2 = {};",
      "#else",
      "      res2.x = __half_as_ushort(__float2half(0));",
      "      res2.y = __half_as_ushort(__float2half(0));",
      "#endif",
      "      half2 v12 = __halves2half2(__int2half_rn(tmp & 0xFF),",
      "                                 __int2half_rn((tmp >> 8) & 0xFF));",
      "      res2 = __hfma2(__hfma2(v12, scales_tmp[0], zeros_tmp[0]),",
      "                     blockvec[m][k + 0], res2);",
      "      half2 v34 = __halves2half2(__int2half_rn((tmp >> 16) & 0xFF),",
      "                                 __int2half_rn((tmp >> 24) & 0xFF));",
      "      res2 = __hfma2(__hfma2(v34, scales_tmp[1], zeros_tmp[1]),",
      "                     blockvec[m][k + 1], res2);",
      "#ifndef USE_ROCM",
      "      res[m] = __hadd(res[m], __hadd(res2.x, res2.y));",
      "#else",
      "      res[m] = __hadd(",
      "          res[m], __hadd(__ushort_as_half(res2.x), __ushort_as_half(res2.y)));",
      "#endif",
      "    }",
      "    i += width;",
      "    k += 2;",
      "  }",
      "  for (int m = 0; m < b_end; m++) {",
      "    atomicAdd(&mul[(b + m) * width + w], res[m]);",
      "  }",
      "}",
      "",
      "void gemm_half_q_half_alt(const half* a, const uint32_t* b_q_weight,",
      "                          const uint32_t* b_gptq_qzeros,",
      "                          const half* b_gptq_scales, const int* b_g_idx,",
      "                          half* c, int size_m, int size_n, int size_k,",
      "                          int bit) {",
      "  dim3 blockDim, gridDim;",
      "  blockDim.x = BLOCK_KN_SIZE;",
      "  blockDim.y = 1;",
      "  blockDim.z = 1;",
      "  gridDim.x = DIVIDE(size_n, BLOCK_KN_SIZE);",
      "  gridDim.y = DIVIDE(size_m, BLOCK_M_SIZE_MAX);",
      "  gridDim.z = DIVIDE(size_k, BLOCK_KN_SIZE);",
      "",
      "  auto kernel = gemm_half_q_half_alt_4bit_kernel;",
      "  if (bit == 8) {",
      "    kernel = gemm_half_q_half_alt_8bit_kernel;",
      "  }",
      "",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  kernel<<<gridDim, blockDim, 0, stream>>>(",
      "      (const half2*)a, b_q_weight, c, b_gptq_scales, b_gptq_qzeros, b_g_idx,",
      "      size_m, size_k / 32 * bit, size_n);",
      "}",
      "",
      "template <class T, int bit>",
      "__global__ void reconstruct_gptq_kernel(const uint32_t* __restrict__ w,",
      "                                        const half* __restrict__ w_scales,",
      "                                        const uint32_t* __restrict__ w_zeros,",
      "                                        const int* __restrict__ g_idx,",
      "                                        const int height, const int width,",
      "                                        const int group,",
      "                                        half* __restrict__ out) {",
      "  // Start of block",
      "",
      "  auto column = BLOCK_KN_SIZE * blockIdx.x + threadIdx.x;",
      "  auto row = blockIdx.y * 32 / bit;",
      "  if (column >= width) return;",
      "",
      "  // Views",
      "",
      "  MatrixView_half_rw out_(out, height, width);",
      "  MatrixView_half w_scales_(w_scales, group, width);",
      "  T w_zeros_(w_zeros, group, width);",
      "",
      "  uint32_t w_read = w[blockIdx.y * width + column];",
      "  half* out_ptr = out_.item_ptr(row, column);",
      "",
      "#pragma unroll",
      "  for (int s = 0; s < 32; s += bit) {",
      "    int group = g_idx[row + s / bit];",
      "    half w_scale = w_scales_.item(group, column);",
      "    uint32_t w_zero = w_zeros_.item(group, column) + 1;",
      "    half w_item =",
      "        __hmul(__int2half_rn((int)((w_read >> s) & ((1 << bit) - 1)) - w_zero),",
      "               w_scale);",
      "    *out_ptr = w_item;",
      "    out_ptr += out_.width;",
      "  }",
      "}",
      "",
      "__global__ void reconstruct_gptq_3bit_kernel(",
      "    const uint32_t* __restrict__ w, const half* __restrict__ w_scales,",
      "    const uint32_t* __restrict__ w_zeros, const int* __restrict__ g_idx,",
      "    const int height, const int width, const int group,",
      "    half* __restrict__ out) {",
      "  // Start of block",
      "  auto column = BLOCK_KN_SIZE * blockIdx.x + threadIdx.x;",
      "  auto row = blockIdx.y * 32;",
      "  if (column >= width) return;",
      "",
      "  // Views",
      "",
      "  MatrixView_half_rw out_(out, height, width);",
      "  MatrixView_half w_scales_(w_scales, group, width);",
      "  MatrixView_q3_row w_zeros_(w_zeros, group, width);",
      "",
      "  uint32_t w1 = w[(blockIdx.y * 3) * width + column];",
      "  uint32_t w2 = w[(blockIdx.y * 3 + 1) * width + column];",
      "  uint32_t w3 = w[(blockIdx.y * 3 + 2) * width + column];",
      "  half* out_ptr = out_.item_ptr(row, column);",
      "",
      "#pragma unroll",
      "  for (int i = 0; i < 32; i += 1) {",
      "    int group = g_idx[row + i];",
      "    half w_scale = w_scales_.item(group, column);",
      "    uint32_t w_zero = w_zeros_.item(group, column) + 1;",
      "    int w_item;",
      "    if (i == 10) {",
      "      w_item = (w1 >> 30) | ((w2 << 2) & 0x4);",
      "    } else if (i == 21) {",
      "      w_item = (w2 >> 31) | ((w3 << 1) & 0x6);",
      "    } else if (i < 10) {",
      "      w_item = ((w1 >> (i * 3)) & 0x7);",
      "    } else if (i < 21) {",
      "      w_item = ((w2 >> (i * 3 - 32)) & 0x7);",
      "    } else {",
      "      w_item = ((w3 >> (i * 3 - 64)) & 0x7);",
      "    }",
      "    *out_ptr = __hmul(__int2half_rn(w_item - w_zero), w_scale);",
      "    out_ptr += out_.width;",
      "  }",
      "}",
      "",
      "void reconstruct_gptq(const uint32_t* b_q_weight, const uint32_t* b_gptq_qzeros,",
      "                      const half* b_gptq_scales, const int* b_g_idx, half* out,",
      "                      int height, int width, int groups, int bit) {",
      "  dim3 blockDim, gridDim;",
      "  blockDim.x = BLOCK_KN_SIZE;",
      "  blockDim.y = 1;",
      "  gridDim.y = DIVIDE(height, 32 / bit);",
      "  gridDim.x = DIVIDE(width, BLOCK_KN_SIZE);",
      "",
      "  auto kernel = reconstruct_gptq_kernel<MatrixView_q4_row, 4>;",
      "  if (bit == 2) {",
      "    kernel = reconstruct_gptq_kernel<MatrixView_q2_row, 2>;",
      "  } else if (bit == 8) {",
      "    kernel = reconstruct_gptq_kernel<MatrixView_q8_row, 8>;",
      "  } else if (bit == 3) {",
      "    kernel = reconstruct_gptq_3bit_kernel;",
      "    gridDim.y = DIVIDE(height, 32);",
      "  }",
      "",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  kernel<<<gridDim, blockDim, 0, stream>>>(b_q_weight, b_gptq_scales,",
      "                                           b_gptq_qzeros, b_g_idx, height,",
      "                                           width, groups, out);",
      "}",
      "",
      "void gemm_half_q_half_cuda(cublasHandle_t cublas_handle, const half* a,",
      "                           const uint32_t* b_q_weight,",
      "                           const uint32_t* b_gptq_qzeros,",
      "                           const half* b_gptq_scales, const int* b_g_idx,",
      "                           half* c, half* temp_dq, int size_m, int size_n,",
      "                           int size_k, int groups, bool use_exllama, int bit) {",
      "  bool use_reconstruct;",
      "  if (use_exllama) {",
      "    use_reconstruct = ((bit == 8 && size_m > MAX_Q_GEMM_ROWS_8BIT) ||",
      "                       (bit != 8 && size_m > MAX_Q_GEMM_ROWS));",
      "  } else {",
      "    // The 2/3-bit kernels are somehow slower than dequant + gemm baseline, so",
      "    // we disabled them for now.",
      "    use_reconstruct = (bit < 4 || size_m > MAX_ALT_GEMM_ROWS);",
      "  }",
      "  if (use_reconstruct) {",
      "    // Reconstruct FP16 matrix, then cuBLAS",
      "    if (use_exllama) {",
      "      reconstruct_exllama(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,",
      "                          temp_dq, size_k, size_n, groups, bit);",
      "    } else {",
      "      reconstruct_gptq(b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,",
      "                       temp_dq, size_k, size_n, groups, bit);",
      "    }",
      "",
      "    const half alpha = __float2half(1.0f);",
      "    const half beta = __float2half(0.0f);",
      "    cublasHgemm(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N, size_n, size_m, size_k,",
      "                &alpha, temp_dq, size_n, a, size_k, &beta, c, size_n);",
      "  } else if (use_exllama) {",
      "    // Quantized matmul",
      "    int max_chunks = size_m / BLOCK_M_SIZE_MAX;",
      "    int last_chunk = max_chunks * BLOCK_M_SIZE_MAX;",
      "    int last_chunk_size = size_m - last_chunk;",
      "",
      "    if (max_chunks) {",
      "      gemm_half_q_half_cuda_part(a, b_q_weight, b_gptq_qzeros, b_gptq_scales,",
      "                                 b_g_idx, c, last_chunk, size_n, size_k,",
      "                                 BLOCK_M_SIZE_MAX, groups, bit);",
      "    }",
      "",
      "    if (last_chunk_size) {",
      "      gemm_half_q_half_cuda_part(a + last_chunk * size_k, b_q_weight,",
      "                                 b_gptq_qzeros, b_gptq_scales, b_g_idx,",
      "                                 c + last_chunk * size_n, last_chunk_size,",
      "                                 size_n, size_k, last_chunk_size, groups, bit);",
      "    }",
      "  } else {",
      "    gemm_half_q_half_alt(a, b_q_weight, b_gptq_qzeros, b_gptq_scales, b_g_idx,",
      "                         c, size_m, size_n, size_k, bit);",
      "  }",
      "}",
      "",
      "__global__ void shuffle_4bit_kernel(uint32_t* __restrict__ b_q_weight,",
      "                                    const int size_k, const int size_n) {",
      "  auto n = blockIdx.x * THREADS_X + threadIdx.x;",
      "  if (n >= size_n) return;",
      "  int k = 0;",
      "  uint32_t* b_ptr = b_q_weight + n;",
      "  while (k < size_k) {",
      "    shuffle_4bit_8(b_ptr, size_n);",
      "    b_ptr += 1 * size_n;",
      "    k += 8;",
      "  }",
      "}",
      "",
      "__global__ void shuffle_8bit_kernel(uint32_t* __restrict__ b_q_weight,",
      "                                    const int size_k, const int size_n) {",
      "  auto n = blockIdx.x * THREADS_X + threadIdx.x;",
      "  if (n >= size_n) return;",
      "  int k = 0;",
      "  uint32_t* b_ptr = b_q_weight + n;",
      "  while (k < size_k) {",
      "    shuffle_8bit_4(b_ptr, size_n);",
      "    b_ptr += 1 * size_n;",
      "    k += 4;",
      "  }",
      "}",
      "",
      "__global__ void shuffle_2bit_kernel(uint32_t* __restrict__ b_q_weight,",
      "                                    const int size_k, const int size_n) {",
      "  auto n = blockIdx.x * THREADS_X + threadIdx.x;",
      "  if (n >= size_n) return;",
      "  int k = 0;",
      "  uint32_t* b_ptr = b_q_weight + n;",
      "  while (k < size_k) {",
      "    shuffle_2bit_16(b_ptr, size_n);",
      "    b_ptr += 1 * size_n;",
      "    k += 16;",
      "  }",
      "}",
      "",
      "__global__ void shuffle_3bit_kernel(uint32_t* __restrict__ b_q_weight,",
      "                                    const int size_k, const int size_n) {",
      "  auto n = blockIdx.x * THREADS_X + threadIdx.x;",
      "  if (n >= size_n) return;",
      "  int k = 0;",
      "  uint32_t* b_ptr = b_q_weight + n;",
      "  while (k < size_k) {",
      "    shuffle_3bit_32(b_ptr, size_n);",
      "    b_ptr += 3 * size_n;",
      "    k += 32;",
      "  }",
      "}",
      "",
      "__global__ void make_sequential_4bit_kernel(const uint32_t* __restrict__ w,",
      "                                            uint32_t* __restrict__ w_new,",
      "                                            const int* __restrict__ q_perm,",
      "                                            const int w_width) {",
      "  const uint64_t* w2 = (uint64_t*)w;",
      "  uint64_t* w_new2 = (uint64_t*)w_new;",
      "  int w2_stride = w_width >> 1;",
      "  auto w2_column = THREADS_X * blockIdx.x + threadIdx.x;",
      "  if (w2_column >= w2_stride) return;",
      "  auto w_new2_row = blockIdx.y;",
      "  int q_perm_idx = w_new2_row << 3;",
      "  uint64_t dst = 0;",
      "",
      "#pragma unroll",
      "  for (int i = 0; i < 8; i++) {",
      "    int source_row = q_perm[q_perm_idx++];",
      "",
      "    int w2_row = source_row >> 3;",
      "    int w2_subrow = source_row & 0x07;",
      "    int w2_row_shift = w2_subrow << 2;",
      "    int wnew2_row_shift = i << 2;",
      "",
      "    uint64_t src = w2[w2_row * w2_stride + w2_column];",
      "    src >>= w2_row_shift;",
      "    src &= 0x0000000f0000000f;",
      "    src <<= wnew2_row_shift;",
      "    dst |= src;",
      "  }",
      "  w_new2[w_new2_row * w2_stride + w2_column] = dst;",
      "}",
      "",
      "__global__ void make_sequential_2bit_kernel(const uint32_t* __restrict__ w,",
      "                                            uint32_t* __restrict__ w_new,",
      "                                            const int* __restrict__ q_perm,",
      "                                            const int w_width) {",
      "  const uint64_t* w2 = (uint64_t*)w;",
      "  uint64_t* w_new2 = (uint64_t*)w_new;",
      "  int w2_stride = w_width >> 1;",
      "  auto w2_column = THREADS_X * blockIdx.x + threadIdx.x;",
      "  if (w2_column >= w2_stride) return;",
      "  auto w_new2_row = blockIdx.y;",
      "  int q_perm_idx = w_new2_row << 4;",
      "  uint64_t dst = 0;",
      "",
      "#pragma unroll",
      "  for (int i = 0; i < 16; i++) {",
      "    int source_row = q_perm[q_perm_idx++];",
      "",
      "    int w2_row = source_row >> 4;",
      "    int w2_subrow = source_row & 0x0f;",
      "    int w2_row_shift = w2_subrow << 1;",
      "    int wnew2_row_shift = i << 1;",
      "",
      "    uint64_t src = w2[w2_row * w2_stride + w2_column];",
      "    src >>= w2_row_shift;",
      "    src &= 0x0000000300000003;",
      "    src <<= wnew2_row_shift;",
      "    dst |= src;",
      "  }",
      "  w_new2[w_new2_row * w2_stride + w2_column] = dst;",
      "}",
      "",
      "__global__ void make_sequential_3bit_kernel(const uint32_t* __restrict__ w,",
      "                                            uint32_t* __restrict__ w_new,",
      "                                            const int* __restrict__ q_perm,",
      "                                            const int w_width) {",
      "  auto w_column = THREADS_X * blockIdx.x + threadIdx.x;",
      "  if (w_column >= w_width) return;",
      "  auto w_new_row = blockIdx.y * 3;",
      "  auto q_perm_idx = blockIdx.y << 5;",
      "  uint32_t dst[3] = {0, 0, 0};",
      "",
      "#pragma unroll",
      "  for (int i = 0; i < 32; i++) {",
      "    int source_row = q_perm[q_perm_idx++];",
      "    int z_w = (source_row / 32) * 3;",
      "    int z_mod = source_row % 32;",
      "    int z_bit;",
      "",
      "    if (z_mod != 10) {",
      "      if (z_mod != 21) {",
      "        z_bit = z_mod;",
      "        if (z_bit > 21) {",
      "          z_bit *= 3;",
      "          z_bit -= 64;",
      "          z_w += 2;",
      "        } else if (z_bit > 10) {",
      "          z_bit *= 3;",
      "          z_bit -= 32;",
      "          z_w += 1;",
      "        } else {",
      "          z_bit *= 3;",
      "        }",
      "      } else {",
      "        z_w += 1;",
      "      }",
      "    }",
      "",
      "    uint64_t src;",
      "    if (z_mod == 10) {",
      "      src = (w[z_w * w_width + w_column] >> 30) |",
      "            ((w[(z_w + 1) * w_width + w_column] << 2) & 0x4);",
      "    } else if (z_mod == 21) {",
      "      src = (w[z_w * w_width + w_column] >> 31) |",
      "            ((w[(z_w + 1) * w_width + w_column] << 1) & 0x6);",
      "    } else {",
      "      src = w[z_w * w_width + w_column];",
      "      src >>= z_bit;",
      "      src &= 0x07;",
      "    }",
      "",
      "    z_w = 0;",
      "    if (i != 10) {",
      "      if (i != 21) {",
      "        z_bit = i;",
      "        if (z_bit > 21) {",
      "          z_bit *= 3;",
      "          z_bit -= 64;",
      "          z_w += 2;",
      "        } else if (z_bit > 10) {",
      "          z_bit *= 3;",
      "          z_bit -= 32;",
      "          z_w += 1;",
      "        } else {",
      "          z_bit *= 3;",
      "        }",
      "      } else {",
      "        z_w += 1;",
      "      }",
      "    }",
      "    if (i == 10) {",
      "      dst[z_w] |= (src & 0x03) << 30;",
      "      dst[z_w + 1] |= ((src & 0x4) >> 2);",
      "    } else if (i == 21) {",
      "      dst[z_w] |= (src & 0x01) << 31;",
      "      dst[z_w + 1] |= ((src & 0x6) >> 1);",
      "    } else {",
      "      dst[z_w] |= (src << z_bit);",
      "    }",
      "  }",
      "  w_new[w_new_row * w_width + w_column] = dst[0];",
      "  w_new[(w_new_row + 1) * w_width + w_column] = dst[1];",
      "  w_new[(w_new_row + 2) * w_width + w_column] = dst[2];",
      "}",
      "",
      "__global__ void make_sequential_8bit_kernel(const uint32_t* __restrict__ w,",
      "                                            uint32_t* __restrict__ w_new,",
      "                                            const int* __restrict__ q_perm,",
      "                                            const int w_width) {",
      "  const uint64_t* w2 = (uint64_t*)w;",
      "  uint64_t* w_new2 = (uint64_t*)w_new;",
      "  int w2_stride = w_width >> 1;",
      "  auto w2_column = THREADS_X * blockIdx.x + threadIdx.x;",
      "  if (w2_column >= w2_stride) return;",
      "  auto w_new2_row = blockIdx.y;",
      "  int q_perm_idx = w_new2_row << 2;",
      "  uint64_t dst = 0;",
      "",
      "#pragma unroll",
      "  for (int i = 0; i < 4; i++) {",
      "    int source_row = q_perm[q_perm_idx++];",
      "",
      "    int w2_row = source_row >> 2;",
      "    int w2_subrow = source_row & 0x03;",
      "    int w2_row_shift = w2_subrow << 3;",
      "    int wnew2_row_shift = i << 3;",
      "",
      "    uint64_t src = w2[w2_row * w2_stride + w2_column];",
      "    src >>= w2_row_shift;",
      "    src &= 0x000000ff000000ff;",
      "    src <<= wnew2_row_shift;",
      "    dst |= src;",
      "  }",
      "  w_new2[w_new2_row * w2_stride + w2_column] = dst;",
      "}",
      "",
      "void shuffle_exllama_weight(uint32_t* q_weight, int* q_perm, int height,",
      "                            int width, int bit) {",
      "  if (q_perm) {",
      "    uint32_t* new_qweight = NULL;",
      "    cudaMalloc(&new_qweight, height / 32 * bit * width * sizeof(uint32_t));",
      "",
      "    dim3 blockDim, gridDim;",
      "    blockDim.x = THREADS_X;",
      "    blockDim.y = 1;",
      "    gridDim.x = DIVIDE(width, THREADS_X);",
      "    gridDim.y = height / 32 * bit;",
      "",
      "    auto kernel = make_sequential_4bit_kernel;",
      "    if (bit == 2) {",
      "      kernel = make_sequential_2bit_kernel;",
      "    } else if (bit == 3) {",
      "      kernel = make_sequential_3bit_kernel;",
      "      gridDim.y = height / 32;",
      "    } else if (bit == 8) {",
      "      kernel = make_sequential_8bit_kernel;",
      "    }",
      "    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "    kernel<<<gridDim, blockDim, 0, stream>>>(q_weight, new_qweight, q_perm,",
      "                                             width);",
      "    // Replace qweights",
      "    cudaMemcpyAsync(q_weight, new_qweight,",
      "                    height / 32 * bit * width * sizeof(uint32_t),",
      "                    cudaMemcpyDeviceToDevice);",
      "    // Cleanup",
      "    cudaDeviceSynchronize();",
      "    cudaFree(new_qweight);",
      "  }",
      "  dim3 blockDim, gridDim;",
      "  blockDim.x = THREADS_X;",
      "  blockDim.y = 1;",
      "  gridDim.x = DIVIDE(width, THREADS_X);",
      "  gridDim.y = 1;",
      "  auto shuffle_kernel = shuffle_4bit_kernel;",
      "  if (bit == 2) {",
      "    shuffle_kernel = shuffle_2bit_kernel;",
      "  } else if (bit == 3) {",
      "    shuffle_kernel = shuffle_3bit_kernel;",
      "  } else if (bit == 8) {",
      "    shuffle_kernel = shuffle_8bit_kernel;",
      "  }",
      "  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();",
      "  shuffle_kernel<<<gridDim, blockDim, 0, stream>>>(q_weight, height, width);",
      "}",
      "",
      "}  // namespace gptq",
      "}  // namespace vllm",
      "",
      "torch::Tensor gptq_gemm(torch::Tensor a, torch::Tensor b_q_weight,",
      "                        torch::Tensor b_gptq_qzeros,",
      "                        torch::Tensor b_gptq_scales, torch::Tensor b_g_idx,",
      "                        bool use_exllama, int64_t bit) {",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(a));",
      "  auto options = torch::TensorOptions().dtype(a.dtype()).device(a.device());",
      "  at::Tensor c = torch::empty({a.size(0), b_q_weight.size(1)}, options);",
      "  at::Tensor temp_dq = torch::empty(",
      "      {b_q_weight.size(0) * 32 / bit, b_q_weight.size(1)}, options);",
      "",
      "  vllm::gptq::gemm_half_q_half_cuda(",
      "      at::cuda::getCurrentCUDABlasHandle(), (const half*)a.data_ptr(),",
      "      (const uint32_t*)b_q_weight.data_ptr(),",
      "      (const uint32_t*)b_gptq_qzeros.data_ptr(),",
      "      (const half*)b_gptq_scales.data_ptr(),",
      "      b_g_idx.device().is_meta() ? NULL : (const int*)b_g_idx.data_ptr(),",
      "      (half*)c.data_ptr(), (half*)temp_dq.data_ptr(),",
      "      c.size(0),              // m",
      "      c.size(1),              // n",
      "      a.size(1),              // k",
      "      b_gptq_qzeros.size(0),  // group number",
      "      use_exllama, bit);",
      "  return c;",
      "}",
      "",
      "void gptq_shuffle(torch::Tensor q_weight, torch::Tensor q_perm, int64_t bit) {",
      "  const at::cuda::OptionalCUDAGuard device_guard(device_of(q_weight));",
      "  vllm::gptq::shuffle_exllama_weight(",
      "      (uint32_t*)q_weight.data_ptr(),",
      "      q_perm.device().is_meta() || q_perm.numel() == 0",
      "          ? NULL",
      "          : (int*)q_perm.data_ptr(),",
      "      q_weight.size(0) * 32 / bit, q_weight.size(1), bit);",
      "}"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq/qdq_3.cuh",
    "source": [
      "#ifndef _qdq_3_cuh",
      "#define _qdq_3_cuh",
      "",
      "#include \"qdq_util.cuh\"",
      "",
      "namespace vllm {",
      "namespace gptq {",
      "// Permutation:",
      "//",
      "// v9997775 55333111  u8886664 44222000  (u, v lsb)",
      "// vjjjhhhf ffdddbbb  uiiiggge eecccaaa",
      "// vtttrrrp ppnnnlll  usssqqqo oommmkkk",
      "",
      "__forceinline__ __device__ void shuffle_3bit_32(uint32_t* q, int stride) {",
      "  uint32_t qa = q[0 * stride];",
      "  uint32_t qb = q[1 * stride];",
      "  uint32_t qc = q[2 * stride];",
      "",
      "  // qa: aa999888 77766655  54443332 22111000",
      "  // qb: lkkkjjji iihhhggg  fffeeedd dcccbbba",
      "  // qc: vvvuuutt tsssrrrq  qqpppooo nnnmmmll",
      "",
      "  uint32_t qd = qc >> 26;",
      "  qc <<= 4;",
      "  qc |= qb >> 28;",
      "  qb <<= 2;",
      "  qb |= qa >> 30;",
      "",
      "  // qa: ..999888 77766655  54443332 22111000",
      "  // qb: ..jjjiii hhhgggff  feeedddc ccbbbaaa",
      "  // qc: ..tttsss rrrqqqpp  pooonnnm mmlllkkk",
      "  // qd:                               vvvuuu",
      "",
      "  uint32_t za = 0;",
      "  uint32_t zb = 0;",
      "  uint32_t zc = 0;",
      "",
      "  for (int i = 0; i < 5; i++) {",
      "    uint32_t t0 = qa & 0x07;",
      "    uint32_t t1 = (qa & 0x38) >> 3;",
      "    qa >>= 6;",
      "    za |= (t0 << (i * 3));",
      "    za |= (t1 << (i * 3 + 16));",
      "  }",
      "  for (int i = 0; i < 5; i++) {",
      "    uint32_t t0 = qb & 0x07;",
      "    uint32_t t1 = (qb & 0x38) >> 3;",
      "    qb >>= 6;",
      "    zb |= (t0 << (i * 3));",
      "    zb |= (t1 << (i * 3 + 16));",
      "  }",
      "  for (int i = 0; i < 5; i++) {",
      "    uint32_t t0 = qc & 0x07;",
      "    uint32_t t1 = (qc & 0x38) >> 3;",
      "    qc >>= 6;",
      "    zc |= (t0 << (i * 3));",
      "    zc |= (t1 << (i * 3 + 16));",
      "  }",
      "",
      "  // za:  9997775 55333111   8886664 44222000",
      "  // zb:  jjjhhhf ffdddbbb   iiiggge eecccaaa",
      "  // zc:  tttrrrp ppnnnlll   sssqqqo oommmkkk",
      "  // qd:                               vvvuuu",
      "",
      "  za |= ((qd & 0x01) >> 0) << 15;",
      "  zb |= ((qd & 0x02) >> 1) << 15;",
      "  zc |= ((qd & 0x04) >> 2) << 15;",
      "  za |= ((qd & 0x08) >> 3) << 31;",
      "  zb |= ((qd & 0x10) >> 4) << 31;",
      "  zc |= ((qd & 0x20) >> 5) << 31;",
      "",
      "  // za: v9997775 55333111  u8886664 44222000  (u, v lsb)",
      "  // zb: vjjjhhhf ffdddbbb  uiiiggge eecccaaa",
      "  // zc: vtttrrrp ppnnnlll  usssqqqo oommmkkk",
      "",
      "  q[0 * stride] = za;",
      "  q[1 * stride] = zb;",
      "  q[2 * stride] = zc;",
      "}",
      "",
      "__forceinline__ __device__ void dequant_3bit_32(const uint32_t q_0,",
      "                                                const uint32_t q_1,",
      "                                                const uint32_t q_2,",
      "                                                half2 (&dq)[16], int stride,",
      "                                                const uint32_t zero) {",
      "  const uint32_t c0 = 0x64006400;",
      "  const half y8_ = __float2half_rn(1.0f / 8.0f);",
      "  const half y64_ = __float2half_rn(1.0f / 64.0f);",
      "  const half2 y8 = __halves2half2(y8_, y8_);",
      "  const half2 y64 = __halves2half2(y64_, y64_);",
      "  const half_uint16 z1_(0xe400 | zero);  // half(-1024.0f - zero);",
      "  const half z8_ = __hsub(__int2half_rn(-128), __int2half_rn(zero));",
      "  const half z64_ = __hsub(__int2half_rn(-16), __int2half_rn(zero));",
      "  const half2 z1 = __halves2half2(z1_.as_half, z1_.as_half);",
      "  const half2 z8 = __halves2half2(z8_, z8_);",
      "  const half2 z64 = __halves2half2(z64_, z64_);",
      "",
      "  uint32_t qa = q_0;",
      "  uint32_t qb = q_1;",
      "  uint32_t qc = q_2;",
      "",
      "  half2_uint32 q0((qa & 0x00070007) | c0);  // half2(q[ 0], q[ 1])      + 1024",
      "  half2_uint32 q1((qa & 0x00380038) | c0);  // half2(q[ 2], q[ 3]) *  8 + 1024",
      "  qa >>= 6;",
      "  half2_uint32 q2((qa & 0x00070007) | c0);  // half2(q[ 4], q[ 5])      + 1024",
      "  half2_uint32 q3((qa & 0x00380038) | c0);  // half2(q[ 6], q[ 7]) *  8 + 1024",
      "  half2_uint32 q4((qa & 0x01c001c0) | c0);  // half2(q[ 8], q[ 9]) * 64 + 1024",
      "  qa >>= 9;",
      "  qa &= 0x00010001;",
      "  half2_uint32 q5((qb & 0x00070007) | c0);  // half2(q[10], q[11])      + 1024",
      "  half2_uint32 q6((qb & 0x00380038) | c0);  // half2(q[12], q[13]) *  8 + 1024",
      "  qb >>= 6;",
      "  half2_uint32 q7((qb & 0x00070007) | c0);  // half2(q[14], q[15])      + 1024",
      "  half2_uint32 q8((qb & 0x00380038) | c0);  // half2(q[16], q[17]) *  8 + 1024",
      "  half2_uint32 q9((qb & 0x01c001c0) | c0);  // half2(q[18], q[19]) * 64 + 1024",
      "  qb >>= 8;",
      "  qb &= 0x00020002;",
      "  half2_uint32 q10((qc & 0x00070007) | c0);  // half2(q[20], q[21])      + 1024",
      "  half2_uint32 q11((qc & 0x00380038) | c0);  // half2(q[22], q[23]) *  8 + 1024",
      "  qc >>= 6;",
      "  half2_uint32 q12((qc & 0x00070007) | c0);  // half2(q[24], q[25])      + 1024",
      "  half2_uint32 q13((qc & 0x00380038) | c0);  // half2(q[26], q[27]) *  8 + 1024",
      "  half2_uint32 q14((qc & 0x01c001c0) | c0);  // half2(q[28], q[29]) * 64 + 1024",
      "  qc >>= 7;",
      "  qc &= 0x00040004;",
      "  half2_uint32 q15((qa | qb | qc) | c0);",
      "",
      "  dq[0] = __hadd2(q0.as_half2, z1);",
      "  dq[1] = __hfma2(q1.as_half2, y8, z8);",
      "  dq[2] = __hadd2(q2.as_half2, z1);",
      "  dq[3] = __hfma2(q3.as_half2, y8, z8);",
      "  dq[4] = __hfma2(q4.as_half2, y64, z64);",
      "  dq[5] = __hadd2(q5.as_half2, z1);",
      "  dq[6] = __hfma2(q6.as_half2, y8, z8);",
      "  dq[7] = __hadd2(q7.as_half2, z1);",
      "  dq[8] = __hfma2(q8.as_half2, y8, z8);",
      "  dq[9] = __hfma2(q9.as_half2, y64, z64);",
      "  dq[10] = __hadd2(q10.as_half2, z1);",
      "  dq[11] = __hfma2(q11.as_half2, y8, z8);",
      "  dq[12] = __hadd2(q12.as_half2, z1);",
      "  dq[13] = __hfma2(q13.as_half2, y8, z8);",
      "  dq[14] = __hfma2(q14.as_half2, y64, z64);",
      "  dq[15] = __hadd2(q15.as_half2, z1);",
      "}",
      "",
      "}  // namespace gptq",
      "}  // namespace vllm",
      "",
      "#endif"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq/matrix_view.cuh",
    "source": [
      "/*",
      "Adapted from https://github.com/turboderp/exllamav2 and",
      "https://github.com/turboderp/exllama",
      "*/",
      "",
      "#ifndef _matrix_view_cuh",
      "#define _matrix_view_cuh",
      "",
      "#include <cuda_runtime.h>",
      "#include <cuda_fp16.h>",
      "",
      "#include \"qdq_util.cuh\"",
      "",
      "namespace vllm {",
      "namespace gptq {",
      "",
      "class MatrixView_half {",
      " public:",
      "  const half* data;",
      "  const int height;",
      "  const int width;",
      "",
      "  __device__ __forceinline__ MatrixView_half(const half* data, const int height,",
      "                                             const int width)",
      "      : data(data), height(height), width(width) {}",
      "",
      "  __device__ __forceinline__ half item(int row, int column) const {",
      "    return data[row * width + column];",
      "  }",
      "  __device__ __forceinline__ half2 item_half2(int row, int column) const {",
      "    return ((half2*)data)[(row * width + column) / 2];",
      "  }",
      "  __device__ __forceinline__ half2 item_half2half2(int row, int column) const {",
      "    return __half2half2(data[row * width + column]);",
      "  }",
      "  __device__ __forceinline__ const half* item_ptr(int row, int column) const {",
      "    return &data[row * width + column];",
      "  }",
      "",
      "  __device__ __forceinline__ void item4(half (&items)[4], int row,",
      "                                        int column) const {",
      "    half2* ptr = (half2*)item_ptr(row, column);",
      "    half2 i01 = ptr[0];",
      "    half2 i23 = ptr[1];",
      "    items[0] = __low2half(i01);",
      "    items[1] = __high2half(i01);",
      "    items[2] = __low2half(i23);",
      "    items[3] = __high2half(i23);",
      "  }",
      "  __device__ __forceinline__ void item4_f(float (&items)[4], int row,",
      "                                          int column) const {",
      "    half2* ptr = (half2*)item_ptr(row, column);",
      "    half2 i01 = ptr[0];",
      "    half2 i23 = ptr[1];",
      "    items[0] = __half2float(__low2half(i01));",
      "    items[1] = __half2float(__high2half(i01));",
      "    items[2] = __half2float(__low2half(i23));",
      "    items[3] = __half2float(__high2half(i23));",
      "  }",
      "",
      "  __device__ __forceinline__ void item4_h2(half2 (&items)[4], int row,",
      "                                           int column) const {",
      "    half2* ptr = (half2*)item_ptr(row, column);",
      "    half2 i01 = ptr[0];",
      "    half2 i23 = ptr[1];",
      "    items[0] = __half2half2(__low2half(i01));",
      "    items[1] = __half2half2(__high2half(i01));",
      "    items[2] = __half2half2(__low2half(i23));",
      "    items[3] = __half2half2(__high2half(i23));",
      "  }",
      "};",
      "",
      "class MatrixView_half_rw {",
      " public:",
      "  half* data;",
      "  const int height;",
      "  const int width;",
      "",
      "  __device__ __forceinline__ MatrixView_half_rw(half* data, const int height,",
      "                                                const int width)",
      "      : data(data), height(height), width(width) {}",
      "",
      "  __device__ __forceinline__ half item(int row, int column) const {",
      "    return data[row * width + column];",
      "  }",
      "  __device__ __forceinline__ half2 item_half2(int row, int column) const {",
      "    return ((half2*)data)[(row * width + column) / 2];",
      "  }",
      "  __device__ __forceinline__ half* item_ptr(int row, int column) {",
      "    return &data[row * width + column];",
      "  }",
      "  __device__ __forceinline__ void set(int row, int column, half value) {",
      "    data[row * width + column] = value;",
      "  }",
      "  __device__ __forceinline__ void set_half2(int row, int column, half2 value) {",
      "    ((half2*)data)[(row * width + column) / 2] = value;",
      "  }",
      "",
      "  __device__ __forceinline__ void set4(int row, int column, half v0, half v1,",
      "                                       half v2, half v3) {",
      "    half2 v01 = __halves2half2(v0, v1);",
      "    half2 v23 = __halves2half2(v2, v3);",
      "    half2* ptr = (half2*)item_ptr(row, column);",
      "    ptr[0] = v01;",
      "    ptr[1] = v23;",
      "  }",
      "};",
      "",
      "class MatrixView_q4_row {",
      " public:",
      "  const uint32_t* data;",
      "  const int height;",
      "  const int width;",
      "",
      "  __device__ __forceinline__ MatrixView_q4_row(const uint32_t* data,",
      "                                               const int height,",
      "                                               const int width)",
      "      : data(data), height(height), width(width) {}",
      "",
      "  __device__ __forceinline__ int item(int row, int column) const {",
      "    int shift = (column & 0x07) * 4;",
      "    return (data[row * width / 8 + column / 8] >> shift) & 0x0f;",
      "  }",
      "",
      "  __device__ __forceinline__ void item2(int (&items)[2], int row,",
      "                                        int column) const {",
      "    int shift = (column & 0x07) * 4;",
      "    uint32_t d = data[row * width / 8 + column / 8] >> shift;",
      "    items[0] = d & 0x0f;",
      "    items[1] = (d >> 4) & 0x0f;",
      "  }",
      "",
      "  __device__ __forceinline__ void item4(int (&items)[4], int row,",
      "                                        int column) const {",
      "    int shift = (column & 0x07) * 4;",
      "    uint32_t d = data[row * width / 8 + column / 8] >> shift;",
      "    items[0] = d & 0x0f;",
      "    items[1] = (d >> 4) & 0x0f;",
      "    items[2] = (d >> 8) & 0x0f;",
      "    items[3] = (d >> 12) & 0x0f;",
      "  }",
      "};",
      "",
      "class MatrixView_q4_column {",
      " public:",
      "  const uint32_t* data;",
      "  const int height;",
      "  const int width;",
      "",
      "  __device__ __forceinline__ MatrixView_q4_column(const uint32_t* data,",
      "                                                  const int height,",
      "                                                  const int width)",
      "      : data(data), height(height), width(width) {}",
      "",
      "  __device__ __forceinline__ int item(int row, int column) const {",
      "    int shift = (row & 0x07) * 4;",
      "    return (data[row / 8 * width + column] >> shift) & 0x0f;",
      "  }",
      "",
      "  __device__ __forceinline__ uint32_t item_uint32_t(int row, int column) {",
      "    return data[row / 8 * width + column];",
      "  }",
      "  __device__ __forceinline__ const uint32_t* item_uint32_ptr(int row,",
      "                                                             int column) {",
      "    return &data[row / 8 * width + column];",
      "  }",
      "};",
      "",
      "class MatrixView_q2_row {",
      " public:",
      "  const uint32_t* data;",
      "  const int height;",
      "  const int width;",
      "",
      "  __device__ __forceinline__ MatrixView_q2_row(const uint32_t* data,",
      "                                               const int height,",
      "                                               const int width)",
      "      : data(data), height(height), width(width) {}",
      "",
      "  __device__ __forceinline__ int item(int row, int column) const {",
      "    int shift = (column & 0x0f) * 2;",
      "    return (data[row * width / 16 + column / 16] >> shift) & 0x03;",
      "  }",
      "",
      "  __device__ __forceinline__ void item2(int (&items)[2], int row,",
      "                                        int column) const {",
      "    int shift = (column & 0x0f) * 2;",
      "    uint32_t d = data[row * width / 16 + column / 16] >> shift;",
      "    items[0] = d & 0x03;",
      "    items[1] = (d >> 2) & 0x03;",
      "  }",
      "",
      "  __device__ __forceinline__ void item4(int (&items)[4], int row,",
      "                                        int column) const {",
      "    int shift = (column & 0x0f) * 2;",
      "    uint32_t d = data[row * width / 16 + column / 16] >> shift;",
      "    items[0] = d & 0x03;",
      "    items[1] = (d >> 2) & 0x03;",
      "    items[2] = (d >> 4) & 0x03;",
      "    items[3] = (d >> 6) & 0x03;",
      "  }",
      "};",
      "",
      "class MatrixView_q3_row {",
      " public:",
      "  const uint32_t* data;",
      "  const int height;",
      "  const int width;",
      "",
      "  __device__ __forceinline__ MatrixView_q3_row(const uint32_t* data,",
      "                                               const int height,",
      "                                               const int width)",
      "      : data(data), height(height), width(width) {}",
      "",
      "  __device__ __forceinline__ int item(int row, int column) const {",
      "    int z_w = column * 3 / 32;",
      "    int z_mod = column & 0x1f;",
      "",
      "    if (z_mod == 10) {",
      "      return (data[row * width * 3 / 32 + z_w] >> 30) |",
      "             ((data[row * width * 3 / 32 + (z_w + 1)] << 2) & 0x4);",
      "    } else if (z_mod == 21) {",
      "      return (data[row * width * 3 / 32 + z_w] >> 31) |",
      "             ((data[row * width * 3 / 32 + (z_w + 1)] << 1) & 0x6);",
      "    } else if (z_mod < 10) {",
      "      return (data[row * width * 3 / 32 + z_w] >> (z_mod * 3)) & 0x07;",
      "    } else if (z_mod < 21) {",
      "      return (data[row * width * 3 / 32 + z_w] >> (z_mod * 3 - 32)) & 0x07;",
      "    } else {",
      "      return (data[row * width * 3 / 32 + z_w] >> (z_mod * 3 - 64)) & 0x07;",
      "    }",
      "  }",
      "",
      "  __device__ __forceinline__ void item4(int (&items)[4], int row,",
      "                                        int column) const {",
      "    int shift = (column & 0x1f);",
      "    uint32_t d;",
      "    if (shift <= 4) {",
      "      d = data[row * width / 32 * 3 + column * 3 / 32] >> (shift * 3);",
      "    } else if (shift == 8) {",
      "      d = (data[row * width / 32 * 3 + column * 3 / 32] >> 24) |",
      "          ((data[row * width / 32 * 3 + column * 3 / 32 + 1] & 0x0f) << 8);",
      "    } else if (shift <= 16) {",
      "      d = data[row * width / 32 * 3 + column * 3 / 32] >> (shift * 3 - 32);",
      "    } else if (shift == 20) {",
      "      d = (data[row * width / 32 * 3 + column * 3 / 32] >> 28) |",
      "          ((data[row * width / 32 * 3 + column * 3 / 32 + 1] & 0xff) << 4);",
      "    } else {",
      "      d = data[row * width / 32 * 3 + column * 3 / 32] >> (shift * 3 - 64);",
      "    }",
      "    items[0] = d & 0x07;",
      "    items[1] = (d >> 3) & 0x07;",
      "    items[2] = (d >> 6) & 0x07;",
      "    items[3] = (d >> 9) & 0x07;",
      "  }",
      "};",
      "",
      "class MatrixView_q8_row {",
      " public:",
      "  const uint32_t* data;",
      "  const int height;",
      "  const int width;",
      "",
      "  __device__ __forceinline__ MatrixView_q8_row(const uint32_t* data,",
      "                                               const int height,",
      "                                               const int width)",
      "      : data(data), height(height), width(width) {}",
      "",
      "  __device__ __forceinline__ int item(int row, int column) const {",
      "    int shift = (column & 0x03) * 8;",
      "    return (data[row * width / 4 + column / 4] >> shift) & 0xff;",
      "  }",
      "",
      "  __device__ __forceinline__ void item2(int (&items)[2], int row,",
      "                                        int column) const {",
      "    int shift = (column & 0x03) * 8;",
      "    uint32_t d = data[row * width / 4 + column / 4] >> shift;",
      "    items[0] = d & 0xff;",
      "    items[1] = (d >> 8) & 0xff;",
      "  }",
      "",
      "  __device__ __forceinline__ void item4(int (&items)[4], int row,",
      "                                        int column) const {",
      "    int shift = (column & 0x03) * 2;",
      "    uint32_t d = data[row * width / 4 + column / 4] >> shift;",
      "    items[0] = d & 0xff;",
      "    items[1] = (d >> 8) & 0xff;",
      "    items[2] = (d >> 16) & 0xff;",
      "    items[3] = (d >> 24) & 0xff;",
      "  }",
      "};",
      "",
      "}  // namespace gptq",
      "}  // namespace vllm",
      "#endif"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq/compat.cuh",
    "source": [
      "/*",
      "Copied from https://github.com/turboderp/exllamav2",
      "*/",
      "",
      "#ifndef _compat_cuh",
      "#define _compat_cuh",
      "",
      "namespace vllm {",
      "namespace gptq {",
      "// atomicAdd for half types, to support CC < 7.x",
      "",
      "__device__ __forceinline__ void atomicAdd_half(half* address, half val) {",
      "  unsigned int* address_as_ui =",
      "      (unsigned int*)((char*)address - ((size_t)address & 2));",
      "  unsigned int old = *address_as_ui;",
      "  unsigned int assumed;",
      "",
      "  do {",
      "    assumed = old;",
      "    __half_raw hsum;",
      "    hsum.x = (size_t)address & 2 ? (old >> 16) : (old & 0xffff);",
      "    half tmpres = __hadd(hsum, val);",
      "    hsum = __half_raw(tmpres);",
      "    old = (size_t)address & 2 ? (old & 0xffff) | (hsum.x << 16)",
      "                              : (old & 0xffff0000) | hsum.x;",
      "    old = atomicCAS(address_as_ui, assumed, old);",
      "  } while (assumed != old);",
      "}",
      "",
      "// atomicAdd for half2 types",
      "",
      "__device__ __forceinline__ void atomicAdd_half2(half2* address, half2 val) {",
      "  unsigned int* address_as_ui = (unsigned int*)address;",
      "  unsigned int old = *address_as_ui;",
      "  unsigned int assumed;",
      "  do {",
      "    assumed = old;",
      "    half2 old_val = *((half2*)&old);",
      "    half2 new_val = __hadd2(old_val, val);",
      "    old = atomicCAS(address_as_ui, assumed, *((unsigned int*)&new_val));",
      "  } while (assumed != old);",
      "}",
      "",
      "//",
      "",
      "#if defined(__CUDA_ARCH__) || defined(USE_ROCM)",
      "  #if __CUDA_ARCH__ < 700 || defined(USE_ROCM)",
      "",
      "__device__ __forceinline__ void atomicAdd(half* address, half val) {",
      "  atomicAdd_half(address, val);",
      "}",
      "",
      "    #if __CUDA_ARCH__ < 600 || defined(USE_ROCM)",
      "__device__ __forceinline__ void atomicAdd(half2* address, half2 val) {",
      "  atomicAdd_half2(address, val);",
      "}",
      "    #endif",
      "",
      "  #endif",
      "#endif",
      "",
      "}  // namespace gptq",
      "}  // namespace vllm",
      "#endif"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq/qdq_util.cuh",
    "source": [
      "/*",
      "Copied from https://github.com/turboderp/exllamav2",
      "*/",
      "",
      "#ifndef _qdq_util_cuh",
      "#define _qdq_util_cuh",
      "",
      "namespace vllm {",
      "namespace gptq {",
      "",
      "union half2_uint32 {",
      "  uint32_t as_uint32;",
      "  half2 as_half2;",
      "  __device__ half2_uint32(uint32_t val) : as_uint32(val) {}",
      "  __device__ half2_uint32(half2 val) : as_half2(val) {}",
      "};",
      "",
      "union half_uint16 {",
      "  uint16_t as_uint16;",
      "  half as_half;",
      "  __device__ half_uint16(uint16_t val) : as_uint16(val) {}",
      "  __device__ half_uint16(half val) : as_half(val) {}",
      "};",
      "",
      "// Max_scale premultiplied by 1/256",
      "",
      "__forceinline__ __device__ half dq_scale(const int qs, const half max_scale) {",
      "  int qs_i = qs + 1;",
      "  half qs_h = __int2half_rn(qs_i * qs_i);",
      "  qs_h = __hmul(qs_h, max_scale);",
      "  return qs_h;",
      "}",
      "",
      "__forceinline__ __device__ half dq(const int q, const int qzero,",
      "                                   const half scale) {",
      "  return __hmul(__int2half_rn(q - qzero), scale);",
      "}",
      "",
      "__forceinline__ __device__ half dq_ns(const int q, const int qzero) {",
      "  // return __hsub(__int2half_rn(q), __int2half_rn(qzero));",
      "  return __int2half_rn(q - qzero);",
      "}",
      "",
      "__forceinline__ __device__ int exb(const uint32_t q, const int shift,",
      "                                   const int mask) {",
      "  return (int)((q >> shift) & mask);",
      "}",
      "",
      "__forceinline__ __device__ int exb(const uint32_t q1, const uint32_t q0,",
      "                                   const int shift, const int mask) {",
      "  return (int)(__funnelshift_rc(q0, q1, shift) & mask);",
      "}",
      "",
      "}  // namespace gptq",
      "}  // namespace vllm",
      "#endif"
    ]
  },
  {
    "type": "cuda",
    "path": "csrc/quantization/gptq/qdq_4.cuh",
    "source": [
      "/*",
      "Copied from https://github.com/turboderp/exllamav2",
      "*/",
      "",
      "#ifndef _qdq_4_cuh",
      "#define _qdq_4_cuh",
      "",
      "#include \"qdq_util.cuh\"",
      "",
      "namespace vllm {",
      "namespace gptq {",
      "// Permutation:",
      "//",
      "// 77775555 33331111  66664444 22220000",
      "",
      "__forceinline__ __device__ void shuffle_4bit_8(uint32_t* q, int stride) {",
      "  uint32_t qa = q[0];",
      "  uint32_t qb = 0;",
      "",
      "#pragma unroll",
      "  for (int i = 0; i < 4; i++) {",
      "    uint32_t qa0 = qa & 0x0f;",
      "    uint32_t qa1 = (qa & 0xf0) >> 4;",
      "    qa >>= 8;",
      "    qb |= (qa1 << (i * 4 + 16));",
      "    qb |= (qa0 << (i * 4));",
      "  }",
      "  q[0] = qb;",
      "}",
      "",
      "__forceinline__ __device__ void dequant_4bit_8(const uint32_t q_0,",
      "                                               half2 (&dq)[4], int stride,",
      "                                               const uint32_t zero) {",
      "  const uint32_t c0 = 0x64006400;",
      "  const half y16_ = __float2half_rn(1.0f / 16.0f);",
      "  const half2 y16 = __halves2half2(y16_, y16_);",
      "  const half_uint16 z1_(0xe400 | zero);  // half(-1024.0f - zero);",
      "  const half z16_ = __hsub(__int2half_rn(-64), __int2half_rn(zero));",
      "  const half2 z1 = __half2half2(z1_.as_half);",
      "  const half2 z16 = __half2half2(z16_);",
      "",
      "  uint32_t qa = q_0;",
      "  half2_uint32 q0((qa & 0x000f000f) | c0);  // half2(q[ 0], q[ 1])      + 1024",
      "  half2_uint32 q1((qa & 0x00f000f0) | c0);  // half2(q[ 2], q[ 3]) * 16 + 1024",
      "  qa >>= 8;",
      "  half2_uint32 q2((qa & 0x000f000f) | c0);  // half2(q[ 4], q[ 5])      + 1024",
      "  half2_uint32 q3((qa & 0x00f000f0) | c0);  // half2(q[ 6], q[ 7]) * 16 + 1024",
      "",
      "  dq[0] = __hadd2(q0.as_half2, z1);",
      "  dq[1] = __hfma2(q1.as_half2, y16, z16);",
      "  dq[2] = __hadd2(q2.as_half2, z1);",
      "  dq[3] = __hfma2(q3.as_half2, y16, z16);",
      "}",
      "",
      "__forceinline__ __device__ void dequant_4bit_8_prep_zero_scale(",
      "    const uint32_t zero, const half scale, half2 (&z1z16)[2],",
      "    half2 (&y1y16)[2]) {",
      "  half_uint16 z1(0xe400 | zero);  // half(-1024.0f - zero);",
      "  half z16 = __hsub(__int2half_rn(-64), __int2half_rn(zero));",
      "",
      "  half2 scale2 = __half2half2(scale);",
      "",
      "  z1z16[0] = __hmul2(scale2, __half2half2(z1.as_half));",
      "  z1z16[1] = __hmul2(scale2, __half2half2(z16));",
      "",
      "  const half y1 = __float2half_rn(1.0f);",
      "  const half y16 = __float2half_rn(1.0f / 16.0f);",
      "",
      "  y1y16[0] = __hmul2(scale2, __half2half2(y1));",
      "  y1y16[1] = __hmul2(scale2, __half2half2(y16));",
      "}",
      "",
      "__forceinline__ __device__ void dequant_4bit_8_prep_zero(const uint32_t zero,",
      "                                                         half2 (&z1z16)[2],",
      "                                                         half2 (&y1y16)[2]) {",
      "  half_uint16 z1(0xe400 | zero);  // half(-1024.0f - zero);",
      "  half z16 = __hsub(__int2half_rn(-64), __int2half_rn(zero));",
      "",
      "  z1z16[0] = __half2half2(z1.as_half);",
      "  z1z16[1] = __half2half2(z16);",
      "",
      "  const half y1 = __float2half_rn(1.0f);",
      "  const half y16 = __float2half_rn(1.0f / 16.0f);",
      "",
      "  y1y16[0] = __half2half2(y1);",
      "  y1y16[1] = __half2half2(y16);",
      "}",
      "",
      "__forceinline__ __device__ void dequant_4bit_8_gptq(const uint32_t q_0,",
      "                                                    half2 (&dq)[4],",
      "                                                    half2 (&z1z16)[2],",
      "                                                    half2 (&y1y16)[2],",
      "                                                    int stride, bool scaled) {",
      "  const uint32_t c0 = 0x64006400;",
      "",
      "  uint32_t qa = q_0;",
      "  half2_uint32 q0((qa & 0x000f000f) |",
      "                  c0);  // half2( q[0]      + 1024, q[1]      + 1024 )",
      "  half2_uint32 q1((qa & 0x00f000f0) |",
      "                  c0);  // half2( q[2] * 16 + 1024, q[3] * 16 + 1024 )",
      "  qa >>= 8;",
      "  half2_uint32 q2((qa & 0x000f000f) |",
      "                  c0);  // half2( q[4]      + 1024, q[5]      + 1024 )",
      "  half2_uint32 q3((qa & 0x00f000f0) |",
      "                  c0);  // half2( q[6] * 16 + 1024, q[7] * 16 + 1024 )",
      "",
      "  if (scaled) {",
      "    dq[0] = __hfma2(q0.as_half2, y1y16[0],",
      "                    z1z16[0]);  // half2( q[0] * s - z * s, q[1] * s - z * s)",
      "    dq[1] = __hfma2(q1.as_half2, y1y16[1],",
      "                    z1z16[1]);  // half2( q[2] * s - z * s, q[3] * s - z * s)",
      "    dq[2] = __hfma2(q2.as_half2, y1y16[0], z1z16[0]);",
      "    dq[3] = __hfma2(q3.as_half2, y1y16[1], z1z16[1]);",
      "  } else {",
      "    dq[0] = __hadd2(q0.as_half2, z1z16[0]);  // half2( q[0] - z, q[1] - z )",
      "    dq[1] = __hfma2(q1.as_half2, y1y16[1],",
      "                    z1z16[1]);               // half2( q[2] - z, q[3] - z )",
      "    dq[2] = __hadd2(q2.as_half2, z1z16[0]);  // half2( q[4] - z, q[5] - z )",
      "    dq[3] = __hfma2(q3.as_half2, y1y16[1],",
      "                    z1z16[1]);  // half2( q[6] - z, q[7] - z )",
      "  }",
      "}",
      "}  // namespace gptq",
      "}  // namespace vllm",
      "",
      "#endif"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/attention/ops/chunked_prefill_paged_decode.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "# Authors:",
      "#  - Burkhard Ringlein <ngl@zurich.ibm.com>",
      "#  - Jan van Lunteren <jvl@zurich.ibm.com>",
      "#  - Chih-Chieh Yang <chih.chieh.yang@ibm.com>",
      "#  - Thomas Parnell <tpa@zurich.ibm.com>",
      "",
      "import torch",
      "",
      "from vllm import _custom_ops as ops",
      "from vllm.platforms import current_platform",
      "from vllm.triton_utils import tl, triton",
      "",
      "from .prefix_prefill import context_attention_fwd",
      "",
      "",
      "@triton.jit",
      "def cdiv_fn(x, y):",
      "    return (x + y - 1) // y",
      "",
      "",
      "@triton.jit",
      "def kernel_paged_attention_2d(",
      "        output_ptr,  # [num_tokens, num_query_heads, head_size]",
      "        query_ptr,  # [num_tokens, num_query_heads, head_size]",
      "        key_cache_ptr,  # [num_blks, num_kv_heads, head_size // x, blk_size, x]",
      "        value_cache_ptr,  # [num_blks, num_kv_heads, head_size, blk_size]",
      "        sink_ptr,  # [num_query_heads]",
      "        block_tables_ptr,  # [num_seqs, max_num_blocks_per_seq]",
      "        seq_lens_ptr,  # [num_seqs]",
      "        alibi_slopes_ptr,  # [num_query_heads]",
      "        scale,  # float32",
      "        k_scale,  # float32",
      "        v_scale,  # float32",
      "        num_query_heads: tl.constexpr,  # int",
      "        num_queries_per_kv: tl.constexpr,  # int",
      "        num_queries_per_kv_padded: tl.constexpr,  # int",
      "        block_table_stride: tl.int64,  # int",
      "        query_stride_0: tl.int64,  # int",
      "        query_stride_1: tl.int64,  # int, should be equal to head_size",
      "        output_stride_0: tl.int64,  # int",
      "        output_stride_1: tl.int64,  # int, should be equal to head_size",
      "        BLOCK_SIZE: tl.constexpr,  # int",
      "        HEAD_SIZE: tl.constexpr,  # int",
      "        HEAD_SIZE_PADDED: tl.constexpr,  # int, must be power of 2",
      "        USE_ALIBI_SLOPES: tl.constexpr,  # bool",
      "        SLIDING_WINDOW: tl.constexpr,  # int",
      "        x: tl.constexpr,  # int",
      "        stride_k_cache_0: tl.int64,  # int",
      "        stride_k_cache_1: tl.int64,  # int",
      "        stride_k_cache_2: tl.int64,  # int",
      "        stride_k_cache_3: tl.int64,  # int",
      "        stride_k_cache_4: tl.int64,  # int",
      "        stride_v_cache_0: tl.int64,  # int",
      "        stride_v_cache_1: tl.int64,  # int",
      "        stride_v_cache_2: tl.int64,  # int",
      "        stride_v_cache_3: tl.int64,  # int",
      "        filter_by_query_len: tl.constexpr,  # bool",
      "        query_start_len_ptr,  # [num_seqs+1]",
      "        USE_SINKS: tl.constexpr,  # bool",
      "):",
      "    seq_idx = tl.program_id(0)",
      "    kv_head_idx = tl.program_id(1)",
      "",
      "    if filter_by_query_len:",
      "        cur_batch_in_all_start_index = tl.load(query_start_len_ptr + seq_idx)",
      "        cur_batch_in_all_stop_index = tl.load(query_start_len_ptr + seq_idx +",
      "                                              1)",
      "        cur_batch_query_len = cur_batch_in_all_stop_index \\",
      "            - cur_batch_in_all_start_index",
      "        if cur_batch_query_len > 1:",
      "            return",
      "    else:",
      "        cur_batch_in_all_start_index = seq_idx",
      "",
      "    query_head_idx = kv_head_idx * num_queries_per_kv + tl.arange(",
      "        0, num_queries_per_kv_padded)",
      "",
      "    query_offset = (cur_batch_in_all_start_index * query_stride_0 +",
      "                    query_head_idx[:, None] * query_stride_1)",
      "",
      "    head_mask = query_head_idx < (kv_head_idx + 1) * num_queries_per_kv",
      "    head_mask = head_mask & (query_head_idx < num_query_heads)",
      "",
      "    dim_mask = tl.where(tl.arange(0, HEAD_SIZE_PADDED) < HEAD_SIZE, 1,",
      "                        0).to(tl.int1)",
      "",
      "    # Q : (num_queries_per_kv, HEAD_SIZE,)",
      "    Q = tl.load(",
      "        query_ptr + query_offset + tl.arange(0, HEAD_SIZE_PADDED)[None, :],",
      "        mask=dim_mask[None, :] & head_mask[:, None],",
      "        other=0.0,",
      "    )",
      "",
      "    block_table_offset = seq_idx * block_table_stride",
      "",
      "    if not USE_SINKS:",
      "        M = tl.full([num_queries_per_kv_padded],",
      "                    float(\"-inf\"),",
      "                    dtype=tl.float32)",
      "    else:",
      "        M = tl.load(",
      "            sink_ptr + query_head_idx,",
      "            mask=head_mask,",
      "            other=float(\"-inf\"),",
      "        ).to(dtype=tl.float32)",
      "",
      "    L = tl.full([num_queries_per_kv_padded], 1.0, dtype=tl.float32)",
      "    acc = tl.zeros([num_queries_per_kv_padded, HEAD_SIZE_PADDED],",
      "                   dtype=tl.float32)",
      "",
      "    # sequence len for this particular sequence",
      "    seq_len = tl.load(seq_lens_ptr + seq_idx)",
      "",
      "    # alibi slope for this head",
      "    if USE_ALIBI_SLOPES:",
      "        alibi_slope = tl.load(alibi_slopes_ptr + query_head_idx,",
      "                              mask=head_mask,",
      "                              other=0.0)",
      "",
      "    num_blocks = cdiv_fn(seq_len, BLOCK_SIZE)",
      "",
      "    # iterate through tiles",
      "    for j in range(0, num_blocks):",
      "",
      "        physical_block_idx = tl.load(block_tables_ptr + block_table_offset + j)",
      "",
      "        offs_n = tl.arange(0, BLOCK_SIZE)",
      "        offs_d = tl.arange(0, HEAD_SIZE_PADDED)",
      "",
      "        v_offset = (physical_block_idx * stride_v_cache_0 +",
      "                    kv_head_idx * stride_v_cache_1 +",
      "                    offs_d[None, :] * stride_v_cache_2 +",
      "                    offs_n[:, None] * stride_v_cache_3)",
      "",
      "        k_offset = (physical_block_idx * stride_k_cache_0 +",
      "                    kv_head_idx * stride_k_cache_1 +",
      "                    (offs_d[:, None] // x) * stride_k_cache_2 +",
      "                    offs_n[None, :] * stride_k_cache_3 +",
      "                    (offs_d[:, None] % x) * stride_k_cache_4)",
      "",
      "        # K : (HEAD_SIZE, BLOCK_SIZE)",
      "        K_load = tl.load(key_cache_ptr + k_offset,",
      "                         mask=dim_mask[:, None],",
      "                         other=0.0)",
      "",
      "        if K_load.dtype.is_fp8():",
      "            K = (K_load.to(tl.float32) * tl.load(k_scale)).to(Q.dtype)",
      "        else:",
      "            K = K_load",
      "",
      "        # V : (BLOCK_SIZE, HEAD_SIZE)",
      "        V_load = tl.load(value_cache_ptr + v_offset,",
      "                         mask=dim_mask[None, :],",
      "                         other=0.0)",
      "",
      "        if V_load.dtype.is_fp8():",
      "            V = (V_load.to(tl.float32) * tl.load(v_scale)).to(Q.dtype)",
      "        else:",
      "            V = V_load",
      "",
      "        seq_offset = j * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "        boundary = tl.full([BLOCK_SIZE], seq_len, dtype=tl.int32)",
      "        seq_mask = seq_offset[None, :] < boundary",
      "",
      "        # S : (num_queries_per_kv, BLOCK_SIZE,)",
      "        S = tl.where(head_mask[:, None] & seq_mask, 0.0,",
      "                     float(\"-inf\")).to(tl.float32)",
      "        S += scale * tl.dot(Q, K)",
      "",
      "        context_len = seq_len - 1",
      "",
      "        if SLIDING_WINDOW > 0:",
      "            S = tl.where((context_len - seq_offset) < SLIDING_WINDOW, S,",
      "                         -10000)",
      "",
      "        if USE_ALIBI_SLOPES:",
      "            S += alibi_slope[:, None] * (seq_offset - context_len)",
      "",
      "        # compute running maximum",
      "        # m_j : (num_queries_per_kv,)",
      "        m_j = tl.maximum(M, tl.max(S, axis=1))",
      "",
      "        # P : (num_queries_per_kv, BLOCK_SIZE,)",
      "        P = tl.exp(S - m_j[:, None])",
      "",
      "        # l_j : (num_queries_per_kv,)",
      "        l_j = tl.sum(P, axis=1)",
      "",
      "        # alpha : (num_queries_per_kv, )",
      "        alpha = tl.exp(M - m_j)",
      "",
      "        # acc : (num_queries_per_kv, BLOCK_SIZE,)",
      "        acc = acc * alpha[:, None]",
      "",
      "        # update constants",
      "        L = L * alpha + l_j",
      "        M = m_j",
      "",
      "        # acc : (num_queries_per_kv, BLOCK_SIZE,)",
      "        acc += tl.dot(P.to(V.dtype), V)",
      "",
      "    # epilogue",
      "    acc = acc / L[:, None]",
      "",
      "    output_offset = (cur_batch_in_all_start_index * output_stride_0 +",
      "                     query_head_idx * output_stride_1)",
      "",
      "    tl.store(",
      "        output_ptr + output_offset[:, None] +",
      "        tl.arange(0, HEAD_SIZE_PADDED)[None, :],",
      "        acc,",
      "        mask=dim_mask[None, :] & head_mask[:, None],",
      "    )",
      "",
      "",
      "def chunked_prefill_paged_decode(",
      "    query,",
      "    key,",
      "    value,",
      "    output,",
      "    kv_cache_dtype,",
      "    key_cache,",
      "    value_cache,",
      "    block_table,",
      "    query_start_loc,",
      "    seq_lens,",
      "    max_seq_len,",
      "    max_query_len,",
      "    k_scale,",
      "    v_scale,",
      "    alibi_slopes=None,",
      "    sliding_window=None,",
      "    sm_scale=None,",
      "    # Optional tensor for sinks",
      "    sinks=None,",
      "):",
      "",
      "    if sm_scale is None:",
      "        sm_scale = 1.0 / (query.shape[1]**0.5)",
      "",
      "    use_alibi_slopes = alibi_slopes is not None",
      "",
      "    if sliding_window is None or sliding_window <= 0:",
      "        sliding_window = 0",
      "",
      "    if max_query_len > 1:",
      "        context_attention_fwd(",
      "            q=query,",
      "            k=key,",
      "            v=value,",
      "            o=output,",
      "            kv_cache_dtype=kv_cache_dtype,",
      "            k_cache=key_cache,",
      "            v_cache=value_cache,",
      "            b_loc=block_table,",
      "            b_start_loc=query_start_loc,",
      "            b_seq_len=seq_lens,",
      "            max_seq_len=max_seq_len,",
      "            max_input_len=max_query_len,",
      "            k_scale=k_scale,",
      "            v_scale=v_scale,",
      "            alibi_slopes=alibi_slopes,",
      "            sliding_window=sliding_window,",
      "            sm_scale=sm_scale,",
      "            skip_decode=True,",
      "            sinks=sinks,",
      "        )",
      "",
      "    block_size = value_cache.shape[3]",
      "    num_seqs = len(seq_lens)",
      "    num_query_heads = query.shape[1]",
      "    num_kv_heads = key.shape[1]",
      "    num_queries_per_kv = query.shape[1] // key.shape[1]",
      "    head_size = query.shape[2]",
      "",
      "    # Conversion of FP8 Tensor from uint8 storage to",
      "    # appropriate torch.dtype for interpretation by Triton",
      "    if \"fp8\" in kv_cache_dtype:",
      "        assert key_cache.dtype in [torch.uint8, current_platform.fp8_dtype()]",
      "        assert value_cache.dtype in [torch.uint8, current_platform.fp8_dtype()]",
      "",
      "        if kv_cache_dtype in (\"fp8\", \"fp8_e4m3\"):",
      "            target_dtype = current_platform.fp8_dtype()",
      "        elif kv_cache_dtype == \"fp8_e5m2\":",
      "            target_dtype = torch.float8_e5m2",
      "        else:",
      "            raise ValueError(\"Unsupported FP8 dtype:\", kv_cache_dtype)",
      "",
      "        key_cache = key_cache.view(target_dtype)",
      "        value_cache = value_cache.view(target_dtype)",
      "",
      "    num_queries_per_kv_padded = max(triton.next_power_of_2(num_queries_per_kv),",
      "                                    16)",
      "",
      "    from vllm.platforms.rocm import use_rocm_custom_paged_attention",
      "    use_custom = use_rocm_custom_paged_attention(",
      "        query.dtype,",
      "        head_size,",
      "        block_size,",
      "        num_queries_per_kv,",
      "        max_seq_len,",
      "        sliding_window,",
      "        kv_cache_dtype,",
      "        alibi_slopes,",
      "        sinks,",
      "    )",
      "    if use_custom:",
      "        _PARTITION_SIZE_ROCM = 256",
      "        max_num_partitions = ((max_seq_len + _PARTITION_SIZE_ROCM - 1) //",
      "                              _PARTITION_SIZE_ROCM)",
      "        assert _PARTITION_SIZE_ROCM % block_size == 0",
      "        total_num_seq = block_table.shape[0]",
      "        tmp_output = torch.empty(",
      "            size=(total_num_seq, num_query_heads, max_num_partitions,",
      "                  head_size),",
      "            dtype=output.dtype,",
      "            device=output.device,",
      "        )",
      "        exp_sums = torch.empty(",
      "            size=(total_num_seq, num_query_heads, max_num_partitions),",
      "            dtype=torch.float32,",
      "            device=output.device,",
      "        )",
      "        max_logits = torch.empty_like(exp_sums)",
      "",
      "        ops.paged_attention_rocm(",
      "            output,",
      "            exp_sums,",
      "            max_logits,",
      "            tmp_output,",
      "            query,",
      "            key_cache,",
      "            value_cache,",
      "            num_kv_heads,",
      "            scale=sm_scale,",
      "            block_tables=block_table,",
      "            seq_lens=seq_lens,",
      "            query_start_loc=query_start_loc,",
      "            block_size=block_size,",
      "            max_seq_len=max_seq_len,",
      "            alibi_slopes=alibi_slopes,",
      "            kv_cache_dtype=kv_cache_dtype,",
      "            k_scale=k_scale,",
      "            v_scale=v_scale,",
      "        )",
      "    else:",
      "        kernel_paged_attention_2d[(",
      "            num_seqs,",
      "            num_kv_heads,",
      "        )](",
      "            output_ptr=output,",
      "            query_ptr=query,",
      "            key_cache_ptr=key_cache,",
      "            value_cache_ptr=value_cache,",
      "            sink_ptr=sinks,",
      "            block_tables_ptr=block_table,",
      "            seq_lens_ptr=seq_lens,",
      "            alibi_slopes_ptr=alibi_slopes,",
      "            scale=sm_scale,",
      "            k_scale=k_scale,",
      "            v_scale=v_scale,",
      "            num_query_heads=num_query_heads,",
      "            num_queries_per_kv=num_queries_per_kv,",
      "            num_queries_per_kv_padded=num_queries_per_kv_padded,",
      "            block_table_stride=block_table.stride(0),",
      "            query_stride_0=query.stride(0),",
      "            query_stride_1=query.stride(1),",
      "            output_stride_0=output.stride(0),",
      "            output_stride_1=output.stride(1),",
      "            BLOCK_SIZE=block_size,",
      "            HEAD_SIZE=head_size,",
      "            HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),",
      "            USE_ALIBI_SLOPES=use_alibi_slopes,",
      "            SLIDING_WINDOW=sliding_window,",
      "            x=key_cache.shape[4],",
      "            stride_k_cache_0=key_cache.stride(0),",
      "            stride_k_cache_1=key_cache.stride(1),",
      "            stride_k_cache_2=key_cache.stride(2),",
      "            stride_k_cache_3=key_cache.stride(3),",
      "            stride_k_cache_4=key_cache.stride(4),",
      "            stride_v_cache_0=value_cache.stride(0),",
      "            stride_v_cache_1=value_cache.stride(1),",
      "            stride_v_cache_2=value_cache.stride(2),",
      "            stride_v_cache_3=value_cache.stride(3),",
      "            filter_by_query_len=True,",
      "            query_start_len_ptr=query_start_loc,",
      "            USE_SINKS=sinks is not None,",
      "        )"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/attention/ops/triton_unified_attention.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "# Authors:",
      "#  - Burkhard Ringlein <ngl@zurich.ibm.com>",
      "#  - Jan van Lunteren <jvl@zurich.ibm.com>",
      "#  - Chih-Chieh Yang <chih.chieh.yang@ibm.com>",
      "#  - Thomas Parnell <tpa@zurich.ibm.com>",
      "",
      "import torch",
      "",
      "from vllm.logger import init_logger",
      "from vllm.triton_utils import tl, triton",
      "",
      "logger = init_logger(__name__)",
      "",
      "",
      "@triton.jit",
      "def cdiv_fn(x, y):",
      "    return (x + y - 1) // y",
      "",
      "",
      "@triton.jit",
      "def apply_softcap(S, x):",
      "    Sdiv = S / x",
      "    p1 = tl.exp(Sdiv)",
      "    p2 = tl.exp(-Sdiv)",
      "    return x * (p1 - p2) / (p1 + p2)",
      "",
      "",
      "@triton.jit",
      "def find_seq_idx(query_start_len_ptr, target_idx, num_seqs,",
      "                 BLOCK_Q: tl.constexpr, use_q_block_mode: tl.constexpr):",
      "    left: tl.int32 = 0",
      "    right = num_seqs",
      "    while left < right:",
      "        mid = (left + right) // 2",
      "        val = tl.load(query_start_len_ptr + mid)",
      "        mid_val = val // BLOCK_Q + mid if use_q_block_mode else val",
      "",
      "        if mid_val <= target_idx:",
      "            left = mid + 1",
      "        else:",
      "            right = mid",
      "",
      "    return left - 1",
      "",
      "",
      "@triton.jit",
      "def kernel_unified_attention_2d(",
      "        output_ptr,  # [num_tokens, num_query_heads, head_size]",
      "        query_ptr,  # [num_tokens, num_query_heads, head_size]",
      "        key_cache_ptr,  # [num_blks, blk_size, num_kv_heads, head_size]",
      "        value_cache_ptr,  # [num_blks, blk_size, num_kv_heads, head_size]",
      "        sink_ptr,  # [num_query_heads]",
      "        block_tables_ptr,  # [num_seqs, max_num_blocks_per_seq]",
      "        seq_lens_ptr,  # [num_seqs]",
      "        alibi_slopes_ptr,  # [num_query_heads]",
      "        qq_bias_ptr,  # [num_query_tokens, num_query_tokens]",
      "        scale,  # float32",
      "        k_scale,  # float32",
      "        v_scale,  # float32",
      "        softcap,  # float32",
      "        num_query_heads: tl.constexpr,  # int",
      "        num_queries_per_kv: tl.constexpr,  # int",
      "        block_table_stride: tl.int64,  # int",
      "        query_stride_0: tl.int64,  # int",
      "        query_stride_1: tl.int64,  # int, should be equal to head_size",
      "        output_stride_0: tl.int64,  # int",
      "        output_stride_1: tl.int64,  # int, should be equal to head_size",
      "        qq_bias_stride_0: tl.int64,  # int",
      "        BLOCK_SIZE: tl.constexpr,  # int",
      "        HEAD_SIZE: tl.constexpr,  # int",
      "        HEAD_SIZE_PADDED: tl.constexpr,  # int, must be power of 2",
      "        USE_ALIBI_SLOPES: tl.constexpr,  # bool",
      "        USE_QQ_BIAS: tl.constexpr,  # bool",
      "        USE_SOFTCAP: tl.constexpr,  # bool",
      "        USE_SINKS: tl.constexpr,  # bool",
      "        SLIDING_WINDOW: tl.constexpr,  # int",
      "        stride_k_cache_0: tl.int64,  # int",
      "        stride_k_cache_1: tl.int64,  # int",
      "        stride_k_cache_2: tl.int64,  # int",
      "        stride_k_cache_3: tl.constexpr,  # int",
      "        stride_v_cache_0: tl.int64,  # int",
      "        stride_v_cache_1: tl.int64,  # int",
      "        stride_v_cache_2: tl.int64,  # int",
      "        stride_v_cache_3: tl.constexpr,  # int",
      "        query_start_len_ptr,  # [num_seqs+1]",
      "        BLOCK_Q: tl.constexpr,  # int",
      "        num_seqs: tl.int32,",
      "        BLOCK_M: tl.constexpr,  # int",
      "):",
      "    q_block_global_idx = tl.program_id(0)",
      "    kv_head_idx = tl.program_id(1)",
      "",
      "    seq_idx = find_seq_idx(query_start_len_ptr, q_block_global_idx, num_seqs,",
      "                           BLOCK_Q, True)",
      "",
      "    q_block_start_idx = tl.load(query_start_len_ptr +",
      "                                seq_idx) // BLOCK_Q + seq_idx",
      "",
      "    q_block_local_idx = q_block_global_idx - q_block_start_idx",
      "",
      "    cur_batch_in_all_start_index = tl.load(query_start_len_ptr + seq_idx)",
      "    cur_batch_in_all_stop_index = tl.load(query_start_len_ptr + seq_idx + 1)",
      "",
      "    cur_batch_query_len = cur_batch_in_all_stop_index \\",
      "        - cur_batch_in_all_start_index",
      "",
      "    if q_block_local_idx * BLOCK_Q >= cur_batch_query_len:",
      "        return",
      "",
      "    offs_m = tl.arange(0, BLOCK_M)",
      "    offs_d = tl.arange(0, HEAD_SIZE_PADDED)",
      "    query_pos = q_block_local_idx * BLOCK_Q + offs_m // num_queries_per_kv",
      "",
      "    query_offset_0 = cur_batch_in_all_start_index + query_pos",
      "    query_offset_1 = kv_head_idx * num_queries_per_kv + \\",
      "        offs_m % num_queries_per_kv",
      "    query_offset = (query_offset_0[:, None] * query_stride_0 +",
      "                    query_offset_1[:, None] * query_stride_1 + offs_d[None, :])",
      "",
      "    dim_mask = tl.where(offs_d < HEAD_SIZE, 1, 0).to(tl.int1)",
      "    query_mask_0 = tl.where(query_pos < cur_batch_query_len, 1, 0).to(tl.int1)",
      "    query_mask_1 = tl.where(query_offset_1 < num_query_heads, 1, 0).to(tl.int1)",
      "",
      "    # Q : (BLOCK_M, HEAD_SIZE_PADDED)",
      "    Q = tl.load(",
      "        query_ptr + query_offset,",
      "        mask=dim_mask[None, :] & query_mask_0[:, None] & query_mask_1[:, None],",
      "        other=0.0,",
      "    )",
      "",
      "    block_table_offset = seq_idx * block_table_stride",
      "",
      "    if not USE_SINKS:",
      "        M = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)",
      "    else:",
      "        M = tl.load(",
      "            sink_ptr + query_offset_1,",
      "            mask=query_mask_1,",
      "            other=float(\"-inf\"),",
      "        ).to(dtype=tl.float32)",
      "",
      "    L = tl.full([BLOCK_M], 1.0, dtype=tl.float32)",
      "    acc = tl.zeros([BLOCK_M, HEAD_SIZE_PADDED], dtype=tl.float32)",
      "",
      "    # sequence len for this particular sequence",
      "    seq_len = tl.load(seq_lens_ptr + seq_idx)",
      "",
      "    # context length for this particular sequences",
      "    context_len = seq_len - cur_batch_query_len",
      "",
      "    # alibi slope for this head",
      "    if USE_ALIBI_SLOPES:",
      "        alibi_slope = tl.load(alibi_slopes_ptr + query_offset_1,",
      "                              mask=query_mask_1,",
      "                              other=0.0)",
      "",
      "    # query-query attention bias",
      "    if USE_QQ_BIAS:",
      "        qq_bias_row_ptrs = (qq_bias_ptr + query_pos[:, None] * qq_bias_stride_0",
      "                            )  # shape: [BLOCK_M]",
      "",
      "    # compute the length of the longest sequence prefix spanned by any",
      "    # query token in the current q_block (q_block_local_idx)",
      "    max_seq_prefix_len = context_len + q_block_local_idx * BLOCK_Q + (",
      "        BLOCK_M - 1) // num_queries_per_kv + 1",
      "",
      "    # adjust for potential padding in the last q_block by considering the",
      "    # actual sequence length",
      "    max_seq_prefix_len = tl.minimum(max_seq_prefix_len, seq_len)",
      "",
      "    # calculate the number of tiles (blocks) that need to be processed to",
      "    # cover the longest sequence prefix (due to causal masking, blocks beyond",
      "    # this prefix can be skipped)",
      "    num_blocks = cdiv_fn(max_seq_prefix_len, BLOCK_SIZE)",
      "",
      "    # iterate through tiles",
      "    for j in range(0, num_blocks):",
      "",
      "        physical_block_idx = tl.load(block_tables_ptr + block_table_offset + j)",
      "",
      "        offs_n = tl.arange(0, BLOCK_SIZE)",
      "",
      "        v_offset = (physical_block_idx * stride_v_cache_0 +",
      "                    kv_head_idx * stride_v_cache_2 +",
      "                    offs_d[None, :] * stride_v_cache_3 +",
      "                    offs_n[:, None] * stride_v_cache_1)",
      "",
      "        k_offset = (physical_block_idx * stride_k_cache_0 +",
      "                    kv_head_idx * stride_k_cache_2 +",
      "                    offs_d[:, None] * stride_k_cache_3 +",
      "                    offs_n[None, :] * stride_k_cache_1)",
      "",
      "        # K : (HEAD_SIZE, BLOCK_SIZE)",
      "        K_load = tl.load(key_cache_ptr + k_offset,",
      "                         mask=dim_mask[:, None],",
      "                         other=0.0)",
      "",
      "        if K_load.dtype.is_fp8():",
      "            if Q.dtype.is_fp8():",
      "                K = K_load",
      "            else:",
      "                K = (K_load.to(tl.float32) * tl.load(k_scale)).to(Q.dtype)",
      "        else:",
      "            K = K_load",
      "",
      "        # V : (BLOCK_SIZE, HEAD_SIZE)",
      "        V_load = tl.load(value_cache_ptr + v_offset,",
      "                         mask=dim_mask[None, :],",
      "                         other=0.0)",
      "",
      "        if V_load.dtype.is_fp8():",
      "            if Q.dtype.is_fp8():",
      "                V = V_load",
      "            else:",
      "                V = (V_load.to(tl.float32) * tl.load(v_scale)).to(Q.dtype)",
      "        else:",
      "            V = V_load",
      "",
      "        seq_offset = j * BLOCK_SIZE + offs_n",
      "",
      "        seq_mask = seq_offset[None, :] < context_len + query_pos[:, None] + 1",
      "",
      "        # S : (BLOCK_M, BLOCK_SIZE)",
      "        S = tl.zeros(shape=(BLOCK_M, BLOCK_SIZE), dtype=tl.float32)",
      "",
      "        S += scale * tl.dot(Q, K)",
      "",
      "        if USE_SOFTCAP:",
      "            S = apply_softcap(S, softcap)",
      "",
      "        S = tl.where(query_mask_1[:, None] & query_mask_0[:, None] & seq_mask,",
      "                     S, float(\"-inf\"))",
      "",
      "        if SLIDING_WINDOW > 0:",
      "            S = tl.where((context_len + query_pos[:, None] - seq_offset)",
      "                         < SLIDING_WINDOW, S, float(\"-inf\"))",
      "",
      "        if USE_ALIBI_SLOPES:",
      "            S += alibi_slope[:, None] * (seq_offset - context_len)",
      "",
      "        if USE_QQ_BIAS:",
      "            # compute key positions relative to query section",
      "            key_rel_pos = seq_offset - context_len  # shape: [BLOCK_SIZE]",
      "            # load bias only for keys that correspond to queries",
      "            is_query_key = key_rel_pos >= 0 and key_rel_pos < qq_bias_stride_0",
      "            qq_bias = tl.load(",
      "                qq_bias_row_ptrs + key_rel_pos[None, :],",
      "                mask=is_query_key[None, :],  # avoid OOB for context keys",
      "                other=0.0,",
      "            )",
      "            S += qq_bias",
      "",
      "        # compute running maximum",
      "        # m_j : (BLOCK_M,)",
      "        m_j = tl.maximum(M, tl.max(S, axis=1))",
      "        # For sliding window there's a chance the max is -inf due to masking of",
      "        # the entire row. In this case we need to set m_j 0 to avoid NaN",
      "        m_j = tl.where(m_j > float(\"-inf\"), m_j, 0.0)",
      "",
      "        # P : (BLOCK_M, BLOCK_SIZE)",
      "        P = tl.exp(S - m_j[:, None])",
      "",
      "        # l_j : (BLOCK_M,)",
      "        l_j = tl.sum(P, axis=1)",
      "",
      "        # alpha : (BLOCK_M, )",
      "        alpha = tl.exp(M - m_j)",
      "",
      "        # acc : (BLOCK_M, HEAD_SIZE_PADDED)",
      "        acc = acc * alpha[:, None]",
      "",
      "        # update constants",
      "        L = L * alpha + l_j",
      "        M = m_j",
      "",
      "        # acc : (BLOCK_M, HEAD_SIZE_PADDED)",
      "        acc += tl.dot(P.to(V.dtype), V)",
      "",
      "    # epilogue",
      "    acc = acc / L[:, None]",
      "",
      "    output_offset = (query_offset_0[:, None] * output_stride_0 +",
      "                     query_offset_1[:, None] * output_stride_1 +",
      "                     offs_d[None, :])",
      "",
      "    tl.store(",
      "        output_ptr + output_offset,",
      "        acc,",
      "        mask=dim_mask[None, :] & query_mask_0[:, None] & query_mask_1[:, None],",
      "    )",
      "",
      "",
      "@triton.jit",
      "def kernel_unified_attention_3d(",
      "        segm_output_ptr,",
      "        # [num_tokens, num_query_heads, num_segments, head_size]",
      "        segm_max_ptr,  # [num_tokens, num_query_heads, num_segments]",
      "        segm_expsum_ptr,  # [num_tokens, num_query_heads, num_segments]",
      "        query_ptr,  # [num_tokens, num_query_heads, head_size]",
      "        key_cache_ptr,  # [num_blks, num_kv_heads, head_size // x, blk_size, x]",
      "        value_cache_ptr,  # [num_blks, num_kv_heads, head_size, blk_size]",
      "        sink_ptr,  # [num_query_heads]",
      "        block_tables_ptr,  # [num_seqs, max_num_blocks_per_seq]",
      "        seq_lens_ptr,  # [num_seqs]",
      "        alibi_slopes_ptr,  # [num_query_heads]",
      "        qq_bias_ptr,  # [num_query_tokens, num_query_tokens]",
      "        scale,  # float32",
      "        k_scale,  # float32",
      "        v_scale,  # float32",
      "        softcap,  # float32",
      "        num_query_heads: tl.constexpr,  # int",
      "        num_queries_per_kv: tl.constexpr,  # int",
      "        block_table_stride: tl.int64,  # int",
      "        query_stride_0: tl.int64,  # int",
      "        query_stride_1: tl.int64,  # int, should be equal to head_size",
      "        qq_bias_stride_0: tl.int64,  # int",
      "        BLOCK_SIZE: tl.constexpr,  # int",
      "        HEAD_SIZE: tl.constexpr,  # int",
      "        HEAD_SIZE_PADDED: tl.constexpr,  # int, must be power of 2",
      "        USE_ALIBI_SLOPES: tl.constexpr,  # bool",
      "        USE_QQ_BIAS: tl.constexpr,  # bool",
      "        USE_SOFTCAP: tl.constexpr,  # bool",
      "        USE_SINKS: tl.constexpr,  # bool",
      "        SLIDING_WINDOW: tl.constexpr,  # int",
      "        stride_k_cache_0: tl.int64,  # int",
      "        stride_k_cache_1: tl.int64,  # int",
      "        stride_k_cache_2: tl.int64,  # int",
      "        stride_k_cache_3: tl.constexpr,  # int",
      "        stride_v_cache_0: tl.int64,  # int",
      "        stride_v_cache_1: tl.int64,  # int",
      "        stride_v_cache_2: tl.int64,  # int",
      "        stride_v_cache_3: tl.constexpr,  # int",
      "        query_start_len_ptr,  # [num_seqs+1]",
      "        BLOCK_Q: tl.constexpr,  # int",
      "        num_seqs: tl.int32,",
      "        BLOCK_M: tl.constexpr,  # int",
      "        NUM_SEGMENTS_PER_SEQ: tl.constexpr,  # int",
      "):",
      "    q_block_global_idx = tl.program_id(0)",
      "    kv_head_idx = tl.program_id(1)",
      "    segm_idx = tl.program_id(2)",
      "",
      "    seq_idx = find_seq_idx(query_start_len_ptr, q_block_global_idx, num_seqs,",
      "                           BLOCK_Q, True)",
      "",
      "    q_block_start_idx = tl.load(query_start_len_ptr +",
      "                                seq_idx) // BLOCK_Q + seq_idx",
      "",
      "    q_block_local_idx = q_block_global_idx - q_block_start_idx",
      "",
      "    cur_batch_in_all_start_index = tl.load(query_start_len_ptr + seq_idx)",
      "    cur_batch_in_all_stop_index = tl.load(query_start_len_ptr + seq_idx + 1)",
      "",
      "    cur_batch_query_len = cur_batch_in_all_stop_index \\",
      "        - cur_batch_in_all_start_index",
      "",
      "    if q_block_local_idx * BLOCK_Q >= cur_batch_query_len:",
      "        return",
      "",
      "    # sequence len for this particular sequence",
      "    seq_len = tl.load(seq_lens_ptr + seq_idx)",
      "",
      "    # number of segments for this particular sequence",
      "    num_segments = NUM_SEGMENTS_PER_SEQ",
      "    blocks_per_segment = cdiv_fn(seq_len, num_segments * BLOCK_SIZE)",
      "",
      "    if segm_idx * blocks_per_segment * BLOCK_SIZE >= seq_len:",
      "        return",
      "",
      "    offs_m = tl.arange(0, BLOCK_M)",
      "    offs_d = tl.arange(0, HEAD_SIZE_PADDED)",
      "",
      "    query_pos = q_block_local_idx * BLOCK_Q + offs_m // num_queries_per_kv",
      "",
      "    query_offset_0 = cur_batch_in_all_start_index + query_pos",
      "    query_offset_1 = kv_head_idx * num_queries_per_kv + \\",
      "        offs_m % num_queries_per_kv",
      "",
      "    query_offset = (query_offset_0[:, None] * query_stride_0 +",
      "                    query_offset_1[:, None] * query_stride_1 + offs_d[None, :])",
      "",
      "    dim_mask = tl.where(offs_d < HEAD_SIZE, 1, 0).to(tl.int1)",
      "    query_mask_0 = tl.where(query_pos < cur_batch_query_len, 1, 0).to(tl.int1)",
      "    query_mask_1 = tl.where(query_offset_1 < num_query_heads, 1, 0).to(tl.int1)",
      "",
      "    # Q : (BLOCK_M, HEAD_SIZE_PADDED)",
      "    Q = tl.load(",
      "        query_ptr + query_offset,",
      "        mask=dim_mask[None, :] & query_mask_0[:, None] & query_mask_1[:, None],",
      "        other=0.0,",
      "    )",
      "",
      "    block_table_offset = seq_idx * block_table_stride",
      "",
      "    if USE_SINKS:",
      "        if segm_idx == 0:",
      "            M = tl.load(",
      "                sink_ptr + query_offset_1,",
      "                mask=query_mask_1,",
      "                other=float(\"-inf\"),",
      "            ).to(dtype=tl.float32)",
      "        else:",
      "            M = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)",
      "    else:",
      "        M = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)",
      "",
      "    L = tl.full([BLOCK_M], 1.0, dtype=tl.float32)",
      "    acc = tl.zeros([BLOCK_M, HEAD_SIZE_PADDED], dtype=tl.float32)",
      "",
      "    # context length for this particular sequences",
      "    context_len = seq_len - cur_batch_query_len",
      "",
      "    # alibi slope for this head",
      "    if USE_ALIBI_SLOPES:",
      "        alibi_slope = tl.load(alibi_slopes_ptr + query_offset_1,",
      "                              mask=query_mask_1,",
      "                              other=0.0)",
      "",
      "    # query-query attention bias",
      "    if USE_QQ_BIAS:",
      "        qq_bias_row_ptrs = (qq_bias_ptr + query_pos[:, None] * qq_bias_stride_0",
      "                            )  # shape: [BLOCK_M]",
      "",
      "    num_blocks = cdiv_fn(seq_len, BLOCK_SIZE)",
      "",
      "    # iterate through tiles within current segment",
      "    for j in range(",
      "            segm_idx * blocks_per_segment,",
      "            min((segm_idx + 1) * blocks_per_segment, num_blocks),",
      "    ):",
      "        physical_block_idx = tl.load(block_tables_ptr + block_table_offset + j)",
      "",
      "        offs_n = tl.arange(0, BLOCK_SIZE)",
      "",
      "        v_offset = (physical_block_idx * stride_v_cache_0 +",
      "                    kv_head_idx * stride_v_cache_2 +",
      "                    offs_d[None, :] * stride_v_cache_3 +",
      "                    offs_n[:, None] * stride_v_cache_1)",
      "",
      "        k_offset = (physical_block_idx * stride_k_cache_0 +",
      "                    kv_head_idx * stride_k_cache_2 +",
      "                    offs_d[:, None] * stride_k_cache_3 +",
      "                    offs_n[None, :] * stride_k_cache_1)",
      "",
      "        # K : (HEAD_SIZE, BLOCK_SIZE)",
      "        K_load = tl.load(key_cache_ptr + k_offset,",
      "                         mask=dim_mask[:, None],",
      "                         other=0.0)",
      "",
      "        if K_load.dtype.is_fp8():",
      "            if Q.dtype.is_fp8():",
      "                K = K_load",
      "            else:",
      "                K = (K_load.to(tl.float32) * tl.load(k_scale)).to(Q.dtype)",
      "        else:",
      "            K = K_load",
      "",
      "        # V : (BLOCK_SIZE, HEAD_SIZE)",
      "        V_load = tl.load(value_cache_ptr + v_offset,",
      "                         mask=dim_mask[None, :],",
      "                         other=0.0)",
      "",
      "        if V_load.dtype.is_fp8():",
      "            if Q.dtype.is_fp8():",
      "                V = V_load",
      "            else:",
      "                V = (V_load.to(tl.float32) * tl.load(v_scale)).to(Q.dtype)",
      "        else:",
      "            V = V_load",
      "",
      "        seq_offset = j * BLOCK_SIZE + offs_n",
      "",
      "        seq_mask = seq_offset[None, :] < context_len + query_pos[:, None] + 1",
      "",
      "        # S : (BLOCK_M, BLOCK_SIZE)",
      "        S = tl.zeros(shape=(BLOCK_M, BLOCK_SIZE), dtype=tl.float32)",
      "",
      "        S += scale * tl.dot(Q, K)",
      "",
      "        if USE_SOFTCAP:",
      "            S = apply_softcap(S, softcap)",
      "",
      "        S = tl.where(query_mask_1[:, None] & query_mask_0[:, None] & seq_mask,",
      "                     S, float(\"-inf\"))",
      "",
      "        if SLIDING_WINDOW > 0:",
      "            S = tl.where((context_len + query_pos[:, None] - seq_offset)",
      "                         < SLIDING_WINDOW, S, float(\"-inf\"))",
      "",
      "        if USE_ALIBI_SLOPES:",
      "            S += alibi_slope[:, None] * (seq_offset - context_len)",
      "",
      "        if USE_QQ_BIAS:",
      "            # compute key positions relative to query section",
      "            key_rel_pos = seq_offset - context_len  # shape: [BLOCK_SIZE]",
      "            # load bias only for keys that correspond to queries",
      "            is_query_key = key_rel_pos >= 0 and key_rel_pos < qq_bias_stride_0",
      "            qq_bias = tl.load(",
      "                qq_bias_row_ptrs + key_rel_pos[None, :],",
      "                mask=is_query_key[None, :],  # avoid OOB for context keys",
      "                other=0.0,",
      "            )",
      "            S += qq_bias",
      "",
      "        # compute running maximum",
      "        # m_j : (BLOCK_M,)",
      "        m_j = tl.maximum(M, tl.max(S, axis=1))",
      "        # For sliding window there's a chance the max is -inf due to masking of",
      "        # the entire row. In this case we need to set m_j 0 to avoid NaN",
      "        m_j = tl.where(m_j > float(\"-inf\"), m_j, 0.0)",
      "",
      "        # P : (BLOCK_M, BLOCK_SIZE,)",
      "        P = tl.exp(S - m_j[:, None])",
      "",
      "        # l_j : (BLOCK_M,)",
      "        l_j = tl.sum(P, axis=1)",
      "",
      "        # alpha : (BLOCK_M, )",
      "        alpha = tl.exp(M - m_j)",
      "",
      "        # acc : (BLOCK_M, HEAD_SIZE_PADDED)",
      "        acc = acc * alpha[:, None]",
      "",
      "        # update constants",
      "        L = L * alpha + l_j",
      "        M = m_j",
      "",
      "        # acc : (BLOCK_M, HEAD_SIZE_PADDED)",
      "        acc += tl.dot(P.to(V.dtype), V)",
      "",
      "    segm_output_offset = (",
      "        query_offset_0[:, None].to(tl.int64) *",
      "        (num_query_heads * NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED) +",
      "        query_offset_1[:, None] * (NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED) +",
      "        segm_idx * HEAD_SIZE_PADDED + tl.arange(0, HEAD_SIZE_PADDED)[None, :])",
      "    tl.store(",
      "        segm_output_ptr + segm_output_offset,",
      "        acc,",
      "        mask=dim_mask[None, :] & query_mask_0[:, None] & query_mask_1[:, None],",
      "    )",
      "    segm_offset = (query_offset_0.to(tl.int64) *",
      "                   (num_query_heads * NUM_SEGMENTS_PER_SEQ) +",
      "                   query_offset_1 * NUM_SEGMENTS_PER_SEQ + segm_idx)",
      "    tl.store(segm_max_ptr + segm_offset, M, mask=query_mask_0 & query_mask_1)",
      "    tl.store(segm_expsum_ptr + segm_offset,",
      "             L,",
      "             mask=query_mask_0 & query_mask_1)",
      "",
      "",
      "@triton.jit",
      "def reduce_segments(",
      "        output_ptr,  # [num_tokens, num_query_heads, head_size]",
      "        segm_output_ptr,",
      "        #[num_tokens, num_query_heads, max_num_segments, head_size]",
      "        segm_max_ptr,  # [num_tokens, num_query_heads, max_num_segments]",
      "        segm_expsum_ptr,  # [num_tokens, num_query_heads, max_num_segments]",
      "        seq_lens_ptr,  # [num_seqs]",
      "        num_seqs,  # int",
      "        num_query_heads: tl.constexpr,  # int",
      "        output_stride_0: tl.int64,  # int",
      "        output_stride_1: tl.int64,  # int, should be equal to head_size",
      "        block_table_stride: tl.int64,  # int",
      "        BLOCK_SIZE: tl.constexpr,  # int",
      "        HEAD_SIZE: tl.constexpr,  # int, must be power of 2",
      "        HEAD_SIZE_PADDED: tl.constexpr,  # int, must be power of 2",
      "        query_start_len_ptr,  # [num_seqs+1]",
      "        BLOCK_Q: tl.constexpr,  # int",
      "        NUM_SEGMENTS_PER_SEQ: tl.constexpr,  # int",
      "):",
      "    query_token_idx = tl.program_id(0)",
      "    query_head_idx = tl.program_id(1)",
      "",
      "    seq_idx = find_seq_idx(query_start_len_ptr, query_token_idx, num_seqs,",
      "                           BLOCK_Q, False)",
      "",
      "    # sequence len for this particular sequence",
      "    seq_len = tl.load(seq_lens_ptr + seq_idx)",
      "",
      "    # number of segments for this particular sequence",
      "    num_segments = NUM_SEGMENTS_PER_SEQ",
      "    blocks_per_segment = cdiv_fn(seq_len, num_segments * BLOCK_SIZE)",
      "",
      "    # create masks for subsequent loads",
      "    act_num_segments = cdiv_fn(seq_len, blocks_per_segment * BLOCK_SIZE)",
      "    segm_mask = tl.arange(0, NUM_SEGMENTS_PER_SEQ) < tl.full(",
      "        [NUM_SEGMENTS_PER_SEQ], act_num_segments, dtype=tl.int32)",
      "    dim_mask = tl.where(tl.arange(0, HEAD_SIZE_PADDED) < HEAD_SIZE, 1,",
      "                        0).to(tl.int1)",
      "",
      "    # load segment maxima",
      "    segm_offset = (query_token_idx.to(tl.int64) *",
      "                   (num_query_heads * NUM_SEGMENTS_PER_SEQ) +",
      "                   query_head_idx * NUM_SEGMENTS_PER_SEQ +",
      "                   tl.arange(0, NUM_SEGMENTS_PER_SEQ))",
      "    segm_max = tl.load(segm_max_ptr + segm_offset,",
      "                       mask=segm_mask,",
      "                       other=float(\"-inf\"))",
      "    overall_max = tl.max(segm_max)",
      "",
      "    # load and rescale segment exp sums",
      "    segm_expsum = tl.load(segm_expsum_ptr + segm_offset,",
      "                          mask=segm_mask,",
      "                          other=0.0)",
      "    segm_expsum = segm_expsum * tl.exp(segm_max - overall_max)",
      "    overall_expsum = tl.sum(segm_expsum)",
      "",
      "    # load, rescale, and add segment attention outputs",
      "    segm_output_offset = (",
      "        query_token_idx.to(tl.int64) *",
      "        (num_query_heads * NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED) +",
      "        query_head_idx * (NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED) +",
      "        tl.arange(0, NUM_SEGMENTS_PER_SEQ)[:, None] * HEAD_SIZE_PADDED +",
      "        tl.arange(0, HEAD_SIZE_PADDED)[None, :])",
      "    segm_output = tl.load(",
      "        segm_output_ptr + segm_output_offset,",
      "        mask=segm_mask[:, None] & dim_mask[None, :],",
      "        other=0.0,",
      "    )",
      "    segm_output *= tl.exp(segm_max - overall_max)[:, None]",
      "    acc_sum = tl.sum(segm_output, axis=0)",
      "    # safely divide by overall_expsum, returning 0.0 if overall_expsum is 0",
      "    acc = tl.where(overall_expsum == 0.0, 0.0, acc_sum / overall_expsum)",
      "",
      "    # write result",
      "    output_offset = (query_token_idx * output_stride_0 +",
      "                     query_head_idx * output_stride_1 +",
      "                     tl.arange(0, HEAD_SIZE_PADDED))",
      "    tl.store(output_ptr + output_offset, acc, mask=dim_mask)",
      "",
      "",
      "def unified_attention(",
      "    q,",
      "    k,",
      "    v,",
      "    out,",
      "    cu_seqlens_q,",
      "    max_seqlen_q,",
      "    seqused_k,",
      "    max_seqlen_k,",
      "    softmax_scale,",
      "    causal,",
      "    window_size,",
      "    block_table,",
      "    softcap,",
      "    q_descale,",
      "    k_descale,",
      "    v_descale,",
      "    alibi_slopes=None,",
      "    qq_bias=None,",
      "    # Optional tensor for sinks",
      "    sinks=None,",
      "):",
      "    assert causal, \"Only causal attention is supported\"",
      "    assert q_descale is None, \"Q scales not supported\"",
      "",
      "    block_size = v.shape[1]",
      "    assert q.element_size() >= 2 or block_size >= 32, \\",
      "        \"Block size must be at least 32 for fp8\"",
      "",
      "    if sinks is not None:",
      "        assert sinks.shape[0] == q.shape[1], \\",
      "        \"Sinks must be num_query_heads size\"",
      "",
      "    use_alibi_slopes = alibi_slopes is not None",
      "    use_qq_bias = qq_bias is not None",
      "",
      "    block_size = v.shape[1]",
      "    num_seqs = len(seqused_k)",
      "    num_query_heads = q.shape[1]",
      "    num_kv_heads = k.shape[2]",
      "    num_queries_per_kv = num_query_heads // num_kv_heads",
      "    head_size = q.shape[2]",
      "",
      "    BLOCK_M = 16 if num_queries_per_kv <= 16 else triton.next_power_of_2(",
      "        num_queries_per_kv)",
      "    BLOCK_Q = BLOCK_M // num_queries_per_kv",
      "",
      "    # Ideally we would launch with kernel with:",
      "    # \\sum_i[ceil(query_len[i] / BLOCK_Q)] blocks.",
      "    # However, it is slow to realize the query_lens on cpu.",
      "    # Instead we use upper-bound:",
      "    # \\sum_i[ceil(query_len[i] / BLOCK_Q)]",
      "    #   <= \\sum_i[floor(query_len[i] / BLOCK_Q) + 1]",
      "    #    = \\sum_i[floor(query_len[i] / BLOCK_Q)] + num_seqs",
      "    #   <= floor(\\sum_i(query_len[i]) / BLOCK_Q) + num_seqs",
      "    #    = floor(q.shape[0] / BLOCK_Q) + num_seqs",
      "    total_num_q_blocks = q.shape[0] // BLOCK_Q + num_seqs",
      "",
      "    # if batch contains a prefill",
      "    if max_seqlen_q > 1 or total_num_q_blocks * num_kv_heads > 128:",
      "        kernel_unified_attention_2d[(",
      "            total_num_q_blocks,",
      "            num_kv_heads,",
      "        )](",
      "            output_ptr=out,",
      "            query_ptr=q,",
      "            key_cache_ptr=k,",
      "            value_cache_ptr=v,",
      "            sink_ptr=sinks,",
      "            block_tables_ptr=block_table,",
      "            seq_lens_ptr=seqused_k,",
      "            alibi_slopes_ptr=alibi_slopes,",
      "            qq_bias_ptr=qq_bias,",
      "            scale=softmax_scale,",
      "            k_scale=k_descale,",
      "            v_scale=v_descale,",
      "            softcap=softcap,",
      "            num_query_heads=num_query_heads,",
      "            num_queries_per_kv=num_queries_per_kv,",
      "            block_table_stride=block_table.stride(0),",
      "            query_stride_0=q.stride(0),",
      "            query_stride_1=q.stride(1),",
      "            output_stride_0=out.stride(0),",
      "            output_stride_1=out.stride(1),",
      "            qq_bias_stride_0=qq_bias.stride(0) if use_qq_bias else 0,",
      "            BLOCK_SIZE=block_size,",
      "            HEAD_SIZE=head_size,",
      "            HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),",
      "            USE_ALIBI_SLOPES=use_alibi_slopes,",
      "            USE_QQ_BIAS=use_qq_bias,",
      "            USE_SOFTCAP=(softcap > 0),",
      "            USE_SINKS=(sinks is not None),",
      "            SLIDING_WINDOW=(1 + window_size[0]),",
      "            stride_k_cache_0=k.stride(0),",
      "            stride_k_cache_1=k.stride(1),",
      "            stride_k_cache_2=k.stride(2),",
      "            stride_k_cache_3=k.stride(3),",
      "            stride_v_cache_0=v.stride(0),",
      "            stride_v_cache_1=v.stride(1),",
      "            stride_v_cache_2=v.stride(2),",
      "            stride_v_cache_3=v.stride(3),",
      "            query_start_len_ptr=cu_seqlens_q,",
      "            BLOCK_Q=BLOCK_Q,",
      "            num_seqs=num_seqs,",
      "            BLOCK_M=BLOCK_M,",
      "        )",
      "    else:",
      "        # for initial version, NUM_SEGMENTS = 16 is chosen as a default",
      "        # value that showed good performance in tests",
      "        NUM_SEGMENTS = 16",
      "",
      "        segm_output = torch.empty(",
      "            q.shape[0],",
      "            num_query_heads,",
      "            NUM_SEGMENTS,",
      "            triton.next_power_of_2(head_size),",
      "            dtype=torch.float32,",
      "            device=q.device,",
      "        )",
      "        segm_max = torch.empty(",
      "            q.shape[0],",
      "            num_query_heads,",
      "            NUM_SEGMENTS,",
      "            dtype=torch.float32,",
      "            device=q.device,",
      "        )",
      "        segm_expsum = torch.empty(",
      "            q.shape[0],",
      "            num_query_heads,",
      "            NUM_SEGMENTS,",
      "            dtype=torch.float32,",
      "            device=q.device,",
      "        )",
      "",
      "        kernel_unified_attention_3d[(",
      "            total_num_q_blocks, num_kv_heads, NUM_SEGMENTS)](",
      "                segm_output_ptr=segm_output,",
      "                segm_max_ptr=segm_max,",
      "                segm_expsum_ptr=segm_expsum,",
      "                query_ptr=q,",
      "                key_cache_ptr=k,",
      "                value_cache_ptr=v,",
      "                sink_ptr=sinks,",
      "                block_tables_ptr=block_table,",
      "                seq_lens_ptr=seqused_k,",
      "                alibi_slopes_ptr=alibi_slopes,",
      "                qq_bias_ptr=qq_bias,",
      "                scale=softmax_scale,",
      "                k_scale=k_descale,",
      "                v_scale=v_descale,",
      "                softcap=softcap,",
      "                num_query_heads=num_query_heads,",
      "                num_queries_per_kv=num_queries_per_kv,",
      "                block_table_stride=block_table.stride(0),",
      "                query_stride_0=q.stride(0),",
      "                query_stride_1=q.stride(1),",
      "                qq_bias_stride_0=qq_bias.stride(0) if use_qq_bias else 0,",
      "                BLOCK_SIZE=block_size,",
      "                HEAD_SIZE=head_size,",
      "                HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),",
      "                USE_ALIBI_SLOPES=use_alibi_slopes,",
      "                USE_QQ_BIAS=use_qq_bias,",
      "                USE_SOFTCAP=(softcap > 0),",
      "                USE_SINKS=(sinks is not None),",
      "                SLIDING_WINDOW=(1 + window_size[0]),",
      "                stride_k_cache_0=k.stride(0),",
      "                stride_k_cache_1=k.stride(1),",
      "                stride_k_cache_2=k.stride(2),",
      "                stride_k_cache_3=k.stride(3),",
      "                stride_v_cache_0=v.stride(0),",
      "                stride_v_cache_1=v.stride(1),",
      "                stride_v_cache_2=v.stride(2),",
      "                stride_v_cache_3=v.stride(3),",
      "                query_start_len_ptr=cu_seqlens_q,",
      "                BLOCK_Q=BLOCK_Q,",
      "                num_seqs=num_seqs,",
      "                BLOCK_M=BLOCK_M,",
      "                NUM_SEGMENTS_PER_SEQ=NUM_SEGMENTS,",
      "            )",
      "",
      "        reduce_segments[(q.shape[0], num_query_heads)](",
      "            output_ptr=out,",
      "            segm_output_ptr=segm_output,",
      "            segm_max_ptr=segm_max,",
      "            segm_expsum_ptr=segm_expsum,",
      "            seq_lens_ptr=seqused_k,",
      "            num_seqs=num_seqs,",
      "            num_query_heads=num_query_heads,",
      "            output_stride_0=out.stride(0),",
      "            output_stride_1=out.stride(1),",
      "            block_table_stride=block_table.stride(0),",
      "            BLOCK_SIZE=block_size,",
      "            HEAD_SIZE=head_size,",
      "            HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),",
      "            query_start_len_ptr=cu_seqlens_q,",
      "            BLOCK_Q=BLOCK_Q,",
      "            NUM_SEGMENTS_PER_SEQ=NUM_SEGMENTS,",
      "        )"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/attention/ops/triton_flash_attention.py",
    "source": [
      "#!/usr/bin/env python",
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "\"\"\"",
      "Fused Attention",
      "===============",
      "",
      "This is a Triton implementation of the Flash Attention v2 algorithm from Tri Dao",
      "(https://tridao.me/publications/flash2/flash2.pdf)",
      "Credits: OpenAI kernel team, AMD ML Frameworks Triton team",
      "",
      "Features supported:",
      "",
      "1) Fwd with causal masking",
      "2) Any sequence lengths without padding (currently fwd kernel only)",
      "3) Support for different sequence lengths for q and k",
      "4) Nested tensor API currently does not support dropout or bias.",
      "",
      "Not currently supported:",
      "",
      "1) Non power of two head dims",
      "",
      "\"\"\"",
      "",
      "import torch",
      "",
      "from vllm.platforms import current_platform",
      "from vllm.triton_utils import tl, triton",
      "",
      "# Avoid misleading ROCm warning.",
      "if current_platform.is_rocm():",
      "    from vllm.platforms.rocm import on_gfx1x",
      "else:",
      "    on_gfx1x = lambda *args, **kwargs: False",
      "",
      "torch_dtype: tl.constexpr = torch.float16",
      "",
      "",
      "@triton.jit",
      "def cdiv_fn(x, y):",
      "    return (x + y - 1) // y",
      "",
      "",
      "@triton.jit",
      "def max_fn(x, y):",
      "    return tl.math.max(x, y)",
      "",
      "",
      "@triton.jit",
      "def dropout_offsets(philox_seed, philox_offset, dropout_p, m, n, stride):",
      "    ms = tl.arange(0, m)",
      "    ns = tl.arange(0, n)",
      "    return philox_offset + ms[:, None] * stride + ns[None, :]",
      "",
      "",
      "@triton.jit",
      "def dropout_rng(philox_seed, philox_offset, dropout_p, m, n, stride):",
      "    rng_offsets = dropout_offsets(philox_seed, philox_offset, dropout_p, m, n,",
      "                                  stride).to(tl.uint32)",
      "    # TODO: use tl.randint for better performance",
      "    return tl.rand(philox_seed, rng_offsets)",
      "",
      "",
      "@triton.jit",
      "def dropout_mask(philox_seed, philox_offset, dropout_p, m, n, stride):",
      "    rng_output = dropout_rng(philox_seed, philox_offset, dropout_p, m, n,",
      "                             stride)",
      "    rng_keep = rng_output > dropout_p",
      "    return rng_keep",
      "",
      "",
      "@triton.jit",
      "def load_fn(block_ptr, first, second, pad):",
      "    if first and second:",
      "        tensor = tl.load(block_ptr, boundary_check=(0, 1), padding_option=pad)",
      "    elif first:",
      "        tensor = tl.load(block_ptr, boundary_check=(0, ), padding_option=pad)",
      "    elif second:",
      "        tensor = tl.load(block_ptr, boundary_check=(1, ), padding_option=pad)",
      "    else:",
      "        tensor = tl.load(block_ptr)",
      "    return tensor",
      "",
      "",
      "@triton.jit",
      "def _attn_fwd_inner(",
      "    acc,",
      "    l_i,",
      "    m_i,",
      "    q,",
      "    K_block_ptr,",
      "    V_block_ptr,",
      "    start_m,",
      "    actual_seqlen_k,",
      "    dropout_p,",
      "    philox_seed,",
      "    batch_philox_offset,",
      "    encoded_softmax_block_ptr,",
      "    block_min,",
      "    block_max,",
      "    offs_n_causal,",
      "    masked_blocks,",
      "    n_extra_tokens,",
      "    bias_ptr,",
      "    IS_CAUSAL: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    OFFS_M: tl.constexpr,",
      "    OFFS_N: tl.constexpr,",
      "    PRE_LOAD_V: tl.constexpr,",
      "    MASK_STEPS: tl.constexpr,",
      "    ENABLE_DROPOUT: tl.constexpr,",
      "    RETURN_ENCODED_SOFTMAX: tl.constexpr,",
      "    PADDED_HEAD: tl.constexpr,",
      "    USE_FP8: tl.constexpr,",
      "    qk_scale,",
      "    p_descale,",
      "):",
      "    # loop over k, v, and update accumulator",
      "    for start_n in range(block_min, block_max, BLOCK_N):",
      "        # For padded blocks, we will overrun the tensor size if",
      "        # we load all BLOCK_N. For others, the blocks are all within range.",
      "        k = load_fn(",
      "            K_block_ptr,",
      "            PADDED_HEAD,",
      "            MASK_STEPS and (n_extra_tokens != 0),",
      "            \"zero\",",
      "        )",
      "        if PRE_LOAD_V:",
      "            v = load_fn(",
      "                V_block_ptr,",
      "                MASK_STEPS and (n_extra_tokens != 0),",
      "                PADDED_HEAD,",
      "                \"zero\",",
      "            )",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        # We start from end of seqlen_k so only the first iteration would need",
      "        # to be checked for padding if it is not a multiple of block_n",
      "        # TODO: This can be optimized to only be true for the padded block.",
      "        if MASK_STEPS:  # noqa: SIM102",
      "            # If this is the last block / iteration, we want to",
      "            # mask if the sequence length is not a multiple of block size",
      "            # a solution is to always do BLOCK_M // BLOCK_N + 1 steps",
      "            # if not is_modulo_mn. last step might get wasted but that is okay.",
      "            # check if this masking works for that case.",
      "            if (start_n + BLOCK_N == block_max) and (n_extra_tokens != 0):",
      "                boundary_m = tl.full([BLOCK_M],",
      "                                     actual_seqlen_k,",
      "                                     dtype=tl.int32)",
      "                size_n = start_n + OFFS_N[None, :]",
      "                mask = size_n < boundary_m[:, None]",
      "                qk = tl.where(mask, qk, float(\"-inf\"))",
      "        if IS_CAUSAL:",
      "            causal_boundary = start_n + offs_n_causal",
      "            causal_mask = OFFS_M[:, None] >= causal_boundary[None, :]",
      "            qk = tl.where(causal_mask, qk, float(\"-inf\"))",
      "        # -- compute qk ----",
      "        qk += tl.dot(q, k)",
      "        if USE_FP8:",
      "            qk *= qk_scale",
      "        if bias_ptr is not None:",
      "            bias = load_fn(bias_ptr, False, MASK_STEPS",
      "                           and (n_extra_tokens != 0), \"zero\")",
      "            # While bias is added after multiplying qk with sm_scale, our",
      "            # optimization to use 2^x instead of e^x results in an additional",
      "            # scale factor of log2(e) which we must also multiply the bias with.",
      "            qk += bias * 1.44269504089",
      "        m_ij = tl.maximum(m_i, tl.max(qk, 1))",
      "        qk = qk - m_ij[:, None]",
      "        p = tl.math.exp2(qk)",
      "",
      "        # CAVEAT: Must update l_ij before applying dropout",
      "        l_ij = tl.sum(p, 1)",
      "        if ENABLE_DROPOUT:",
      "            philox_offset = (batch_philox_offset +",
      "                             start_m * BLOCK_M * actual_seqlen_k + start_n -",
      "                             BLOCK_N)",
      "            keep = dropout_mask(",
      "                philox_seed,",
      "                philox_offset,",
      "                dropout_p,",
      "                BLOCK_M,",
      "                BLOCK_N,",
      "                actual_seqlen_k,",
      "            )",
      "            if RETURN_ENCODED_SOFTMAX:",
      "                tl.store(",
      "                    encoded_softmax_block_ptr,",
      "                    tl.where(keep, p,",
      "                             -p).to(encoded_softmax_block_ptr.type.element_ty),",
      "                )",
      "            p = tl.where(keep, p, 0.0)",
      "        elif RETURN_ENCODED_SOFTMAX:",
      "            tl.store(",
      "                encoded_softmax_block_ptr,",
      "                p.to(encoded_softmax_block_ptr.type.element_ty),",
      "            )",
      "        # -- update output accumulator --",
      "        alpha = tl.math.exp2(m_i - m_ij)",
      "        acc = acc * alpha[:, None]",
      "        if not PRE_LOAD_V:",
      "            v = load_fn(",
      "                V_block_ptr,",
      "                MASK_STEPS and (n_extra_tokens != 0),",
      "                PADDED_HEAD,",
      "                \"zero\",",
      "            )",
      "        # -- update m_i and l_i",
      "        l_i = l_i * alpha + l_ij",
      "        # update m_i and l_i",
      "        m_i = m_ij",
      "",
      "        if USE_FP8:",
      "            p *= p_descale",
      "",
      "        acc += tl.dot(p.to(V_block_ptr.type.element_ty), v)",
      "",
      "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))",
      "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))",
      "        if bias_ptr is not None:",
      "            bias_ptr = tl.advance(bias_ptr, (0, BLOCK_N))",
      "        if RETURN_ENCODED_SOFTMAX:",
      "            encoded_softmax_block_ptr = tl.advance(encoded_softmax_block_ptr,",
      "                                                   (0, BLOCK_N))",
      "    return acc, l_i, m_i",
      "",
      "",
      "def get_cdna_autotune_configs():",
      "    return [",
      "        triton.Config(",
      "            {",
      "                'BLOCK_M': 256,",
      "                'BLOCK_N': 64,",
      "                'waves_per_eu': 2,",
      "                'PRE_LOAD_V': False",
      "            },",
      "            num_stages=1,",
      "            num_warps=8),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_M': 128,",
      "                'BLOCK_N': 128,",
      "                'waves_per_eu': 2,",
      "                'PRE_LOAD_V': False",
      "            },",
      "            num_stages=1,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_M': 256,",
      "                'BLOCK_N': 128,",
      "                'waves_per_eu': 2,",
      "                'PRE_LOAD_V': False",
      "            },",
      "            num_stages=1,",
      "            num_warps=8),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_M': 128,",
      "                'BLOCK_N': 64,",
      "                'waves_per_eu': 1,",
      "                'PRE_LOAD_V': False",
      "            },",
      "            num_stages=1,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_M': 128,",
      "                'BLOCK_N': 64,",
      "                'waves_per_eu': 3,",
      "                'PRE_LOAD_V': True",
      "            },",
      "            num_stages=1,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_M': 128,",
      "                'BLOCK_N': 64,",
      "                'waves_per_eu': 3,",
      "                'PRE_LOAD_V': False",
      "            },",
      "            num_stages=1,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_M': 64,",
      "                'BLOCK_N': 64,",
      "                'waves_per_eu': 4,",
      "                'PRE_LOAD_V': False",
      "            },",
      "            num_stages=1,",
      "            num_warps=8),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_M': 32,",
      "                'BLOCK_N': 32,",
      "                'waves_per_eu': 4,",
      "                'PRE_LOAD_V': False",
      "            },",
      "            num_stages=1,",
      "            num_warps=8),",
      "        # TODO: This config fails with head_size not pow2 with data mismatches.",
      "        #    triton.Config({'BLOCK_M': 32, 'BLOCK_N': 16, 'waves_per_eu': 1,",
      "        #                   'PRE_LOAD_V': False}, num_stages=1, num_warps=4),",
      "",
      "        # Fails in AccelerateAMDMatmul (Triton) assert when using FP8:",
      "        # triton.Config(",
      "        #     {",
      "        #         \"BLOCK_M\": 16,",
      "        #         \"BLOCK_N\": 16,",
      "        #         \"waves_per_eu\": 1,",
      "        #         \"PRE_LOAD_V\": False,",
      "        #     },",
      "        #     num_stages=1,",
      "        #     num_warps=4,",
      "        # ),",
      "    ], ['IS_CAUSAL', 'dropout_p', 'BLOCK_DMODEL', 'USE_FP8']",
      "",
      "",
      "def get_rdna_autotune_configs():",
      "    return [",
      "        triton.Config(",
      "            {",
      "                'BLOCK_M': 32,",
      "                'BLOCK_N': 32,",
      "                'waves_per_eu': 4,",
      "                'PRE_LOAD_V': False",
      "            },",
      "            num_stages=1,",
      "            num_warps=2),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_M': 32,",
      "                'BLOCK_N': 32,",
      "                'waves_per_eu': 2,",
      "                'PRE_LOAD_V': False",
      "            },",
      "            num_stages=1,",
      "            num_warps=2),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_M': 32,",
      "                'BLOCK_N': 16,",
      "                'waves_per_eu': 4,",
      "                'PRE_LOAD_V': False",
      "            },",
      "            num_stages=1,",
      "            num_warps=2),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_M': 32,",
      "                'BLOCK_N': 16,",
      "                'waves_per_eu': 2,",
      "                'PRE_LOAD_V': False",
      "            },",
      "            num_stages=1,",
      "            num_warps=2),",
      "        # Fails in AccelerateAMDMatmul (Triton) assert when using FP8:",
      "        # triton.Config(",
      "        #     {",
      "        #         'BLOCK_M': 16,",
      "        #         'BLOCK_N': 16,",
      "        #         'waves_per_eu': 4,",
      "        #         'PRE_LOAD_V': False",
      "        #     },",
      "        #     num_stages=1,",
      "        #     num_warps=2),",
      "        # triton.Config(",
      "        #     {",
      "        #         'BLOCK_M': 16,",
      "        #         'BLOCK_N': 16,",
      "        #         'waves_per_eu': 2,",
      "        #         'PRE_LOAD_V': False",
      "        #     },",
      "        #     num_stages=1,",
      "        #     num_warps=2),",
      "        # # Fall-back config.",
      "        # triton.Config(",
      "        #     {",
      "        #         'BLOCK_M': 16,",
      "        #         'BLOCK_N': 16,",
      "        #         'waves_per_eu': 1,",
      "        #         'PRE_LOAD_V': False",
      "        #     },",
      "        #     num_stages=1,",
      "        #     num_warps=2),",
      "    ], ['IS_CAUSAL', 'dropout_p', 'BLOCK_DMODEL', 'USE_FP8']",
      "",
      "",
      "def get_autotune_configs():",
      "    if on_gfx1x():",
      "        return get_rdna_autotune_configs()",
      "    else:",
      "        return get_cdna_autotune_configs()",
      "",
      "",
      "autotune_configs, autotune_keys = get_autotune_configs()",
      "",
      "float8_info = torch.finfo(current_platform.fp8_dtype())",
      "",
      "",
      "@triton.autotune(",
      "    configs=autotune_configs,",
      "    key=autotune_keys,",
      ")",
      "@triton.jit",
      "def attn_fwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    bias,",
      "    sm_scale,",
      "    q_scale,",
      "    k_scale,",
      "    v_scale,",
      "    p_scale,",
      "    p_descale,",
      "    o_descale,",
      "    L,",
      "    Out,",
      "    stride_qz: tl.int64,",
      "    stride_qh: tl.int64,",
      "    stride_qm: tl.int64,",
      "    stride_qk: tl.int64,",
      "    stride_kz: tl.int64,",
      "    stride_kh: tl.int64,",
      "    stride_kn: tl.int64,",
      "    stride_kk: tl.int64,",
      "    stride_vz: tl.int64,",
      "    stride_vh: tl.int64,",
      "    stride_vk: tl.int64,",
      "    stride_vn: tl.int64,",
      "    stride_oz: tl.int64,",
      "    stride_oh: tl.int64,",
      "    stride_om: tl.int64,",
      "    stride_on: tl.int64,",
      "    stride_bz: tl.int64,",
      "    stride_bh: tl.int64,",
      "    stride_bm: tl.int64,",
      "    stride_bn: tl.int64,",
      "    cu_seqlens_q,",
      "    cu_seqlens_k,",
      "    dropout_p,",
      "    philox_seed,",
      "    philox_offset_base,",
      "    encoded_softmax,",
      "    HQ: tl.constexpr,",
      "    HK: tl.constexpr,",
      "    ACTUAL_BLOCK_DMODEL: tl.constexpr,",
      "    MAX_SEQLENS_Q: tl.constexpr,",
      "    MAX_SEQLENS_K: tl.constexpr,",
      "    VARLEN: tl.constexpr,",
      "    IS_CAUSAL: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    USE_FP8: tl.constexpr,",
      "    USE_FP8_OUT: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    PRE_LOAD_V: tl.constexpr,",
      "    BIAS_TYPE: tl.constexpr,",
      "    ENABLE_DROPOUT: tl.constexpr,",
      "    RETURN_ENCODED_SOFTMAX: tl.constexpr,",
      "    FP8_MIN: tl.constexpr = float8_info.min,",
      "    FP8_MAX: tl.constexpr = float8_info.max,",
      "):",
      "    start_m = tl.program_id(0)",
      "    off_h_q = tl.program_id(1)",
      "    off_z = tl.program_id(2)",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "    if VARLEN:",
      "        cu_seqlens_q_start = tl.load(cu_seqlens_q + off_z)",
      "        cu_seqlens_q_end = tl.load(cu_seqlens_q + off_z + 1)",
      "        seqlen_q = cu_seqlens_q_end - cu_seqlens_q_start",
      "        # We have a one-size-fits-all grid in id(0). Some seqlens might be too",
      "        # small for all start_m so for those we return early.",
      "        if start_m * BLOCK_M > seqlen_q:",
      "            return",
      "        cu_seqlens_k_start = tl.load(cu_seqlens_k + off_z)",
      "        cu_seqlens_k_end = tl.load(cu_seqlens_k + off_z + 1)",
      "        seqlen_k = cu_seqlens_k_end - cu_seqlens_k_start",
      "    else:",
      "        cu_seqlens_q_start = 0",
      "        cu_seqlens_k_start = 0",
      "        seqlen_q = MAX_SEQLENS_Q",
      "        seqlen_k = MAX_SEQLENS_K",
      "",
      "    # Now we compute whether we need to exit early due to causal masking.",
      "    # This is because for seqlen_q > seqlen_k, M rows of the attn scores",
      "    # are completely masked, resulting in 0s written to the output, and",
      "    # inf written to LSE. We don't need to do any GEMMs in this case.",
      "    # This block of code determines what N is, and if this WG is operating",
      "    # on those M rows.",
      "    n_blocks = cdiv_fn(seqlen_k, BLOCK_N)",
      "    if IS_CAUSAL:",
      "        # If seqlen_q == seqlen_k, the attn scores are a square matrix.",
      "        # If seqlen_q != seqlen_k, attn scores are rectangular which means",
      "        # the causal mask boundary is bottom right aligned, and ends at either",
      "        # the top edge (seqlen_q < seqlen_k) or left edge.",
      "        # This captures the decrease in n_blocks if we have a rectangular attn",
      "        # matrix",
      "        n_blocks_seqlen = cdiv_fn(",
      "            (start_m + 1) * BLOCK_M + seqlen_k - seqlen_q, BLOCK_N)",
      "        # This is what adjusts the block_max for the current WG, only",
      "        # if IS_CAUSAL. Otherwise we want to always iterate through all n_blocks",
      "        n_blocks = min(n_blocks, n_blocks_seqlen)",
      "        # If we have no blocks after adjusting for seqlen deltas, this WG is",
      "        # part of the blocks that are all 0. We exit early.",
      "        if n_blocks <= 0:",
      "            o_offset = (off_z * stride_oz + cu_seqlens_q_start * stride_om +",
      "                        off_h_q * stride_oh)",
      "            O_block_ptr = tl.make_block_ptr(",
      "                base=Out + o_offset,",
      "                shape=(seqlen_q, BLOCK_DMODEL),",
      "                strides=(stride_om, stride_on),",
      "                offsets=(start_m * BLOCK_M, 0),",
      "                block_shape=(BLOCK_M, BLOCK_DMODEL),",
      "                order=(1, 0),",
      "            )",
      "            acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=Out.type.element_ty)",
      "            # We still need to write 0s to the result",
      "            # tl.store(O_block_ptr,",
      "            # acc.to(Out.type.element_ty), boundary_check=(0,1))",
      "            # l_ptrs = L + off_z * HQ * MAX_SEQLENS_Q + off_h_q * MAX_SEQLENS_Q",
      "            #          + offs_m",
      "            # We store inf to LSE, not -inf because in the bwd pass,",
      "            # we subtract this",
      "            # from qk which makes it -inf, such that exp(qk - inf) = 0",
      "            # for these masked blocks.",
      "            # l = tl.full([BLOCK_M], value=float(\"inf\"), dtype=tl.float32)",
      "            # tl.store(l_ptrs, l)",
      "            # TODO: Should dropout and return encoded softmax be handled here?",
      "            return",
      "",
      "    # If MQA / GQA, set the K and V head offsets appropriately.",
      "    GROUP_SIZE: tl.constexpr = HQ // HK",
      "    off_h_k = off_h_q // GROUP_SIZE if GROUP_SIZE != 1 else off_h_q",
      "",
      "    n_extra_tokens = 0",
      "    if seqlen_k < BLOCK_N:",
      "        n_extra_tokens = BLOCK_N - seqlen_k",
      "    elif seqlen_k % BLOCK_N:",
      "        n_extra_tokens = seqlen_k % BLOCK_N",
      "    padded_head = ACTUAL_BLOCK_DMODEL != BLOCK_DMODEL",
      "",
      "    # Compute pointers for all the tensors used in this kernel.",
      "    q_offset = (off_z * stride_qz + off_h_q * stride_qh +",
      "                cu_seqlens_q_start * stride_qm)",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + q_offset,",
      "        shape=(seqlen_q, ACTUAL_BLOCK_DMODEL),",
      "        strides=(stride_qm, stride_qk),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "    k_offset = (off_z * stride_kz + off_h_k * stride_kh +",
      "                cu_seqlens_k_start * stride_kn)",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + k_offset,",
      "        shape=(ACTUAL_BLOCK_DMODEL, seqlen_k),",
      "        strides=(stride_kk, stride_kn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_DMODEL, BLOCK_N),",
      "        order=(0, 1),",
      "    )",
      "    v_offset = (off_z * stride_vz + off_h_k * stride_vh +",
      "                cu_seqlens_k_start * stride_vk)",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + v_offset,",
      "        shape=(seqlen_k, ACTUAL_BLOCK_DMODEL),",
      "        strides=(stride_vk, stride_vn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_N, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "    if BIAS_TYPE != 0:",
      "        bias_ptr = tl.make_block_ptr(",
      "            base=bias + off_h_q * stride_bh,",
      "            shape=(seqlen_q, seqlen_k),",
      "            strides=(stride_bm, stride_bn),",
      "            offsets=(start_m * BLOCK_M, 0),",
      "            block_shape=(BLOCK_M, BLOCK_N),",
      "            order=(1, 0),",
      "        )",
      "    else:",
      "        bias_ptr = None",
      "    if ENABLE_DROPOUT:",
      "        batch_philox_offset = philox_offset_base \\",
      "                              + (off_z * HQ + off_h_q) \\",
      "                              * seqlen_q * seqlen_k",
      "    else:",
      "        batch_philox_offset = 0",
      "    # We can ask to return the dropout mask without actually doing any dropout.",
      "    # In this case, we return an invalid pointer so indicate the mask is not i",
      "    # valid.",
      "    # TODO: Fix encoded softmax. It currently uses just h_q in the base offset.",
      "    if RETURN_ENCODED_SOFTMAX:",
      "        encoded_softmax_block_ptr = tl.make_block_ptr(",
      "            base=encoded_softmax + off_h_q * seqlen_q * seqlen_k,",
      "            shape=(seqlen_q, seqlen_k),",
      "            strides=(seqlen_k, 1),",
      "            offsets=(start_m * BLOCK_M, 0),",
      "            block_shape=(BLOCK_M, BLOCK_N),",
      "            order=(1, 0),",
      "        )",
      "    else:",
      "        encoded_softmax_block_ptr = 0",
      "    # initialize pointer to m and l",
      "    m_i = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)",
      "    l_i = tl.full([BLOCK_M], 1.0, dtype=tl.float32)",
      "    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)",
      "    # scale sm_scale by log_2(e) and use 2^x in the loop as we do not",
      "    # have native e^x support in HW.",
      "    qk_scale = sm_scale * 1.44269504089",
      "    # Q is loaded once at the beginning and shared by all N blocks.",
      "    q = load_fn(Q_block_ptr, True, padded_head, \"zero\")",
      "    if not USE_FP8:",
      "        q = (q * qk_scale).to(Q_block_ptr.type.element_ty)",
      "        acc_scale = 1.0",
      "    else:",
      "        qk_scale *= q_scale * k_scale",
      "        acc_scale = p_scale * v_scale",
      "",
      "    # Here we compute how many full and masked blocks we have.",
      "    padded_block_k = n_extra_tokens != 0",
      "    is_modulo_mn = not padded_block_k and (seqlen_q % BLOCK_M == 0)",
      "    if IS_CAUSAL:",
      "        # There are always at least BLOCK_M // BLOCK_N masked blocks.",
      "        # Additionally there might be one more due to dissimilar seqlens.",
      "        masked_blocks = BLOCK_M // BLOCK_N + (not is_modulo_mn)",
      "    else:",
      "        # Padding on Q does not need to be masked in the FA loop.",
      "        masked_blocks = padded_block_k",
      "    # if IS_CAUSAL, not is_modulo_mn does not always result in an additional",
      "    # block. In this case we might exceed n_blocks so pick the min.",
      "    masked_blocks = min(masked_blocks, n_blocks)",
      "    n_full_blocks = n_blocks - masked_blocks",
      "    block_min = 0",
      "    block_max = n_blocks * BLOCK_N",
      "    # Compute for full blocks. Here we set causal to false regardless of its",
      "    # value because there is no masking. Similarly we do not need padding.",
      "    if n_full_blocks > 0:",
      "        block_max = (n_blocks - masked_blocks) * BLOCK_N",
      "        acc, l_i, m_i = _attn_fwd_inner(",
      "            acc,",
      "            l_i,",
      "            m_i,",
      "            q,",
      "            K_block_ptr,",
      "            V_block_ptr,",
      "            start_m,",
      "            seqlen_k,",
      "            dropout_p,",
      "            philox_seed,",
      "            batch_philox_offset,",
      "            encoded_softmax_block_ptr,",
      "            # _, _, offs_n_causal, masked_blocks, n_extra_tokens, _",
      "            block_min,",
      "            block_max,",
      "            0,",
      "            0,",
      "            0,",
      "            bias_ptr,",
      "            # IS_CAUSAL, ....",
      "            False,",
      "            BLOCK_M,",
      "            BLOCK_DMODEL,",
      "            BLOCK_N,",
      "            offs_m,",
      "            offs_n,",
      "            # _, MASK_STEPS, ...",
      "            PRE_LOAD_V,",
      "            False,",
      "            ENABLE_DROPOUT,",
      "            RETURN_ENCODED_SOFTMAX,",
      "            padded_head,",
      "            USE_FP8,",
      "            qk_scale,",
      "            p_descale,",
      "        )",
      "        block_min = block_max",
      "        block_max = n_blocks * BLOCK_N",
      "",
      "    tl.debug_barrier()",
      "    # Remaining blocks, if any, are full / not masked.",
      "    if masked_blocks > 0:",
      "        offs_n_causal = offs_n + (seqlen_q - seqlen_k) if IS_CAUSAL else 0",
      "        K_block_ptr = tl.advance(K_block_ptr, (0, n_full_blocks * BLOCK_N))",
      "        V_block_ptr = tl.advance(V_block_ptr, (n_full_blocks * BLOCK_N, 0))",
      "        if bias_ptr is not None:",
      "            bias_ptr = tl.advance(bias_ptr, (0, n_full_blocks * BLOCK_N))",
      "        if RETURN_ENCODED_SOFTMAX:",
      "            encoded_softmax_block_ptr = tl.advance(encoded_softmax_block_ptr,",
      "                                                   (0, n_full_blocks))",
      "        acc, l_i, m_i = _attn_fwd_inner(",
      "            acc,",
      "            l_i,",
      "            m_i,",
      "            q,",
      "            K_block_ptr,",
      "            V_block_ptr,",
      "            start_m,",
      "            seqlen_k,",
      "            dropout_p,",
      "            philox_seed,",
      "            batch_philox_offset,",
      "            encoded_softmax_block_ptr,",
      "            block_min,",
      "            block_max,",
      "            offs_n_causal,",
      "            masked_blocks,",
      "            n_extra_tokens,",
      "            bias_ptr,",
      "            IS_CAUSAL,",
      "            BLOCK_M,",
      "            BLOCK_DMODEL,",
      "            BLOCK_N,",
      "            offs_m,",
      "            offs_n,",
      "            # _, MASK_STEPS, ...",
      "            PRE_LOAD_V,",
      "            True,",
      "            ENABLE_DROPOUT,",
      "            RETURN_ENCODED_SOFTMAX,",
      "            padded_head,",
      "            USE_FP8,",
      "            qk_scale,",
      "            p_descale,",
      "        )",
      "    # epilogue",
      "",
      "    if USE_FP8:",
      "        acc *= acc_scale",
      "    acc = acc / l_i[:, None]",
      "    if ENABLE_DROPOUT:",
      "        acc = acc / (1 - dropout_p)",
      "    # If seqlen_q > seqlen_k but the delta is not a multiple of BLOCK_M,",
      "    # then we have one block with a row of all NaNs which come from computing",
      "    # softmax over a row of all -infs (-inf - inf = NaN). We check for that here",
      "    # and store 0s where there are NaNs as these rows should've been zeroed out.",
      "    end_m_idx = (start_m + 1) * BLOCK_M",
      "    start_m_idx = start_m * BLOCK_M",
      "    causal_start_idx = seqlen_q - seqlen_k",
      "    if USE_FP8_OUT:",
      "        acc *= o_descale",
      "        acc = tl.clamp(acc, FP8_MIN, FP8_MAX)",
      "    acc = acc.to(Out.type.element_ty)",
      "    if IS_CAUSAL:  # noqa: SIM102",
      "        if causal_start_idx > start_m_idx and causal_start_idx < end_m_idx:",
      "            out_mask_boundary = tl.full((BLOCK_DMODEL, ),",
      "                                        causal_start_idx,",
      "                                        dtype=tl.int32)",
      "            mask_m_offsets = start_m_idx + tl.arange(0, BLOCK_M)",
      "            out_ptrs_mask = (mask_m_offsets[:, None]",
      "                             >= out_mask_boundary[None, :])",
      "            z = tl.zeros((1, ), tl.float32)",
      "            acc = tl.where(out_ptrs_mask, acc, z.to(acc.type.element_ty))",
      "    # write back LSE",
      "    # l_ptrs = L + off_z * HQ * MAX_SEQLENS_Q + off_h_q * MAX_SEQLENS_Q + offs_m",
      "    # If seqlen_q not multiple of BLOCK_M, we need to mask out the last",
      "    # few rows. This is only true for the last M block. For others,",
      "    # overflow_size will be -ve",
      "    # overflow_size = end_m_idx - seqlen_q",
      "    # if overflow_size > 0:",
      "    #    boundary = tl.full((BLOCK_M,), BLOCK_M - overflow_size, dtype=tl.int32)",
      "    #    # This is a > check because mask being 0 blocks the store.",
      "    #    l_ptrs_mask = boundary > tl.arange(0, BLOCK_M)",
      "    #    tl.store(l_ptrs, m_i + tl.math.log2(l_i), mask=l_ptrs_mask)",
      "    # else:",
      "    #    tl.store(l_ptrs, m_i + tl.math.log2(l_i))",
      "",
      "    # write back O",
      "    o_offset = (off_z * stride_oz + cu_seqlens_q_start * stride_om +",
      "                off_h_q * stride_oh)",
      "    O_block_ptr = tl.make_block_ptr(",
      "        base=Out + o_offset,",
      "        shape=(seqlen_q, ACTUAL_BLOCK_DMODEL),",
      "        strides=(stride_om, stride_on),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "    # Need boundary check on this to make sure the padding from the",
      "    # Q and KV tensors in both dims are not part of what we store back.",
      "    # TODO: Do the boundary check optionally.",
      "    tl.store(O_block_ptr, acc, boundary_check=(0, 1))",
      "",
      "",
      "def check_args(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    varlen=True,",
      "    max_seqlens=None,",
      "    cu_seqlens_q=None,",
      "    cu_seqlens_k=None,",
      "):",
      "    assert q.dim() == k.dim() and q.dim() == v.dim()",
      "    if varlen:",
      "        assert q.dim() == 3",
      "        total_q, nheads_q, head_size = q.shape",
      "        total_k, nheads_k, _ = k.shape",
      "        assert cu_seqlens_q is not None",
      "        assert cu_seqlens_k is not None",
      "        assert len(cu_seqlens_q) == len(cu_seqlens_k)",
      "    else:",
      "        assert q.dim() == 4",
      "        batch, nheads_q, seqlen_q, head_size = q.shape",
      "        _, nheads_k, seqlen_k, _ = k.shape",
      "        assert max_seqlens > 0",
      "    assert k.shape == v.shape",
      "    assert q.shape[-1] == k.shape[-1] and q.shape[-1] == v.shape[-1]",
      "    # TODO: Change assert if we support qkl f8 and v f16",
      "    assert q.dtype == k.dtype and q.dtype == v.dtype",
      "    assert head_size <= 256",
      "    assert o.shape == q.shape",
      "    assert (nheads_q % nheads_k) == 0",
      "",
      "",
      "class _attention(torch.autograd.Function):",
      "",
      "    @staticmethod",
      "    def forward(",
      "        ctx,",
      "        q,",
      "        k,",
      "        v,",
      "        o,",
      "        cu_seqlens_q,",
      "        cu_seqlens_k,",
      "        max_seqlens_q,",
      "        max_seqlens_k,",
      "        causal=False,",
      "        sm_scale=1.0,",
      "        bias=None,",
      "        fp8_scales=None,",
      "        fp8_out_scale=None,",
      "    ):",
      "        if fp8_scales is not None:",
      "            use_fp8 = True",
      "            (q_scale, k_scale, v_scale, p_scale) = fp8_scales",
      "            float8 = current_platform.fp8_dtype()",
      "",
      "            def check_and_convert(t, scale):",
      "                if t.dtype != float8:",
      "                    descale = 1.0 / scale",
      "                    ts = (t * descale).clamp(min=float8_info.min,",
      "                                             max=float8_info.max)",
      "                    return ts.to(float8)",
      "                else:",
      "                    return t",
      "",
      "            q = check_and_convert(q, q_scale)",
      "            k = check_and_convert(k, k_scale)",
      "            v = check_and_convert(v, v_scale)",
      "        else:",
      "            use_fp8 = False",
      "            q_scale = k_scale = v_scale = p_scale = 1.0",
      "",
      "        if o is None:",
      "            o = torch.empty_like(q, dtype=v.dtype)",
      "",
      "        check_args(",
      "            q,",
      "            k,",
      "            v,",
      "            o,",
      "            varlen=True,",
      "            cu_seqlens_q=cu_seqlens_q,",
      "            cu_seqlens_k=cu_seqlens_k,",
      "        )",
      "        if True:  # varlen",
      "            total_q, nheads_q, head_size = q.shape",
      "            total_k, nheads_k, _ = k.shape",
      "            batch = len(cu_seqlens_q) - 1",
      "            q_strides = (0, q.stride(1), q.stride(0), q.stride(2))",
      "            k_strides = (0, k.stride(1), k.stride(0), k.stride(2))",
      "            v_strides = (0, v.stride(1), v.stride(0), v.stride(2))",
      "            o_strides = (0, o.stride(1), o.stride(0), o.stride(2))",
      "        else:",
      "            batch, seqlen_q, nheads_q, head_size = q.shape",
      "            _, seqlen_k, nheads_k, _ = k.shape",
      "            q_strides = (q.stride(0), q.stride(2), q.stride(1), q.stride(3))",
      "            k_strides = (k.stride(0), k.stride(2), k.stride(1), k.stride(3))",
      "            v_strides = (v.stride(0), v.stride(2), v.stride(1), v.stride(3))",
      "            o_strides = (o.stride(0), o.stride(2), o.stride(1), o.stride(3))",
      "",
      "        # Get closest power of 2 over or equal to 32.",
      "        unpadded_head_dims = {32, 64, 128, 256}",
      "        if head_size not in unpadded_head_dims:",
      "            padded_d_model = None",
      "            for i in unpadded_head_dims:",
      "                if i > head_size:",
      "                    padded_d_model = i",
      "                    break",
      "            assert padded_d_model is not None",
      "        else:",
      "            padded_d_model = head_size",
      "",
      "        grid = lambda META: (",
      "            triton.cdiv(max_seqlens_q, META[\"BLOCK_M\"]),",
      "            nheads_q,",
      "            batch,",
      "        )",
      "",
      "        encoded_softmax = None",
      "",
      "        # Seed the RNG so we get reproducible results for testing.",
      "        philox_seed = 0x1BF52",
      "        philox_offset = 0x1D4B42",
      "",
      "        if bias is not None:",
      "            bias_strides = (",
      "                bias.stride(0),",
      "                bias.stride(1),",
      "                bias.stride(2),",
      "                bias.stride(3),",
      "            )",
      "        else:",
      "            bias_strides = (0, 0, 0, 0)",
      "",
      "        p_descale = 1.0 / p_scale",
      "        o_descale = 1.0 / fp8_out_scale.item(",
      "        ) if fp8_out_scale is not None else 1.0",
      "",
      "        arg_max_seqlens_q = 0 if on_gfx1x() else max_seqlens_q",
      "        arg_max_seqlens_k = 0 if on_gfx1x() else max_seqlens_k",
      "",
      "        attn_fwd[grid](",
      "            q,",
      "            k,",
      "            v,",
      "            bias,",
      "            sm_scale,",
      "            q_scale,",
      "            k_scale,",
      "            v_scale,",
      "            p_scale,",
      "            p_descale,",
      "            o_descale,",
      "            None,",
      "            o,",
      "            *q_strides,",
      "            *k_strides,",
      "            *v_strides,",
      "            *o_strides,",
      "            *bias_strides,",
      "            cu_seqlens_q,",
      "            cu_seqlens_k,",
      "            dropout_p=0.0,",
      "            philox_seed=philox_seed,",
      "            philox_offset_base=philox_offset,",
      "            encoded_softmax=encoded_softmax,",
      "            HQ=nheads_q,",
      "            HK=nheads_k,",
      "            ACTUAL_BLOCK_DMODEL=head_size,",
      "            MAX_SEQLENS_Q=arg_max_seqlens_q,",
      "            MAX_SEQLENS_K=arg_max_seqlens_k,",
      "            IS_CAUSAL=causal,",
      "            VARLEN=True,",
      "            BLOCK_DMODEL=padded_d_model,",
      "            BIAS_TYPE=0 if bias is None else 1,",
      "            ENABLE_DROPOUT=False,",
      "            RETURN_ENCODED_SOFTMAX=False,",
      "            USE_FP8=use_fp8,",
      "            USE_FP8_OUT=fp8_out_scale is not None,",
      "        )",
      "",
      "        ctx.grid = grid",
      "        ctx.sm_scale = sm_scale",
      "        ctx.BLOCK_DMODEL = head_size",
      "        ctx.causal = causal",
      "        ctx.dropout_p = 0.0",
      "        ctx.philox_seed = philox_seed",
      "        ctx.philox_offset = philox_offset",
      "        ctx.encoded_softmax = encoded_softmax",
      "        ctx.return_encoded_softmax = False",
      "        return o, encoded_softmax",
      "",
      "",
      "triton_attention = _attention.apply"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/attention/ops/triton_decode_attention.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "# Adapted from",
      "# https://github.com/sgl-project/sglang/blob/9f635ea50de920aa507f486daafba26a5b837574/python/sglang/srt/layers/attention/triton_ops/decode_attention.py",
      "# which was originally adapted from",
      "# https://github.com/ModelTC/lightllm/blob/96353e868a840db4d103138caf15ed9dbea8c186/lightllm/models/deepseek2/triton_kernel/gqa_flash_decoding_stage1.py",
      "# https://github.com/ModelTC/lightllm/blob/96353e868a840db4d103138caf15ed9dbea8c186/lightllm/models/deepseek2/triton_kernel/gqa_flash_decoding_stage2.py",
      "",
      "# Changes:",
      "# - Add support for page size >= 1.",
      "",
      "# Copyright 2025 vLLM Team",
      "# Copyright 2023-2024 SGLang Team",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");",
      "# you may not use this file except in compliance with the License.",
      "# You may obtain a copy of the License at",
      "#",
      "#     http://www.apache.org/licenses/LICENSE-2.0",
      "#",
      "# Unless required by applicable law or agreed to in writing, software",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
      "# See the License for the specific language governing permissions and",
      "# limitations under the License.",
      "# ==============================================================================",
      "\"\"\"",
      "Memory-efficient attention for decoding.",
      "It supports page size >= 1.",
      "\"\"\"",
      "",
      "import logging",
      "",
      "from packaging import version",
      "",
      "from vllm.platforms import current_platform",
      "from vllm.triton_utils import tl, triton",
      "",
      "is_hip_ = current_platform.is_rocm()",
      "",
      "logger = logging.getLogger(__name__)",
      "",
      "# Only print the following warnings when triton version < 3.2.0.",
      "# The issue won't affect performance or accuracy.",
      "if version.parse(triton.__version__) < version.parse('3.2.0'):",
      "    logger.warning(",
      "        \"The following error message 'operation scheduled before its operands' \"",
      "        \"can be ignored.\")",
      "",
      "",
      "@triton.jit",
      "def tanh(x):",
      "    # Tanh is just a scaled sigmoid",
      "    return 2 * tl.sigmoid(2 * x) - 1",
      "",
      "",
      "@triton.jit",
      "def _fwd_kernel_stage1(",
      "    Q,",
      "    K_Buffer,",
      "    V_Buffer,",
      "    sm_scale,",
      "    Req_to_tokens,",
      "    B_Seqlen,",
      "    Att_Out,",
      "    stride_req_to_tokens_b,",
      "    stride_qbs,",
      "    stride_qh,",
      "    stride_buf_kbs,",
      "    stride_buf_kh,",
      "    stride_buf_vbs,",
      "    stride_buf_vh,",
      "    stride_mid_ob,",
      "    stride_mid_oh,",
      "    stride_mid_os,",
      "    kv_group_num: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_DV: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    NUM_KV_SPLITS: tl.constexpr,",
      "    PAGE_SIZE: tl.constexpr,",
      "    logit_cap: tl.constexpr,",
      "    Lk: tl.constexpr,",
      "    Lv: tl.constexpr,",
      "):",
      "    cur_batch = tl.program_id(0)",
      "    cur_head = tl.program_id(1)",
      "    split_kv_id = tl.program_id(2)",
      "",
      "    cur_kv_head = cur_head // kv_group_num",
      "",
      "    offs_d = tl.arange(0, BLOCK_DMODEL)",
      "    offs_dv = tl.arange(0, BLOCK_DV)",
      "    mask_d = offs_d < Lk",
      "    mask_dv = offs_dv < Lv",
      "    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)",
      "    cur_batch_req_idx = cur_batch",
      "",
      "    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d",
      "    q = tl.load(Q + off_q, mask=mask_d, other=0.0)",
      "",
      "    kv_len_per_split = tl.cdiv(cur_batch_seq_len, NUM_KV_SPLITS)",
      "    split_kv_start = kv_len_per_split * split_kv_id",
      "    split_kv_end = tl.minimum(split_kv_start + kv_len_per_split,",
      "                              cur_batch_seq_len)",
      "",
      "    e_max = -float(\"inf\")",
      "    e_sum = 0.0",
      "    acc = tl.zeros([BLOCK_DV], dtype=tl.float32)",
      "",
      "    if split_kv_end > split_kv_start:",
      "        for start_n in range(split_kv_start, split_kv_end, BLOCK_N):",
      "            offs_n = start_n + tl.arange(0, BLOCK_N)",
      "            kv_page_number = tl.load(",
      "                Req_to_tokens + stride_req_to_tokens_b * cur_batch_req_idx +",
      "                offs_n // PAGE_SIZE,",
      "                mask=offs_n < split_kv_end,",
      "                other=0,",
      "            )",
      "            kv_loc = kv_page_number * PAGE_SIZE + offs_n % PAGE_SIZE",
      "            offs_buf_k = (kv_loc[:, None] * stride_buf_kbs +",
      "                          cur_kv_head * stride_buf_kh + offs_d[None, :])",
      "            k = tl.load(",
      "                K_Buffer + offs_buf_k,",
      "                mask=(offs_n[:, None] < split_kv_end) & (mask_d[None, :]),",
      "                other=0.0,",
      "            )",
      "            qk = tl.sum(q[None, :] * k, 1)",
      "            qk *= sm_scale",
      "",
      "            if logit_cap > 0:",
      "                qk = logit_cap * tanh(qk / logit_cap)",
      "",
      "            qk = tl.where(offs_n < split_kv_end, qk, float(\"-inf\"))",
      "",
      "            offs_buf_v = (kv_loc[:, None] * stride_buf_vbs +",
      "                          cur_kv_head * stride_buf_vh + offs_dv[None, :])",
      "            v = tl.load(",
      "                V_Buffer + offs_buf_v,",
      "                mask=(offs_n[:, None] < split_kv_end) & (mask_dv[None, :]),",
      "                other=0.0,",
      "            )",
      "",
      "            n_e_max = tl.maximum(tl.max(qk, 0), e_max)",
      "            re_scale = tl.exp(e_max - n_e_max)",
      "            p = tl.exp(qk - n_e_max)",
      "            acc *= re_scale",
      "            acc += tl.sum(p[:, None] * v, 0)",
      "",
      "            e_sum = e_sum * re_scale + tl.sum(p, 0)",
      "            e_max = n_e_max",
      "",
      "        offs_mid_o = (cur_batch * stride_mid_ob + cur_head * stride_mid_oh +",
      "                      split_kv_id * stride_mid_os + offs_dv)",
      "",
      "        tl.store(",
      "            Att_Out + offs_mid_o,",
      "            acc / e_sum,",
      "            mask=(mask_dv),",
      "        )",
      "",
      "        offs_mid_o_1 = (cur_batch * stride_mid_ob + cur_head * stride_mid_oh +",
      "                        split_kv_id * stride_mid_os + Lv)",
      "",
      "        tl.store(",
      "            Att_Out + offs_mid_o_1,",
      "            e_max + tl.log(e_sum),",
      "        )",
      "",
      "",
      "def _decode_att_m_fwd(",
      "    q,",
      "    k_buffer,",
      "    v_buffer,",
      "    att_out,",
      "    Req_to_tokens,",
      "    B_Seqlen,",
      "    num_kv_splits,",
      "    sm_scale,",
      "    page_size,",
      "    logit_cap,",
      "):",
      "    BLOCK = 64 if not is_hip_ else 8",
      "",
      "    NUM_KV_SPLITS = num_kv_splits",
      "    Lk = k_buffer.shape[-1]",
      "    Lv = v_buffer.shape[-1]",
      "",
      "    batch, head_num = q.shape[0], q.shape[1]",
      "",
      "    grid = (batch, head_num, NUM_KV_SPLITS)",
      "    kv_group_num = q.shape[1] // k_buffer.shape[-2]",
      "",
      "    num_warps = 4",
      "    if kv_group_num != 1:",
      "        num_warps = 1 if is_hip_ else 2",
      "",
      "    BLOCK_DMODEL = triton.next_power_of_2(Lk)",
      "    BLOCK_DV = triton.next_power_of_2(Lv)",
      "",
      "    _fwd_kernel_stage1[grid](",
      "        q,",
      "        k_buffer,",
      "        v_buffer,",
      "        sm_scale,",
      "        Req_to_tokens,",
      "        B_Seqlen,",
      "        att_out,",
      "        Req_to_tokens.stride(0),",
      "        q.stride(0),",
      "        q.stride(1),",
      "        k_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)",
      "        k_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)",
      "        v_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)",
      "        v_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)",
      "        att_out.stride(0),",
      "        att_out.stride(1),",
      "        att_out.stride(2),",
      "        kv_group_num=kv_group_num,",
      "        BLOCK_DMODEL=BLOCK_DMODEL,",
      "        BLOCK_DV=BLOCK_DV,",
      "        BLOCK_N=BLOCK,",
      "        NUM_KV_SPLITS=NUM_KV_SPLITS,",
      "        PAGE_SIZE=page_size,",
      "        logit_cap=logit_cap,",
      "        num_warps=num_warps,",
      "        num_stages=2,",
      "        Lk=Lk,",
      "        Lv=Lv,",
      "    )",
      "",
      "",
      "@triton.jit",
      "def _fwd_grouped_kernel_stage1(",
      "    Q,",
      "    K_Buffer,",
      "    V_Buffer,",
      "    sm_scale,",
      "    Req_to_tokens,",
      "    B_Seqlen,",
      "    Att_Out,",
      "    stride_req_to_tokens_b,",
      "    stride_qbs,",
      "    stride_qh,",
      "    stride_buf_kbs,",
      "    stride_buf_kh,",
      "    stride_buf_vbs,",
      "    stride_buf_vh,",
      "    stride_mid_ob,",
      "    stride_mid_oh,",
      "    stride_mid_os,",
      "    kv_group_num: tl.constexpr,",
      "    q_head_num: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_DPE: tl.constexpr,",
      "    BLOCK_DV: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_H: tl.constexpr,",
      "    NUM_KV_SPLITS: tl.constexpr,",
      "    PAGE_SIZE: tl.constexpr,",
      "    logit_cap: tl.constexpr,",
      "    Lk: tl.constexpr,",
      "    Lv: tl.constexpr,",
      "):",
      "    cur_batch = tl.program_id(0)",
      "    cur_head_id = tl.program_id(1)",
      "    cur_kv_head = cur_head_id // tl.cdiv(kv_group_num, BLOCK_H)",
      "    split_kv_id = tl.program_id(2)",
      "",
      "    if kv_group_num > BLOCK_H:",
      "        VALID_BLOCK_H: tl.constexpr = BLOCK_H",
      "    else:",
      "        VALID_BLOCK_H: tl.constexpr = kv_group_num",
      "    cur_head = cur_head_id * VALID_BLOCK_H + tl.arange(0, BLOCK_H)",
      "    mask_h = cur_head < (cur_head_id + 1) * VALID_BLOCK_H",
      "    mask_h = mask_h & (cur_head < q_head_num)",
      "",
      "    offs_d = tl.arange(0, BLOCK_DMODEL)",
      "    offs_dv = tl.arange(0, BLOCK_DV)",
      "    mask_d = offs_d < Lk",
      "    mask_dv = offs_dv < Lv",
      "    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)",
      "    cur_batch_req_idx = cur_batch",
      "",
      "    offs_q = cur_batch * stride_qbs + cur_head[:, None] * stride_qh + offs_d[",
      "        None, :]",
      "    q = tl.load(Q + offs_q,",
      "                mask=(mask_h[:, None]) & (mask_d[None, :]),",
      "                other=0.0)",
      "",
      "    if BLOCK_DPE > 0:",
      "        offs_dpe = BLOCK_DMODEL + tl.arange(0, BLOCK_DPE)",
      "        mask_dpe = offs_dpe < Lk",
      "        off_qpe = (cur_batch * stride_qbs + cur_head[:, None] * stride_qh +",
      "                   offs_dpe[None, :])",
      "        qpe = tl.load(Q + off_qpe,",
      "                      mask=(mask_h[:, None]) & (mask_dpe[None, :]),",
      "                      other=0.0)",
      "",
      "    kv_len_per_split = tl.cdiv(cur_batch_seq_len, NUM_KV_SPLITS)",
      "    split_kv_start = kv_len_per_split * split_kv_id",
      "    split_kv_end = tl.minimum(split_kv_start + kv_len_per_split,",
      "                              cur_batch_seq_len)",
      "",
      "    e_max = tl.zeros([BLOCK_H], dtype=tl.float32) - float(\"inf\")",
      "    e_sum = tl.zeros([BLOCK_H], dtype=tl.float32)",
      "    acc = tl.zeros([BLOCK_H, BLOCK_DV], dtype=tl.float32)",
      "",
      "    if split_kv_end > split_kv_start:",
      "        for start_n in range(split_kv_start, split_kv_end, BLOCK_N):",
      "            offs_n = start_n + tl.arange(0, BLOCK_N)",
      "            kv_page_number = tl.load(",
      "                Req_to_tokens + stride_req_to_tokens_b * cur_batch_req_idx +",
      "                offs_n // PAGE_SIZE,",
      "                mask=offs_n < split_kv_end,",
      "                other=0,",
      "            )",
      "            kv_loc = kv_page_number * PAGE_SIZE + offs_n % PAGE_SIZE",
      "            offs_buf_k = (kv_loc[None, :] * stride_buf_kbs +",
      "                          cur_kv_head * stride_buf_kh + offs_d[:, None])",
      "            k = tl.load(",
      "                K_Buffer + offs_buf_k,",
      "                mask=(offs_n[None, :] < split_kv_end) & (mask_d[:, None]),",
      "                other=0.0,",
      "            )",
      "            qk = tl.dot(q, k.to(q.dtype))",
      "            if BLOCK_DPE > 0:",
      "                offs_buf_kpe = (kv_loc[None, :] * stride_buf_kbs +",
      "                                cur_kv_head * stride_buf_kh +",
      "                                offs_dpe[:, None])",
      "                kpe = tl.load(",
      "                    K_Buffer + offs_buf_kpe,",
      "                    mask=(offs_n[None, :] < split_kv_end) &",
      "                    (mask_dpe[:, None]),",
      "                    other=0.0,",
      "                )",
      "                qk += tl.dot(qpe, kpe.to(qpe.dtype))",
      "            qk *= sm_scale",
      "",
      "            if logit_cap > 0:",
      "                qk = logit_cap * tanh(qk / logit_cap)",
      "",
      "            qk = tl.where(mask_h[:, None] & (offs_n[None, :] < split_kv_end),",
      "                          qk, float(\"-inf\"))",
      "",
      "            offs_buf_v = (kv_loc[:, None] * stride_buf_vbs +",
      "                          cur_kv_head * stride_buf_vh + offs_dv[None, :])",
      "            v = tl.load(",
      "                V_Buffer + offs_buf_v,",
      "                mask=(offs_n[:, None] < split_kv_end) & (mask_dv[None, :]),",
      "                other=0.0,",
      "            )",
      "",
      "            n_e_max = tl.maximum(tl.max(qk, 1), e_max)",
      "            re_scale = tl.exp(e_max - n_e_max)",
      "            p = tl.exp(qk - n_e_max[:, None])",
      "            acc *= re_scale[:, None]",
      "            acc += tl.dot(p.to(v.dtype), v)",
      "",
      "            e_sum = e_sum * re_scale + tl.sum(p, 1)",
      "            e_max = n_e_max",
      "",
      "        offs_mid_o = (cur_batch * stride_mid_ob +",
      "                      cur_head[:, None] * stride_mid_oh +",
      "                      split_kv_id * stride_mid_os + offs_dv[None, :])",
      "",
      "        tl.store(",
      "            Att_Out + offs_mid_o,",
      "            acc / e_sum[:, None],",
      "            mask=(mask_h[:, None]) & (mask_dv[None, :]),",
      "        )",
      "",
      "        offs_mid_o_1 = (cur_batch * stride_mid_ob + cur_head * stride_mid_oh +",
      "                        split_kv_id * stride_mid_os + Lv)",
      "",
      "        tl.store(",
      "            Att_Out + offs_mid_o_1,",
      "            e_max + tl.log(e_sum),",
      "            mask=mask_h,",
      "        )",
      "",
      "",
      "def _decode_grouped_att_m_fwd(",
      "    q,",
      "    k_buffer,",
      "    v_buffer,",
      "    att_out,",
      "    Req_to_tokens,",
      "    B_Seqlen,",
      "    num_kv_splits,",
      "    sm_scale,",
      "    page_size,",
      "    logit_cap,",
      "):",
      "    BLOCK = 32",
      "    Lk = k_buffer.shape[-1]",
      "    Lv = v_buffer.shape[-1]",
      "",
      "    # [TODO] work around shmem limit on MI3xx",
      "    if is_hip_ and Lk >= 576:",
      "        BLOCK = 16",
      "",
      "    if Lk == 576:",
      "        BLOCK_DMODEL = 512",
      "        BLOCK_DPE = 64",
      "    elif Lk == 288:",
      "        BLOCK_DMODEL = 256",
      "        BLOCK_DPE = 32",
      "    else:",
      "        BLOCK_DMODEL = triton.next_power_of_2(Lk)",
      "        BLOCK_DPE = 0",
      "    BLOCK_DV = triton.next_power_of_2(Lv)",
      "",
      "    batch, head_num = q.shape[0], q.shape[1]",
      "    kv_group_num = q.shape[1] // k_buffer.shape[-2]",
      "",
      "    BLOCK_H = 16",
      "    NUM_KV_SPLITS = num_kv_splits",
      "    grid = (",
      "        batch,",
      "        triton.cdiv(head_num, min(BLOCK_H, kv_group_num)),",
      "        NUM_KV_SPLITS,",
      "    )",
      "",
      "    extra_kargs = {}",
      "    num_stages = 2",
      "    if is_hip_:",
      "        # https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference-optimization/workload.html#mi300x-triton-kernel-performance-optimization",
      "        # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py",
      "        extra_kargs = {",
      "            \"waves_per_eu\": 1,",
      "            \"matrix_instr_nonkdim\": 16,",
      "            \"kpack\": 2",
      "        }",
      "        num_stages = 1",
      "",
      "    _fwd_grouped_kernel_stage1[grid](",
      "        q,",
      "        k_buffer,",
      "        v_buffer,",
      "        sm_scale,",
      "        Req_to_tokens,",
      "        B_Seqlen,",
      "        att_out,",
      "        Req_to_tokens.stride(0),",
      "        q.stride(0),",
      "        q.stride(1),",
      "        k_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)",
      "        k_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)",
      "        v_buffer.stride(-3),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)",
      "        v_buffer.stride(-2),  # Assume (..., PAGE_SIZE, NUM_HEADS, HEAD_DIM)",
      "        att_out.stride(0),",
      "        att_out.stride(1),",
      "        att_out.stride(2),",
      "        kv_group_num=kv_group_num,",
      "        q_head_num=head_num,",
      "        BLOCK_DMODEL=BLOCK_DMODEL,",
      "        BLOCK_DPE=BLOCK_DPE,",
      "        BLOCK_DV=BLOCK_DV,",
      "        BLOCK_N=BLOCK,",
      "        BLOCK_H=BLOCK_H,",
      "        NUM_KV_SPLITS=NUM_KV_SPLITS,",
      "        PAGE_SIZE=page_size,",
      "        logit_cap=logit_cap,",
      "        num_warps=4,",
      "        num_stages=num_stages,",
      "        Lk=Lk,",
      "        Lv=Lv,",
      "        **extra_kargs,",
      "    )",
      "",
      "",
      "@triton.jit",
      "def _fwd_kernel_stage2(",
      "    Mid_O,",
      "    o,",
      "    B_Seqlen,",
      "    stride_mid_ob,",
      "    stride_mid_oh,",
      "    stride_mid_os,",
      "    stride_obs,",
      "    stride_oh,",
      "    NUM_KV_SPLITS: tl.constexpr,",
      "    BLOCK_DV: tl.constexpr,",
      "    Lv: tl.constexpr,",
      "):",
      "    cur_batch = tl.program_id(0)",
      "    cur_head = tl.program_id(1)",
      "",
      "    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)",
      "",
      "    offs_d = tl.arange(0, BLOCK_DV)",
      "    mask_d = offs_d < Lv",
      "",
      "    e_sum = 0.0",
      "    e_max = -float(\"inf\")",
      "    acc = tl.zeros([BLOCK_DV], dtype=tl.float32)",
      "",
      "    offs_v = cur_batch * stride_mid_ob + cur_head * stride_mid_oh + offs_d",
      "    offs_logic = cur_batch * stride_mid_ob + cur_head * stride_mid_oh + Lv",
      "",
      "    for split_kv_id in range(0, NUM_KV_SPLITS):",
      "        kv_len_per_split = tl.cdiv(cur_batch_seq_len, NUM_KV_SPLITS)",
      "        split_kv_start = kv_len_per_split * split_kv_id",
      "        split_kv_end = tl.minimum(split_kv_start + kv_len_per_split,",
      "                                  cur_batch_seq_len)",
      "",
      "        if split_kv_end > split_kv_start:",
      "            tv = tl.load(Mid_O + offs_v + split_kv_id * stride_mid_os,",
      "                         mask=mask_d,",
      "                         other=0.0)",
      "            tlogic = tl.load(Mid_O + offs_logic + split_kv_id * stride_mid_os)",
      "            n_e_max = tl.maximum(tlogic, e_max)",
      "",
      "            old_scale = tl.exp(e_max - n_e_max)",
      "            acc *= old_scale",
      "            exp_logic = tl.exp(tlogic - n_e_max)",
      "            acc += exp_logic * tv",
      "",
      "            e_sum = e_sum * old_scale + exp_logic",
      "            e_max = n_e_max",
      "",
      "    tl.store(",
      "        o + cur_batch * stride_obs + cur_head * stride_oh + offs_d,",
      "        acc / e_sum,",
      "        mask=mask_d,",
      "    )",
      "",
      "",
      "def _decode_softmax_reducev_fwd(",
      "    logits,",
      "    q,",
      "    o,",
      "    v_buffer,",
      "    b_seq_len,",
      "    num_kv_splits,",
      "):",
      "    batch, head_num = q.shape[0], q.shape[1]",
      "    Lv = v_buffer.shape[-1]",
      "    BLOCK_DV = triton.next_power_of_2(Lv)",
      "",
      "    NUM_KV_SPLITS = num_kv_splits",
      "",
      "    extra_kargs = {}",
      "    if is_hip_:",
      "        # https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/optimizing-triton-kernel.html",
      "        # https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py",
      "        extra_kargs = {",
      "            \"waves_per_eu\": 4,",
      "            \"matrix_instr_nonkdim\": 16,",
      "            \"kpack\": 2",
      "        }",
      "",
      "    grid = (batch, head_num)",
      "    _fwd_kernel_stage2[grid](",
      "        logits,",
      "        o,",
      "        b_seq_len,",
      "        logits.stride(0),",
      "        logits.stride(1),",
      "        logits.stride(2),",
      "        o.stride(0),",
      "        o.stride(1),",
      "        NUM_KV_SPLITS=NUM_KV_SPLITS,",
      "        BLOCK_DV=BLOCK_DV,",
      "        Lv=Lv,",
      "        num_warps=4,",
      "        num_stages=2,",
      "        **extra_kargs,",
      "    )",
      "",
      "",
      "def decode_attention_fwd_normal(",
      "    q,",
      "    k_buffer,",
      "    v_buffer,",
      "    o,",
      "    req_to_token,",
      "    b_seq_len,",
      "    attn_logits,",
      "    num_kv_splits,",
      "    sm_scale,",
      "    page_size,",
      "    logit_cap=0.0,",
      "):",
      "    _decode_att_m_fwd(",
      "        q,",
      "        k_buffer,",
      "        v_buffer,",
      "        attn_logits,",
      "        req_to_token,",
      "        b_seq_len,",
      "        num_kv_splits,",
      "        sm_scale,",
      "        page_size,",
      "        logit_cap,",
      "    )",
      "    _decode_softmax_reducev_fwd(attn_logits, q, o, v_buffer, b_seq_len,",
      "                                num_kv_splits)",
      "",
      "",
      "def decode_attention_fwd_grouped(",
      "    q,",
      "    k_buffer,",
      "    v_buffer,",
      "    o,",
      "    req_to_token,",
      "    b_seq_len,",
      "    attn_logits,",
      "    num_kv_splits,",
      "    sm_scale,",
      "    page_size,",
      "    logit_cap=0.0,",
      "):",
      "    _decode_grouped_att_m_fwd(",
      "        q,",
      "        k_buffer,",
      "        v_buffer,",
      "        attn_logits,",
      "        req_to_token,",
      "        b_seq_len,",
      "        num_kv_splits,",
      "        sm_scale,",
      "        page_size,",
      "        logit_cap,",
      "    )",
      "    _decode_softmax_reducev_fwd(attn_logits, q, o, v_buffer, b_seq_len,",
      "                                num_kv_splits)",
      "",
      "",
      "def decode_attention_fwd(",
      "    q,",
      "    k_buffer,",
      "    v_buffer,",
      "    o,",
      "    req_to_token,",
      "    b_seq_len,",
      "    attn_logits,",
      "    num_kv_splits,",
      "    sm_scale,",
      "    page_size=1,",
      "    logit_cap=0.0,",
      "):",
      "    assert num_kv_splits == attn_logits.shape[2]",
      "    kv_group_num = q.shape[1] // v_buffer.shape[-2]",
      "",
      "    if kv_group_num == 1:",
      "        # MHA",
      "        decode_attention_fwd_normal(",
      "            q,",
      "            k_buffer,",
      "            v_buffer,",
      "            o,",
      "            req_to_token,",
      "            b_seq_len,",
      "            attn_logits,",
      "            num_kv_splits,",
      "            sm_scale,",
      "            page_size,",
      "            logit_cap,",
      "        )",
      "    else:",
      "        # GQA/MQA/MLA",
      "        decode_attention_fwd_grouped(",
      "            q,",
      "            k_buffer,",
      "            v_buffer,",
      "            o,",
      "            req_to_token,",
      "            b_seq_len,",
      "            attn_logits,",
      "            num_kv_splits,",
      "            sm_scale,",
      "            page_size,",
      "            logit_cap,",
      "        )"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/attention/ops/triton_merge_attn_states.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "from typing import Optional",
      "",
      "import torch",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "",
      "# Implements section 2.2 of https://www.arxiv.org/pdf/2501.01005",
      "# can be used to combine partial attention results (in the split-KV case)",
      "def merge_attn_states(",
      "    output: torch.Tensor,",
      "    prefix_output: torch.Tensor,",
      "    prefix_lse: torch.Tensor,",
      "    suffix_output: torch.Tensor,",
      "    suffix_lse: torch.Tensor,",
      "    output_lse: Optional[torch.Tensor] = None,",
      ") -> None:",
      "    num_tokens = output.shape[0]",
      "    num_query_heads = output.shape[1]",
      "    head_size = output.shape[2]",
      "    padded_head_size = triton.next_power_of_2(head_size)",
      "",
      "    # TODO(woosuk): Use CUDA kernel instead of Triton to minimize CPU overhead.",
      "    merge_attn_states_kernel[(num_tokens, num_query_heads)](",
      "        output,",
      "        output_lse,",
      "        prefix_output,",
      "        prefix_lse,",
      "        suffix_output,",
      "        suffix_lse,",
      "        head_size,",
      "        padded_head_size,",
      "        output_lse is not None,",
      "    )",
      "",
      "",
      "@triton.jit",
      "def merge_attn_states_kernel(",
      "    output,  # [NUM_TOKENS, NUM_HEADS, HEAD_SIZE]",
      "    output_lse,  # [NUM_HEADS, NUM_TOKENS]",
      "    prefix_output,  # [NUM_TOKENS, NUM_HEADS, HEAD_SIZE]",
      "    prefix_lse,  # [NUM_HEADS, NUM_TOKENS]",
      "    suffix_output,  # [NUM_TOKENS, NUM_HEADS, HEAD_SIZE]",
      "    suffix_lse,  # [NUM_HEADS, NUM_TOKENS]",
      "    HEAD_SIZE: tl.constexpr,",
      "    PADDED_HEAD_SIZE: tl.constexpr,",
      "    OUTPUT_LSE: tl.constexpr,",
      "):",
      "    token_idx = tl.program_id(0)",
      "    num_tokens = tl.num_programs(0)",
      "    head_idx = tl.program_id(1)",
      "    num_heads = tl.num_programs(1)",
      "",
      "    p_lse = tl.load(prefix_lse + head_idx * num_tokens + token_idx)",
      "    s_lse = tl.load(suffix_lse + head_idx * num_tokens + token_idx)",
      "",
      "    # FA2 and FA3 have different behavior for when the sum-exp is 0, this namely",
      "    # arises with 0 len seqlens. FA3 returns -inf here while FA2 returns inf.",
      "    # If we see an inf assume FA2 and convert inf to -inf for consistency",
      "    # and correctness. Inf generally doesn't make sense in this context outside",
      "    # of undefined-behavior/FA2-case, so I think this a safe assumption.",
      "    p_lse = float('-inf') if p_lse == float('inf') else p_lse",
      "    s_lse = float('-inf') if s_lse == float('inf') else s_lse",
      "",
      "    max_lse = tl.maximum(p_lse, s_lse)",
      "    p_lse = p_lse - max_lse",
      "    s_lse = s_lse - max_lse",
      "    # Will reuse precomputed Exp values for scale factor computation.",
      "    p_se = tl.exp(p_lse)",
      "    s_se = tl.exp(s_lse)",
      "    out_se = (p_se + s_se)",
      "",
      "    if OUTPUT_LSE:",
      "        out_lse = tl.log(out_se) + max_lse",
      "        tl.store(output_lse + head_idx * num_tokens + token_idx, out_lse)",
      "",
      "    head_arange = tl.arange(0, PADDED_HEAD_SIZE)",
      "    head_mask = head_arange < HEAD_SIZE",
      "    p_out = tl.load(prefix_output + token_idx * num_heads * HEAD_SIZE +",
      "                    head_idx * HEAD_SIZE + head_arange,",
      "                    mask=head_mask)",
      "    s_out = tl.load(suffix_output + token_idx * num_heads * HEAD_SIZE +",
      "                    head_idx * HEAD_SIZE + head_arange,",
      "                    mask=head_mask)",
      "",
      "    # NOTE(woosuk): Be careful with the numerical stability.",
      "    # We should compute the scale first, and then multiply it with the output.",
      "    # Do not multiply the output with tl.exp(p_lse) or tl.exp(s_lse) directly.",
      "    p_scale = p_se / out_se",
      "    s_scale = s_se / out_se",
      "    out = p_out * p_scale + s_out * s_scale",
      "    tl.store(output + token_idx * num_heads * HEAD_SIZE +",
      "             head_idx * HEAD_SIZE + head_arange,",
      "             out,",
      "             mask=head_mask)"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/attention/ops/common.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "import torch",
      "",
      "from vllm.distributed.parallel_state import GroupCoordinator",
      "from vllm.triton_utils import tl, triton",
      "",
      "",
      "@triton.jit",
      "def _correct_attn_cp_out_kernel(outputs_ptr, new_output_ptr, lses_ptr,",
      "                                vlse_ptr, outputs_stride_B, outputs_stride_H,",
      "                                outputs_stride_D, lses_stride_N, lses_stride_B,",
      "                                lses_stride_H, lse_idx, HEAD_DIM: tl.constexpr,",
      "                                N_ROUNDED: tl.constexpr):",
      "    \"\"\"",
      "    Apply the all-gathered lses to correct each local rank's attention",
      "    output. we still need perform a cross-rank reduction to obtain the",
      "    final attention output.",
      "",
      "    Args:",
      "        output: [ B, H, D ]",
      "        lses   : [ N, B, H ]",
      "        cp, batch, q_heads, v_head_dim",
      "    Return:",
      "        output: [ B, H, D ]",
      "        lse   : [ B, H ]",
      "    \"\"\"",
      "    batch_idx = tl.program_id(axis=0).to(tl.int64)",
      "    head_idx = tl.program_id(axis=1).to(tl.int64)",
      "    d_offsets = tl.arange(0, HEAD_DIM)",
      "    num_n_offsets = tl.arange(0, N_ROUNDED)",
      "",
      "    # shape = [N]",
      "    lse_offsets = num_n_offsets * lses_stride_N + batch_idx * \\",
      "        lses_stride_B + head_idx * lses_stride_H",
      "",
      "    # calc final lse",
      "    lse = tl.load(lses_ptr + lse_offsets)",
      "    lse = tl.where((lse != lse) | (lse == float('inf')), -float('inf'), lse)",
      "    lse_max = tl.max(lse, axis=0)",
      "    lse -= lse_max",
      "    lse_exp = tl.exp(lse)",
      "    lse_acc = tl.sum(lse_exp, axis=0)",
      "    lse = tl.log(lse_acc)",
      "    lse += lse_max",
      "",
      "    lse_offsets = batch_idx * lses_stride_B + head_idx * lses_stride_H",
      "    tl.store(vlse_ptr + lse_offsets, lse)",
      "",
      "    # shape = [D]",
      "    output_offsets = batch_idx * outputs_stride_B + \\",
      "                    head_idx * outputs_stride_H + \\",
      "                    d_offsets * outputs_stride_D",
      "",
      "    # correct output",
      "    lse_offset = lse_idx * lses_stride_N + batch_idx * \\",
      "        lses_stride_B + head_idx * lses_stride_H",
      "    lse_tmp = tl.load(lses_ptr + lse_offset)",
      "    lse_finally = lse_tmp - lse",
      "    lse_finally = tl.where(",
      "        (lse_finally != lse_finally) | (lse_finally == float('inf')),",
      "        -float('inf'), lse_finally)",
      "    factor = tl.exp(lse_finally)",
      "    output = tl.load(outputs_ptr + output_offsets)",
      "    output = output * factor",
      "",
      "    tl.store(new_output_ptr + output_offsets, output)",
      "",
      "",
      "class CPTritonContext:",
      "    \"\"\" The CPTritonContext is used to avoid recompilation of the Triton JIT.",
      "    \"\"\"",
      "",
      "    def __init__(self):",
      "        self.inner_kernel = None",
      "",
      "    def call_kernel(self, kernel, grid, *regular_args, **const_args):",
      "        if self.inner_kernel is None:",
      "            self.inner_kernel = kernel[grid](*regular_args, **const_args)",
      "        else:",
      "            self.inner_kernel[grid](*regular_args)",
      "",
      "",
      "def correct_attn_out(out: torch.Tensor, lses: torch.Tensor, cp_rank: int,",
      "                     ctx: CPTritonContext):",
      "    \"\"\"",
      "    Apply the all-gathered lses to correct each local rank's attention",
      "    output. we still need perform a cross-rank reduction to obtain the",
      "    final attention output.",
      "",
      "    Args:",
      "        output: [ B, H, D ]",
      "        lses   : [ N, B, H ]",
      "    Return:",
      "        output: [ B, H, D ]",
      "        lse   : [ B, H ]",
      "    \"\"\"",
      "    if ctx is None:",
      "        ctx = CPTritonContext()",
      "",
      "    lse = torch.empty_like(lses[0])",
      "",
      "    grid = (out.shape[0], out.shape[1], 1)",
      "    regular_args = (out, out, lses, lse, *out.stride(), *lses.stride(),",
      "                    cp_rank)",
      "    const_args = {",
      "        \"HEAD_DIM\": out.shape[-1],",
      "        \"N_ROUNDED\": lses.shape[0],",
      "    }",
      "",
      "    ctx.call_kernel(_correct_attn_cp_out_kernel, grid, *regular_args,",
      "                    **const_args)",
      "    return out, lse",
      "",
      "",
      "def cp_lse_ag_out_rs(cp_attn_out: torch.Tensor,",
      "                     cp_attn_lse: torch.Tensor,",
      "                     cp_group: GroupCoordinator,",
      "                     ctx: CPTritonContext = None):",
      "    \"\"\"",
      "    cp_attn_out: [ B, H, D ]",
      "    cp_attn_lse: [ B, H ]",
      "    \"\"\"",
      "    if cp_group.world_size == 1:",
      "        return cp_attn_out",
      "",
      "    if ctx is None:",
      "        ctx = CPTritonContext()",
      "",
      "    lses = torch.empty((cp_group.world_size, ) + cp_attn_lse.shape,",
      "                       dtype=cp_attn_lse.dtype,",
      "                       device=cp_attn_lse.device)",
      "",
      "    cp_attn_lse = cp_attn_lse.contiguous()",
      "    lses = cp_group.all_gather(cp_attn_lse, dim=0).view_as(lses)",
      "    out, _ = correct_attn_out(cp_attn_out, lses, cp_group.rank_in_group, ctx)",
      "    assert out.is_contiguous()",
      "    out = cp_group.reduce_scatter(out, dim=1)",
      "    return out"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/attention/ops/prefix_prefill.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "# The kernels in this file are adapted from LightLLM's context_attention_fwd:",
      "# https://github.com/ModelTC/lightllm/blob/main/lightllm/models/llama/triton_kernel/context_flashattention_nopad.py",
      "",
      "import torch",
      "",
      "from vllm.platforms import current_platform",
      "from vllm.triton_utils import tl, triton",
      "",
      "# Static kernels parameters",
      "BASE_BLOCK = 128 if current_platform.has_device_capability(80) else 64",
      "NUM_WARPS = 4 if current_platform.is_rocm() else 8",
      "",
      "# To check compatibility",
      "IS_TURING = current_platform.get_device_capability() == (7, 5)",
      "",
      "",
      "# Here's an example autotuner config for this kernel. This config does provide",
      "# a performance improvement, but dramatically increases first call latency in",
      "# triton 3.2. Because of this tradeoff, it's currently commented out.",
      "# @triton.autotune(",
      "#     configs=[",
      "#         triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, \\",
      "#                         \"num_unroll_cache\": 4, \\",
      "#                         \"num_unroll_request\": 1 } | \\",
      "#                         ({\"kpack\": 2, \"waves_per_eu\": 2} \\",
      "#                             if current_platform.is_rocm() else {}), \\",
      "#                         num_warps=4, \\",
      "#                         num_stages=1)",
      "#     ],",
      "#     key=[\"BLOCK_SIZE\", \"MAX_Q_LEN\", \"MAX_CTX_LEN\"]",
      "# )",
      "@triton.jit",
      "def _fwd_kernel(Q,",
      "                K,",
      "                V,",
      "                K_cache,",
      "                V_cache,",
      "                sink_ptr,",
      "                B_Loc,",
      "                sm_scale,",
      "                k_scale,",
      "                v_scale,",
      "                B_Start_Loc,",
      "                B_Seqlen,",
      "                x: tl.constexpr,",
      "                Out,",
      "                stride_b_loc_b,",
      "                stride_b_loc_s,",
      "                stride_qbs,",
      "                stride_qh,",
      "                stride_qd,",
      "                stride_kbs,",
      "                stride_kh,",
      "                stride_kd,",
      "                stride_vbs,",
      "                stride_vh,",
      "                stride_vd,",
      "                stride_obs,",
      "                stride_oh,",
      "                stride_od,",
      "                stride_k_cache_bs,",
      "                stride_k_cache_h,",
      "                stride_k_cache_d,",
      "                stride_k_cache_bl: tl.constexpr,",
      "                stride_k_cache_x,",
      "                stride_v_cache_bs,",
      "                stride_v_cache_h,",
      "                stride_v_cache_d,",
      "                stride_v_cache_bl,",
      "                num_queries_per_kv: tl.constexpr,",
      "                IN_PRECISION: tl.constexpr,",
      "                BLOCK_M: tl.constexpr,",
      "                BLOCK_DMODEL: tl.constexpr,",
      "                BLOCK_DMODEL_PADDED: tl.constexpr,",
      "                BLOCK_SIZE: tl.constexpr,",
      "                BLOCK_N: tl.constexpr,",
      "                SLIDING_WINDOW: tl.constexpr,",
      "                num_unroll_cache: tl.constexpr,",
      "                num_unroll_request: tl.constexpr,",
      "                SKIP_DECODE: tl.constexpr,",
      "                USE_SINKS: tl.constexpr,",
      "                MAX_Q_LEN: tl.constexpr = 0,",
      "                MAX_CTX_LEN: tl.constexpr = 0):",
      "",
      "    cur_batch = tl.program_id(0)",
      "    cur_head = tl.program_id(1)",
      "    start_m = tl.program_id(2)",
      "",
      "    cur_kv_head = cur_head // num_queries_per_kv",
      "",
      "    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)",
      "    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)",
      "    cur_batch_in_all_stop_index = tl.load(B_Start_Loc + cur_batch + 1)",
      "    cur_batch_query_len = (cur_batch_in_all_stop_index -",
      "                           cur_batch_in_all_start_index)",
      "    cur_batch_ctx_len = cur_batch_seq_len - cur_batch_query_len",
      "",
      "    if SKIP_DECODE and cur_batch_query_len == 1:",
      "        return",
      "",
      "    # start position inside of the query",
      "    # generally, N goes over kv, while M goes over query_len",
      "    block_start_loc = BLOCK_M * start_m",
      "",
      "    # initialize offsets",
      "    # [BLOCK_SIZE]; starts at 0",
      "    offs_bs_n = tl.arange(0, BLOCK_SIZE)",
      "    # [N]; starts at 0",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "    # [D]; starts at 0",
      "    offs_d = tl.arange(0, BLOCK_DMODEL_PADDED)",
      "    # [M]; starts at current position in query",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    # [M,D]",
      "    off_q = ((cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs +",
      "             cur_head * stride_qh + offs_d[None, :] * stride_qd)",
      "",
      "    dim_mask = tl.where(",
      "        tl.arange(0, BLOCK_DMODEL_PADDED) < BLOCK_DMODEL, 1,",
      "        0).to(tl.int1)  # [D]",
      "",
      "    q = tl.load(Q + off_q,",
      "                mask=dim_mask[None, :] &",
      "                (offs_m[:, None] < cur_batch_query_len),",
      "                other=0.0)  # [M,D]",
      "",
      "    # initialize pointer to m and l",
      "    if not USE_SINKS:",
      "        m_i = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)",
      "    else:",
      "        m_i = tl.load(",
      "            sink_ptr + tl.full([BLOCK_M], cur_head, dtype=tl.int64),",
      "            mask=(offs_m < cur_batch_query_len),",
      "            other=float(\"-inf\"),",
      "        ).to(dtype=tl.float32)",
      "",
      "    l_i = tl.full([BLOCK_M], 1.0, dtype=tl.float32)",
      "    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL_PADDED], dtype=tl.float32)  # [M,D]",
      "",
      "    # compute query against context (no causal mask here)",
      "    for start_n in tl.range(0, cur_batch_ctx_len, BLOCK_SIZE, \\",
      "                            loop_unroll_factor=num_unroll_cache):",
      "        start_n = tl.multiple_of(start_n, BLOCK_SIZE)",
      "        # -- compute qk ----",
      "        bn = tl.load(B_Loc + cur_batch * stride_b_loc_b +",
      "                     (start_n // BLOCK_SIZE) * stride_b_loc_s).to(tl.int64)",
      "        # [D,BLOCK_SIZE]",
      "        off_k = (",
      "            bn[None, :] * stride_k_cache_bs + cur_kv_head * stride_k_cache_h +",
      "            (offs_d[:, None] // x) * stride_k_cache_d +",
      "            ((start_n + offs_bs_n[None, :]) % BLOCK_SIZE) * stride_k_cache_bl +",
      "            (offs_d[:, None] % x) * stride_k_cache_x)",
      "",
      "        # [BLOCK_SIZE,D]",
      "        off_v = (bn[:, None] * stride_v_cache_bs +",
      "                 cur_kv_head * stride_v_cache_h +",
      "                 offs_d[None, :] * stride_v_cache_d +",
      "                 offs_bs_n[:, None] * stride_v_cache_bl)",
      "",
      "        if start_n + BLOCK_SIZE > cur_batch_ctx_len or \\",
      "            BLOCK_DMODEL != BLOCK_DMODEL_PADDED:",
      "            k_load = tl.load(",
      "                K_cache + off_k,",
      "                mask=dim_mask[:, None] &",
      "                ((start_n + offs_bs_n[None, :]) < cur_batch_ctx_len),",
      "                other=0.0)  # [D,N]",
      "        else:",
      "            k_load = tl.load(K_cache + off_k)",
      "",
      "        if k_load.dtype.is_fp8():",
      "            k = (k_load.to(tl.float32) * tl.load(k_scale)).to(q.dtype)",
      "        else:",
      "            k = k_load",
      "",
      "        qk = tl.zeros([BLOCK_M, BLOCK_SIZE], dtype=tl.float32)  # [M,N]",
      "        qk = tl.dot(q, k, acc=qk, input_precision=IN_PRECISION)",
      "        qk = tl.where((start_n + offs_bs_n[None, :]) < cur_batch_ctx_len, qk,",
      "                      float(\"-inf\"))",
      "        qk *= sm_scale",
      "        if SLIDING_WINDOW > 0:",
      "            # (cur_batch_ctx_len + offs_m[:, None]) are the positions of",
      "            # Q entries in sequence",
      "            # (start_n + offs_bs_n[None, :]) are the positions of",
      "            # KV entries in sequence",
      "            # So the condition makes sure each entry in Q only attends",
      "            # to KV entries not more than SLIDING_WINDOW away.",
      "            #",
      "            # We can't use -inf here, because the",
      "            # sliding window may lead to the entire row being masked.",
      "            # This then makes m_ij contain -inf, which causes NaNs in",
      "            # exp().",
      "            qk = tl.where((cur_batch_ctx_len + offs_m[:, None]) -",
      "                          (start_n + offs_bs_n[None, :]) < SLIDING_WINDOW, qk,",
      "                          -10000)",
      "",
      "        # compute running maximum",
      "        m_ij = tl.maximum(m_i, tl.max(qk, axis=1))",
      "        p = tl.exp(qk - m_ij[:, None])",
      "        l_ij = tl.sum(p, axis=1)",
      "        alpha = tl.exp(m_i - m_ij)",
      "        acc = acc * alpha[:, None]",
      "",
      "        # update acc",
      "        if start_n + BLOCK_SIZE > cur_batch_ctx_len or \\",
      "            BLOCK_DMODEL != BLOCK_DMODEL_PADDED:",
      "            v_load = tl.load(",
      "                V_cache + off_v,",
      "                mask=dim_mask[None, :] &",
      "                ((start_n + offs_bs_n[:, None]) < cur_batch_ctx_len),",
      "                other=0.0)  # [N,D]",
      "        else:",
      "            v_load = tl.load(V_cache + off_v)",
      "",
      "        if v_load.dtype.is_fp8():",
      "            v = (v_load.to(tl.float32) * tl.load(v_scale)).to(q.dtype)",
      "        else:",
      "            v = v_load",
      "        p = p.to(v.dtype)",
      "",
      "        acc = tl.dot(p, v, acc=acc, input_precision=IN_PRECISION)",
      "        # # update m_i and l_i",
      "        l_i = l_i * alpha + l_ij",
      "        m_i = m_ij",
      "",
      "    off_k = (offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh +",
      "             offs_d[:, None] * stride_kd)",
      "    off_v = (offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh +",
      "             offs_d[None, :] * stride_vd)",
      "    k_ptrs = K + off_k",
      "    v_ptrs = V + off_v",
      "",
      "    # block_mask is 0 when we're already past the current query length",
      "    block_mask = tl.where(block_start_loc < cur_batch_query_len, 1, 0)",
      "",
      "    # compute query against itself (with causal mask)",
      "    for start_n in tl.range(0, \\",
      "                        block_mask * (start_m + 1) * BLOCK_M, BLOCK_N, \\",
      "                        loop_unroll_factor=num_unroll_request):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "        # -- compute qk ----",
      "        k = tl.load(k_ptrs +",
      "                    (cur_batch_in_all_start_index + start_n) * stride_kbs,",
      "                    mask=dim_mask[:, None] &",
      "                    ((start_n + offs_n[None, :]) < cur_batch_query_len),",
      "                    other=0.0)",
      "",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk = tl.dot(q, k, acc=qk, input_precision=IN_PRECISION)",
      "        qk *= sm_scale",
      "        # apply causal mask",
      "        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk,",
      "                      float(\"-inf\"))",
      "        if SLIDING_WINDOW > 0:",
      "            qk = tl.where(",
      "                offs_m[:, None] - (start_n + offs_n[None, :]) < SLIDING_WINDOW,",
      "                qk, -10000)",
      "",
      "        # compute running maximum",
      "        m_ij = tl.maximum(m_i, tl.max(qk, axis=1))",
      "        p = tl.exp(qk - m_ij[:, None])",
      "        l_ij = tl.sum(p, axis=1)",
      "        alpha = tl.exp(m_i - m_ij)",
      "        acc = acc * alpha[:, None]",
      "",
      "        # update acc",
      "        v = tl.load(v_ptrs +",
      "                    (cur_batch_in_all_start_index + start_n) * stride_vbs,",
      "                    mask=dim_mask[None, :] &",
      "                    ((start_n + offs_n[:, None]) < cur_batch_query_len),",
      "                    other=0.0)",
      "        p = p.to(v.dtype)",
      "",
      "        acc = tl.dot(p, v, acc=acc, input_precision=IN_PRECISION)",
      "        # update m_i and l_i",
      "        l_i = l_i * alpha + l_ij",
      "        m_i = m_ij",
      "",
      "    acc = acc / l_i[:, None]",
      "",
      "    # initialize pointers to output",
      "    off_o = ((cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs +",
      "             cur_head * stride_oh + offs_d[None, :] * stride_od)",
      "    out_ptrs = Out + off_o",
      "    tl.store(out_ptrs,",
      "             acc,",
      "             mask=dim_mask[None, :] & (offs_m[:, None] < cur_batch_query_len))",
      "    return",
      "",
      "",
      "@triton.jit",
      "def _fwd_kernel_flash_attn_v2(",
      "    Q,",
      "    K,",
      "    V,",
      "    K_cache,",
      "    V_cache,",
      "    B_Loc,",
      "    sm_scale,",
      "    B_Start_Loc,",
      "    B_Seqlen,",
      "    B_Ctxlen,",
      "    block_size,",
      "    x,",
      "    Out,",
      "    stride_b_loc_b,",
      "    stride_b_loc_s,",
      "    stride_qbs,",
      "    stride_qh,",
      "    stride_qd,",
      "    stride_kbs,",
      "    stride_kh,",
      "    stride_kd,",
      "    stride_vbs,",
      "    stride_vh,",
      "    stride_vd,",
      "    stride_obs,",
      "    stride_oh,",
      "    stride_od,",
      "    stride_k_cache_bs,",
      "    stride_k_cache_h,",
      "    stride_k_cache_d,",
      "    stride_k_cache_bl,",
      "    stride_k_cache_x,",
      "    stride_v_cache_bs,",
      "    stride_v_cache_h,",
      "    stride_v_cache_d,",
      "    stride_v_cache_bl,",
      "    num_queries_per_kv: int,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    cur_batch = tl.program_id(0)",
      "    cur_head = tl.program_id(1)",
      "    start_m = tl.program_id(2)",
      "",
      "    cur_kv_head = cur_head // num_queries_per_kv",
      "",
      "    cur_batch_ctx_len = tl.load(B_Ctxlen + cur_batch)",
      "    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)",
      "    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)",
      "",
      "    block_start_loc = BLOCK_M * start_m",
      "",
      "    # initialize offsets",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "    offs_d = tl.arange(0, BLOCK_DMODEL)",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    off_q = ((cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs +",
      "             cur_head * stride_qh + offs_d[None, :] * stride_qd)",
      "",
      "    q = tl.load(Q + off_q,",
      "                mask=offs_m[:, None] < cur_batch_seq_len - cur_batch_ctx_len,",
      "                other=0.0)",
      "",
      "    # # initialize pointer to m and l",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)",
      "    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)",
      "",
      "    for start_n in range(0, cur_batch_ctx_len, BLOCK_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "        # -- compute qk ----",
      "        bn = tl.load(B_Loc + cur_batch * stride_b_loc_b +",
      "                     ((start_n + offs_n) // block_size) * stride_b_loc_s,",
      "                     mask=(start_n + offs_n) < cur_batch_ctx_len,",
      "                     other=0).to(tl.int64)",
      "        off_k = (",
      "            bn[None, :] * stride_k_cache_bs + cur_kv_head * stride_k_cache_h +",
      "            (offs_d[:, None] // x) * stride_k_cache_d +",
      "            ((start_n + offs_n[None, :]) % block_size) * stride_k_cache_bl +",
      "            (offs_d[:, None] % x) * stride_k_cache_x)",
      "        off_v = (bn[:, None] * stride_v_cache_bs +",
      "                 cur_kv_head * stride_v_cache_h +",
      "                 offs_d[None, :] * stride_v_cache_d +",
      "                 (start_n + offs_n[:, None]) % block_size * stride_v_cache_bl)",
      "        k = tl.load(K_cache + off_k,",
      "                    mask=(start_n + offs_n[None, :]) < cur_batch_ctx_len,",
      "                    other=0.0)",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk += tl.dot(q, k)",
      "        qk = tl.where((start_n + offs_n[None, :]) < cur_batch_ctx_len, qk,",
      "                      float(\"-inf\"))",
      "        qk *= sm_scale",
      "",
      "        # -- compute m_ij, p, l_ij",
      "        m_ij = tl.max(qk, 1)",
      "        m_i_new = tl.maximum(m_i, m_ij)",
      "        p = tl.math.exp(qk - m_i_new[:, None])",
      "        l_ij = tl.sum(p, 1)",
      "        # -- update m_i and l_i",
      "",
      "        alpha = tl.math.exp(m_i - m_i_new)",
      "        l_i_new = alpha * l_i + l_ij",
      "        # -- update output accumulator --",
      "        # scale p",
      "        # scale acc",
      "        acc_scale = alpha",
      "        # acc_scale = l_i / l_i_new * alpha",
      "        acc = acc * acc_scale[:, None]",
      "        # update acc",
      "        v = tl.load(V_cache + off_v,",
      "                    mask=(start_n + offs_n[:, None]) < cur_batch_ctx_len,",
      "                    other=0.0)",
      "",
      "        p = p.to(v.dtype)",
      "        acc += tl.dot(p, v)",
      "        # update m_i and l_i",
      "        l_i = l_i_new",
      "        m_i = m_i_new",
      "",
      "    off_k = (offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh +",
      "             offs_d[:, None] * stride_kd)",
      "    off_v = (offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh +",
      "             offs_d[None, :] * stride_vd)",
      "    k_ptrs = K + off_k",
      "    v_ptrs = V + off_v",
      "",
      "    block_mask = tl.where(",
      "        block_start_loc < cur_batch_seq_len - cur_batch_ctx_len, 1, 0)",
      "",
      "    for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "        # -- compute qk ----",
      "        k = tl.load(k_ptrs +",
      "                    (cur_batch_in_all_start_index + start_n) * stride_kbs,",
      "                    mask=(start_n + offs_n[None, :])",
      "                    < cur_batch_seq_len - cur_batch_ctx_len,",
      "                    other=0.0)",
      "",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk += tl.dot(q, k)",
      "        qk *= sm_scale",
      "        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk,",
      "                      float(\"-inf\"))",
      "",
      "        # -- compute m_ij, p, l_ij",
      "        m_ij = tl.max(qk, 1)",
      "        m_i_new = tl.maximum(m_i, m_ij)",
      "        p = tl.math.exp(qk - m_i_new[:, None])",
      "        l_ij = tl.sum(p, 1)",
      "        # -- update m_i and l_i",
      "",
      "        alpha = tl.math.exp(m_i - m_i_new)",
      "        l_i_new = alpha * l_i + l_ij",
      "        # -- update output accumulator --",
      "        # scale p",
      "        # scale acc",
      "        acc_scale = alpha",
      "        # acc_scale = l_i / l_i_new * alpha",
      "        acc = acc * acc_scale[:, None]",
      "        # update acc",
      "        v = tl.load(v_ptrs +",
      "                    (cur_batch_in_all_start_index + start_n) * stride_vbs,",
      "                    mask=(start_n + offs_n[:, None])",
      "                    < cur_batch_seq_len - cur_batch_ctx_len,",
      "                    other=0.0)",
      "",
      "        p = p.to(v.dtype)",
      "        acc += tl.dot(p, v)",
      "        # update m_i and l_i",
      "        l_i = l_i_new",
      "        m_i = m_i_new",
      "",
      "    # acc /= l_i[:, None]",
      "    # initialize pointers to output",
      "    off_o = ((cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs +",
      "             cur_head * stride_oh + offs_d[None, :] * stride_od)",
      "    out_ptrs = Out + off_o",
      "    tl.store(out_ptrs,",
      "             acc,",
      "             mask=offs_m[:, None] < cur_batch_seq_len - cur_batch_ctx_len)",
      "    return",
      "",
      "",
      "@triton.jit",
      "def _fwd_kernel_alibi(",
      "    Q,",
      "    K,",
      "    V,",
      "    K_cache,",
      "    V_cache,",
      "    B_Loc,",
      "    sm_scale,",
      "    k_scale,",
      "    v_scale,",
      "    B_Start_Loc,",
      "    B_Seqlen,",
      "    Alibi_slopes,",
      "    block_size,",
      "    x,",
      "    Out,",
      "    stride_b_loc_b,",
      "    stride_b_loc_s,",
      "    stride_qbs,",
      "    stride_qh,",
      "    stride_qd,",
      "    stride_kbs,",
      "    stride_kh,",
      "    stride_kd,",
      "    stride_vbs,",
      "    stride_vh,",
      "    stride_vd,",
      "    stride_obs,",
      "    stride_oh,",
      "    stride_od,",
      "    stride_k_cache_bs,",
      "    stride_k_cache_h,",
      "    stride_k_cache_d,",
      "    stride_k_cache_bl,",
      "    stride_k_cache_x,",
      "    stride_v_cache_bs,",
      "    stride_v_cache_h,",
      "    stride_v_cache_d,",
      "    stride_v_cache_bl,",
      "    num_queries_per_kv: int,",
      "    IN_PRECISION: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,  # head size",
      "    BLOCK_DMODEL_PADDED: tl.constexpr,  # head size padded to a power of 2",
      "    BLOCK_N: tl.constexpr,",
      "    SKIP_DECODE: tl.constexpr,",
      "):",
      "    # attn_bias[]",
      "    cur_batch = tl.program_id(0)",
      "    cur_head = tl.program_id(1)",
      "    start_m = tl.program_id(2)",
      "",
      "    cur_kv_head = cur_head // num_queries_per_kv",
      "",
      "    # cur_batch_seq_len: the length of prompts",
      "    # cur_batch_ctx_len: the length of prefix",
      "    # cur_batch_in_all_start_index: the start id of the dim=0",
      "    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)",
      "    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)",
      "    cur_batch_in_all_stop_index = tl.load(B_Start_Loc + cur_batch + 1)",
      "    cur_batch_query_len = (cur_batch_in_all_stop_index -",
      "                           cur_batch_in_all_start_index)",
      "    cur_batch_ctx_len = cur_batch_seq_len - cur_batch_query_len",
      "",
      "    if SKIP_DECODE and cur_batch_query_len == 1:",
      "        return",
      "",
      "    block_start_loc = BLOCK_M * start_m",
      "",
      "    # initialize offsets",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "    offs_d = tl.arange(0, BLOCK_DMODEL_PADDED)",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    off_q = ((cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs +",
      "             cur_head * stride_qh + offs_d[None, :] * stride_qd)",
      "",
      "    dim_mask = tl.where(",
      "        tl.arange(0, BLOCK_DMODEL_PADDED) < BLOCK_DMODEL, 1, 0).to(tl.int1)",
      "",
      "    q = tl.load(Q + off_q,",
      "                mask=dim_mask[None, :] &",
      "                (offs_m[:, None] < cur_batch_seq_len - cur_batch_ctx_len),",
      "                other=0.0)",
      "",
      "    # # initialize pointer to m and l",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)",
      "    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL_PADDED], dtype=tl.float32)",
      "",
      "    alibi_slope = tl.load(Alibi_slopes + cur_head)",
      "    alibi_start_q = tl.arange(0, BLOCK_M) + block_start_loc + cur_batch_ctx_len",
      "    alibi_start_k = 0",
      "    for start_n in range(0, cur_batch_ctx_len, BLOCK_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "        # -- compute qk ----",
      "        bn = tl.load(B_Loc + cur_batch * stride_b_loc_b +",
      "                     ((start_n + offs_n) // block_size) * stride_b_loc_s,",
      "                     mask=(start_n + offs_n) < cur_batch_ctx_len,",
      "                     other=0).to(tl.int64)",
      "        off_k = (",
      "            bn[None, :] * stride_k_cache_bs + cur_kv_head * stride_k_cache_h +",
      "            (offs_d[:, None] // x) * stride_k_cache_d +",
      "            ((start_n + offs_n[None, :]) % block_size) * stride_k_cache_bl +",
      "            (offs_d[:, None] % x) * stride_k_cache_x)",
      "        off_v = (bn[:, None] * stride_v_cache_bs +",
      "                 cur_kv_head * stride_v_cache_h +",
      "                 offs_d[None, :] * stride_v_cache_d +",
      "                 (start_n + offs_n[:, None]) % block_size * stride_v_cache_bl)",
      "        k_load = tl.load(K_cache + off_k,",
      "                         mask=dim_mask[:, None] &",
      "                         ((start_n + offs_n[None, :]) < cur_batch_ctx_len),",
      "                         other=0.0)  # [D,N]",
      "",
      "        if k_load.dtype.is_fp8():",
      "            k = (k_load.to(tl.float32) * tl.load(k_scale)).to(q.dtype)",
      "        else:",
      "            k = k_load",
      "",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk = tl.dot(q, k, acc=qk, input_precision=IN_PRECISION)",
      "        qk = tl.where((start_n + offs_n[None, :]) < cur_batch_ctx_len, qk,",
      "                      float(\"-inf\"))",
      "        qk *= sm_scale",
      "",
      "        # load alibi",
      "        alibi = (tl.arange(0, BLOCK_N)[None, :] + alibi_start_k -",
      "                 alibi_start_q[:, None]) * alibi_slope",
      "        alibi = tl.where(",
      "            (alibi <= 0) & (alibi_start_q[:, None] < cur_batch_seq_len), alibi,",
      "            float(\"-inf\"))",
      "        qk += alibi",
      "        alibi_start_k += BLOCK_N",
      "",
      "        # -- compute m_ij, p, l_ij",
      "        m_ij = tl.max(qk, 1)",
      "        m_i_new = tl.maximum(m_i, m_ij)",
      "        p = tl.math.exp(qk - m_i_new[:, None])",
      "        l_ij = tl.sum(p, 1)",
      "        # -- update m_i and l_i",
      "",
      "        alpha = tl.math.exp(m_i - m_i_new)",
      "        l_i_new = alpha * l_i + l_ij",
      "        # -- update output accumulator --",
      "        # scale p",
      "        # scale acc",
      "        acc_scale = alpha",
      "        # acc_scale = l_i / l_i_new * alpha",
      "        acc = acc * acc_scale[:, None]",
      "        # update acc",
      "        v_load = tl.load(V_cache + off_v,",
      "                         mask=dim_mask[None, :] &",
      "                         ((start_n + offs_n[:, None]) < cur_batch_ctx_len),",
      "                         other=0.0)",
      "        if v_load.dtype.is_fp8():",
      "            v = (v_load.to(tl.float32) * tl.load(v_scale)).to(q.dtype)",
      "        else:",
      "            v = v_load",
      "        p = p.to(v.dtype)",
      "",
      "        acc = tl.dot(p, v, acc=acc, input_precision='ieee')",
      "        # update m_i and l_i",
      "        l_i = l_i_new",
      "        m_i = m_i_new",
      "",
      "    off_k = (offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh +",
      "             offs_d[:, None] * stride_kd)",
      "    off_v = (offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh +",
      "             offs_d[None, :] * stride_vd)",
      "    k_ptrs = K + off_k",
      "    v_ptrs = V + off_v",
      "",
      "    block_mask = tl.where(",
      "        block_start_loc < cur_batch_seq_len - cur_batch_ctx_len, 1, 0)",
      "",
      "    # init alibi",
      "    alibi_slope = tl.load(Alibi_slopes + cur_head)",
      "    alibi_start_q = tl.arange(0, BLOCK_M) + block_start_loc + cur_batch_ctx_len",
      "    alibi_start_k = cur_batch_ctx_len",
      "    # # init debugger",
      "    # offset_db_q = tl.arange(0, BLOCK_M) + block_start_loc",
      "    # offset_db_k = tl.arange(0, BLOCK_N)",
      "    # calc q[BLOCK_M, BLOCK_MODEL] mul k[prefix_len: , BLOCK_DMODEL]",
      "    for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "        # -- compute qk ----",
      "        k = tl.load(",
      "            k_ptrs + (cur_batch_in_all_start_index + start_n) * stride_kbs,",
      "            mask=dim_mask[:, None] & ((start_n + offs_n[None, :])",
      "                                      < cur_batch_seq_len - cur_batch_ctx_len),",
      "            other=0.0)",
      "",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk = tl.dot(q, k, acc=qk, input_precision='ieee')",
      "        qk *= sm_scale",
      "        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk,",
      "                      float(\"-inf\"))",
      "",
      "        # load alibi",
      "        alibi = (tl.arange(0, BLOCK_N)[None, :] + alibi_start_k -",
      "                 alibi_start_q[:, None]) * alibi_slope",
      "        alibi = tl.where(",
      "            (alibi <= 0) & (alibi_start_q[:, None] < cur_batch_seq_len), alibi,",
      "            float(\"-inf\"))",
      "        qk += alibi",
      "        alibi_start_k += BLOCK_N",
      "",
      "        # -- compute m_ij, p, l_ij",
      "        m_ij = tl.max(qk, 1)",
      "        m_i_new = tl.maximum(m_i, m_ij)",
      "        p = tl.math.exp(qk - m_i_new[:, None])",
      "        l_ij = tl.sum(p, 1)",
      "        # -- update m_i and l_i",
      "",
      "        alpha = tl.math.exp(m_i - m_i_new)",
      "        l_i_new = alpha * l_i + l_ij",
      "        # -- update output accumulator --",
      "        # scale p",
      "        # scale acc",
      "        acc_scale = alpha",
      "        # acc_scale = l_i / l_i_new * alpha",
      "        acc = acc * acc_scale[:, None]",
      "        # update acc",
      "        v = tl.load(",
      "            v_ptrs + (cur_batch_in_all_start_index + start_n) * stride_vbs,",
      "            mask=dim_mask[None, :] & ((start_n + offs_n[:, None])",
      "                                      < cur_batch_seq_len - cur_batch_ctx_len),",
      "            other=0.0)",
      "        p = p.to(v.dtype)",
      "",
      "        acc = tl.dot(p, v, acc=acc, input_precision='ieee')",
      "        # update m_i and l_i",
      "        l_i = l_i_new",
      "        m_i = m_i_new",
      "",
      "    acc = acc / l_i[:, None]",
      "",
      "    # initialize pointers to output",
      "    off_o = ((cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs +",
      "             cur_head * stride_oh + offs_d[None, :] * stride_od)",
      "    out_ptrs = Out + off_o",
      "    tl.store(out_ptrs,",
      "             acc,",
      "             mask=dim_mask[None, :] &",
      "             (offs_m[:, None] < cur_batch_seq_len - cur_batch_ctx_len))",
      "    return",
      "",
      "",
      "@torch.inference_mode()",
      "def context_attention_fwd(q,",
      "                          k,",
      "                          v,",
      "                          o,",
      "                          kv_cache_dtype: str,",
      "                          k_cache,",
      "                          v_cache,",
      "                          b_loc,",
      "                          b_start_loc,",
      "                          b_seq_len,",
      "                          max_seq_len,",
      "                          max_input_len,",
      "                          k_scale: torch.Tensor,",
      "                          v_scale: torch.Tensor,",
      "                          alibi_slopes=None,",
      "                          sliding_window=None,",
      "                          sm_scale=None,",
      "                          skip_decode=False,",
      "                          sinks=None):",
      "",
      "    q_dtype_is_f32 = q.dtype is torch.float32",
      "",
      "    # Turing does have tensor core for float32 multiplication",
      "    # use ieee as fallback for triton kernels work. There is also",
      "    # warning on vllm/config.py to inform users this fallback",
      "    # implementation",
      "    IN_PRECISION = 'ieee' if IS_TURING and q_dtype_is_f32 else None",
      "",
      "    # Conversion of FP8 Tensor from uint8 storage to",
      "    # appropriate torch.dtype for interpretation by Triton",
      "    if \"fp8\" in kv_cache_dtype:",
      "        assert k_cache.dtype in [torch.uint8, current_platform.fp8_dtype()]",
      "        assert v_cache.dtype in [torch.uint8, current_platform.fp8_dtype()]",
      "",
      "        if kv_cache_dtype in (\"fp8\", \"fp8_e4m3\"):",
      "            target_dtype = current_platform.fp8_dtype()",
      "        elif kv_cache_dtype == \"fp8_e5m2\":",
      "            target_dtype = torch.float8_e5m2",
      "        else:",
      "            raise ValueError(\"Unsupported FP8 dtype:\", kv_cache_dtype)",
      "",
      "        k_cache = k_cache.view(target_dtype)",
      "        v_cache = v_cache.view(target_dtype)",
      "",
      "    if (k_cache.dtype == torch.uint8",
      "            or v_cache.dtype == torch.uint8 and kv_cache_dtype == \"auto\"):",
      "        raise ValueError(\"kv_cache_dtype='auto' unsupported for\\",
      "            FP8 KV Cache prefill kernel\")",
      "",
      "    # shape constraints",
      "    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]",
      "    assert Lq == Lk and Lk == Lv",
      "    # round up Lk to a power of 2 - this is required for Triton block size",
      "    Lk_padded = triton.next_power_of_2(Lk)",
      "",
      "    if sm_scale is None:",
      "        sm_scale = 1.0 / (Lq**0.5)",
      "    batch, head = b_seq_len.shape[0], q.shape[1]",
      "    num_queries_per_kv = q.shape[1] // k.shape[1]",
      "",
      "    assert batch + 1 == len(b_start_loc)",
      "",
      "    # 0 means \"disable\"",
      "    if sliding_window is None or sliding_window <= 0:",
      "        sliding_window = 0",
      "",
      "    if alibi_slopes is not None:",
      "        assert sinks is None, \"Sinks arg is not supported with alibi\"",
      "        # need to reduce num. blocks when using fp32",
      "        # due to increased use of GPU shared memory",
      "        # if q.dtype is torch.float32:",
      "        BLOCK = BASE_BLOCK // 2 if q_dtype_is_f32 else BASE_BLOCK",
      "        # batch, head,",
      "        grid = (batch, head, triton.cdiv(max_input_len, BLOCK))",
      "        _fwd_kernel_alibi[grid](",
      "            q,",
      "            k,",
      "            v,",
      "            k_cache,",
      "            v_cache,",
      "            b_loc,",
      "            sm_scale,",
      "            k_scale,",
      "            v_scale,",
      "            b_start_loc,",
      "            b_seq_len,",
      "            alibi_slopes,",
      "            v_cache.shape[3],",
      "            k_cache.shape[4],",
      "            o,",
      "            b_loc.stride(0),",
      "            b_loc.stride(1),",
      "            q.stride(0),",
      "            q.stride(1),",
      "            q.stride(2),",
      "            k.stride(0),",
      "            k.stride(1),",
      "            k.stride(2),",
      "            v.stride(0),",
      "            v.stride(1),",
      "            v.stride(2),",
      "            o.stride(0),",
      "            o.stride(1),",
      "            o.stride(2),",
      "            k_cache.stride(0),",
      "            k_cache.stride(1),",
      "            k_cache.stride(2),",
      "            k_cache.stride(3),",
      "            k_cache.stride(",
      "                4),  #[num_blocks, num_kv_heads, head_size/x, block_size, x]",
      "            v_cache.stride(0),",
      "            v_cache.stride(1),",
      "            v_cache.stride(2),",
      "            v_cache.stride(",
      "                3),  #[num_blocks, num_kv_heads, head_size, block_size]",
      "            num_queries_per_kv=num_queries_per_kv,",
      "            IN_PRECISION=IN_PRECISION,",
      "            BLOCK_M=BLOCK,",
      "            BLOCK_DMODEL=Lk,",
      "            BLOCK_DMODEL_PADDED=Lk_padded,",
      "            BLOCK_N=BLOCK,",
      "            SKIP_DECODE=skip_decode,",
      "            num_warps=NUM_WARPS,",
      "            num_stages=1,",
      "        )",
      "        return",
      "",
      "    max_seq_len = 0 if max_seq_len is None else max_seq_len",
      "    extra_kargs = {}",
      "    if current_platform.is_rocm():",
      "        extra_kargs = {\"kpack\": 1, \"waves_per_eu\": 2}",
      "",
      "    grid = lambda META: (batch, head,",
      "                         triton.cdiv(max_input_len, META[\"BLOCK_M\"]))",
      "    _fwd_kernel[grid](",
      "        q,",
      "        k,",
      "        v,",
      "        k_cache,",
      "        v_cache,",
      "        sinks,",
      "        b_loc,",
      "        sm_scale,",
      "        k_scale,",
      "        v_scale,",
      "        b_start_loc,",
      "        b_seq_len,",
      "        k_cache.shape[4],",
      "        o,",
      "        b_loc.stride(0),",
      "        b_loc.stride(1),",
      "        q.stride(0),",
      "        q.stride(1),",
      "        q.stride(2),",
      "        k.stride(0),",
      "        k.stride(1),",
      "        k.stride(2),",
      "        v.stride(0),",
      "        v.stride(1),",
      "        v.stride(2),",
      "        o.stride(0),",
      "        o.stride(1),",
      "        o.stride(2),",
      "        k_cache.stride(0),",
      "        k_cache.stride(1),",
      "        k_cache.stride(2),",
      "        k_cache.stride(3),",
      "        k_cache.stride(",
      "            4),  #[num_blocks, num_kv_heads, head_size/x, block_size, x]",
      "        v_cache.stride(0),",
      "        v_cache.stride(1),",
      "        v_cache.stride(2),",
      "        v_cache.stride(3),  #[num_blocks, num_kv_heads, head_size, block_size]",
      "        BLOCK_SIZE=v_cache.shape[3],",
      "        num_queries_per_kv=num_queries_per_kv,",
      "        IN_PRECISION=IN_PRECISION,",
      "        BLOCK_DMODEL=Lk,",
      "        BLOCK_DMODEL_PADDED=Lk_padded,",
      "        SLIDING_WINDOW=sliding_window,",
      "        SKIP_DECODE=skip_decode,",
      "        BLOCK_M=128,",
      "        BLOCK_N=64,",
      "        num_unroll_cache=4,",
      "        num_unroll_request=1,",
      "        num_warps=4,",
      "        num_stages=1,",
      "        USE_SINKS=sinks is not None,",
      "        **extra_kargs)",
      "    return"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/lora/ops/triton_ops/kernel_utils.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "\"\"\"",
      "Utilities for Punica kernel construction.",
      "\"\"\"",
      "from vllm.triton_utils import tl, triton",
      "",
      "",
      "@triton.jit",
      "def mm_k(a_ptr, b_ptr, ak_stride, bk_stride, offset_k, K: tl.constexpr,",
      "         BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,",
      "         EVEN_K: tl.constexpr, SPLIT_K: tl.constexpr, CAST_TYPE: tl.constexpr,",
      "         b_dtype: tl.constexpr):",
      "    \"\"\"",
      "    Given a_ptr and b_ptr, that identify the rows of A (m x k) and columns of",
      "    B (k x n), iterate, through the K dimension to compute the partial/complete",
      "    matrix block product.",
      "    If SPLIT_K == 1, the output m x n product is complete.",
      "    If SPLIT_K > 1, the thread block computes partial outputs. The partial",
      "    outputs are then atomically summed in the caller code. ",
      "    Args:",
      "        a_ptr: Array of pointers, identifying rows of A ",
      "        b_ptr: Array of pointers, identifying columns of B",
      "        ak_stride: K dimension stride of the A matrix",
      "        bk_stride: K dimension stride of the B matrix",
      "        K: Length of the K dimension",
      "        BLOCK_M: M dimension of the output block m x n",
      "        BLOCK_N: N dimension of the output block m x n",
      "        BLOCK_K: K dimension atom",
      "        EVEN_K: True if the blocks of A and B can be loaded without any",
      "          masking.",
      "        SPLIT_K: Parameter signifying parallelism in the K dimension. ",
      "        CAST_TYPE: if True, cast the values from the A matrix to the B",
      "          matrix dtype.",
      "        b_dtype: datatype of the B matrix",
      "    \"\"\"",
      "    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)",
      "    for k in range(tl.cdiv(K, BLOCK_K * SPLIT_K)):",
      "        if EVEN_K:",
      "            tiled_a = tl.load(a_ptr)",
      "            tiled_b = tl.load(b_ptr)",
      "        else:",
      "            tiled_a = tl.load(a_ptr,",
      "                              mask=offset_k[None, :]",
      "                              < K - k * (BLOCK_K * SPLIT_K),",
      "                              other=0)",
      "            tiled_b = tl.load(b_ptr,",
      "                              mask=offset_k[:, None]",
      "                              < K - k * (BLOCK_K * SPLIT_K),",
      "                              other=0)",
      "        if CAST_TYPE:",
      "            tiled_a = tiled_a.to(b_dtype)",
      "        accumulator += tl.dot(",
      "            tiled_a,",
      "            tiled_b,",
      "        )",
      "        a_ptr += BLOCK_K * SPLIT_K * ak_stride",
      "        b_ptr += BLOCK_K * SPLIT_K * bk_stride",
      "    return accumulator",
      "",
      "",
      "@triton.jit",
      "def do_expand_kernel(",
      "    pid_n,",
      "    lora_index,",
      "    slice_id,",
      "    input_ptr,",
      "    lora_ptr,",
      "    out_ptr,",
      "    N,",
      "    K,",
      "    M_LEN,",
      "    ram,  # array identifying the rows of Input ptr to operate on",
      "    slice_start_loc,",
      "    # input ptr strides",
      "    input_d0_stride,",
      "    input_d1_stride,",
      "    input_d2_stride,",
      "    # lora ptr strides",
      "    ls_d0_ptr,",
      "    ls_d1_ptr,",
      "    ls_d2_ptr,",
      "    # out ptr strides",
      "    output_d0_stride,",
      "    output_d1_stride,",
      "    # constants",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    SAME_STRIDE: tl.constexpr,",
      "    SLICE_NUM: tl.constexpr,",
      "    EVEN_K: tl.constexpr,",
      "    CAST_TYPE: tl.constexpr,",
      "    ADD_INPUTS: tl.constexpr,",
      "):",
      "    \"\"\"",
      "    Given an array of integers that identifies the rows of A, ram,",
      "    a lora index that identifies which LoRA to use from lora_ptr, lora_index,",
      "    a slice_id that identifies the input/output slice,",
      "    compute the matrix product and store in the appropriate output location.",
      "    Given that this is an expand kernel, we don't perform any split-K reduction",
      "    as the K dimension is assumed to be small.",
      "    \"\"\"",
      "",
      "    # ls_d*_ptr can be either an integer or a pointer",
      "    if SAME_STRIDE:",
      "        # integer",
      "        cur_lora_d0_stride = ls_d0_ptr",
      "        cur_lora_d1_stride = ls_d1_ptr",
      "        cur_lora_d2_stride = ls_d2_ptr",
      "    else:",
      "        # pointer",
      "        cur_lora_d0_stride = tl.load(ls_d0_ptr + slice_id)",
      "        cur_lora_d1_stride = tl.load(ls_d1_ptr + slice_id)",
      "        cur_lora_d2_stride = tl.load(ls_d2_ptr + slice_id)",
      "",
      "    # Identify the input_ptr and lora_ptr from slice_id.",
      "    if SLICE_NUM == 1:",
      "        cur_input_ptr = input_ptr",
      "        cur_lora_ptr = lora_ptr",
      "    else:",
      "        cur_input_ptr = input_ptr + slice_id * input_d0_stride",
      "        cur_lora_ptr = tl.load(lora_ptr + slice_id).to(",
      "            tl.pointer_type(out_ptr.dtype.element_ty))",
      "",
      "    # Identify the column indices of B to process.",
      "    offset_n = tl.arange(0, BLOCK_N) + pid_n * BLOCK_N",
      "    rbn = tl.max_contiguous(tl.multiple_of(offset_n % N, BLOCK_N), BLOCK_N)",
      "",
      "    # Identify A and B block pointers",
      "    offset_k = tl.arange(0, BLOCK_K)",
      "    a_ptr = (cur_input_ptr + ram[:, None] * input_d1_stride +",
      "             offset_k[None, :] * input_d2_stride)",
      "    b_ptr = (cur_lora_ptr + cur_lora_d0_stride * lora_index +",
      "             offset_k[:, None] * cur_lora_d2_stride +",
      "             rbn[None, :] * cur_lora_d1_stride)",
      "",
      "    # Compute the block matrix product.",
      "    SPLIT_K = 1",
      "    accumulator = mm_k(a_ptr, b_ptr, input_d2_stride, cur_lora_d2_stride,",
      "                       offset_k, K, BLOCK_M, BLOCK_N, BLOCK_K, EVEN_K, SPLIT_K,",
      "                       CAST_TYPE, cur_lora_ptr.dtype.element_ty)",
      "",
      "    tiled_c = accumulator.to(cur_lora_ptr.dtype.element_ty)",
      "    if SLICE_NUM == 1:",
      "        cur_slice_start = slice_start_loc",
      "    else:",
      "        cur_slice_start = tl.load(slice_start_loc + slice_id)",
      "",
      "    # Identify the C output pointers to store the results of the accumulator.",
      "    offset_cn = tl.arange(0, BLOCK_N) + pid_n * BLOCK_N + cur_slice_start",
      "    offset_cm = tl.arange(0, BLOCK_M)",
      "    c_ptr = (out_ptr + ram[:, None] * output_d0_stride +",
      "             offset_cn[None, :] * output_d1_stride)",
      "    c_mask = (offset_cm[:, None] < M_LEN) & (offset_cn[None, :]",
      "                                             < (cur_slice_start + N))",
      "",
      "    if ADD_INPUTS:",
      "        tiled_out = tl.load(c_ptr, mask=c_mask)",
      "        tiled_c += tiled_out",
      "    tl.store(c_ptr, tiled_c, mask=c_mask)",
      "",
      "",
      "@triton.jit",
      "def do_shrink_kernel(",
      "    pid_n,",
      "    pid_sk,",
      "    slice_id,",
      "    lora_index,",
      "    input_ptr,",
      "    lora_ptr,",
      "    out_ptr,",
      "    N,",
      "    K,",
      "    M_LEN,",
      "    ram,",
      "    # input strides",
      "    input_d0_stride,",
      "    input_d1_stride,",
      "    # lora strides",
      "    lora_d0_stride,",
      "    lora_d1_stride,",
      "    lora_d2_stride,",
      "    # output strides",
      "    output_d0_stride,",
      "    output_d1_stride,",
      "    output_d2_stride,",
      "    scaling,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    EVEN_K: tl.constexpr,",
      "    SPLIT_K: tl.constexpr,",
      "    SLICE_NUM: tl.constexpr,",
      "):",
      "    \"\"\"",
      "    Given an array of integers that identifies the rows of A, ram,",
      "    a lora index that identifies which LoRA to use from lora_ptr, lora_index,",
      "    a slice_id that identifies the input/output slice, compute the",
      "    matrix product and store in the appropriate output location.",
      "    \"\"\"",
      "",
      "    # Identify the lora_ptr from slice_id.",
      "    if SLICE_NUM == 1:",
      "        # current lora ptr",
      "        cur_lora_ptr = lora_ptr",
      "    else:",
      "        # current lora ptr",
      "        cur_lora_ptr = tl.load(lora_ptr + slice_id).to(",
      "            tl.pointer_type(input_ptr.dtype.element_ty))",
      "",
      "    # Identify the column indices of B to process.",
      "    offset_n = tl.arange(0, BLOCK_N) + pid_n * BLOCK_N",
      "    rbn = tl.max_contiguous(tl.multiple_of(offset_n % N, BLOCK_N), BLOCK_N)",
      "",
      "    # Identify A and B block pointers",
      "    offset_k = pid_sk * BLOCK_K + tl.arange(0, BLOCK_K)",
      "    a_ptr = (input_ptr + ram[:, None] * input_d0_stride +",
      "             offset_k[None, :] * input_d1_stride)",
      "    b_ptr = (cur_lora_ptr + lora_d0_stride * lora_index +",
      "             rbn[None, :] * lora_d1_stride +",
      "             offset_k[:, None] * lora_d2_stride)",
      "",
      "    # Compute partial/complete block matrix product.",
      "    accumulator = mm_k(a_ptr, b_ptr, input_d1_stride, lora_d2_stride, offset_k,",
      "                       K, BLOCK_M, BLOCK_N, BLOCK_K, EVEN_K, SPLIT_K, False,",
      "                       cur_lora_ptr.dtype.element_ty)",
      "",
      "    # Identify the C output pointers to store the results of the accumulator.",
      "    offset_cn = tl.arange(0, BLOCK_N) + pid_n * BLOCK_N",
      "    offset_cm = tl.arange(0, BLOCK_M)",
      "    cur_out_ptr = (out_ptr if SLICE_NUM == 1 else out_ptr +",
      "                   slice_id * output_d0_stride)",
      "    c_ptr = cur_out_ptr + ram[:, None] * output_d1_stride + offset_cn[",
      "        None, :] * output_d2_stride",
      "    c_mask = (offset_cm[:, None] < M_LEN) & (offset_cn[None, :] < N)",
      "",
      "    accumulator *= scaling",
      "    # handles write-back with reduction-splitting",
      "    if SPLIT_K == 1:",
      "        tl.store(c_ptr, accumulator, mask=c_mask)",
      "    else:",
      "        tl.atomic_add(c_ptr, accumulator, mask=c_mask)"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/lora/ops/triton_ops/lora_expand_op.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "\"\"\"",
      "Based on:",
      "Chen, L., Ye, Z., Wu, Y., Zhuo, D., Ceze, L., & Krishnamurthy, A. (2023).",
      "Punica: Multi-Tenant LoRA Serving.",
      "https://arxiv.org/abs/2310.18547",
      "\"\"\"",
      "",
      "import torch",
      "",
      "from vllm.lora.ops.triton_ops.kernel_utils import do_expand_kernel",
      "from vllm.lora.ops.triton_ops.utils import _get_lora_b_ptr",
      "from vllm.platforms import current_platform",
      "from vllm.triton_utils import tl, triton",
      "from vllm.utils import direct_register_custom_op",
      "",
      "",
      "@triton.jit",
      "def _lora_expand_kernel(",
      "        input_ptr,",
      "        lora_ptr,",
      "        out_ptr,",
      "        M,",
      "        N,",
      "        K,",
      "        token_indices_sorted_by_lora_ids,",
      "        num_tokens_per_lora,",
      "        lora_token_start_loc,",
      "        lora_ids,",
      "        slice_start_loc,",
      "        input_d0_stride,",
      "        input_d1_stride,",
      "        input_d2_stride,  # 1",
      "        ls_d0_ptr,",
      "        ls_d1_ptr,",
      "        ls_d2_ptr,  # 1",
      "        output_d0_stride,",
      "        output_d1_stride,  # 1",
      "        output_hs_ptr,",
      "        BLOCK_M: tl.constexpr,",
      "        BLOCK_N: tl.constexpr,",
      "        BLOCK_K: tl.constexpr,",
      "        EVEN_K: tl.constexpr,",
      "        ADD_INPUTS: tl.constexpr,",
      "        CAST_TYPE: tl.constexpr,",
      "        SLICE_NUM: tl.constexpr,",
      "        SAME_STRIDE: tl.constexpr):",
      "",
      "    cta_n_num = tl.cdiv(N, BLOCK_N)",
      "    cta_m_num = tl.cdiv(M, BLOCK_M)",
      "",
      "    pid_mn = tl.program_id(axis=0)",
      "    pid_m = pid_mn % cta_m_num",
      "    pid_n = (pid_mn // cta_m_num) % cta_n_num",
      "",
      "    slice_id = tl.program_id(axis=1)",
      "    lora_idx = tl.program_id(axis=2)",
      "",
      "    lora_id = tl.load(lora_ids + lora_idx)",
      "    if lora_id == -1:",
      "        # Early exit for the no-lora case.",
      "        return",
      "",
      "    lora_m_size = tl.load(num_tokens_per_lora + lora_idx)",
      "",
      "    cta_m_offset = pid_m * BLOCK_M",
      "    if cta_m_offset >= lora_m_size:",
      "        # Early exit CTA.",
      "        return",
      "",
      "    # When the output dimensions of each slice are the same,cur_n=N, otherwise",
      "    # cur_n=tl.load(output_hs_ptr + slice_id), this situation exists in GQA's",
      "    # qkv linear.",
      "    curr_N = N if SAME_STRIDE else tl.load(output_hs_ptr + slice_id)",
      "    if pid_n * BLOCK_N >= curr_N:",
      "        # Early exit CTA.",
      "        return",
      "",
      "    # num rows this CTA should process.",
      "    cta_m_len = min(BLOCK_M, lora_m_size - cta_m_offset)",
      "",
      "    # Identify all rows that this CTA should process.",
      "    lora_m_indices_start = tl.load(lora_token_start_loc + lora_idx)",
      "    cta_lora_seq_indices = (token_indices_sorted_by_lora_ids +",
      "                            lora_m_indices_start + cta_m_offset)",
      "",
      "    # Load all relevant row indices.",
      "    offset_m = tl.arange(0, BLOCK_M) % cta_m_len",
      "    ram = tl.load(cta_lora_seq_indices + offset_m)",
      "",
      "    do_expand_kernel(",
      "        pid_n,",
      "        lora_id,",
      "        slice_id,",
      "        input_ptr,",
      "        lora_ptr,",
      "        out_ptr,",
      "        curr_N,",
      "        K,",
      "        cta_m_len,",
      "        ram,  # array identifying the rows of Input ptr to operate on",
      "        slice_start_loc,",
      "        # input ptr strides",
      "        input_d0_stride,",
      "        input_d1_stride,",
      "        input_d2_stride,",
      "        # lora ptr strides",
      "        ls_d0_ptr,",
      "        ls_d1_ptr,",
      "        ls_d2_ptr,",
      "        # out ptr strides",
      "        output_d0_stride,",
      "        output_d1_stride,",
      "        # constants",
      "        BLOCK_M,",
      "        BLOCK_N,",
      "        BLOCK_K,",
      "        SAME_STRIDE,",
      "        SLICE_NUM,",
      "        EVEN_K,",
      "        CAST_TYPE,",
      "        ADD_INPUTS)",
      "",
      "",
      "@torch.inference_mode()",
      "def _lora_expand(",
      "    inputs: torch.Tensor,  # shape [num_slices, num_tokens, lora_rank]",
      "    lora_b_weights: list[",
      "        torch.Tensor],  # shape [num_lora, hidden_size, lora_rank]",
      "    output_tensor: torch.",
      "    Tensor,  # shape [num_tokens, hidden_size * num_slices]",
      "    token_lora_mapping: torch.Tensor,  # shape [num_tokens]",
      "    token_indices_sorted_by_lora_ids: torch.Tensor,  # shape [num_tokens]",
      "    num_tokens_per_lora: torch.Tensor,  # shape [max-loras + 1]",
      "    lora_token_start_loc: torch.Tensor,  # shape [max-loras + 2]",
      "    lora_ids: torch.Tensor,  # shape [max-loras + 1]",
      "    no_lora_flag_cpu: torch.Tensor,  # shape [1] ",
      "    offset_start: int = 0,",
      "    add_inputs: bool = False,",
      ") -> None:",
      "    \"\"\"",
      "    Args:",
      "        inputs (torch.Tensor): input tensor",
      "        lora_b_weights (list[torch.Tensor]): lora'b weight",
      "        output_tensor (torch.Tensor): output tensor",
      "        token_lora_mapping (torch.Tensor): A tensor mapping each input token",
      "            to the lora-id related to that token. A value of -1 indicates that",
      "            LoRA doesn't apply to that token.",
      "        token_indices_sorted_by_lora_ids (torch.Tensor): Row/Token indices from",
      "            the A matrix grouped by LoRA IDs.",
      "        num_tokens_per_lora (torch.Tensor): num_tokens_per_lora[i] is the number",
      "            of tokens that are to be processed by LoRA ID lora_ids[i] ",
      "        lora_token_start_loc (torch.Tensor): A cumulative sum of",
      "            num_tokens_per_lora. lora_token_start_loc[0] is always 0 so that",
      "            lora_token_start_loc[i], along with num_tokens_per_lora[i]",
      "            identifies the region in token_indices_sorted_by_lora_ids that",
      "            LoRA lora_ids[i] should process.",
      "        lora_ids (torch.Tensor): LoRA ids to process.",
      "        no_lora_flag_cpu (torch.Tensor): A CPU tensor of size 1, that indicates",
      "            if there are any requests that require LoRA.",
      "        offset_start (int, optional): Offset start for output_tensor. ",
      "            Defaults to 0.",
      "        add_inputs (bool, optional): Whether to add the input tensor to the ",
      "            output tensor. Defaults to False.",
      "    \"\"\"",
      "",
      "    assert no_lora_flag_cpu.numel() == 1",
      "    if no_lora_flag_cpu.item():",
      "        # None of the inputs require LoRA.",
      "        return",
      "",
      "    assert inputs.dtype in [torch.float16, torch.bfloat16, torch.float32]",
      "    for weight in lora_b_weights:",
      "        assert weight.dtype in [torch.float16, torch.bfloat16]",
      "",
      "    assert inputs.size(0) == len(lora_b_weights)",
      "    assert output_tensor.is_contiguous()",
      "",
      "    # metadata sanity check.",
      "    M = inputs.size(1)",
      "    assert token_lora_mapping.size(0) == M",
      "    assert token_lora_mapping.size(0) == token_indices_sorted_by_lora_ids.size(",
      "        0)",
      "    assert lora_ids.size(0) == num_tokens_per_lora.size(0)",
      "    assert lora_token_start_loc.size(0) == lora_ids.size(0) + 1",
      "",
      "    (slice_start_tensor, lora_ptr_tensor, lora_strides_d0_tensor,",
      "     lora_strides_d1_tensor, lora_strides_d2_tensor, hidden_sizes_tensor,",
      "     same_stride, MAX_N) = _get_lora_b_ptr(lora_b_weights, offset_start,",
      "                                           inputs.device)",
      "",
      "    K = lora_b_weights[0].shape[-1]  # K= rank",
      "    ADD_INPUTS = add_inputs",
      "    MAX_LORAS = lora_ids.size(0)",
      "    CAST_TYPE = False",
      "    NUM_SLICES = len(lora_b_weights)",
      "",
      "    # Triton kernel configs.",
      "    BLOCK_M = 64",
      "    BLOCK_N = 128",
      "    BLOCK_K = 16",
      "    NUM_WARPS = 4",
      "    NUM_CTAS = 1",
      "    NUM_STAGES = 2",
      "",
      "    EVEN_K = K % BLOCK_K == 0  # type: ignore",
      "",
      "    if inputs.dtype == torch.float32 and lora_b_weights[0].dtype in [",
      "            torch.float16,",
      "            torch.bfloat16,",
      "    ]:",
      "        CAST_TYPE = True",
      "",
      "    # TODO (varun): This grid formulation maximizes parallelization at the",
      "    # cost of wasteful thread block launch when only a few input tokens require",
      "    # LoRA. This might not be the best in all cases.",
      "    grid = (",
      "        triton.cdiv(M, BLOCK_M) * triton.cdiv(MAX_N, BLOCK_N),",
      "        NUM_SLICES,",
      "        # Each LoRA receives its own set of thread blocks for output",
      "        # computation. If some LoRA doesn't have any tokens to process, its",
      "        # thread blocks simply exit.",
      "        MAX_LORAS,",
      "    )",
      "",
      "    _lora_expand_kernel[grid](",
      "        inputs,",
      "        lora_ptr_tensor,",
      "        output_tensor,",
      "        M,",
      "        MAX_N,",
      "        K,",
      "        token_indices_sorted_by_lora_ids,",
      "        num_tokens_per_lora,",
      "        lora_token_start_loc,",
      "        lora_ids,",
      "        slice_start_tensor,",
      "        inputs.stride(0),",
      "        inputs.stride(1),",
      "        inputs.stride(2),",
      "        lora_strides_d0_tensor,",
      "        lora_strides_d1_tensor,",
      "        lora_strides_d2_tensor,",
      "        output_tensor.stride(0),",
      "        output_tensor.stride(1),",
      "        hidden_sizes_tensor,",
      "        BLOCK_M,",
      "        BLOCK_N,",
      "        BLOCK_K,",
      "        EVEN_K,",
      "        ADD_INPUTS,",
      "        CAST_TYPE,",
      "        NUM_SLICES,",
      "        same_stride,",
      "        num_warps=NUM_WARPS,",
      "        num_ctas=NUM_CTAS,",
      "        num_stages=NUM_STAGES,",
      "    )",
      "",
      "    return",
      "",
      "",
      "def _lora_expand_fake(",
      "    inputs: torch.Tensor,",
      "    lora_b_weights: list[torch.Tensor],",
      "    output_tensor: torch.Tensor,",
      "    token_lora_mapping: torch.Tensor,",
      "    token_indices_sorted_by_lora_ids: torch.Tensor,",
      "    num_tokens_per_lora: torch.Tensor,",
      "    lora_token_start_loc: torch.Tensor,",
      "    lora_ids: torch.Tensor,",
      "    no_lora_flag_cpu: torch.Tensor,",
      "    offset_start: int = 0,",
      "    add_inputs: bool = False,",
      ") -> None:",
      "    return",
      "",
      "",
      "try:",
      "    direct_register_custom_op(",
      "        op_name=\"lora_expand\",",
      "        op_func=_lora_expand,",
      "        mutates_args=[\"output_tensor\"],",
      "        fake_impl=_lora_expand_fake,",
      "        dispatch_key=current_platform.dispatch_key,",
      "    )",
      "    lora_expand = torch.ops.vllm.lora_expand",
      "",
      "except AttributeError:",
      "    lora_expand = _lora_expand"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/lora/ops/triton_ops/lora_shrink_op.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "\"\"\"",
      "Based on:",
      "Chen, L., Ye, Z., Wu, Y., Zhuo, D., Ceze, L., & Krishnamurthy, A. (2023). ",
      "Punica: Multi-Tenant LoRA Serving. ",
      "https://arxiv.org/abs/2310.18547",
      "\"\"\"",
      "",
      "import torch",
      "",
      "from vllm.lora.ops.triton_ops.kernel_utils import do_shrink_kernel",
      "from vllm.lora.ops.triton_ops.utils import _get_lora_a_ptr",
      "from vllm.platforms import current_platform",
      "from vllm.triton_utils import tl, triton",
      "from vllm.utils import direct_register_custom_op",
      "",
      "",
      "@triton.jit",
      "def _lora_shrink_kernel(input_ptr, lora_ptr, out_ptr, M, N, K,",
      "                        token_indices_sorted_by_lora_ids, num_tokens_per_lora,",
      "                        lora_token_start_loc, lora_ids, scaling,",
      "                        input_d0_stride, input_d1_stride, lora_d0_stride,",
      "                        lora_d1_stride, lora_d2_stride, output_d0_stride,",
      "                        output_d1_stride, output_d2_stride,",
      "                        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,",
      "                        BLOCK_K: tl.constexpr, EVEN_K: tl.constexpr,",
      "                        SPLIT_K: tl.constexpr, SLICE_NUM: tl.constexpr):",
      "",
      "    cta_n_num = tl.cdiv(N, BLOCK_N)",
      "    cta_m_num = tl.cdiv(M, BLOCK_M)",
      "",
      "    pid_sk_m_n = tl.program_id(axis=0)",
      "    pid_sk = pid_sk_m_n % SPLIT_K",
      "    pid_m = (pid_sk_m_n // SPLIT_K) % cta_m_num",
      "    pid_n = pid_sk_m_n // (SPLIT_K * cta_m_num) % cta_n_num",
      "",
      "    slice_id = tl.program_id(axis=1)",
      "    lora_idx = tl.program_id(axis=2)",
      "",
      "    lora_id = tl.load(lora_ids + lora_idx)",
      "    if lora_id == -1:",
      "        # Early exit for the no-lora case.",
      "        return",
      "",
      "    lora_m_size = tl.load(num_tokens_per_lora + lora_idx)",
      "",
      "    cta_m_offset = pid_m * BLOCK_M",
      "    if cta_m_offset >= lora_m_size:",
      "        # Early exit CTA.",
      "        return",
      "",
      "    # num rows this CTA should process.",
      "    cta_m_len = min(BLOCK_M, lora_m_size - cta_m_offset)",
      "",
      "    # Identify all rows that this CTA should process.",
      "    lora_m_indices_start = tl.load(lora_token_start_loc + lora_idx)",
      "    cta_lora_seq_indices = (token_indices_sorted_by_lora_ids +",
      "                            lora_m_indices_start + cta_m_offset)",
      "",
      "    # Load all relevant row indices.",
      "    offset_m = tl.arange(0, BLOCK_M) % cta_m_len",
      "    ram = tl.load(cta_lora_seq_indices + offset_m)",
      "",
      "    do_shrink_kernel(",
      "        pid_n,",
      "        pid_sk,",
      "        slice_id,",
      "        lora_id,",
      "        input_ptr,",
      "        lora_ptr,",
      "        out_ptr,",
      "        N,",
      "        K,",
      "        cta_m_len,",
      "        ram,  # array identifying the rows of Input ptr to operate on",
      "        # input strides",
      "        input_d0_stride,",
      "        input_d1_stride,",
      "        # lora strides",
      "        lora_d0_stride,",
      "        lora_d1_stride,",
      "        lora_d2_stride,",
      "        # output strides",
      "        output_d0_stride,",
      "        output_d1_stride,",
      "        output_d2_stride,",
      "        scaling,",
      "        BLOCK_M,",
      "        BLOCK_N,",
      "        BLOCK_K,",
      "        EVEN_K,",
      "        SPLIT_K,",
      "        SLICE_NUM)",
      "",
      "",
      "@torch.inference_mode()",
      "def _lora_shrink(",
      "    inputs: torch.Tensor,  #  shape [num_tokens, hidden_size]",
      "    lora_a_weights: list[",
      "        torch.Tensor],  # shape [num_loras, lora_rank, hidden_size]",
      "    output_tensor: torch.Tensor,  # shape [num_slices, num_tokens, lora_rank]",
      "    token_lora_mapping: torch.Tensor,  # shape [num_tokens]",
      "    token_indices_sorted_by_lora_ids: torch.Tensor,  # shape [num_tokens] ",
      "    num_tokens_per_lora: torch.Tensor,  # shape [max-loras + 1]",
      "    lora_token_start_loc: torch.Tensor,  # shape [max-loras + 2]",
      "    lora_ids: torch.Tensor,  # shape [max-loras + 1]",
      "    no_lora_flag_cpu: torch.Tensor,  # shape [1]",
      "    scaling: float,",
      ") -> None:",
      "    \"\"\"",
      "    Args:",
      "        inputs (torch.Tensor): Input tensor",
      "        lora_a_weights (list[torch.Tensor]): LoRA weights",
      "        output_tensor (torch.Tensor): output tensor",
      "        token_lora_mapping (torch.Tensor): A tensor mapping each input token",
      "            to the lora-id related to that token. A value of -1 indicates that",
      "            LoRA doesn't apply to that token.",
      "        token_indices_sorted_by_lora_ids (torch.Tensor): Row/Token indices from",
      "            the A matrix grouped by LoRA IDs.",
      "        num_tokens_per_lora (torch.Tensor): num_tokens_per_lora[i] is the number",
      "            of tokens that are to be processed by LoRA ID lora_ids[i] ",
      "        lora_token_start_loc (torch.Tensor): A cumulative sum of",
      "            num_tokens_per_lora. lora_token_start_loc[0] is always 0 so that",
      "            lora_token_start_loc[i], along with num_tokens_per_lora[i]",
      "            identifies the region in token_indices_sorted_by_lora_ids that",
      "            LoRA lora_ids[i] should process.",
      "        lora_ids (torch.Tensor): LoRA ids to process.",
      "        no_lora_flag_cpu (torch.Tensor): A CPU tensor of size 1, that indicates",
      "            if there are any requests that require LoRA.",
      "        scaling (float): Scaling factor.",
      "    \"\"\"",
      "",
      "    assert no_lora_flag_cpu.numel() == 1",
      "    if no_lora_flag_cpu.item():",
      "        # None of the inputs require LoRA.",
      "        return",
      "",
      "    assert inputs.dtype == lora_a_weights[0].dtype",
      "    assert inputs.dtype in [torch.float16, torch.bfloat16]",
      "    for weight in lora_a_weights:",
      "        assert weight.dtype in [torch.float16, torch.bfloat16]",
      "",
      "    assert inputs.size(1) == lora_a_weights[0].size(-1)",
      "    assert inputs.is_contiguous()",
      "    assert output_tensor.is_contiguous()",
      "",
      "    # metadata sanity check",
      "    M = inputs.size(0)",
      "    assert token_lora_mapping.size(0) == M",
      "    assert token_lora_mapping.size(0) == token_indices_sorted_by_lora_ids.size(",
      "        0)",
      "    assert lora_ids.size(0) == num_tokens_per_lora.size(0)",
      "    assert lora_token_start_loc.size(0) == lora_ids.size(0) + 1",
      "",
      "    (lora_ptr_tensor, lora_strides_d0, lora_strides_d1,",
      "     lora_strides_d2) = _get_lora_a_ptr(lora_a_weights, inputs.device)",
      "    N, K = lora_a_weights[0].shape[-2:]  # K=hidden_size,N=rank",
      "    NUM_SLICES = len(lora_a_weights)",
      "    MAX_LORAS = lora_ids.size(0)",
      "",
      "    # Triton kernel configs",
      "    BLOCK_M = 32",
      "    BLOCK_N = 16",
      "    BLOCK_K = 256 if M < 128 else 32",
      "    SPLIT_K = 64 if M < 128 else 8",
      "    NUM_WARPS = 4",
      "    NUM_CTAS = 1",
      "    NUM_STAGES = 2",
      "",
      "    EVEN_K = K % (BLOCK_K * SPLIT_K) == 0  # type: ignore",
      "",
      "    # TODO (varun): This grid formulation maximizes parallelization at the",
      "    # cost of wasteful thread block launch when only few of the input tokens",
      "    # require LoRA. This might not be the best in all cases.",
      "    grid = (",
      "        SPLIT_K * triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),",
      "        NUM_SLICES,",
      "        # Each LoRA receives its own set of thread blocks for output",
      "        # computation. If some LoRA doesn't have any tokens to process, its",
      "        # thread blocks exit early.",
      "        MAX_LORAS,",
      "    )",
      "",
      "    _lora_shrink_kernel[grid](",
      "        inputs,",
      "        lora_ptr_tensor,",
      "        output_tensor,",
      "        M,",
      "        N,",
      "        K,",
      "        token_indices_sorted_by_lora_ids,",
      "        num_tokens_per_lora,",
      "        lora_token_start_loc,",
      "        lora_ids,",
      "        scaling,",
      "        inputs.stride(0),",
      "        inputs.stride(1),",
      "        lora_strides_d0,",
      "        lora_strides_d1,",
      "        lora_strides_d2,",
      "        output_tensor.stride(0),",
      "        output_tensor.stride(1),",
      "        output_tensor.stride(2),",
      "        BLOCK_M,",
      "        BLOCK_N,",
      "        BLOCK_K,",
      "        EVEN_K,",
      "        SPLIT_K,",
      "        NUM_SLICES,",
      "        num_warps=NUM_WARPS,",
      "        num_ctas=NUM_CTAS,",
      "        num_stages=NUM_STAGES,",
      "    )",
      "",
      "    return",
      "",
      "",
      "def _lora_shrink_fake(",
      "    inputs: torch.Tensor,",
      "    lora_a_weights: list[torch.Tensor],",
      "    output_tensor: torch.Tensor,",
      "    token_lora_mapping: torch.Tensor,",
      "    token_indices_sorted_by_lora_ids: torch.Tensor,",
      "    num_tokens_per_lora: torch.Tensor,",
      "    lora_token_start_loc: torch.Tensor,",
      "    lora_ids: torch.Tensor,",
      "    no_lora_flag_cpu: torch.Tensor,",
      "    scaling: float,",
      ") -> None:",
      "    return",
      "",
      "",
      "try:",
      "    direct_register_custom_op(",
      "        op_name=\"lora_shrink\",",
      "        op_func=_lora_shrink,",
      "        mutates_args=[\"output_tensor\"],",
      "        fake_impl=_lora_shrink_fake,",
      "        dispatch_key=current_platform.dispatch_key,",
      "    )",
      "    lora_shrink = torch.ops.vllm.lora_shrink",
      "",
      "except AttributeError:",
      "    lora_shrink = _lora_shrink"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/v1/attention/backends/flashinfer.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "\"\"\"Attention layer with FlashInfer.\"\"\"",
      "from __future__ import annotations",
      "",
      "from dataclasses import dataclass",
      "from typing import ClassVar, Optional, Union",
      "",
      "import numpy as np",
      "import torch",
      "from flashinfer import (BatchDecodeWithPagedKVCacheWrapper,",
      "                        BatchPrefillWithPagedKVCacheWrapper,",
      "                        MultiLevelCascadeAttentionWrapper)",
      "from flashinfer.decode import _get_range_buf, trtllm_batch_decode_with_kv_cache",
      "from flashinfer.prefill import trtllm_batch_context_with_kv_cache",
      "from flashinfer.utils import FP4Tensor",
      "",
      "from vllm import _custom_ops as ops",
      "from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,",
      "                                              AttentionType)",
      "from vllm.config import CUDAGraphMode, VllmConfig",
      "from vllm.logger import init_logger",
      "from vllm.model_executor.layers.quantization.utils.quant_utils import (",
      "    QuantKey, kFp8StaticTensorSym, kNvfp4Quant)",
      "from vllm.platforms import current_platform",
      "from vllm.triton_utils import tl, triton",
      "from vllm.utils import cdiv, is_pin_memory_available",
      "from vllm.utils.flashinfer import (supports_trtllm_attention,",
      "                                   use_trtllm_attention)",
      "from vllm.v1.attention.backends.flash_attn import use_cascade_attention",
      "# yapf conflicts with isort for this block",
      "# yapf: disable",
      "from vllm.v1.attention.backends.utils import (AttentionCGSupport,",
      "                                              AttentionMetadataBuilder,",
      "                                              CommonAttentionMetadata,",
      "                                              get_kv_cache_layout,",
      "                                              get_per_layer_parameters,",
      "                                              infer_global_hyperparameters,",
      "                                              split_decodes_and_prefills)",
      "# yapf: enable",
      "from vllm.v1.kv_cache_interface import AttentionSpec",
      "",
      "FLASHINFER_WORKSPACE_BUFFER_SIZE = 256 * 1024 * 1024",
      "",
      "FP8_DTYPE = current_platform.fp8_dtype()",
      "FP4_DTYPE = torch.uint8",
      "",
      "logger = init_logger(__name__)",
      "",
      "",
      "class FlashInferBackend(AttentionBackend):",
      "",
      "    accept_output_buffer: bool = True",
      "",
      "    @classmethod",
      "    def get_supported_dtypes(cls) -> list[torch.dtype]:",
      "        return [torch.float16, torch.bfloat16]",
      "",
      "    @classmethod",
      "    def get_supported_head_sizes(cls) -> list[int]:",
      "        # https://github.com/flashinfer-ai/flashinfer/blob/3d55c71a62052c590c130897d3a3db49b14fcc34/include/flashinfer/utils.cuh#L157",
      "        return [64, 128, 256]",
      "",
      "    @classmethod",
      "    def validate_head_size(cls, head_size: int) -> None:",
      "        supported_head_sizes = cls.get_supported_head_sizes()",
      "        if head_size not in supported_head_sizes:",
      "            attn_type = cls.__name__.removesuffix(\"Backend\")",
      "            raise ValueError(",
      "                f\"Head size {head_size} is not supported by {attn_type}. \"",
      "                f\"Supported head sizes are: {supported_head_sizes}. \"",
      "                \"Set VLLM_ATTENTION_BACKEND=FLEX_ATTENTION to use \"",
      "                \"FlexAttention backend which supports all head sizes.\")",
      "",
      "    @staticmethod",
      "    def get_name() -> str:",
      "        return \"FLASHINFER_VLLM_V1\"",
      "",
      "    @staticmethod",
      "    def get_impl_cls() -> type[FlashInferImpl]:",
      "        return FlashInferImpl",
      "",
      "    @staticmethod",
      "    def get_metadata_cls() -> type[FlashInferMetadata]:",
      "        return FlashInferMetadata",
      "",
      "    @staticmethod",
      "    def get_builder_cls() -> type[FlashInferMetadataBuilder]:",
      "        return FlashInferMetadataBuilder",
      "",
      "    @staticmethod",
      "    def get_kv_cache_shape(",
      "        num_blocks: int,",
      "        block_size: int,",
      "        num_kv_heads: int,",
      "        head_size: int,",
      "    ) -> tuple[int, ...]:",
      "        return (num_blocks, 2, block_size, num_kv_heads, head_size)",
      "",
      "    @staticmethod",
      "    def get_kv_cache_stride_order() -> tuple[int, ...]:",
      "        # `stride_order` indicates the permutation that gets us from",
      "        # `get_kv_cache_shape` to the actual memory layout we want.",
      "        cache_layout = get_kv_cache_layout()",
      "        if cache_layout == \"NHD\":",
      "            stride_order = (0, 1, 2, 3, 4)",
      "        elif cache_layout == \"HND\":",
      "            stride_order = (0, 1, 3, 2, 4)",
      "        else:",
      "            raise ValueError(f\"Unknown cache layout format {cache_layout}.\")",
      "        return stride_order",
      "",
      "    @staticmethod",
      "    def get_fp8_dtype_for_flashinfer(kv_cache_dtype: str) -> torch.dtype:",
      "        if kv_cache_dtype in (\"fp8\", \"fp8_e4m3\"):",
      "            return torch.float8_e4m3fn",
      "        elif kv_cache_dtype == \"fp8_e5m2\":",
      "            return torch.float8_e5m2",
      "        else:",
      "            raise ValueError(f\"Unrecognized FP8 dtype: {kv_cache_dtype}\")",
      "",
      "",
      "@dataclass",
      "class FlashInferMetadata:",
      "",
      "    num_actual_tokens: int  # Number of tokens excluding padding.",
      "",
      "    # The data type of the query",
      "    q_data_type: torch.dtype",
      "",
      "    slot_mapping: torch.Tensor",
      "",
      "    # For flashinfer trtllm batch decode",
      "    max_q_len: int",
      "    max_seq_len: int",
      "    seq_lens: torch.Tensor",
      "    block_table_tensor: torch.Tensor",
      "    prefill_use_trtllm: bool",
      "    decode_use_trtllm: bool",
      "",
      "    # For handling prefill decode split",
      "    num_decodes: int",
      "    num_decode_tokens: int",
      "    num_prefills: int",
      "    num_prefill_tokens: int",
      "",
      "    # For cascade attention (CPU for planning).",
      "    use_cascade: bool",
      "",
      "    prefill_wrapper: Optional[BatchPrefillWithPagedKVCacheWrapper] = None",
      "    decode_wrapper: Optional[BatchDecodeWithPagedKVCacheWrapper] = None",
      "    cascade_wrapper: Optional[MultiLevelCascadeAttentionWrapper] = None",
      "",
      "    qo_indptr_gpu: Optional[torch.Tensor] = None",
      "    paged_kv_indptr_gpu: Optional[torch.Tensor] = None",
      "",
      "",
      "class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):",
      "    cudagraph_support: ClassVar[AttentionCGSupport] = \\",
      "        AttentionCGSupport.UNIFORM_SINGLE_TOKEN_DECODE",
      "",
      "    reorder_batch_threshold: ClassVar[int] = 1",
      "",
      "    def __init__(self, kv_cache_spec: AttentionSpec, layer_names: list[str],",
      "                 vllm_config: VllmConfig, device: torch.device):",
      "        self.device = device",
      "        self.vllm_config = vllm_config",
      "        self.cache_config = vllm_config.cache_config",
      "        self.model_config = vllm_config.model_config",
      "        self.kv_cache_spec = kv_cache_spec",
      "        self._workspace_buffer = None",
      "        self._prefill_wrapper = None  # Wrapper for prefill/append",
      "        self._decode_wrapper = None  # Wrapper for decode (general shape)",
      "",
      "        self.compilation_config = vllm_config.compilation_config",
      "        max_num_pages_per_req = cdiv(self.model_config.max_model_len,",
      "                                     self.kv_cache_spec.block_size)",
      "        max_num_reqs = vllm_config.scheduler_config.max_num_seqs",
      "        max_num_pages = max_num_reqs * max_num_pages_per_req",
      "        self.enable_cuda_graph = self.compilation_config.cudagraph_mode.\\",
      "            decode_mode() == CUDAGraphMode.FULL",
      "        if self.enable_cuda_graph:",
      "            # For full cudagraph capture, one `decode_wrapper` for each batch",
      "            # size is needed for FlashInfer.",
      "            self._decode_wrappers_cudagraph: dict[",
      "                int, BatchDecodeWithPagedKVCacheWrapper] = {}",
      "            self._decode_cudagraph_max_bs = min(",
      "                max_num_reqs, self.compilation_config.max_capture_size)",
      "",
      "        self.num_qo_heads = self.model_config.get_num_attention_heads(",
      "            self.vllm_config.parallel_config)",
      "        self.num_kv_heads = self.kv_cache_spec.num_kv_heads",
      "        self.head_dim = self.kv_cache_spec.head_size",
      "        FlashInferBackend.validate_head_size(self.head_dim)",
      "        self.page_size = self.kv_cache_spec.block_size",
      "",
      "        self.cache_dtype = self.cache_config.cache_dtype",
      "        if self.cache_dtype.startswith(\"fp8\"):",
      "            self.kv_cache_dtype = (",
      "                FlashInferBackend.get_fp8_dtype_for_flashinfer(",
      "                    self.cache_dtype))",
      "        else:",
      "            assert self.kv_cache_spec.dtype == self.model_config.dtype",
      "            self.kv_cache_dtype = self.kv_cache_spec.dtype",
      "        self.q_data_type = self.kv_cache_dtype",
      "",
      "        self._cascade_wrapper = None  # Wrapper for cascade attention",
      "",
      "        # Global hyperparameters shared by all attention layers",
      "        # TODO: discard this for trtllm-gen backend",
      "        self.global_hyperparameters = infer_global_hyperparameters(",
      "            get_per_layer_parameters(vllm_config, layer_names, FlashInferImpl))",
      "        self.sm_scale = self.global_hyperparameters.sm_scale",
      "        self.window_left = self.global_hyperparameters.window_left",
      "        self.logits_soft_cap = self.global_hyperparameters.logits_soft_cap",
      "        self.has_sinks = self.global_hyperparameters.has_sinks",
      "",
      "        # Preparing persistent buffers (device-side)",
      "        self.paged_kv_indptr = torch.zeros(max_num_reqs + 1,",
      "                                           dtype=torch.int32,",
      "                                           device=self.device)",
      "        self.paged_kv_indices = torch.zeros(",
      "            max_num_pages,  # max num pages possible",
      "            dtype=torch.int32,",
      "            device=self.device)",
      "        self.paged_kv_last_page_len = torch.zeros(max_num_reqs,",
      "                                                  dtype=torch.int32,",
      "                                                  device=self.device)",
      "        # host-side buffer",
      "        pin_memory = is_pin_memory_available()",
      "        self.paged_kv_indptr_cpu = torch.zeros(max_num_reqs + 1,",
      "                                               dtype=torch.int32,",
      "                                               device=\"cpu\",",
      "                                               pin_memory=pin_memory)",
      "        self.paged_kv_indptr_np = self.paged_kv_indptr_cpu.numpy()",
      "        self.paged_kv_indptr_buffer = torch.zeros_like(",
      "            self.paged_kv_indptr_cpu, pin_memory=pin_memory)",
      "        self.paged_kv_indices_cpu = torch.zeros(max_num_pages,",
      "                                                dtype=torch.int32,",
      "                                                device=\"cpu\",",
      "                                                pin_memory=pin_memory)",
      "        self.paged_kv_last_page_len_cpu = torch.zeros(max_num_reqs,",
      "                                                      dtype=torch.int32,",
      "                                                      device=\"cpu\",",
      "                                                      pin_memory=pin_memory)",
      "        self.paged_kv_last_page_len_np = (",
      "            self.paged_kv_last_page_len_cpu.numpy())",
      "",
      "    def _get_workspace_buffer(self):",
      "        if self._workspace_buffer is None:",
      "            self._workspace_buffer = torch.zeros(",
      "                FLASHINFER_WORKSPACE_BUFFER_SIZE,",
      "                dtype=torch.uint8,",
      "                device=self.device)",
      "        return self._workspace_buffer",
      "",
      "    def _get_prefill_wrapper(self):",
      "        if self._prefill_wrapper is None:",
      "            self._prefill_wrapper = BatchPrefillWithPagedKVCacheWrapper(",
      "                self._get_workspace_buffer(), get_kv_cache_layout())",
      "        return self._prefill_wrapper",
      "",
      "    def _get_decode_wrapper(self,",
      "                            batch_size: int,",
      "                            use_cudagraph: bool = False):",
      "        if use_cudagraph:",
      "            decode_wrapper = self._decode_wrappers_cudagraph.get(",
      "                batch_size, None)",
      "        else:",
      "            decode_wrapper = self._decode_wrapper",
      "",
      "        if decode_wrapper is None:",
      "            if use_cudagraph:",
      "                paged_kv_indptr = self.paged_kv_indptr[:batch_size + 1]",
      "                paged_kv_indices = self.paged_kv_indices",
      "                paged_kv_last_page_len = self.paged_kv_last_page_len[:",
      "                                                                     batch_size]",
      "            else:",
      "                paged_kv_indptr = None",
      "                paged_kv_indices = None",
      "                paged_kv_last_page_len = None",
      "            decode_wrapper = BatchDecodeWithPagedKVCacheWrapper(",
      "                self._get_workspace_buffer(),",
      "                get_kv_cache_layout(),",
      "                use_cuda_graph=use_cudagraph,",
      "                paged_kv_indptr_buffer=paged_kv_indptr,",
      "                paged_kv_indices_buffer=paged_kv_indices,",
      "                paged_kv_last_page_len_buffer=paged_kv_last_page_len,",
      "                # Tensor cores are enabled by default because the perf would be",
      "                # at least as good as cuda cores for all attention ops in latest",
      "                # gpus.",
      "                use_tensor_cores=True,",
      "            )",
      "",
      "            # save the decode wrapper",
      "            if use_cudagraph:",
      "                self._decode_wrappers_cudagraph[batch_size] = decode_wrapper",
      "            else:",
      "                self._decode_wrapper = decode_wrapper",
      "",
      "        return decode_wrapper",
      "",
      "    def _get_cascade_wrapper(self):",
      "        if self._cascade_wrapper is None:",
      "            self._cascade_wrapper = MultiLevelCascadeAttentionWrapper(",
      "                2, self._get_workspace_buffer(), get_kv_cache_layout())",
      "        return self._cascade_wrapper",
      "",
      "    def build(self,",
      "              common_prefix_len: int,",
      "              common_attn_metadata: CommonAttentionMetadata,",
      "              fast_build: bool = False) -> FlashInferMetadata:",
      "        num_reqs = common_attn_metadata.num_reqs",
      "        num_actual_tokens = common_attn_metadata.num_actual_tokens",
      "        num_decodes, num_prefills, num_decode_tokens, num_prefill_tokens =\\",
      "            split_decodes_and_prefills(common_attn_metadata,",
      "                                       decode_threshold=self.reorder_batch_threshold)",
      "",
      "        page_size = self.page_size",
      "        max_q_len = common_attn_metadata.max_query_len",
      "        max_seq_len = common_attn_metadata.max_seq_len",
      "        seq_lens = common_attn_metadata.seq_lens",
      "        seq_lens_cpu = common_attn_metadata.seq_lens_cpu",
      "        seq_lens_np = seq_lens_cpu.numpy()",
      "        block_table_tensor = common_attn_metadata.block_table_tensor",
      "",
      "        num_blocks_np = (seq_lens_np + (page_size - 1)) // page_size",
      "",
      "        use_cascade = common_prefix_len > 0",
      "        if use_cascade:",
      "            # Grab the blocks of the shared prefix from the first request.",
      "            assert common_prefix_len % page_size == 0",
      "            num_common_kv_blocks = common_prefix_len // page_size",
      "",
      "            # Create CPU versions directly for cascade (no GPU versions needed)",
      "            shared_qo_indptr_cpu = torch.tensor([0, num_actual_tokens],",
      "                                                dtype=torch.int32,",
      "                                                device='cpu')",
      "            shared_kv_page_indptr_cpu = torch.tensor([0, num_common_kv_blocks],",
      "                                                     dtype=torch.int32,",
      "                                                     device='cpu')",
      "            shared_kv_page_indices_cpu = block_table_tensor[",
      "                0, :num_common_kv_blocks]",
      "            shared_kv_last_page_len_cpu = torch.tensor([page_size],",
      "                                                       dtype=torch.int32,",
      "                                                       device='cpu')",
      "",
      "            # Remove the blocks of the shared prefix from all requests.",
      "            block_table_tensor = block_table_tensor[:, num_common_kv_blocks:]",
      "            num_blocks_np -= num_common_kv_blocks",
      "        else:",
      "            shared_qo_indptr_cpu = None",
      "            shared_kv_page_indptr_cpu = None",
      "            shared_kv_page_indices_cpu = None",
      "            shared_kv_last_page_len_cpu = None",
      "",
      "        # write self.paged_kv_indptr_cpu inplace (0-index is always 0)",
      "        np.cumsum(",
      "            num_blocks_np,",
      "            dtype=np.int32,",
      "            out=self.paged_kv_indptr_np[1:num_reqs + 1],",
      "        )",
      "        # NOTE(woosuk): Because self.paged_kv_indptr_cpu can be modified",
      "        # after this line (e.g., for cuda graphs), we need to copy the data to",
      "        # self.paged_kv_indptr_buffer to avoid race condition.",
      "        self.paged_kv_indptr_buffer[:num_reqs +",
      "                                    1] = (self.paged_kv_indptr_cpu[:num_reqs +",
      "                                                                   1])",
      "        paged_kv_indptr = self.paged_kv_indptr[:num_reqs + 1]",
      "        paged_kv_indptr.copy_(self.paged_kv_indptr_buffer[:num_reqs + 1],",
      "                              non_blocking=True)",
      "",
      "        # write self.paged_kv_indices inplace",
      "        num_actual_pages = self.paged_kv_indptr_np[num_reqs]",
      "        paged_kv_indices = self.paged_kv_indices[:num_actual_pages]",
      "        _copy_page_indices_kernel[(num_reqs, )](",
      "            paged_kv_indices,",
      "            block_table_tensor,",
      "            block_table_tensor.stride(0),",
      "            paged_kv_indptr,",
      "            BLOCK_SIZE=1024,",
      "        )",
      "",
      "        # write self.paged_kv_last_page_len_cpu inplace",
      "        paged_kv_last_page_len_np = seq_lens_np % page_size",
      "        self.paged_kv_last_page_len_np[:num_reqs] = np.where(",
      "            paged_kv_last_page_len_np == 0,",
      "            page_size,",
      "            paged_kv_last_page_len_np,",
      "        )",
      "",
      "        # Check if any layer uses sinks (requires TRTLLM attention)",
      "        prefill_use_trtllm = use_trtllm_attention(self.num_qo_heads,",
      "                                                  self.num_kv_heads,",
      "                                                  num_prefill_tokens,",
      "                                                  max_seq_len,",
      "                                                  self.cache_dtype,",
      "                                                  self.q_data_type,",
      "                                                  is_prefill=True,",
      "                                                  has_sinks=self.has_sinks)",
      "        decode_use_trtllm = use_trtllm_attention(self.num_qo_heads,",
      "                                                 self.num_kv_heads,",
      "                                                 num_decode_tokens,",
      "                                                 max_seq_len,",
      "                                                 self.cache_dtype,",
      "                                                 self.q_data_type,",
      "                                                 is_prefill=False,",
      "                                                 has_sinks=self.has_sinks)",
      "",
      "        attn_metadata = FlashInferMetadata(",
      "            num_actual_tokens=num_actual_tokens,",
      "            q_data_type=self.q_data_type,",
      "            slot_mapping=common_attn_metadata.slot_mapping,",
      "            max_q_len=max_q_len,",
      "            max_seq_len=max_seq_len,",
      "            seq_lens=seq_lens,",
      "            block_table_tensor=block_table_tensor,",
      "            prefill_use_trtllm=prefill_use_trtllm,",
      "            decode_use_trtllm=decode_use_trtllm,",
      "            num_decodes=num_decodes,",
      "            num_decode_tokens=num_decode_tokens,",
      "            num_prefills=num_prefills,",
      "            num_prefill_tokens=num_prefill_tokens,",
      "            use_cascade=use_cascade,",
      "        )",
      "",
      "        qo_indptr_cpu = common_attn_metadata.query_start_loc_cpu",
      "        paged_kv_indptr_cpu = self.paged_kv_indptr_cpu[:1 + num_reqs]",
      "        paged_kv_last_page_len_cpu = self.paged_kv_last_page_len_cpu[:num_reqs]",
      "",
      "        if attn_metadata.use_cascade:",
      "            attn_metadata.cascade_wrapper = self._get_cascade_wrapper()",
      "            attn_metadata.cascade_wrapper.plan(",
      "                [shared_qo_indptr_cpu, qo_indptr_cpu],",
      "                [shared_kv_page_indptr_cpu, paged_kv_indptr_cpu],",
      "                [shared_kv_page_indices_cpu, paged_kv_indices],",
      "                [shared_kv_last_page_len_cpu, paged_kv_last_page_len_cpu],",
      "                self.num_qo_heads,",
      "                self.num_kv_heads,",
      "                self.head_dim,",
      "                self.page_size,",
      "                causal=True,",
      "                sm_scale=self.sm_scale,",
      "                window_left=self.window_left,",
      "                logits_soft_cap=self.logits_soft_cap,",
      "                q_data_type=self.q_data_type,",
      "                kv_data_type=self.kv_cache_dtype,",
      "            )",
      "        else:",
      "            # Regular attention (common case).",
      "            # Decodes are at the front and prefills are at the back,",
      "            # according to reorder_batch()",
      "            num_prefills = attn_metadata.num_prefills",
      "            num_decodes = attn_metadata.num_decodes",
      "            if num_prefills > 0:",
      "                # Decodes are first so prefills start after the last decode",
      "                prefill_start = num_decodes",
      "                attn_metadata.prefill_wrapper = self._get_prefill_wrapper()",
      "                assert qo_indptr_cpu[prefill_start:].shape[",
      "                    0] == num_prefills + 1",
      "                assert paged_kv_indptr_cpu[prefill_start:].shape[",
      "                    0] == num_prefills + 1",
      "                assert paged_kv_last_page_len_cpu[prefill_start:].shape[",
      "                    0] == num_prefills",
      "                # Since prefill_wrapper.run() will be called with",
      "                # query[num_decode_tokens:] we need to adjust the qo_indptr",
      "                # to be relative to the start of the prefill queries.",
      "                qo_indptr_cpu = qo_indptr_cpu[prefill_start:] - qo_indptr_cpu[",
      "                    prefill_start]",
      "                paged_kv_indptr_cpu = paged_kv_indptr_cpu[prefill_start:]",
      "                if not attn_metadata.prefill_use_trtllm:",
      "                    attn_metadata.prefill_wrapper.plan(",
      "                        qo_indptr_cpu,",
      "                        paged_kv_indptr_cpu,",
      "                        paged_kv_indices,",
      "                        paged_kv_last_page_len_cpu[prefill_start:],",
      "                        self.num_qo_heads,",
      "                        self.num_kv_heads,",
      "                        self.head_dim,",
      "                        self.page_size,",
      "                        causal=True,",
      "                        sm_scale=self.sm_scale,",
      "                        window_left=self.window_left,",
      "                        logits_soft_cap=self.logits_soft_cap,",
      "                        q_data_type=self.q_data_type,",
      "                        kv_data_type=self.kv_cache_dtype,",
      "                    )",
      "                else:",
      "                    attn_metadata.qo_indptr_gpu = qo_indptr_cpu.to(self.device)",
      "                    attn_metadata.paged_kv_indptr_gpu = paged_kv_indptr_cpu.to(",
      "                        self.device)",
      "",
      "            if num_decodes > 0:",
      "                pure_decode = num_prefills == 0",
      "                # possible required padding for cudagraph replay",
      "                use_cudagraph = (self.enable_cuda_graph and pure_decode and",
      "                                 num_decodes <= self._decode_cudagraph_max_bs)",
      "                if use_cudagraph:",
      "                    num_input_tokens = (",
      "                        self.vllm_config.pad_for_cudagraph(num_decodes))",
      "                    # Carefully fulfill the padding region with reasonable value",
      "                    # on cpu.",
      "                    # Make sure paged_kv_indptr_cpu is not decreasing",
      "                    self.paged_kv_indptr_cpu[1 + num_decodes:1 +",
      "                                             num_input_tokens].fill_(",
      "                                                 paged_kv_indptr_cpu[-1])",
      "                    # Fill the remaining paged_kv_last_page_len_cpu with 1.",
      "                    # This is because flashinfer treats 0 as a full page",
      "                    # instead of empty.",
      "                    self.paged_kv_last_page_len_cpu[",
      "                        num_decodes:num_input_tokens].fill_(1)",
      "",
      "                else:",
      "                    num_input_tokens = num_decodes",
      "",
      "                attn_metadata.decode_wrapper = self._get_decode_wrapper(",
      "                    num_input_tokens, use_cudagraph)",
      "                if not attn_metadata.decode_use_trtllm:",
      "                    # Use the persistent buffer with padding length,",
      "                    # instead of the same address but chunked version",
      "                    # in atten_metadata when using cudagraph.",
      "                    fast_plan_decode(",
      "                        attn_metadata.decode_wrapper,",
      "                        self.paged_kv_indptr_cpu[:num_input_tokens + 1],",
      "                        paged_kv_indices,",
      "                        self.paged_kv_last_page_len_cpu[:num_input_tokens],",
      "                        seq_lens_cpu[:num_input_tokens],",
      "                        self.num_qo_heads,",
      "                        self.num_kv_heads,",
      "                        self.head_dim,",
      "                        self.page_size,",
      "                        # Disable flashinfer's pos encoding and use vllm's rope.",
      "                        pos_encoding_mode=\"NONE\",",
      "                        sm_scale=self.sm_scale,",
      "                        window_left=self.window_left,",
      "                        logits_soft_cap=self.logits_soft_cap,",
      "                        q_data_type=self.q_data_type,",
      "                        kv_data_type=self.kv_cache_dtype,",
      "                    )",
      "        return attn_metadata",
      "",
      "    def build_for_cudagraph_capture(",
      "            self, common_attn_metadata: CommonAttentionMetadata):",
      "        \"\"\"",
      "        This method builds the metadata for full cudagraph capture.",
      "        Currently, only decode is supported for full cudagraphs with FlashInfer.",
      "        \"\"\"",
      "        m = common_attn_metadata",
      "",
      "        assert m.num_reqs == m.num_actual_tokens, \\",
      "            \"FlashInfer only supports decode-only full CUDAGraph capture. \" \\",
      "            \"Make sure all cudagraph capture sizes <= max_num_seq.\"",
      "",
      "        m.max_query_len = 1  # decode-only",
      "",
      "        return self.build(0, m)",
      "",
      "    def use_cascade_attention(self, *args, **kwargs) -> bool:",
      "        if self.kv_cache_spec.dtype != self.vllm_config.model_config.dtype:",
      "            # TODO: The cascade wrapper currently does not support setting",
      "            # kv cache dtype to something different from query dtype.",
      "            return False",
      "        return use_cascade_attention(*args, **kwargs)",
      "",
      "",
      "class FlashInferImpl(AttentionImpl):",
      "",
      "    def __init__(",
      "        self,",
      "        num_heads: int,",
      "        head_size: int,",
      "        scale: float,",
      "        num_kv_heads: int,",
      "        alibi_slopes: Optional[list[float]],",
      "        sliding_window: Optional[int],",
      "        kv_cache_dtype: str,",
      "        logits_soft_cap: Optional[float] = None,",
      "        attn_type: AttentionType = AttentionType.DECODER,",
      "        kv_sharing_target_layer_name: Optional[int] = None,",
      "        sinks: Optional[torch.Tensor] = None,",
      "    ) -> None:",
      "        self.num_heads = num_heads",
      "        self.head_size = head_size",
      "        self.scale = float(scale)",
      "        self.num_kv_heads = num_kv_heads",
      "        if alibi_slopes is not None:",
      "            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)",
      "        self.alibi_slopes = alibi_slopes",
      "        if sliding_window is None:",
      "            self.sliding_window = (-1, -1)",
      "        else:",
      "            self.sliding_window = (sliding_window - 1, 0)",
      "        self.window_left = (self.sliding_window[0]",
      "                            if self.sliding_window is not None else -1)",
      "        self.kv_cache_dtype = kv_cache_dtype",
      "        self.logits_soft_cap = logits_soft_cap",
      "        self.kv_sharing_target_layer_name = kv_sharing_target_layer_name",
      "",
      "        self.num_queries_per_kv = self.num_heads // self.num_kv_heads",
      "",
      "        if attn_type != AttentionType.DECODER:",
      "            raise NotImplementedError(\"Encoder self-attention and \"",
      "                                      \"encoder/decoder cross-attention \"",
      "                                      \"are not implemented for \"",
      "                                      \"FlashInferImpl\")",
      "",
      "        self.sinks: Optional[torch.Tensor] = None",
      "        if sinks is not None:",
      "            if sinks.shape[0] != num_heads:",
      "                raise ValueError(",
      "                    \"Sinks must have the same number of heads as the number of \"",
      "                    f\"heads in the layer. Expected {num_heads}, but got \"",
      "                    f\"{sinks.shape[0]}.\")",
      "            self.sinks = sinks",
      "",
      "        self.support_trtllm_attn = (supports_trtllm_attention()",
      "                                    and num_heads % num_kv_heads == 0)",
      "        self.bmm1_scale: Optional[float] = None",
      "        self.bmm2_scale: Optional[float] = None",
      "        self.o_sf_scale: Optional[float] = None",
      "",
      "    def fused_output_quant_supported(self, quant_key: QuantKey):",
      "        return (self.support_trtllm_attn",
      "                and self.kv_cache_dtype.startswith(\"fp8\")",
      "                and quant_key in (kFp8StaticTensorSym, kNvfp4Quant))",
      "",
      "    def forward(",
      "        self,",
      "        layer: torch.nn.Module,",
      "        query: torch.Tensor,",
      "        key: torch.Tensor,",
      "        value: torch.Tensor,",
      "        kv_cache: torch.Tensor,",
      "        attn_metadata: FlashInferMetadata,",
      "        output: Optional[torch.Tensor] = None,",
      "        output_scale: Optional[torch.Tensor] = None,",
      "        output_block_scale: Optional[torch.Tensor] = None,",
      "    ) -> torch.Tensor:",
      "        \"\"\"Forward pass with FlashInfer.",
      "",
      "        Args:",
      "            query: shape = [num_tokens, num_heads, head_size]",
      "            key: shape = [num_tokens, num_kv_heads, head_size]",
      "            value: shape = [num_tokens, num_kv_heads, head_size]",
      "            kv_cache: KV cache tensor with different possible shapes:",
      "                - NHD: [num_blocks, 2, block_size, num_kv_heads, head_size]",
      "                - HND: [num_blocks, 2, num_kv_heads, block_size, head_size]",
      "            attn_metadata: Metadata for attention.",
      "        Returns:",
      "            shape = [num_tokens, num_heads * head_size]",
      "        \"\"\"",
      "        assert output is not None, \"Output tensor must be provided.\"",
      "",
      "        if attn_metadata is None:",
      "            # Profiling run.",
      "            return output",
      "",
      "        if self.bmm1_scale is None:",
      "            self.bmm1_scale = (layer._q_scale_float * layer._k_scale_float *",
      "                               self.scale)",
      "",
      "        if self.bmm2_scale is None:",
      "            self.bmm2_scale = layer._v_scale_float",
      "",
      "        # The attn+quant fusion happens when output_scale is provided.",
      "        if output_scale is None:",
      "            assert output_block_scale is None, \"output_block_scale \"\\",
      "                \"is not supported when fusion has not happened\"",
      "        else:",
      "            assert attn_metadata.q_data_type == FP8_DTYPE, \\",
      "                \"Query must be FP8 when attn+quant fusion happened.\"",
      "            assert (attn_metadata.prefill_use_trtllm and",
      "                    attn_metadata.decode_use_trtllm), \"Must use TRT-LLM attn\"",
      "",
      "            if output.dtype == FP8_DTYPE:",
      "                assert output_block_scale is None, \\",
      "                    \"output_block_scale should not be provided for fp8 output\"",
      "            elif output.dtype == FP4_DTYPE:",
      "                assert output_block_scale is not None, \\",
      "                    \"output_block_scale is required for nvfp4 output\"",
      "            else:",
      "                raise ValueError(f\"Unsupported output dtype: {output.dtype}\")",
      "",
      "            # TRTLLM attn kernel requires to scale to pass as a host scalar,",
      "            # store the o scale as a host scalar in warmup run with cuda graph",
      "            # not enabled",
      "            if layer._o_scale_float is None:",
      "                layer._o_scale_float = output_scale.cpu().item()",
      "                if output.dtype == FP8_DTYPE:",
      "                    self.bmm2_scale = self.bmm2_scale / layer._o_scale_float",
      "                elif output.dtype == FP4_DTYPE:",
      "                    self.o_sf_scale = layer._o_scale_float",
      "",
      "        # Insert FP8 quant for query",
      "        if attn_metadata.q_data_type == FP8_DTYPE:",
      "            num_tokens, num_heads, head_size = query.shape",
      "            query, _ = ops.scaled_fp8_quant(",
      "                query.reshape(",
      "                    (num_tokens, num_heads * head_size)).contiguous(),",
      "                layer._q_scale)",
      "            query = query.reshape((num_tokens, num_heads, head_size))",
      "",
      "        # IMPORTANT!",
      "        # NOTE(woosuk): With piece-wise CUDA graphs, this method is executed in",
      "        # eager-mode PyTorch. Thus, we need to be careful about any CPU overhead",
      "        # in this method. For example, `view` and `slice` (or `[:n]`) operations",
      "        # are surprisingly slow even in the case they do not invoke any GPU ops.",
      "        # Minimize the PyTorch ops in this method as much as possible.",
      "        # Whenever making a change in this method, please benchmark the",
      "        # performance to make sure it does not introduce any overhead.",
      "",
      "        num_actual_tokens = attn_metadata.num_actual_tokens",
      "",
      "        if self.kv_sharing_target_layer_name is None:",
      "            # Reshape the input keys and values and store them in the cache.",
      "            # Skip this if sharing KV cache with an earlier attention layer.",
      "            # NOTE(woosuk): Here, key and value are padded while slot_mapping is",
      "            # not padded. However, we don't need to do key[:num_actual_tokens]",
      "            # and value[:num_actual_tokens] because the reshape_and_cache_flash",
      "            # op uses the slot_mapping's shape to determine the number of",
      "            # actual tokens.",
      "            torch.ops._C_cache_ops.reshape_and_cache_flash(",
      "                key,",
      "                value,",
      "                kv_cache[:, 0],",
      "                kv_cache[:, 1],",
      "                attn_metadata.slot_mapping,",
      "                self.kv_cache_dtype,",
      "                layer._k_scale,",
      "                layer._v_scale,",
      "            )",
      "",
      "            # The FlashInfer api requires data to be in fp8_e4m3 or fp8_e5m2",
      "            # to process the cache when the kv_cache_dtype is fp8",
      "            if self.kv_cache_dtype.startswith(\"fp8\"):",
      "                torch_dtype = FlashInferBackend.get_fp8_dtype_for_flashinfer(",
      "                    self.kv_cache_dtype)",
      "                kv_cache = kv_cache.view(torch_dtype)",
      "",
      "        # Inputs and outputs may be padded for CUDA graphs",
      "        query = query[:num_actual_tokens]",
      "        output_padded = output",
      "        output = output[:num_actual_tokens]",
      "",
      "        if attn_metadata.use_cascade:",
      "            # Cascade attention (rare case).",
      "            assert attn_metadata.cascade_wrapper is not None",
      "            output.copy_(attn_metadata.cascade_wrapper.run(query, kv_cache))",
      "            return output",
      "",
      "        num_decode_tokens = attn_metadata.num_decode_tokens",
      "        num_prefill_tokens = attn_metadata.num_prefill_tokens",
      "",
      "        stride_order = FlashInferBackend.get_kv_cache_stride_order()",
      "        kv_cache_permute = kv_cache.permute(*stride_order)",
      "        # Regular attention (common case).",
      "        # Decodes are at the front and prefills are at the back,",
      "        # according to reorder_batch()",
      "        if num_prefill_tokens > 0:",
      "            prefill_wrapper = attn_metadata.prefill_wrapper",
      "            prefill_query = query[num_decode_tokens:]",
      "            assert prefill_query.shape[0] == num_prefill_tokens",
      "            assert prefill_wrapper is not None",
      "",
      "            if not attn_metadata.prefill_use_trtllm:",
      "                assert prefill_wrapper._causal",
      "                assert prefill_wrapper._window_left == self.window_left",
      "                assert prefill_wrapper._logits_soft_cap == (",
      "                    self.logits_soft_cap or 0.0)",
      "                assert prefill_wrapper._sm_scale == self.scale",
      "                prefill_wrapper.run(",
      "                    prefill_query,",
      "                    kv_cache_permute,",
      "                    k_scale=layer._k_scale_float,",
      "                    v_scale=layer._v_scale_float,",
      "                    out=output[num_decode_tokens:],",
      "                )",
      "            else:",
      "                # prefill_query may be non-contiguous",
      "                prefill_query = prefill_query.contiguous()",
      "                workspace_buffer = prefill_wrapper._float_workspace_buffer",
      "                block_tables_prefill = attn_metadata.block_table_tensor[",
      "                    num_decode_tokens:]",
      "                seq_lens_prefill = attn_metadata.seq_lens[num_decode_tokens:]",
      "",
      "                # This path needs to be enabled with VLLM_KV_CACHE_LAYOUT = HND",
      "                assert get_kv_cache_layout() == \"HND\"",
      "                assert prefill_query.is_contiguous()",
      "                assert kv_cache_permute.is_contiguous()",
      "                assert workspace_buffer.is_contiguous()",
      "                assert block_tables_prefill.is_contiguous()",
      "                assert seq_lens_prefill.is_contiguous()",
      "",
      "                if output.dtype == FP4_DTYPE:",
      "                    assert self.o_sf_scale is not None",
      "                    out = FP4Tensor(data=output[num_decode_tokens:],",
      "                                    scale=output_block_scale,",
      "                                    scale_start_index=num_decode_tokens,",
      "                                    original_shape=prefill_query.shape)",
      "                else:",
      "                    assert self.o_sf_scale is None",
      "                    out = output[num_decode_tokens:]",
      "",
      "                trtllm_batch_context_with_kv_cache(",
      "                    query=prefill_query,",
      "                    kv_cache=kv_cache_permute,",
      "                    workspace_buffer=workspace_buffer,",
      "                    block_tables=block_tables_prefill,",
      "                    seq_lens=seq_lens_prefill,",
      "                    max_q_len=attn_metadata.max_q_len,",
      "                    max_kv_len=attn_metadata.max_seq_len,",
      "                    bmm1_scale=self.bmm1_scale,",
      "                    bmm2_scale=self.bmm2_scale,",
      "                    batch_size=attn_metadata.num_prefills,",
      "                    cum_seq_lens_q=attn_metadata.qo_indptr_gpu,",
      "                    cum_seq_lens_kv=attn_metadata.paged_kv_indptr_gpu,",
      "                    window_left=self.window_left,",
      "                    sinks=self.sinks,",
      "                    o_sf_scale=self.o_sf_scale,",
      "                    out=out,",
      "                )",
      "",
      "        if num_decode_tokens > 0:",
      "            decode_wrapper = attn_metadata.decode_wrapper",
      "            decode_query = query[:num_decode_tokens]",
      "            assert decode_query.shape[0] == num_decode_tokens",
      "            assert decode_wrapper is not None",
      "",
      "            if not attn_metadata.decode_use_trtllm:",
      "                assert decode_wrapper._window_left == self.window_left",
      "                assert decode_wrapper._logits_soft_cap == (self.logits_soft_cap",
      "                                                           or 0.0)",
      "                assert decode_wrapper._sm_scale == self.scale",
      "                decode_wrapper.run(",
      "                    decode_query,",
      "                    kv_cache_permute,",
      "                    k_scale=layer._k_scale_float,",
      "                    v_scale=layer._v_scale_float,",
      "                    out=output[:num_decode_tokens],",
      "                )",
      "            else:",
      "                # decode_query may be non-contiguous",
      "                decode_query = decode_query.contiguous()",
      "                workspace_buffer = decode_wrapper._float_workspace_buffer",
      "                block_tables_decode = attn_metadata.\\",
      "                        block_table_tensor[:num_decode_tokens]",
      "                seq_lens_decode = attn_metadata.seq_lens[:num_decode_tokens]",
      "",
      "                # This path needs to be enabled with VLLM_KV_CACHE_LAYOUT = HND",
      "                assert get_kv_cache_layout() == \"HND\"",
      "                assert decode_query.is_contiguous()",
      "                assert kv_cache_permute.is_contiguous()",
      "                assert workspace_buffer.is_contiguous()",
      "                assert block_tables_decode.is_contiguous()",
      "                assert seq_lens_decode.is_contiguous()",
      "",
      "                if output.dtype == FP4_DTYPE:",
      "                    assert self.o_sf_scale is not None",
      "                    out = FP4Tensor(data=output[:num_decode_tokens],",
      "                                    scale=output_block_scale,",
      "                                    scale_start_index=0,",
      "                                    original_shape=decode_query.shape)",
      "                else:",
      "                    assert self.o_sf_scale is None",
      "                    out = output[:num_decode_tokens]",
      "",
      "                trtllm_batch_decode_with_kv_cache(",
      "                    query=decode_query,",
      "                    kv_cache=kv_cache_permute,",
      "                    workspace_buffer=workspace_buffer,",
      "                    block_tables=block_tables_decode,",
      "                    seq_lens=seq_lens_decode,",
      "                    max_seq_len=attn_metadata.max_seq_len,",
      "                    bmm1_scale=self.bmm1_scale,",
      "                    bmm2_scale=self.bmm2_scale,",
      "                    window_left=self.window_left,",
      "                    sinks=self.sinks,",
      "                    o_sf_scale=self.o_sf_scale,",
      "                    out=out,",
      "                )",
      "        return output_padded",
      "",
      "",
      "def fast_plan_decode(",
      "    self,  # decode wrapper",
      "    indptr_cpu: torch.Tensor,",
      "    indices: torch.Tensor,",
      "    last_page_len_cpu: torch.Tensor,",
      "    seq_lens_cpu: torch.Tensor,",
      "    num_qo_heads: int,",
      "    num_kv_heads: int,",
      "    head_dim: int,",
      "    page_size: int,",
      "    pos_encoding_mode: str = \"NONE\",",
      "    window_left: int = -1,",
      "    logits_soft_cap: Optional[float] = None,",
      "    q_data_type: Optional[Union[str, torch.dtype]] = \"float16\",",
      "    kv_data_type: Optional[Union[str, torch.dtype]] = None,",
      "    data_type: Optional[Union[str, torch.dtype]] = None,",
      "    sm_scale: Optional[float] = None,",
      "    rope_scale: Optional[float] = None,",
      "    rope_theta: Optional[float] = None,",
      "    non_blocking: bool = True,",
      ") -> None:",
      "    \"\"\"",
      "    A faster version of BatchDecodeWithPagedKVCacheWrapper::plan used for",
      "    cudagraph capture/replay, while the no cudagraph version turns back",
      "    to the original plan.",
      "    using original plan after passing host-side buffers:",
      "    - only host-to-device copy of indptr and last_page_len buffers",
      "    Modifications for cudagraph:",
      "    - only host-to-device copy of indptr and last_page_len buffers.",
      "    - avoid device-to-device copy of indices buffer.",
      "",
      "    Part of the code get inspiration from the original plan from FlashInfer repo",
      "    and the implementation of fast_decode_plan for FlashInfer in SGlang repo.",
      "    \"\"\"",
      "    # Warm up with the original plan if it is first call, and always run the",
      "    # original plan if we run for dynamic shape. For fixed shape (cudagraph),",
      "    # this warm up is to generate the _cached_module for the decode wrapper.",
      "    if not self.is_cuda_graph_enabled or \\",
      "        getattr(self, \"vllm_first_call\", True):",
      "        self.plan(",
      "            indptr_cpu,",
      "            indices,",
      "            last_page_len_cpu,",
      "            num_qo_heads,",
      "            num_kv_heads,",
      "            head_dim,",
      "            page_size,",
      "            pos_encoding_mode,",
      "            window_left,",
      "            logits_soft_cap,",
      "            q_data_type,",
      "            kv_data_type,",
      "            data_type,",
      "            sm_scale,",
      "            rope_scale,",
      "            rope_theta,",
      "            non_blocking,",
      "        )",
      "        self.vllm_first_call = False",
      "        return",
      "",
      "    assert self.is_cuda_graph_enabled, \"Should be cudagraph only here\"",
      "",
      "    batch_size = len(last_page_len_cpu)",
      "    if logits_soft_cap is None:",
      "        logits_soft_cap = 0.0",
      "",
      "    # Handle data types consistently",
      "    if data_type is not None:",
      "        if q_data_type is None:",
      "            q_data_type = data_type",
      "        if kv_data_type is None:",
      "            kv_data_type = data_type",
      "    elif q_data_type is None:",
      "        q_data_type = \"float16\"",
      "",
      "    if kv_data_type is None:",
      "        kv_data_type = q_data_type",
      "    q_data_type = getattr(torch, q_data_type) if isinstance(",
      "        q_data_type, str) else q_data_type",
      "    kv_data_type = getattr(torch, kv_data_type) if isinstance(",
      "        kv_data_type, str) else kv_data_type",
      "",
      "    if batch_size != self._fixed_batch_size:",
      "        raise ValueError(",
      "            \"The batch size should be fixed in cudagraph mode, the runtime \"",
      "            \"batch size {} mismatches the batch size set during \"",
      "            \"initialization {}\".format(batch_size, self._fixed_batch_size))",
      "    if len(indices) > len(self._paged_kv_indices_buf):",
      "        raise ValueError(",
      "            \"The size of indices should be less than or equal to the \"",
      "            \"allocated buffer\")",
      "",
      "    # host-to-device copy for the indptr buffer",
      "    self._paged_kv_indptr_buf.copy_(indptr_cpu, non_blocking=True)",
      "    # host-to-device copy for the last_page_len buffer",
      "    self._paged_kv_last_page_len_buf.copy_(last_page_len_cpu,",
      "                                           non_blocking=True)",
      "",
      "    qo_indptr_host = _get_range_buf(batch_size + 1, \"cpu\")",
      "",
      "    try:",
      "        # Make sure we pass exactly 15 arguments for tensor core version",
      "        self._plan_info = self._cached_module.plan(",
      "            self._float_workspace_buffer,",
      "            self._int_workspace_buffer,",
      "            self._pin_memory_int_workspace_buffer,",
      "            qo_indptr_host,",
      "            indptr_cpu,",
      "            seq_lens_cpu,",
      "            batch_size,  # total_num_rows",
      "            batch_size,",
      "            num_qo_heads,",
      "            num_kv_heads,",
      "            page_size,",
      "            self.is_cuda_graph_enabled,",
      "            head_dim,",
      "            head_dim,",
      "            False,  # causal",
      "        )",
      "    except Exception as e:",
      "        raise RuntimeError(f\"Error in tensor core plan: {e}\") from e",
      "",
      "    self._pos_encoding_mode = pos_encoding_mode",
      "    self._window_left = window_left",
      "    self._logits_soft_cap = logits_soft_cap",
      "    self._sm_scale = sm_scale",
      "    self._rope_scale = rope_scale",
      "    self._rope_theta = rope_theta",
      "",
      "",
      "@triton.jit",
      "def _copy_page_indices_kernel(",
      "    page_indices,",
      "    block_table,",
      "    block_table_stride,",
      "    cu_num_blocks,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    req_idx = tl.program_id(0)",
      "    row_ptr = block_table + req_idx * block_table_stride",
      "    start_idx = tl.load(cu_num_blocks + req_idx)",
      "    end_idx = tl.load(cu_num_blocks + req_idx + 1)",
      "    num_blocks = end_idx - start_idx",
      "",
      "    offset = tl.arange(0, BLOCK_SIZE)",
      "    for i in tl.range(0, num_blocks, BLOCK_SIZE):",
      "        block_ids = tl.load(row_ptr + i + offset, mask=i + offset < num_blocks)",
      "        tl.store(page_indices + start_idx + i + offset,",
      "                 block_ids,",
      "                 mask=i + offset < num_blocks)"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/v1/attention/backends/rocm_aiter_fa.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "\"\"\"Attention layer with AiterFlashAttention.\"\"\"",
      "from dataclasses import dataclass",
      "from typing import Optional",
      "",
      "import torch",
      "",
      "from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,",
      "                                              AttentionMetadata, AttentionType)",
      "from vllm.config import VllmConfig",
      "from vllm.logger import init_logger",
      "from vllm.platforms import current_platform",
      "from vllm.v1.attention.backends.utils import (AttentionCGSupport,",
      "                                              AttentionMetadataBuilder,",
      "                                              CommonAttentionMetadata)",
      "from vllm.v1.kv_cache_interface import AttentionSpec",
      "",
      "_PARTITION_SIZE_ROCM = 256",
      "",
      "if current_platform.is_rocm():",
      "    import aiter",
      "",
      "    from vllm.triton_utils import tl, triton",
      "    from vllm.utils import direct_register_custom_op",
      "",
      "    @triton.jit",
      "    def _vllm_layout_trans_kernel(",
      "        k_buffer_ptr,",
      "        v_buffer_ptr,",
      "        k_values_ptr,",
      "        v_values_ptr,",
      "        b_query_lens_loc,",
      "        b_seq_lens_loc,",
      "        block_table,",
      "        block_table_stride_0,",
      "        k_scale,",
      "        v_scale,",
      "        output_dtype: tl.constexpr,",
      "        E_DIM: tl.constexpr,",
      "        BLOCK_SIZE: tl.constexpr,",
      "    ):",
      "        batch_idx = tl.program_id(0)",
      "        block_idx = tl.program_id(1)",
      "",
      "        batch_query_indexes = tl.load(b_query_lens_loc + batch_idx +",
      "                                      tl.arange(0, 2))",
      "        batch_query_start, batch_query_end = tl.split(batch_query_indexes)",
      "        query_len = batch_query_end - batch_query_start",
      "",
      "        if query_len <= 1:",
      "            return",
      "",
      "        batch_token_indexes = tl.load(b_seq_lens_loc + batch_idx +",
      "                                      tl.arange(0, 2))",
      "        batch_token_start, batch_token_end = tl.split(batch_token_indexes)",
      "        seq_len = batch_token_end - batch_token_start",
      "",
      "        if block_idx * BLOCK_SIZE < seq_len:",
      "            block_mask = (block_idx * BLOCK_SIZE +",
      "                          tl.arange(0, BLOCK_SIZE)[:, None]) < seq_len",
      "",
      "            kv_idx = tl.load(block_table + batch_idx * block_table_stride_0 +",
      "                             block_idx).to(tl.int64)",
      "",
      "            kv_buffer_off = kv_idx * BLOCK_SIZE * E_DIM + tl.arange(",
      "                0, BLOCK_SIZE)[:, None] * E_DIM + tl.arange(0, E_DIM)[None, :]",
      "            k_vals = tl.load(k_buffer_ptr + kv_buffer_off,",
      "                             mask=block_mask,",
      "                             other=0.0)",
      "            if k_vals.dtype.is_fp8():",
      "                k_vals = (k_vals.to(tl.float32) *",
      "                          tl.load(k_scale)).to(output_dtype)",
      "            else:",
      "                k_vals = k_vals.to(output_dtype)",
      "",
      "            v_vals = tl.load(v_buffer_ptr + kv_buffer_off,",
      "                             mask=block_mask,",
      "                             other=0.0)",
      "            if v_vals.dtype.is_fp8():",
      "                v_vals = (v_vals.to(tl.float32) *",
      "                          tl.load(v_scale)).to(output_dtype)",
      "            else:",
      "                v_vals = v_vals.to(output_dtype)",
      "            kv_values_off = batch_token_start * E_DIM + \\",
      "                block_idx * BLOCK_SIZE * E_DIM + \\",
      "                tl.arange(0, BLOCK_SIZE)[:, None] * E_DIM + \\",
      "                tl.arange(0, E_DIM)[None, :]",
      "            tl.store(k_values_ptr + kv_values_off, k_vals, mask=block_mask)",
      "            tl.store(v_values_ptr + kv_values_off, v_vals, mask=block_mask)",
      "",
      "    def vllm_layout_trans(b_query_lens_loc, b_seq_lens_loc, block_table,",
      "                          k_cache, v_cache, max_seq_len, k_scale, v_scale,",
      "                          output_dtype, total_tokens):",
      "        H_KV = v_cache.shape[2]",
      "        D = v_cache.shape[3]",
      "        BLOCK_SIZE = v_cache.shape[1]",
      "",
      "        k_values = torch.empty(",
      "            (total_tokens, H_KV, D),",
      "            dtype=output_dtype,",
      "            device=k_cache.device,",
      "        )",
      "        v_values = torch.empty(",
      "            (total_tokens, H_KV, D),",
      "            dtype=output_dtype,",
      "            device=v_cache.device,",
      "        )",
      "",
      "        grid = (block_table.shape[0],",
      "                (max_seq_len + BLOCK_SIZE - 1) // BLOCK_SIZE)",
      "",
      "        if output_dtype == torch.float16:",
      "            output_dtype = tl.float16",
      "        elif output_dtype == torch.bfloat16:",
      "            output_dtype = tl.bfloat16",
      "        else:",
      "            raise ValueError(f\"Unsupported output dtype: {output_dtype}\")",
      "",
      "        _vllm_layout_trans_kernel[grid](k_cache,",
      "                                        v_cache,",
      "                                        k_values,",
      "                                        v_values,",
      "                                        b_query_lens_loc,",
      "                                        b_seq_lens_loc,",
      "                                        block_table,",
      "                                        block_table.stride(0),",
      "                                        k_scale,",
      "                                        v_scale,",
      "                                        output_dtype=output_dtype,",
      "                                        E_DIM=H_KV * D,",
      "                                        BLOCK_SIZE=BLOCK_SIZE)",
      "",
      "        return k_values, v_values",
      "",
      "    def flash_attn_varlen_func_impl(",
      "        q: torch.Tensor,",
      "        k_cache: torch.Tensor,",
      "        v_cache: torch.Tensor,",
      "        out: torch.Tensor,",
      "        cu_seqlens_q: torch.Tensor,",
      "        cu_seqlens_k: torch.Tensor,",
      "        max_seqlen_q: int,",
      "        max_seqlen_k: int,",
      "        softmax_scale: float,",
      "        window_size: Optional[list[int]],  # -1 means infinite context window",
      "        alibi_slopes: Optional[list[float]],",
      "        block_table: torch.Tensor,",
      "        k_scale: torch.Tensor,",
      "        v_scale: torch.Tensor,",
      "        total_tokens: int = 0,",
      "    ) -> torch.Tensor:",
      "        if total_tokens == 0:",
      "            total_tokens = int(cu_seqlens_k[-1].item())",
      "        k, v = vllm_layout_trans(cu_seqlens_q, cu_seqlens_k, block_table,",
      "                                 k_cache, v_cache, max_seqlen_k, k_scale,",
      "                                 v_scale, q.dtype, total_tokens)",
      "",
      "        output = aiter.flash_attn_varlen_func(",
      "            q=q,",
      "            k=k,",
      "            v=v,",
      "            cu_seqlens_q=cu_seqlens_q,",
      "            max_seqlen_q=max_seqlen_q,",
      "            min_seqlen_q=1,",
      "            cu_seqlens_k=cu_seqlens_k,",
      "            max_seqlen_k=max_seqlen_k,",
      "            softmax_scale=softmax_scale,",
      "            causal=True,",
      "            alibi_slopes=alibi_slopes,",
      "            window_size=window_size,",
      "            out=out,",
      "        )",
      "        return output",
      "",
      "    def flash_attn_varlen_func_fake(",
      "        q: torch.Tensor,",
      "        k_cache: torch.Tensor,",
      "        v_cache: torch.Tensor,",
      "        out: torch.Tensor,",
      "        cu_seqlens_q: torch.Tensor,",
      "        cu_seqlens_k: torch.Tensor,",
      "        max_seqlen_q: int,",
      "        max_seqlen_k: int,",
      "        softmax_scale: float,",
      "        window_size: Optional[list[int]],  # -1 means infinite context window",
      "        alibi_slopes: Optional[list[float]],",
      "        block_table: torch.Tensor,",
      "        k_scale: torch.Tensor,",
      "        v_scale: torch.Tensor,",
      "        total_tokens: int = 0,",
      "    ) -> torch.Tensor:",
      "        return torch.empty(q.shape[0],",
      "                           q.shape[1],",
      "                           v_cache.shape[-2],",
      "                           dtype=q.dtype,",
      "                           device=q.device)",
      "",
      "    direct_register_custom_op(\"flash_attn_varlen_func\",",
      "                              flash_attn_varlen_func_impl, [\"out\"],",
      "                              flash_attn_varlen_func_fake,",
      "                              dispatch_key=current_platform.dispatch_key)",
      "",
      "logger = init_logger(__name__)",
      "",
      "",
      "@dataclass",
      "class AiterFlashAttentionMetadata:",
      "    # NOTE(sang): Definition of context_len, query_len, and seq_len.",
      "    # |---------- N-1 iteration --------|",
      "    # |---------------- N iteration ---------------------|",
      "    # |- tokenA -|......................|-- newTokens ---|",
      "    # |---------- context_len ----------|",
      "    # |-------------------- seq_len ---------------------|",
      "    #                                   |-- query_len ---|",
      "",
      "    num_actual_tokens: int  # Number of tokens excluding padding.",
      "    num_actual_kv_tokens: int",
      "    max_query_len: int",
      "    query_start_loc: torch.Tensor",
      "    max_seq_len: int",
      "    seq_lens: torch.Tensor",
      "    slot_mapping: torch.Tensor",
      "    block_table: torch.Tensor",
      "    cu_seq_lens: Optional[torch.Tensor]",
      "",
      "    # For cascade attention.",
      "    use_cascade: bool",
      "    common_prefix_len: int",
      "    total_tokens: int",
      "",
      "",
      "class AiterFlashAttentionMetadataBuilder(",
      "        AttentionMetadataBuilder[AiterFlashAttentionMetadata]):",
      "    cudagraph_support = AttentionCGSupport.ALWAYS",
      "",
      "    def __init__(self, kv_cache_spec: AttentionSpec, layer_names: list[str],",
      "                 vllm_config: VllmConfig, device: torch.device):",
      "        self.vllm_config = vllm_config",
      "        self.model_config = vllm_config.model_config",
      "        self.parallel_config = vllm_config.parallel_config",
      "        self.cache_config = vllm_config.cache_config",
      "        self.device = device",
      "",
      "        self.num_heads_q = self.model_config.get_num_attention_heads(",
      "            self.parallel_config)",
      "        self.num_heads_kv = self.model_config.get_num_kv_heads(",
      "            self.parallel_config)",
      "        self.headdim = self.model_config.get_head_size()",
      "        self.block_size = kv_cache_spec.block_size",
      "        self.kv_cache_spec = kv_cache_spec",
      "        # Sliding window size to be used with the AOT scheduler will be",
      "        # populated on first build() call.",
      "        self.aot_sliding_window: Optional[tuple[int, int]] = None",
      "        self.total_tokens: int = 0",
      "",
      "    def build_for_cudagraph_capture(",
      "            self, common_attn_metadata: CommonAttentionMetadata):",
      "        self.total_tokens = self.model_config.max_model_len \\",
      "            * self.vllm_config.scheduler_config.max_num_partial_prefills",
      "        res = self.build(common_prefix_len=0,",
      "                         common_attn_metadata=common_attn_metadata)",
      "        self.total_tokens = 0",
      "        return res",
      "",
      "    def build(self,",
      "              common_prefix_len: int,",
      "              common_attn_metadata: CommonAttentionMetadata,",
      "              fast_build: bool = False) -> 'AiterFlashAttentionMetadata':",
      "",
      "        num_actual_tokens = common_attn_metadata.num_actual_tokens",
      "        max_query_len = common_attn_metadata.max_query_len",
      "        max_seq_len = common_attn_metadata.max_seq_len",
      "        query_start_loc = common_attn_metadata.query_start_loc",
      "        seq_lens = common_attn_metadata.seq_lens",
      "        block_table_tensor = common_attn_metadata.block_table_tensor",
      "        slot_mapping = common_attn_metadata.slot_mapping",
      "        if max_query_len > 1:",
      "            # We pre-compute cumulative seq len needed for prefill attention",
      "            # here to avoid recomputing it for every layer",
      "            cu_seq_lens = torch.zeros(seq_lens.shape[0] + 1,",
      "                                      dtype=torch.int32,",
      "                                      device=seq_lens.device)",
      "            torch.cumsum(seq_lens,",
      "                         dim=0,",
      "                         dtype=cu_seq_lens.dtype,",
      "                         out=cu_seq_lens[1:])",
      "            num_actual_kv_tokens = int(cu_seq_lens[-1].item())",
      "        else:",
      "            cu_seq_lens = None",
      "            num_actual_kv_tokens = 0",
      "",
      "        def schedule(batch_size, cu_query_lens, max_query_len, seqlens,",
      "                     max_seq_len, causal):",
      "            return None",
      "",
      "        use_cascade = common_prefix_len > 0",
      "",
      "        attn_metadata = AiterFlashAttentionMetadata(",
      "            num_actual_tokens=num_actual_tokens,",
      "            num_actual_kv_tokens=num_actual_kv_tokens,",
      "            max_query_len=max_query_len,",
      "            query_start_loc=query_start_loc,",
      "            max_seq_len=max_seq_len,",
      "            seq_lens=seq_lens,",
      "            block_table=block_table_tensor,",
      "            slot_mapping=slot_mapping,",
      "            cu_seq_lens=cu_seq_lens,",
      "            use_cascade=use_cascade,",
      "            common_prefix_len=common_prefix_len,",
      "            total_tokens=self.total_tokens,",
      "        )",
      "        return attn_metadata",
      "",
      "    def use_cascade_attention(self, *args, **kwargs) -> bool:",
      "        return False",
      "",
      "",
      "class AiterFlashAttentionBackend(AttentionBackend):",
      "",
      "    accept_output_buffer: bool = True",
      "",
      "    @classmethod",
      "    def get_supported_dtypes(cls) -> list[torch.dtype]:",
      "        return [torch.float16, torch.bfloat16]",
      "",
      "    @classmethod",
      "    def get_supported_head_sizes(cls) -> list[int]:",
      "        return [64, 128, 256]",
      "",
      "    @classmethod",
      "    def validate_head_size(cls, head_size: int) -> None:",
      "        supported_head_sizes = cls.get_supported_head_sizes()",
      "        if head_size not in supported_head_sizes:",
      "            attn_type = cls.__name__.removesuffix(\"Backend\")",
      "            raise ValueError(",
      "                f\"Head size {head_size} is not supported by {attn_type}. \"",
      "                f\"Supported head sizes are: {supported_head_sizes}. \"",
      "                \"Set VLLM_ATTENTION_BACKEND=FLEX_ATTENTION to use \"",
      "                \"FlexAttention backend which supports all head sizes.\")",
      "",
      "    @staticmethod",
      "    def get_name() -> str:",
      "        return \"FLASH_ATTN_VLLM_V1\"",
      "",
      "    @staticmethod",
      "    def get_impl_cls() -> type[\"AiterFlashAttentionImpl\"]:",
      "        return AiterFlashAttentionImpl",
      "",
      "    @staticmethod",
      "    def get_metadata_cls() -> type[\"AttentionMetadata\"]:",
      "        return AiterFlashAttentionMetadata",
      "",
      "    @staticmethod",
      "    def get_builder_cls() -> type[\"AiterFlashAttentionMetadataBuilder\"]:",
      "        return AiterFlashAttentionMetadataBuilder",
      "",
      "    @staticmethod",
      "    def get_kv_cache_shape(",
      "        num_blocks: int,",
      "        block_size: int,",
      "        num_kv_heads: int,",
      "        head_size: int,",
      "    ) -> tuple[int, ...]:",
      "        if block_size % 16 != 0:",
      "            raise ValueError(\"Block size must be a multiple of 16.\")",
      "        return (2, num_blocks, block_size, num_kv_heads, head_size)",
      "",
      "",
      "class AiterFlashAttentionImpl(AttentionImpl):",
      "",
      "    def __init__(",
      "        self,",
      "        num_heads: int,",
      "        head_size: int,",
      "        scale: float,",
      "        num_kv_heads: int,",
      "        alibi_slopes: Optional[list[float]],",
      "        sliding_window: Optional[int],",
      "        kv_cache_dtype: str,",
      "        logits_soft_cap: Optional[float] = None,",
      "        attn_type: AttentionType = AttentionType.DECODER,",
      "        kv_sharing_target_layer_name: Optional[int] = None,",
      "    ) -> None:",
      "        self.num_heads = num_heads",
      "        self.head_size = head_size",
      "        self.scale = float(scale)",
      "        self.num_kv_heads = num_kv_heads",
      "        if alibi_slopes is not None:",
      "            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)",
      "        self.alibi_slopes = alibi_slopes",
      "        if sliding_window is None:",
      "            self.sliding_window = [-1, -1]",
      "        else:",
      "            self.sliding_window = [sliding_window - 1, 0]",
      "        self.kv_cache_dtype = kv_cache_dtype",
      "        if logits_soft_cap is None:",
      "            # In flash-attn, setting logits_soft_cap as 0 means no soft cap.",
      "            logits_soft_cap = 0.",
      "        self.logits_soft_cap = logits_soft_cap",
      "        self.kv_sharing_target_layer_name = kv_sharing_target_layer_name",
      "",
      "        assert self.num_heads % self.num_kv_heads == 0",
      "        self.num_queries_per_kv = self.num_heads // self.num_kv_heads",
      "",
      "        AiterFlashAttentionBackend.validate_head_size(head_size)",
      "",
      "        if attn_type != AttentionType.DECODER:",
      "            raise NotImplementedError(\"Encoder self-attention and \"",
      "                                      \"encoder/decoder cross-attention \"",
      "                                      \"are not implemented for \"",
      "                                      \"FlashAttentionImpl\")",
      "",
      "    def forward(",
      "        self,",
      "        layer: torch.nn.Module,",
      "        query: torch.Tensor,",
      "        key: torch.Tensor,",
      "        value: torch.Tensor,",
      "        kv_cache: torch.Tensor,",
      "        attn_metadata: AiterFlashAttentionMetadata,",
      "        output: Optional[torch.Tensor] = None,",
      "        output_scale: Optional[torch.Tensor] = None,",
      "        output_block_scale: Optional[torch.Tensor] = None,",
      "    ) -> torch.Tensor:",
      "        \"\"\"Forward pass with AiterFlashAttention.",
      "",
      "        Args:",
      "            query: shape = [num_tokens, num_heads, head_size]",
      "            key: shape = [num_tokens, num_kv_heads, head_size]",
      "            value: shape = [num_tokens, num_kv_heads, head_size]",
      "            kv_cache: shape =",
      "                [2, num_blocks, block_size, num_kv_heads, head_size]",
      "            attn_metadata: Metadata for attention.",
      "        Returns:",
      "            shape = [num_tokens, num_heads * head_size]",
      "        NOTE: FP8 quantization, flash-attn expect the size of",
      "              {q,k,v}_descale to be (num_sequences, num_kv_heads).",
      "              We use torch's .expand() to avoid duplicating values",
      "        \"\"\"",
      "        assert output is not None, \"Output tensor must be provided.\"",
      "",
      "        if output_scale is not None or output_block_scale is not None:",
      "            raise NotImplementedError(",
      "                \"fused output quantization is not yet supported\"",
      "                \" for FlashAttentionImpl\")",
      "",
      "        if attn_metadata is None:",
      "            # Profiling run.",
      "            return output",
      "",
      "        # IMPORTANT!",
      "        # NOTE(woosuk): With piece-wise CUDA graphs, this method is executed in",
      "        # eager-mode PyTorch. Thus, we need to be careful about any CPU overhead",
      "        # in this method. For example, `view` and `slice` (or `[:n]`) operations",
      "        # are surprisingly slow even in the case they do not invoke any GPU ops.",
      "        # Minimize the PyTorch ops in this method as much as possible.",
      "        # Whenever making a change in this method, please benchmark the",
      "        # performance to make sure it does not introduce any overhead.",
      "",
      "        num_actual_tokens = attn_metadata.num_actual_tokens",
      "        key_cache, value_cache = kv_cache.unbind(0)",
      "        if self.kv_sharing_target_layer_name is None:",
      "            # Reshape the input keys and values and store them in the cache.",
      "            # Skip this if sharing KV cache with an earlier attention layer.",
      "            # NOTE(woosuk): Here, key and value are padded while slot_mapping is",
      "            # not padded. However, we don't need to do key[:num_actual_tokens]",
      "            # and value[:num_actual_tokens] because the reshape_and_cache_flash",
      "            # op uses the slot_mapping's shape to determine the number of",
      "            # actual tokens.",
      "            torch.ops._C_cache_ops.reshape_and_cache_flash(",
      "                key,",
      "                value,",
      "                key_cache,",
      "                value_cache,",
      "                attn_metadata.slot_mapping,",
      "                self.kv_cache_dtype,",
      "                layer._k_scale,",
      "                layer._v_scale,",
      "            )",
      "",
      "        if self.kv_cache_dtype.startswith(\"fp8\"):",
      "            key_cache = key_cache.view(torch.float8_e4m3fnuz)",
      "            value_cache = value_cache.view(torch.float8_e4m3fnuz)",
      "",
      "        if not attn_metadata.use_cascade:",
      "            cu_seqlens_q = attn_metadata.query_start_loc",
      "            seqused_k = attn_metadata.seq_lens",
      "            max_seqlen_q = attn_metadata.max_query_len",
      "            max_seqlen_k = attn_metadata.max_seq_len",
      "            block_table = attn_metadata.block_table",
      "",
      "            if max_seqlen_q > 1:",
      "                torch.ops.vllm.flash_attn_varlen_func(",
      "                    query[:num_actual_tokens],",
      "                    key_cache,",
      "                    value_cache,",
      "                    out=output[:num_actual_tokens],",
      "                    cu_seqlens_q=cu_seqlens_q,",
      "                    max_seqlen_q=max_seqlen_q,",
      "                    max_seqlen_k=max_seqlen_k,",
      "                    softmax_scale=self.scale,",
      "                    alibi_slopes=self.alibi_slopes,",
      "                    window_size=self.sliding_window,",
      "                    block_table=block_table,",
      "                    cu_seqlens_k=attn_metadata.cu_seq_lens,",
      "                    k_scale=layer._k_scale,",
      "                    v_scale=layer._v_scale,",
      "                    total_tokens=attn_metadata.num_actual_kv_tokens,",
      "                )",
      "",
      "            _, num_heads, head_size = query.shape",
      "            nbytes_per_qo_elem = torch.finfo(query.dtype).bits // 8",
      "            num_seqs = seqused_k.shape[0]",
      "            max_num_partitions = (max_seqlen_k + _PARTITION_SIZE_ROCM -",
      "                                  1) // _PARTITION_SIZE_ROCM",
      "",
      "            workspace_buffer = torch.empty(",
      "                (num_seqs * num_heads * max_num_partitions * head_size) *",
      "                nbytes_per_qo_elem + 2 *",
      "                (num_seqs * num_heads * max_num_partitions) * 4,",
      "                dtype=torch.uint8,",
      "                device=output.device,",
      "            )",
      "",
      "            torch.ops.aiter.paged_attention_v1(",
      "                output[:num_actual_tokens],",
      "                workspace_buffer,",
      "                query[:num_actual_tokens],",
      "                key_cache,",
      "                value_cache,",
      "                self.scale,",
      "                block_table,",
      "                cu_seqlens_q,",
      "                seqused_k,",
      "                max_seqlen_k,",
      "                self.alibi_slopes,",
      "                self.kv_cache_dtype,",
      "                \"NHD\",",
      "                self.logits_soft_cap,",
      "                layer._k_scale,",
      "                layer._v_scale,",
      "                None,",
      "                _PARTITION_SIZE_ROCM,",
      "            )",
      "            return output",
      "        else:",
      "            raise NotImplementedError(",
      "                \"Cascade attention is not implemented for ROCM AITER\")"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/v1/sample/rejection_sampler.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "from typing import Optional",
      "",
      "import torch",
      "import torch.nn as nn",
      "",
      "from vllm.logger import init_logger",
      "from vllm.triton_utils import tl, triton",
      "from vllm.v1.sample.metadata import SamplingMetadata",
      "from vllm.v1.sample.ops.topk_topp_sampler import apply_top_k_top_p",
      "from vllm.v1.spec_decode.metadata import SpecDecodeMetadata",
      "",
      "logger = init_logger(__name__)",
      "",
      "PLACEHOLDER_TOKEN_ID: tl.constexpr = -1",
      "GREEDY_TEMPERATURE: tl.constexpr = -1",
      "# Maximum number of speculative draft tokens allowed per request in a single",
      "# step. This value is chosen to be large enough to handle typical use cases.",
      "MAX_SPEC_LEN = 32",
      "",
      "",
      "class RejectionSampler(nn.Module):",
      "    \"\"\"",
      "    The implementation strictly follows the algorithm described in",
      "        https://arxiv.org/abs/2211.17192.",
      "    However, we want to clarify the terminology used in the implementation:",
      "    accepted tokens: tokens that are accepted based on the relationship",
      "            between the \"raw\" draft and target probabilities.",
      "    recovered tokens: tokens that are sampled based on the adjusted probability",
      "        distribution, which is derived from both the draft and target",
      "        probabilities.",
      "    bonus tokens:",
      "        If all proposed tokens are accepted, the bonus token is added to the",
      "        end of the sequence. The bonus token is only sampled from the target",
      "        probabilities. We pass in the bonus tokens instead of sampling them",
      "        in the rejection sampler to allow for more flexibility in the",
      "        sampling process. For example, we can use top_p, top_k sampling for",
      "        bonus tokens, while spec decode does not support these sampling",
      "        strategies.",
      "    output tokens:",
      "        Tokens are finally generated with the rejection sampler.",
      "        output tokens = accepted tokens + recovered tokens + bonus tokens",
      "    \"\"\"",
      "",
      "    def forward(",
      "        self,",
      "        metadata: SpecDecodeMetadata,",
      "        # [num_tokens, vocab_size]",
      "        draft_probs: Optional[torch.Tensor],",
      "        # [num_tokens, vocab_size]",
      "        target_logits: torch.Tensor,",
      "        # [batch_size, 1]",
      "        bonus_token_ids: torch.Tensor,",
      "        sampling_metadata: SamplingMetadata,",
      "    ) -> torch.Tensor:",
      "        '''",
      "        Args:",
      "            metadata:",
      "                Metadata for spec decoding.",
      "            draft_probs (Optional[torch.Tensor]):",
      "                Probability distribution for the draft tokens. Shape is",
      "                [num_tokens, vocab_size]. Can be None if probabilities are",
      "                not provided, which is the case for ngram spec decode.",
      "            target_logits (torch.Tensor):",
      "                Target model's logits probability distribution.",
      "                Shape is [num_tokens, vocab_size]. Here, probabilities from",
      "                different requests are flattened into a single tensor because",
      "                this is the shape of the output logits.",
      "                NOTE: `target_logits` can be updated in place to save memory.",
      "            bonus_token_ids (torch.Tensor):",
      "                A tensor containing bonus tokens. Shape is [batch_size, 1].",
      "                Bonus tokens are added to the end of the sequence if all",
      "                proposed tokens are accepted. We generate the bonus tokens",
      "                outside of the rejection sampler with the default sampling",
      "                strategy. It allows for more flexibility in the sampling",
      "                process such as top_p, top_k sampling.",
      "            sampling_metadata (vllm.v1.sample.metadata.SamplingMetadata):",
      "                Additional metadata needed for sampling, such as temperature,",
      "                top-k/top-p parameters, or other relevant information.",
      "        Returns:",
      "            output_token_ids (torch.Tensor):",
      "                A tensor containing the final output token IDs.",
      "        '''",
      "        assert metadata.max_spec_len <= MAX_SPEC_LEN",
      "        # [num_tokens, vocab_size]",
      "        # NOTE(woosuk): `target_logits` can be updated in place inside the",
      "        # `compute_probs` function.",
      "        target_probs = compute_probs(",
      "            target_logits,",
      "            metadata.cu_num_draft_tokens,",
      "            sampling_metadata,",
      "        )",
      "",
      "        output_token_ids = rejection_sample(",
      "            metadata.draft_token_ids,",
      "            metadata.num_draft_tokens,",
      "            metadata.max_spec_len,",
      "            metadata.cu_num_draft_tokens,",
      "            draft_probs,",
      "            target_probs,",
      "            bonus_token_ids,",
      "            sampling_metadata,",
      "        )",
      "        return output_token_ids",
      "",
      "    @staticmethod",
      "    def parse_output(",
      "        output_token_ids: torch.Tensor,",
      "        vocab_size: int,",
      "    ) -> list[list[int]]:",
      "        \"\"\"Parse the output of the rejection sampler.",
      "",
      "        Args:",
      "            output_token_ids: The sampled token IDs in shape",
      "                [batch_size, max_spec_len + 1]. The rejected tokens are",
      "                replaced with `PLACEHOLDER_TOKEN_ID` by the rejection sampler",
      "                and will be filtered out in this function.",
      "            vocab_size: The size of the vocabulary.",
      "",
      "        Returns:",
      "            A list of lists of token IDs.",
      "        \"\"\"",
      "        output_token_ids_np = output_token_ids.cpu().numpy()",
      "        # Create mask for valid tokens.",
      "        valid_mask = ((output_token_ids_np != PLACEHOLDER_TOKEN_ID) &",
      "                      (output_token_ids_np < vocab_size))",
      "        outputs = [",
      "            row[valid_mask[i]].tolist()",
      "            for i, row in enumerate(output_token_ids_np)",
      "        ]",
      "        return outputs",
      "",
      "",
      "def rejection_sample(",
      "    # [num_tokens]",
      "    draft_token_ids: torch.Tensor,",
      "    # [batch_size]",
      "    num_draft_tokens: list[int],",
      "    max_spec_len: int,",
      "    # [batch_size]",
      "    cu_num_draft_tokens: torch.Tensor,",
      "    # [num_tokens, vocab_size]",
      "    draft_probs: Optional[torch.Tensor],",
      "    # [num_tokens, vocab_size]",
      "    target_probs: torch.Tensor,",
      "    # [batch_size, 1]",
      "    bonus_token_ids: torch.Tensor,",
      "    sampling_metadata: SamplingMetadata,",
      ") -> torch.Tensor:",
      "    assert draft_token_ids.ndim == 1",
      "    assert draft_probs is None or draft_probs.ndim == 2",
      "    assert cu_num_draft_tokens.ndim == 1",
      "    assert target_probs.ndim == 2",
      "",
      "    batch_size = len(num_draft_tokens)",
      "    num_tokens = draft_token_ids.shape[0]",
      "    vocab_size = target_probs.shape[-1]",
      "    device = target_probs.device",
      "    assert draft_token_ids.is_contiguous()",
      "    assert draft_probs is None or draft_probs.is_contiguous()",
      "    assert target_probs.is_contiguous()",
      "    assert bonus_token_ids.is_contiguous()",
      "    assert target_probs.shape == (num_tokens, vocab_size)",
      "",
      "    # Create output buffer.",
      "    output_token_ids = torch.empty(",
      "        (batch_size, max_spec_len + 1),",
      "        dtype=torch.int32,  # Consistent with SamplerOutput.sampled_token_ids.",
      "        device=device,",
      "    )",
      "    output_token_ids.fill_(PLACEHOLDER_TOKEN_ID)",
      "",
      "    if sampling_metadata.all_greedy:",
      "        is_greedy = None",
      "    else:",
      "        is_greedy = sampling_metadata.temperature == GREEDY_TEMPERATURE",
      "    if not sampling_metadata.all_random:",
      "        # Rejection sampling for greedy sampling requests.",
      "        target_argmax = target_probs.argmax(dim=-1)",
      "        rejection_greedy_sample_kernel[(batch_size, )](",
      "            output_token_ids,",
      "            cu_num_draft_tokens,",
      "            draft_token_ids,",
      "            target_argmax,",
      "            bonus_token_ids,",
      "            is_greedy,",
      "            max_spec_len,",
      "            num_warps=1,",
      "        )",
      "        if sampling_metadata.all_greedy:",
      "            return output_token_ids",
      "",
      "    # Generate uniform probabilities for rejection sampling.",
      "    # [num_tokens]",
      "    uniform_probs = generate_uniform_probs(",
      "        num_tokens,",
      "        num_draft_tokens,",
      "        sampling_metadata.generators,",
      "        device,",
      "    )",
      "",
      "    # Sample recovered tokens for each position.",
      "    # [num_tokens]",
      "    recovered_token_ids = sample_recovered_tokens(",
      "        max_spec_len,",
      "        num_draft_tokens,",
      "        cu_num_draft_tokens,",
      "        draft_token_ids,",
      "        draft_probs,",
      "        target_probs,",
      "        sampling_metadata,",
      "        device,",
      "    )",
      "",
      "    # Rejection sampling for random sampling requests.",
      "    rejection_random_sample_kernel[(batch_size, )](",
      "        output_token_ids,",
      "        cu_num_draft_tokens,",
      "        draft_token_ids,",
      "        draft_probs,",
      "        target_probs,",
      "        bonus_token_ids,",
      "        recovered_token_ids,",
      "        uniform_probs,",
      "        is_greedy,",
      "        max_spec_len,",
      "        vocab_size,",
      "        NO_DRAFT_PROBS=draft_probs is None,",
      "        num_warps=1,",
      "    )",
      "    return output_token_ids",
      "",
      "",
      "def compute_probs(",
      "    logits: torch.Tensor,  # [num_tokens, vocab_size]",
      "    cu_num_draft_tokens: torch.Tensor,  # [batch_size]",
      "    sampling_metadata: SamplingMetadata,",
      ") -> torch.Tensor:",
      "    \"\"\"Compute probability distribution from logits based on sampling metadata.",
      "",
      "    This function applies temperature scaling to the logits and converts",
      "    them to probabilities using softmax. For greedy decoding, it returns",
      "    the original logits.",
      "",
      "    Args:",
      "        logits: Input logits tensor to be converted to probabilities.",
      "        cu_num_draft_tokens: Cumulative number of draft tokens.",
      "        sampling_metadata: Metadata containing sampling parameters such as",
      "            temperature and whether greedy sampling is used.",
      "",
      "    Returns:",
      "        torch.Tensor: Probability distribution (softmax of scaled logits)",
      "            if non-greedy sampling is used, otherwise returns the",
      "            original logits.",
      "    \"\"\"",
      "    assert logits.ndim == 2",
      "    assert cu_num_draft_tokens.ndim == 1",
      "    if sampling_metadata.all_greedy:",
      "        return logits",
      "",
      "    num_tokens = logits.shape[0]",
      "    temperature = expand_batch_to_tokens(",
      "        sampling_metadata.temperature,",
      "        cu_num_draft_tokens,",
      "        num_tokens,",
      "        replace_from=GREEDY_TEMPERATURE,",
      "        replace_to=1,",
      "    )",
      "    # NOTE(woosuk): Update `logits` in place to avoid allocating a new tensor.",
      "    logits.div_(temperature.unsqueeze(-1))",
      "",
      "    # Get expanded top_k and top_p tensors.",
      "    top_k = None",
      "    if sampling_metadata.top_k is not None:",
      "        top_k = expand_batch_to_tokens(",
      "            sampling_metadata.top_k,",
      "            cu_num_draft_tokens,",
      "            num_tokens,",
      "        )",
      "    top_p = None",
      "    if sampling_metadata.top_p is not None:",
      "        top_p = expand_batch_to_tokens(",
      "            sampling_metadata.top_p,",
      "            cu_num_draft_tokens,",
      "            num_tokens,",
      "        )",
      "",
      "    # NOTE(woosuk): `apply_top_k_top_p` uses sorting to calculate the mask,",
      "    # which is slow for large vocab sizes. This may cause performance issues.",
      "    logits = apply_top_k_top_p(logits, top_k, top_p)",
      "    output_prob = logits.softmax(dim=-1, dtype=torch.float32)",
      "    return output_prob",
      "",
      "",
      "def expand_batch_to_tokens(",
      "    x: torch.Tensor,  # [batch_size]",
      "    cu_num_tokens: torch.Tensor,  # [batch_size]",
      "    num_tokens: int,",
      "    replace_from: int = 0,",
      "    replace_to: int = 0,",
      ") -> torch.Tensor:",
      "    \"\"\"Expand [batch_size] tensor to [num_tokens] tensor based on the number of",
      "    tokens per batch in cu_num_tokens.",
      "",
      "    For example, if x = [a, b, c] and cu_num_tokens = [2, 5, 6], then",
      "    num_tokens = 6, and expanded_x = [a, a, b, b, b, c].",
      "",
      "    Args:",
      "        x: [batch_size] tensor to expand.",
      "        cu_num_tokens: [batch_size] tensor containing the cumulative number of",
      "            tokens per batch. Each element represents the total number of",
      "            tokens up to and including that batch.",
      "        num_tokens: Total number of tokens.",
      "        replace_from: int = 0",
      "            Value to be replaced if it is found in x.",
      "        replace_to: int = 0",
      "            Value to replace with when replace_from is found.",
      "    Returns:",
      "        expanded_x: [num_tokens] tensor.",
      "    \"\"\"",
      "    batch_size = x.shape[0]",
      "    assert cu_num_tokens.shape[0] == batch_size",
      "    expanded_x = x.new_empty(num_tokens)",
      "    expand_kernel[(batch_size, )](",
      "        expanded_x,",
      "        x,",
      "        cu_num_tokens,",
      "        replace_from,",
      "        replace_to,",
      "        MAX_NUM_TOKENS=MAX_SPEC_LEN,  # To avoid recompilation.",
      "        num_warps=1,",
      "    )",
      "    return expanded_x",
      "",
      "",
      "def generate_uniform_probs(",
      "    num_tokens: int,",
      "    num_draft_tokens: list[int],",
      "    generators: dict[int, torch.Generator],",
      "    device: torch.device,",
      ") -> torch.Tensor:",
      "    \"\"\"",
      "    Generates a batch of uniform random samples, with optional seeding",
      "    if available.",
      "",
      "    This method creates a tensor of shape `(num_tokens, )` filled",
      "    with uniform random values in the range [0, 1). If `generators` is provided,",
      "    the requests with their own seeds will use the provided `torch.Generator`",
      "    for reproducibility. The samples for the other requests will be generated",
      "    without a seed.",
      "",
      "    Args:",
      "        num_tokens : int",
      "            Total number of tokens.",
      "        num_draft_tokens : List[List[int]]",
      "            Number of draft tokens per request.",
      "        generators : Optional[Dict[int, torch.Generator]]",
      "            A dictionary mapping indices in the batch to",
      "            `torch.Generator` objects.",
      "        device : torch.device",
      "            The device on which to allocate the tensor.",
      "    Returns:",
      "        uniform_rand : torch.Tensor",
      "            A tensor of shape `(num_tokens, )` containing uniform",
      "            random values in the range [0, 1).",
      "    \"\"\"",
      "    # NOTE(woosuk): We deliberately use float64 instead of float32 here",
      "    # because when using float32, there's a non-negligible chance that",
      "    # uniform_prob is sampled to be exact 0.0 as reported in",
      "    # https://github.com/pytorch/pytorch/issues/16706. Using float64",
      "    # mitigates the issue.",
      "    uniform_probs = torch.rand(",
      "        (num_tokens, ),",
      "        dtype=torch.float64,",
      "        device=device,",
      "    )",
      "    start_idx = 0",
      "    for req_idx, n in enumerate(num_draft_tokens):",
      "        # Do not generate random numbers for requests with no draft tokens.",
      "        # This can be important for reproducibility.",
      "        if n == 0:",
      "            continue",
      "        end_idx = start_idx + n",
      "        generator = generators.get(req_idx)",
      "        if generator is not None:",
      "            uniform_probs[start_idx:end_idx].uniform_(generator=generator)",
      "        start_idx = end_idx",
      "    return uniform_probs",
      "",
      "",
      "def sample_recovered_tokens(",
      "    max_spec_len: int,",
      "    num_draft_tokens: list[int],",
      "    # [batch_size]",
      "    cu_num_draft_tokens: torch.Tensor,",
      "    # [num_tokens]",
      "    draft_token_ids: torch.Tensor,",
      "    # [num_tokens, vocab_size]",
      "    draft_probs: Optional[torch.Tensor],",
      "    # [num_tokens, vocab_size]",
      "    target_probs: torch.Tensor,",
      "    sampling_metadata: SamplingMetadata,",
      "    device: torch.device,",
      ") -> torch.Tensor:",
      "    # NOTE(woosuk): Create only one distribution for each request.",
      "    batch_size = len(num_draft_tokens)",
      "    vocab_size = target_probs.shape[-1]",
      "    q = torch.empty(",
      "        (batch_size, vocab_size),",
      "        dtype=torch.float32,",
      "        device=device,",
      "    )",
      "    q.exponential_()",
      "    for i, generator in sampling_metadata.generators.items():",
      "        # Do not generate random numbers for requests with no draft tokens.",
      "        # This can be important for reproducibility.",
      "        if num_draft_tokens[i] > 0:",
      "            q[i].exponential_(generator=generator)",
      "",
      "    recovered_token_ids = torch.empty_like(draft_token_ids)",
      "    sample_recovered_tokens_kernel[(batch_size, max_spec_len)](",
      "        recovered_token_ids,",
      "        cu_num_draft_tokens,",
      "        draft_token_ids,",
      "        draft_probs,",
      "        target_probs,",
      "        q,",
      "        vocab_size,",
      "        triton.next_power_of_2(vocab_size),",
      "        NO_DRAFT_PROBS=draft_probs is None,",
      "    )",
      "    return recovered_token_ids",
      "",
      "",
      "# NOTE(woosuk): Avoid specialization to prevent unnecessary recompilation.",
      "@triton.jit(do_not_specialize=[\"max_spec_len\"])",
      "def rejection_greedy_sample_kernel(",
      "    output_token_ids_ptr,  # [batch_size, max_spec_len + 1]",
      "    cu_num_draft_tokens_ptr,  # [batch_size]",
      "    draft_token_ids_ptr,  # [num_tokens]",
      "    target_argmax_ptr,  # [num_tokens]",
      "    bonus_token_ids_ptr,  # [batch_size]",
      "    is_greedy_ptr,  # [batch_size] or None",
      "    max_spec_len,",
      "):",
      "    req_idx = tl.program_id(0)",
      "    # FIXME(woosuk): Because is_greedy_ptr is not None at profiling run,",
      "    # re-compilation may happen during runtime when is_greedy_ptr is None.",
      "    if is_greedy_ptr is None:",
      "        is_greedy = True",
      "    else:",
      "        is_greedy = tl.load(is_greedy_ptr + req_idx)",
      "    if not is_greedy:",
      "        # Early exit for non-greedy sampling requests.",
      "        return",
      "",
      "    if req_idx == 0:",
      "        start_idx = 0",
      "    else:",
      "        start_idx = tl.load(cu_num_draft_tokens_ptr + req_idx - 1)",
      "    end_idx = tl.load(cu_num_draft_tokens_ptr + req_idx)",
      "    num_draft_tokens = end_idx - start_idx",
      "",
      "    rejected = False",
      "    for pos in range(num_draft_tokens):",
      "        if not rejected:",
      "            draft_token_id = tl.load(draft_token_ids_ptr + start_idx + pos)",
      "            target_argmax_id = tl.load(target_argmax_ptr + start_idx + pos)",
      "            tl.store(output_token_ids_ptr + req_idx * (max_spec_len + 1) + pos,",
      "                     target_argmax_id)",
      "            if draft_token_id != target_argmax_id:",
      "                # Reject.",
      "                rejected = True",
      "",
      "    if not rejected:",
      "        # If all tokens are accepted, append the bonus token.",
      "        bonus_token_id = tl.load(bonus_token_ids_ptr + req_idx)",
      "        tl.store(",
      "            output_token_ids_ptr + req_idx * (max_spec_len + 1) +",
      "            num_draft_tokens, bonus_token_id)",
      "",
      "",
      "# NOTE(woosuk): Avoid specialization to prevent unnecessary recompilation.",
      "@triton.jit(do_not_specialize=[\"max_spec_len\"])",
      "def rejection_random_sample_kernel(",
      "    output_token_ids_ptr,  # [batch_size, max_spec_len + 1]",
      "    cu_num_draft_tokens_ptr,  # [batch_size]",
      "    draft_token_ids_ptr,  # [num_tokens]",
      "    draft_probs_ptr,  # [num_tokens, vocab_size] or None",
      "    target_probs_ptr,  # [num_tokens, vocab_size]",
      "    bonus_token_ids_ptr,  # [batch_size]",
      "    recovered_token_ids_ptr,  # [num_tokens]",
      "    uniform_probs_ptr,  # [num_tokens]",
      "    is_greedy_ptr,  # [batch_size]",
      "    max_spec_len,",
      "    vocab_size,",
      "    NO_DRAFT_PROBS: tl.constexpr,",
      "):",
      "    req_idx = tl.program_id(0)",
      "    is_greedy = tl.load(is_greedy_ptr + req_idx)",
      "    if is_greedy:",
      "        # Early exit for greedy sampling requests.",
      "        return",
      "",
      "    if req_idx == 0:",
      "        start_idx = 0",
      "    else:",
      "        start_idx = tl.load(cu_num_draft_tokens_ptr + req_idx - 1)",
      "    end_idx = tl.load(cu_num_draft_tokens_ptr + req_idx)",
      "    num_draft_tokens = end_idx - start_idx",
      "",
      "    rejected = False",
      "    for pos in range(num_draft_tokens):",
      "        if not rejected:",
      "            draft_token_id = tl.load(draft_token_ids_ptr + start_idx + pos)",
      "            if NO_DRAFT_PROBS:",
      "                draft_prob = 1",
      "            else:",
      "                draft_prob = tl.load(draft_probs_ptr +",
      "                                     (start_idx + pos) * vocab_size +",
      "                                     draft_token_id)",
      "            target_prob = tl.load(target_probs_ptr +",
      "                                  (start_idx + pos) * vocab_size +",
      "                                  draft_token_id)",
      "            uniform_prob = tl.load(uniform_probs_ptr + start_idx + pos)",
      "            # NOTE(woosuk): While the draft probability should never be 0,",
      "            # we check it to avoid NaNs. If it happens to be 0, we reject.",
      "            if draft_prob > 0 and target_prob / draft_prob >= uniform_prob:",
      "                # Accept.",
      "                token_id = draft_token_id",
      "            else:",
      "                # Reject. Use recovered token.",
      "                rejected = True",
      "                token_id = tl.load(recovered_token_ids_ptr + start_idx + pos)",
      "            tl.store(output_token_ids_ptr + req_idx * (max_spec_len + 1) + pos,",
      "                     token_id)",
      "",
      "    if not rejected:",
      "        # If all tokens are accepted, append the bonus token.",
      "        bonus_token_id = tl.load(bonus_token_ids_ptr + req_idx)",
      "        tl.store(",
      "            output_token_ids_ptr + req_idx * (max_spec_len + 1) +",
      "            num_draft_tokens, bonus_token_id)",
      "",
      "",
      "# NOTE(woosuk): Avoid specialization to prevent unnecessary recompilation.",
      "@triton.jit(do_not_specialize=[\"replace_from\", \"replace_to\"])",
      "def expand_kernel(",
      "    output_ptr,  # [num_tokens]",
      "    input_ptr,  # [batch_size]",
      "    cu_num_tokens_ptr,  # [batch_size]",
      "    replace_from,",
      "    replace_to,",
      "    MAX_NUM_TOKENS: tl.constexpr,",
      "):",
      "    req_idx = tl.program_id(0)",
      "    if req_idx == 0:  # noqa: SIM108",
      "        start_idx = 0",
      "    else:",
      "        start_idx = tl.load(cu_num_tokens_ptr + req_idx - 1)",
      "    end_idx = tl.load(cu_num_tokens_ptr + req_idx)",
      "    num_tokens = end_idx - start_idx",
      "",
      "    src_val = tl.load(input_ptr + req_idx)",
      "    src_val = tl.where(src_val == replace_from, replace_to, src_val)",
      "    offset = tl.arange(0, MAX_NUM_TOKENS)",
      "    tl.store(output_ptr + start_idx + offset,",
      "             src_val,",
      "             mask=offset < num_tokens)",
      "",
      "",
      "@triton.jit",
      "def sample_recovered_tokens_kernel(",
      "    output_token_ids_ptr,  # [num_tokens]",
      "    cu_num_draft_tokens_ptr,  # [batch_size]",
      "    draft_token_ids_ptr,  # [num_tokens]",
      "    draft_probs_ptr,  # [num_tokens, vocab_size] or None",
      "    target_probs_ptr,  # [num_tokens, vocab_size]",
      "    q_ptr,  # [batch_size, vocab_size]",
      "    vocab_size,",
      "    PADDED_VOCAB_SIZE: tl.constexpr,",
      "    NO_DRAFT_PROBS: tl.constexpr,",
      "):",
      "    req_idx = tl.program_id(0)",
      "    if req_idx == 0:",
      "        start_idx = 0",
      "    else:",
      "        start_idx = tl.load(cu_num_draft_tokens_ptr + req_idx - 1)",
      "    end_idx = tl.load(cu_num_draft_tokens_ptr + req_idx)",
      "    num_draft_tokens = end_idx - start_idx",
      "",
      "    # Early exit for out-of-range positions.",
      "    pos = tl.program_id(1)",
      "    if pos >= num_draft_tokens:",
      "        return",
      "",
      "    vocab_offset = tl.arange(0, PADDED_VOCAB_SIZE)",
      "    if NO_DRAFT_PROBS:",
      "        draft_token_id = tl.load(draft_token_ids_ptr + start_idx + pos)",
      "        prob = tl.load(target_probs_ptr + (start_idx + pos) * vocab_size +",
      "                       vocab_offset,",
      "                       mask=((vocab_offset < vocab_size) &",
      "                             (vocab_offset != draft_token_id)),",
      "                       other=0)",
      "    else:",
      "        draft_prob = tl.load(draft_probs_ptr + (start_idx + pos) * vocab_size +",
      "                             vocab_offset,",
      "                             mask=vocab_offset < vocab_size,",
      "                             other=0)",
      "        target_prob = tl.load(target_probs_ptr +",
      "                              (start_idx + pos) * vocab_size + vocab_offset,",
      "                              mask=vocab_offset < vocab_size,",
      "                              other=0)",
      "        prob = tl.maximum(target_prob - draft_prob, 0)",
      "        # NOTE(woosuk): We don't need `prob = prob / tl.sum(prob)` here because",
      "        # `tl.argmax` will select the maximum value.",
      "",
      "    q = tl.load(q_ptr + req_idx * vocab_size + vocab_offset,",
      "                mask=vocab_offset < vocab_size,",
      "                other=float(\"-inf\"))",
      "    recovered_id = tl.argmax(prob / q, axis=-1)",
      "    tl.store(output_token_ids_ptr + start_idx + pos, recovered_id)"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/lightning_attn.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "from typing import Optional",
      "",
      "import torch",
      "from einops import rearrange",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "",
      "@triton.jit",
      "def _fwd_diag_kernel(Q, K, V, Out, S, b: tl.constexpr, h: tl.constexpr, n,",
      "                     d: tl.constexpr, e: tl.constexpr, BLOCK: tl.constexpr,",
      "                     NUM_BLOCK, CBLOCK: tl.constexpr):",
      "    # This kernel computes the diagonal blocks of the attention matrix",
      "    # Each diagonal block represents attention",
      "    # where queries attend to keys in the same block",
      "    off = tl.program_id(0)",
      "    off_bh = off // NUM_BLOCK  # batch-head index",
      "    off_block = off % NUM_BLOCK  # block index within the sequence",
      "    off_cblock = tl.program_id(1)  # sub-block index within a block",
      "",
      "    off_h = off_bh % h  # head index",
      "",
      "    # Calculate base offsets for the current batch and head",
      "    qk_offset = off_bh * n * d",
      "    v_offset = off_bh * n * e",
      "    o_offset = off_bh * n * e",
      "",
      "    # Calculate offsets for the current block",
      "    block_offset = off_block * BLOCK",
      "    qk_block_offset = block_offset * d",
      "    v_block_offset = block_offset * e",
      "    o_block_offset = block_offset * e",
      "",
      "    # Calculate offsets for the current sub-block",
      "    cblock_offset = off_cblock * CBLOCK",
      "    q_cblock_offset = cblock_offset * d",
      "    o_cblock_offset = cblock_offset * e",
      "",
      "    # Calculate pointers to the query, key, value, and output tensors",
      "    Q_block_ptr = (Q + qk_offset + qk_block_offset + q_cblock_offset +",
      "                   tl.arange(0, CBLOCK)[:, None] * d +",
      "                   tl.arange(0, d)[None, :])",
      "    K_trans_block_ptr = (K + qk_offset + qk_block_offset +",
      "                         tl.arange(0, CBLOCK)[None, :] * d +",
      "                         tl.arange(0, d)[:, None])",
      "    V_block_ptr = (V + v_offset + v_block_offset +",
      "                   tl.arange(0, CBLOCK)[:, None] * e +",
      "                   tl.arange(0, e)[None, :])",
      "    O_block_ptr = (Out + o_offset + o_block_offset + o_cblock_offset +",
      "                   tl.arange(0, CBLOCK)[:, None] * e +",
      "                   tl.arange(0, e)[None, :])",
      "",
      "    # Load the decay rate for the current head",
      "    S_block_ptr = S + off_h",
      "    s = tl.load(S_block_ptr)",
      "",
      "    i = off_cblock",
      "    q_index = tl.arange(0, CBLOCK) + i * CBLOCK",
      "",
      "    # Load query values",
      "    q = tl.load(Q_block_ptr,",
      "                mask=block_offset + q_index[:, None] < n,",
      "                other=0.0).to(tl.float32)",
      "",
      "    # Initialize output accumulator",
      "    qkv = tl.zeros([CBLOCK, e], dtype=tl.float32)",
      "",
      "    # Process all sub-blocks up to and",
      "    # including the current one (causal attention)",
      "    for j in range(i + 1):",
      "        kv_index = tl.arange(0, CBLOCK) + j * CBLOCK",
      "        diff = q_index[:, None] - kv_index[None, :]",
      "        s_index = s * diff",
      "        # Apply causal mask: only attend to positions before the current one",
      "        s_index = tl.where(diff >= 0, -s_index, float(\"-inf\"))",
      "        decay = tl.exp(s_index)",
      "",
      "        # Load key and value",
      "        k_trans = tl.load(",
      "            K_trans_block_ptr,",
      "            mask=block_offset + kv_index[None, :] < n,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        v = tl.load(",
      "            V_block_ptr,",
      "            mask=block_offset + kv_index[:, None] < n,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "",
      "        # Compute attention scores and apply decay",
      "        qk = tl.dot(q, k_trans) * decay",
      "",
      "        # Compute weighted values and accumulate",
      "        qkv += tl.dot(qk, v)",
      "",
      "        # Move to the next sub-block",
      "        K_trans_block_ptr += CBLOCK * d",
      "        V_block_ptr += CBLOCK * e",
      "",
      "    # Store the result",
      "    tl.store(",
      "        O_block_ptr,",
      "        qkv.to(O_block_ptr.dtype.element_ty),",
      "        mask=block_offset + q_index[:, None] < n,",
      "    )",
      "",
      "",
      "@triton.jit",
      "def _fwd_kv_parallel(",
      "    K,",
      "    V,",
      "    K_decay,",
      "    KV,",
      "    b: tl.constexpr,",
      "    h: tl.constexpr,",
      "    n,",
      "    d: tl.constexpr,",
      "    e: tl.constexpr,",
      "    BLOCK: tl.constexpr,",
      "    NUM_BLOCK,",
      "    D_FBLOCK: tl.constexpr,",
      "    E_FBLOCK: tl.constexpr,",
      "    NUM_FBLOCK: tl.constexpr,",
      "    CBLOCK: tl.constexpr,",
      "    NUM_CBLOCK: tl.constexpr,",
      "):",
      "    # This kernel computes the key-value outer",
      "    # products for each block in parallel",
      "    off_bh = tl.program_id(0)  # batch-head index",
      "    off_block = tl.program_id(1)  # block index",
      "",
      "    off_h = off_bh % h  # head index",
      "",
      "    block_offset = off_block * BLOCK",
      "",
      "    # Calculate offsets for the current block",
      "    k_block_offset = block_offset * d",
      "    v_block_offset = block_offset * e",
      "    kv_block_offset = off_block * d * e",
      "",
      "    # Calculate base offsets for the current batch and head",
      "    k_offset = off_bh * n * d",
      "    v_offset = off_bh * n * e",
      "    kv_offset = off_bh * NUM_BLOCK * d * e",
      "",
      "    # Calculate pointers to the key, value, and key-value tensors",
      "    K_trans_block_ptr = (K + k_offset + k_block_offset +",
      "                         tl.arange(0, CBLOCK)[None, :] * d +",
      "                         tl.arange(0, D_FBLOCK)[:, None])",
      "    V_block_ptr = (V + v_offset + v_block_offset +",
      "                   tl.arange(0, CBLOCK)[:, None] * e +",
      "                   tl.arange(0, E_FBLOCK)[None, :])",
      "    KV_block_ptr = (KV + kv_offset + kv_block_offset +",
      "                    tl.arange(0, D_FBLOCK)[:, None] * e +",
      "                    tl.arange(0, E_FBLOCK)[None, :])",
      "",
      "    # Load the decay factors for the current head and block",
      "    k_decay_ptr = (K_decay + off_h * BLOCK + tl.arange(0, CBLOCK)[None, :])",
      "",
      "    kv_index = tl.arange(0, CBLOCK)",
      "",
      "    # Initialize the key-value outer product accumulator",
      "    kv = tl.zeros([D_FBLOCK, E_FBLOCK], dtype=tl.float32)",
      "",
      "    # Handle the last block which might be smaller than BLOCK",
      "    if off_block == NUM_BLOCK - 1:",
      "        split_n = n - (NUM_BLOCK - 1) * BLOCK",
      "    else:",
      "        split_n = BLOCK",
      "    left_shift = tl.cdiv(split_n, CBLOCK) * CBLOCK - split_n",
      "    num_blocks = min(tl.cdiv(split_n, CBLOCK), NUM_CBLOCK)",
      "    k_decay_ptr += (NUM_CBLOCK - num_blocks) * CBLOCK",
      "",
      "    # Process all sub-blocks in the current block",
      "    for j in range(num_blocks):",
      "        left_bound = (1 - j) * left_shift",
      "        # Load key and value, handling boundary conditions",
      "        k_trans = tl.load(K_trans_block_ptr - left_shift * d,",
      "                          mask=kv_index[None, :] >= left_bound,",
      "                          other=0.0)",
      "        v = tl.load(V_block_ptr - left_shift * e,",
      "                    mask=kv_index[:, None] >= left_bound,",
      "                    other=0.0)",
      "",
      "        # Load decay factor and compute weighted key-value outer product",
      "        k_decay = tl.load(k_decay_ptr)",
      "        kv += tl.dot(k_trans * k_decay, v)",
      "",
      "        # Move to the next sub-block",
      "        K_trans_block_ptr += CBLOCK * d",
      "        V_block_ptr += CBLOCK * e",
      "        k_decay_ptr += CBLOCK",
      "",
      "    # Store the result",
      "    tl.store(KV_block_ptr, kv.to(KV_block_ptr.dtype.element_ty))",
      "",
      "",
      "@triton.jit",
      "def _fwd_kv_reduce(S, KV, KV_HISTORY, b: tl.constexpr, h: tl.constexpr, n,",
      "                   d: tl.constexpr, e: tl.constexpr, BLOCK: tl.constexpr,",
      "                   NUM_BLOCK, D_FBLOCK: tl.constexpr, E_FBLOCK: tl.constexpr):",
      "    # This kernel reduces the key-value outer products",
      "    # across blocks and updates the KV history",
      "    off_bh = tl.program_id(0)  # batch-head index",
      "    off_h = off_bh % h  # head index",
      "",
      "    kv_offset = off_bh * NUM_BLOCK * d * e",
      "",
      "    # Calculate pointer to the key-value tensor",
      "    KV_block_ptr = (KV + kv_offset + tl.arange(0, D_FBLOCK)[:, None] * e +",
      "                    tl.arange(0, E_FBLOCK)[None, :])",
      "",
      "    # Load the decay rate for the current head",
      "    s_ptrs = S + off_h",
      "    s = tl.load(s_ptrs)",
      "",
      "    # Calculate pointer to the key-value history tensor",
      "    kv_history_offset = off_bh * d * e",
      "    KV_HISTORY_block_ptr = (KV_HISTORY + kv_history_offset +",
      "                            tl.arange(0, D_FBLOCK)[:, None] * e +",
      "                            tl.arange(0, E_FBLOCK)[None, :])",
      "",
      "    # Load the previous key-value history",
      "    kv_pre = tl.load(KV_HISTORY_block_ptr).to(tl.float32)",
      "",
      "    # Process all blocks in reverse order to compute the prefix sum",
      "    for i in range(NUM_BLOCK):",
      "        block_size = min(n - i * BLOCK, BLOCK)",
      "        # Compute decay factor for the current block",
      "        block_decay = tl.exp(-s.to(tl.float32) * block_size)",
      "",
      "        # Load the current key-value outer product",
      "        kv_cur = tl.load(KV_block_ptr).to(tl.float32)",
      "        # Store the previous key-value history to the current block",
      "        tl.store(KV_block_ptr, kv_pre.to(KV_block_ptr.dtype.element_ty))",
      "",
      "        # Update the key-value history with the current block",
      "        kv_pre = block_decay * kv_pre + kv_cur",
      "        KV_block_ptr += d * e",
      "",
      "    # Store the updated key-value history",
      "    tl.store(KV_HISTORY_block_ptr, kv_pre)",
      "",
      "",
      "@triton.jit",
      "def _fwd_none_diag_kernel(",
      "    Q,",
      "    Out,",
      "    S,",
      "    KV,",
      "    b: tl.constexpr,",
      "    h: tl.constexpr,",
      "    n,",
      "    d: tl.constexpr,",
      "    e: tl.constexpr,",
      "    BLOCK: tl.constexpr,",
      "    NUM_BLOCK,",
      "    E_FBLOCK: tl.constexpr,",
      "    CBLOCK: tl.constexpr,",
      "    NUM_CBLOCK: tl.constexpr,",
      "):",
      "    # This kernel computes the non-diagonal blocks of the attention matrix",
      "    # Each non-diagonal block represents attention",
      "    # where queries attend to keys in different blocks",
      "    off_bh = tl.program_id(0)  # batch-head index",
      "    off_h = off_bh % h  # head index",
      "",
      "    off_nc = tl.program_id(1)",
      "    off_n = off_nc // NUM_CBLOCK  # block index",
      "    off_c = off_nc % NUM_CBLOCK  # sub-block index",
      "    off_e = tl.program_id(2)  # output feature block index",
      "",
      "    n_offset = off_n * BLOCK",
      "    c_offset = off_c * CBLOCK",
      "    e_offset = off_e * E_FBLOCK",
      "    block_offset = n_offset + c_offset",
      "",
      "    # Calculate offsets for the current batch, head, and block",
      "    q_offset = off_bh * n * d + (n_offset + c_offset) * d",
      "    o_offset = off_bh * n * e + (n_offset + c_offset) * e + e_offset",
      "    kv_offset = off_bh * NUM_BLOCK * d * e + off_n * d * e + e_offset",
      "",
      "    # Calculate pointers to the query, output, and key-value tensors",
      "    Q_block_ptr = (Q + q_offset + tl.arange(0, CBLOCK)[:, None] * d +",
      "                   tl.arange(0, d)[None, :])",
      "    O_block_ptr = (Out + o_offset + tl.arange(0, CBLOCK)[:, None] * e +",
      "                   tl.arange(0, E_FBLOCK)[None, :])",
      "    KV_block_ptr = (KV + kv_offset + tl.arange(0, d)[:, None] * e +",
      "                    tl.arange(0, E_FBLOCK)[None, :])",
      "",
      "    # Load the decay rate for the current head",
      "    S_block_ptr = S + off_h",
      "    s = tl.load(S_block_ptr)",
      "",
      "    c_array = tl.arange(0, CBLOCK)",
      "",
      "    # Load the key-value outer product for the current block",
      "    kv = tl.load(KV_block_ptr).to(tl.float32)",
      "    q_index = block_offset + tl.arange(0, CBLOCK)",
      "",
      "    # Load query values",
      "    q = tl.load(Q_block_ptr, mask=q_index[:, None] < n,",
      "                other=0.).to(tl.float32)",
      "",
      "    # Compute decay factors for the current sub-block",
      "    q_decay = tl.exp(-s.to(tl.float32) * (off_c * CBLOCK + c_array[:, None]))",
      "",
      "    # Compute non-diagonal attention output",
      "    qkv_none_diag = tl.dot(q, kv) * q_decay",
      "",
      "    # Load diagonal attention output (computed by _fwd_diag_kernel)",
      "    qkv_diag = tl.load(O_block_ptr, mask=q_index[:, None] < n,",
      "                       other=0.).to(tl.float32)",
      "",
      "    # Combine diagonal and non-diagonal attention outputs",
      "    qkv = qkv_diag + qkv_none_diag",
      "",
      "    # Store the result",
      "    tl.store(O_block_ptr,",
      "             qkv.to(O_block_ptr.dtype.element_ty),",
      "             mask=q_index[:, None] < n)",
      "",
      "",
      "class _attention(torch.autograd.Function):",
      "",
      "    @staticmethod",
      "    def forward(ctx, q, k, v, s, kv_history):",
      "        # Forward pass of the lightning attention algorithm",
      "        q = q.contiguous()",
      "        k = k.contiguous()",
      "        v = v.contiguous()",
      "        s = s.contiguous()",
      "",
      "        # Check CUDA compute capability",
      "        capability = torch.cuda.get_device_capability()",
      "        if capability[0] < 8:",
      "            raise RuntimeError(\"Flash attention currently only supported\",",
      "                               \"for compute capability >= 80\")",
      "",
      "        # Get input dimensions",
      "        b, h, n, d = q.shape",
      "        e = v.shape[-1]",
      "",
      "        # Initialize output tensor",
      "        o = torch.empty((b, h, n, e), dtype=q.dtype, device=q.device)",
      "",
      "        # Set block sizes",
      "        BLOCK = 256",
      "        NUM_BLOCK = triton.cdiv(n, BLOCK)",
      "",
      "        CBLOCK = 32",
      "        NUM_CBLOCK = BLOCK // CBLOCK",
      "        assert BLOCK % CBLOCK == 0, \"BLOCK must be a multiple of CBLOCK\"",
      "",
      "        # Compute decay factors for keys",
      "        array = torch.arange(0, BLOCK, device=q.device) + 1",
      "        k_decay = torch.exp(-s * (BLOCK - array.reshape(1, -1)))",
      "",
      "        # Step 1: Compute diagonal blocks of attention",
      "        grid = (b * h * NUM_BLOCK, NUM_CBLOCK)",
      "        _fwd_diag_kernel[grid](q,",
      "                               k,",
      "                               v,",
      "                               o,",
      "                               s,",
      "                               b,",
      "                               h,",
      "                               n,",
      "                               d,",
      "                               e,",
      "                               BLOCK=BLOCK,",
      "                               NUM_BLOCK=NUM_BLOCK,",
      "                               CBLOCK=CBLOCK)",
      "",
      "        # Set feature block sizes",
      "        NUM_FBLOCK = 1",
      "        D_FBLOCK = d // NUM_FBLOCK",
      "        assert d % NUM_FBLOCK == 0",
      "        E_FBLOCK = e // NUM_FBLOCK",
      "        assert e % NUM_FBLOCK == 0",
      "",
      "        CBLOCK = 64",
      "        NUM_CBLOCK = BLOCK // CBLOCK",
      "        assert BLOCK % CBLOCK == 0, \"BLOCK must be a multiple of CBLOCK\"",
      "",
      "        # Step 2: Compute key-value outer products for each block in parallel",
      "        kv = torch.empty((b, h, NUM_BLOCK, d, e),",
      "                         dtype=torch.float32,",
      "                         device=q.device)",
      "        grid = (b * h, NUM_BLOCK)",
      "        _fwd_kv_parallel[grid](",
      "            k,",
      "            v,",
      "            k_decay,",
      "            kv,",
      "            b,",
      "            h,",
      "            n,",
      "            d,",
      "            e,",
      "            BLOCK=BLOCK,",
      "            NUM_BLOCK=NUM_BLOCK,",
      "            D_FBLOCK=D_FBLOCK,",
      "            E_FBLOCK=E_FBLOCK,",
      "            NUM_FBLOCK=NUM_FBLOCK,",
      "            CBLOCK=CBLOCK,",
      "            NUM_CBLOCK=NUM_CBLOCK,",
      "        )",
      "",
      "        # Step 3: Reduce key-value outer products",
      "        # across blocks and update KV history",
      "        grid = (b * h, NUM_FBLOCK)",
      "        _fwd_kv_reduce[grid](s,",
      "                             kv,",
      "                             kv_history,",
      "                             b,",
      "                             h,",
      "                             n,",
      "                             d,",
      "                             e,",
      "                             BLOCK=BLOCK,",
      "                             NUM_BLOCK=NUM_BLOCK,",
      "                             D_FBLOCK=D_FBLOCK,",
      "                             E_FBLOCK=E_FBLOCK)",
      "",
      "        # Step 4: Compute non-diagonal blocks of attention",
      "        grid = (b * h, NUM_BLOCK * NUM_CBLOCK)",
      "        _fwd_none_diag_kernel[grid](",
      "            q,",
      "            o,",
      "            s,",
      "            kv,",
      "            b,",
      "            h,",
      "            n,",
      "            d,",
      "            e,",
      "            BLOCK=BLOCK,",
      "            NUM_BLOCK=NUM_BLOCK,",
      "            E_FBLOCK=E_FBLOCK,",
      "            CBLOCK=CBLOCK,",
      "            NUM_CBLOCK=NUM_CBLOCK,",
      "        )",
      "",
      "        # Save tensors for backward pass",
      "        ctx.save_for_backward(q, k, v, s, kv)",
      "        ctx.BLOCK = BLOCK",
      "",
      "        return o, torch.cat([kv, kv_history.unsqueeze(2)], dim=2)",
      "",
      "",
      "# Apply the lightning attention function",
      "lightning_attention_ = _attention.apply",
      "",
      "",
      "def lightning_attention(",
      "    q: torch.Tensor,",
      "    k: torch.Tensor,",
      "    v: torch.Tensor,",
      "    ed: torch.Tensor,",
      "    block_size: int = 256,",
      "    kv_history: Optional[torch.Tensor] = None",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    \"\"\"",
      "    Apply lightning attention algorithm ",
      "    to compute attention efficiently.",
      "    ",
      "    Args:",
      "        q: Query tensor of shape [batch, heads, seq_len, dim]",
      "        k: Key tensor of shape [batch, heads, seq_len, dim]",
      "        v: Value tensor of shape [batch, heads, seq_len, dim_v]",
      "        ed: Decay rate tensor of shape [heads]",
      "        block_size: Size of blocks for block-sparse attention",
      "        kv_history: Optional key-value history from previous computations",
      "        ",
      "    Returns:",
      "        output: Attention output",
      "        kv: Updated key-value history",
      "    \"\"\"",
      "    d = q.shape[-1]",
      "    e = v.shape[-1]",
      "",
      "    if ed.dim() == 1:",
      "        ed = ed.view(1, -1, 1, 1)",
      "",
      "    # Split the computation into chunks for better parallelism",
      "    m = 128 if d >= 128 else 64",
      "    assert d % m == 0, f\"Dimension d ({d}) must be divisible by m ({m})\"",
      "    arr = [m * i for i in range(d // m + 1)]",
      "    if arr[-1] != d:",
      "        arr.append(d)",
      "    n = len(arr)",
      "    output = 0",
      "",
      "    # Initialize or clone key-value history",
      "    if kv_history is None:",
      "        kv_history = torch.zeros((q.shape[0], q.shape[1], d, e),",
      "                                 dtype=torch.float32,",
      "                                 device=q.device)",
      "    else:",
      "        kv_history = kv_history.clone().contiguous()",
      "",
      "    # Process each chunk and accumulate results",
      "    for i in range(n - 1):",
      "        s = arr[i]",
      "        e = arr[i + 1]",
      "        q1 = q[..., s:e]",
      "        k1 = k[..., s:e]",
      "        o, kv = lightning_attention_(q1, k1, v, ed, kv_history)",
      "        output = output + o",
      "    return output, kv",
      "",
      "",
      "@triton.jit",
      "def _linear_attn_decode_kernel(",
      "    q_ptr,",
      "    k_ptr,",
      "    v_ptr,",
      "    kv_cache_ptr,",
      "    slope_rate,",
      "    slot_idx,",
      "    output_ptr,",
      "    D: tl.constexpr,",
      "    qkv_b_stride,",
      "    qkv_h_stride,",
      "    cache_b_stride,",
      "    cache_h_stride,",
      "    cache_d0_stride,",
      "    cache_d1_stride,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    \"\"\"",
      "    Kernel for linear attention decoding with KV cache.",
      "    ",
      "    This kernel computes attention for a single token using the KV cache.",
      "    \"\"\"",
      "    pid_b = tl.program_id(0)  # batch index",
      "    pid_h = tl.program_id(1)  # head index",
      "    pid_d = tl.program_id(2)  # dimension block index",
      "",
      "    # Load slot index for the current batch",
      "    slot_id = tl.load(slot_idx + pid_b).to(tl.int64)",
      "",
      "    # Skip if slot_id is -1 (padding)",
      "    if slot_id == -1:",
      "        return",
      "",
      "    batch_id = pid_b",
      "    head_id = pid_h",
      "",
      "    # Load decay rate for the current head",
      "    ratio = tl.load(slope_rate + pid_h)",
      "",
      "    # Calculate offsets for dimensions",
      "    qk_d_offsets = tl.arange(0, D)",
      "    v_d_offsets = tl.arange(0, BLOCK_SIZE) + pid_d * BLOCK_SIZE",
      "    cache_d_offsets = qk_d_offsets[:, None] * cache_d0_stride + v_d_offsets[",
      "        None, :] * cache_d1_stride",
      "",
      "    # Calculate offsets for the current batch and head",
      "    q_offset = batch_id * qkv_b_stride + head_id * qkv_h_stride",
      "    k_offset = batch_id * qkv_b_stride + head_id * qkv_h_stride",
      "    v_offset = batch_id * qkv_b_stride + head_id * qkv_h_stride",
      "",
      "    cache_offset = slot_id * cache_b_stride + head_id * cache_h_stride",
      "",
      "    # Create masks for loading tensors",
      "    qk_mask = qk_d_offsets < D",
      "    v_mask = v_d_offsets < D",
      "",
      "    # Load query, key, and value tensors",
      "    q = tl.load(q_ptr + q_offset + qk_d_offsets, mask=qk_mask, other=0.0)",
      "    k = tl.load(k_ptr + k_offset + qk_d_offsets, mask=qk_mask, other=0.0)",
      "    v = tl.load(v_ptr + v_offset + v_d_offsets, mask=v_mask, other=0.0)",
      "",
      "    # Compute key-value outer product",
      "    kv_outer = k[:, None] * v[None, :]",
      "    kv_mask = qk_mask[:, None] & v_mask[None, :]",
      "",
      "    # Apply decay to previous KV cache",
      "    ratio = tl.exp(-ratio)",
      "    kv_ptr = kv_cache_ptr + cache_offset + cache_d_offsets",
      "    kv_cache_old = tl.load(kv_ptr, mask=kv_mask, other=0.0)",
      "    kv_outer = kv_outer + ratio * kv_cache_old",
      "",
      "    # Compute attention output",
      "    output = q[:, None].to(tl.float32) * kv_outer",
      "    output = tl.sum(output, axis=0)",
      "",
      "    # Update KV cache and store output",
      "    tl.store(kv_ptr, kv_outer, mask=kv_mask)",
      "    tl.store(output_ptr + q_offset + v_d_offsets, output, mask=v_mask)",
      "",
      "",
      "def linear_decode_forward_triton(",
      "    q: torch.Tensor,",
      "    k: torch.Tensor,",
      "    v: torch.Tensor,",
      "    kv_caches: torch.Tensor,",
      "    slope_rate: torch.Tensor,",
      "    slot_idx: torch.Tensor,",
      "    BLOCK_SIZE: int = 32,",
      ") -> torch.Tensor:",
      "    \"\"\"",
      "    Perform linear attention decoding using Triton kernels.",
      "    ",
      "    Args:",
      "        q: Query tensor of shape [B, H, 1, D]",
      "        k: Key tensor of shape [B, H, 1, D]",
      "        v: Value tensor of shape [B, H, 1, D]",
      "        kv_caches: Key-value cache tensor",
      "        slope_rate: Decay rate tensor",
      "        slot_idx: Slot indices for batches",
      "        BLOCK_SIZE: Size of blocks for processing",
      "        ",
      "    Returns:",
      "        output: Attention output tensor",
      "    \"\"\"",
      "    B, H, _, D = q.shape",
      "    assert k.shape == (B, H, 1, D)",
      "    assert v.shape == (B, H, 1, D)",
      "",
      "    # Initialize output tensor",
      "    output = torch.empty_like(q)",
      "",
      "    # Set grid dimensions for the kernel",
      "    grid = (B, H, D // BLOCK_SIZE)",
      "",
      "    # Calculate strides for tensors",
      "    qkv_b_stride = q.stride(0)",
      "    qkv_h_stride = q.stride(1)",
      "",
      "    cache_b_stride = kv_caches.stride(0)",
      "    cache_h_stride = kv_caches.stride(1)",
      "    cache_d0_stride = kv_caches.stride(2)",
      "    cache_d1_stride = kv_caches.stride(3)",
      "",
      "    # Launch the kernel",
      "    _linear_attn_decode_kernel[grid](",
      "        q,",
      "        k,",
      "        v,",
      "        kv_caches,",
      "        slope_rate,",
      "        slot_idx,",
      "        output,",
      "        D,",
      "        qkv_b_stride,",
      "        qkv_h_stride,",
      "        cache_b_stride,",
      "        cache_h_stride,",
      "        cache_d0_stride,",
      "        cache_d1_stride,",
      "        BLOCK_SIZE=BLOCK_SIZE,",
      "    )",
      "",
      "    # Reshape output and return",
      "    output = rearrange(output, \"b h n d -> b n (h d)\")",
      "    return output.squeeze(1).contiguous()"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/mamba/ops/ssd_state_passing.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "# Copyright (c) 2024, Tri Dao, Albert Gu.",
      "# Adapted from https://github.com/state-spaces/mamba/blob/v2.2.4/mamba_ssm/ops/triton/ssd_state_passing.py",
      "",
      "# ruff: noqa: E501",
      "",
      "import torch",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "",
      "@triton.autotune(",
      "    configs=[",
      "        triton.Config({'BLOCK_SIZE': 64}),",
      "        triton.Config({'BLOCK_SIZE': 128}),",
      "        triton.Config({'BLOCK_SIZE': 256}),",
      "        triton.Config({'BLOCK_SIZE': 512}),",
      "        triton.Config({'BLOCK_SIZE': 1024}),",
      "        triton.Config({'BLOCK_SIZE': 2048}),",
      "    ],",
      "    key=['dim'],",
      ")",
      "@triton.jit",
      "def _state_passing_fwd_kernel(",
      "    # Pointers to matrices",
      "    states_ptr,",
      "    out_ptr,",
      "    final_states_ptr,",
      "    dA_cs_ptr,",
      "    initstates_ptr,",
      "    seq_idx_ptr,",
      "    chunk_offsets_ptr,",
      "    chunk_meta_num,",
      "    # Matrix dimensions",
      "    dim,",
      "    nchunks,",
      "    seqlen,",
      "    chunk_size,",
      "    # Strides",
      "    stride_states_batch,",
      "    stride_states_chunk,",
      "    stride_states_head,",
      "    stride_states_dim,",
      "    stride_out_batch,",
      "    stride_out_chunk,",
      "    stride_out_head,",
      "    stride_out_dim,",
      "    stride_final_states_batch,",
      "    stride_final_states_head,",
      "    stride_final_states_dim,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_initstates_batch,",
      "    stride_initstates_head,",
      "    stride_initstates_dim,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    # Meta-parameters",
      "    HAS_INITSTATES: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    IS_CONT_BATCHED: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head",
      "    dA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head + (",
      "        chunk_size - 1) * stride_dA_cs_csize",
      "    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head",
      "    final_states_ptr += pid_b * stride_final_states_batch + pid_h * stride_final_states_head",
      "    if HAS_INITSTATES:",
      "        initstates_ptr += pid_h * stride_initstates_head",
      "        if not IS_CONT_BATCHED:",
      "            initstates_ptr += pid_b * stride_initstates_batch",
      "",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += pid_b * stride_seq_idx_batch",
      "",
      "    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    states_ptrs = states_ptr + offs_m * stride_states_dim",
      "    out_ptrs = out_ptr + offs_m * stride_out_dim",
      "    final_states_ptrs = final_states_ptr + offs_m * stride_final_states_dim",
      "",
      "    # - states will be the past state of the sequence that continues on the current check",
      "    if not HAS_INITSTATES:",
      "        states = tl.zeros((BLOCK_SIZE, ), dtype=tl.float32)",
      "    else:",
      "        initstates_ptr += offs_m * stride_initstates_dim",
      "        initstates_ptrs = initstates_ptr",
      "        # - for cont batches, for the first chunk mean it will be the first batch's",
      "        #   init state",
      "        states = tl.load(initstates_ptrs, mask=offs_m < dim,",
      "                         other=0.0).to(tl.float32)",
      "",
      "    tl.store(out_ptrs, states, mask=offs_m < dim)",
      "    out_ptrs += stride_out_chunk",
      "    prev_seq_idx_chunk_end = 0",
      "    logical_chunk_idx = 0",
      "    for c in range(nchunks):",
      "        new_states = tl.load(states_ptrs, mask=offs_m < dim,",
      "                             other=0.0).to(tl.float32)",
      "        dA_cs = tl.load(dA_cs_ptr).to(tl.float32)",
      "        scale_mask = True",
      "        if HAS_SEQ_IDX:",
      "            # - the seq to pass forward is the one that is flushed to the right",
      "            #   boundary.",
      "            # - that is given by seq_idx_chunk_end below: the sequence index at the end of the chunk.",
      "            seq_idx_chunk_end = tl.load(seq_idx_ptr + (min(",
      "                (c + 1) * chunk_size, seqlen) - 1) * stride_seq_idx_seqlen)",
      "            if HAS_INITSTATES:",
      "                if IS_CONT_BATCHED and prev_seq_idx_chunk_end != seq_idx_chunk_end:",
      "                    # this means in the current chunk the rightmost flushed seq",
      "                    # has changed.",
      "                    # - so we do not propagate the state from previous chunk",
      "                    # - but rather we load that sequence's init state",
      "                    initstates_ptrs = initstates_ptr + seq_idx_chunk_end * stride_initstates_batch",
      "",
      "                    # - update state with seq_idx_new's init state",
      "                    states = tl.load(initstates_ptrs,",
      "                                     mask=offs_m < dim,",
      "                                     other=0.0).to(tl.float32)",
      "",
      "                    # - we need to consider the cumsum only of the last sequence in the chunk",
      "                    # - find its starting position (given by c_off of the logical chunk index)",
      "                    # - and subtract the cumsum just before that position from the total cumsum",
      "                    # - first, update the logical chunk index (add the number of sequences in the current physical chunk):",
      "                    # sequence index at the start of the current chunk",
      "                    seq_idx_chunk_start = tl.load(seq_idx_ptr +",
      "                                                  min(c * chunk_size, seqlen) *",
      "                                                  stride_seq_idx_seqlen)",
      "                    logical_chunk_idx += seq_idx_chunk_end - seq_idx_chunk_start",
      "                    # - load the chunk offset:",
      "                    c_off = tl.load(chunk_offsets_ptr + logical_chunk_idx,",
      "                                    mask=logical_chunk_idx < chunk_meta_num,",
      "                                    other=0)",
      "                    # - if offset is 0, then the sequence starts at the beginning of the chunk, and we don't need to subtract anything",
      "                    if c_off > 0:",
      "                        # - dA_cs_ptr currently points to the cumsum at the end of the chunk - subtract the chunk size and add the offset",
      "                        dA_cs_boundary = tl.load(",
      "                            dA_cs_ptr - (chunk_size - 1) * stride_dA_cs_csize +",
      "                            (c_off - 1) * stride_dA_cs_csize,",
      "                            mask=(c_off - 1) > -1 and c_off < chunk_size,",
      "                            other=0.0)",
      "                        dA_cs -= dA_cs_boundary",
      "",
      "                # - increment logical chunk index for every physical chunk",
      "                logical_chunk_idx += 1",
      "            else:",
      "                scale_mask = seq_idx_chunk_end == prev_seq_idx_chunk_end",
      "            prev_seq_idx_chunk_end = seq_idx_chunk_end",
      "",
      "        scale = tl.where(scale_mask, tl.exp(dA_cs), 0.0)",
      "        states = scale * states + new_states",
      "        if c < nchunks - 1:",
      "            tl.store(out_ptrs, states, mask=offs_m < dim)",
      "        else:",
      "            tl.store(final_states_ptrs, states, mask=offs_m < dim)",
      "        states_ptrs += stride_states_chunk",
      "        dA_cs_ptr += stride_dA_cs_chunk",
      "        out_ptrs += stride_out_chunk",
      "",
      "",
      "def _state_passing_fwd(",
      "    states,",
      "    dA_cumsum,",
      "    initial_states=None,",
      "    seq_idx=None,",
      "    chunk_size=None,",
      "    out_dtype=None,",
      "    is_cont_batched=False,",
      "    chunk_offsets=None,",
      "):",
      "    batch, nchunks, nheads, dim = states.shape",
      "    if chunk_size is None:",
      "        chunk_size = dA_cumsum.shape[-1]",
      "    else:",
      "        assert chunk_size == dA_cumsum.shape[-1]",
      "    assert dA_cumsum.shape == (batch, nheads, nchunks, chunk_size)",
      "    if initial_states is not None:",
      "        if is_cont_batched:",
      "            # - if cu_seqlens is provided, then the initial states",
      "            #   are used for continuous batching. In which case we",
      "            #   require seq_idx to be provided",
      "            assert seq_idx is not None, \"seq_idx must be provided for continuous batching\"",
      "            # - we also need chunk_offsets to be provided, to account",
      "            #   for computation of dA_cumsum from the start of the",
      "            #   sequence",
      "            assert chunk_offsets is not None, \"chunk_offsets must be provided for continuous batching\"",
      "        else:",
      "            # - this is the regular batching case, where initial",
      "            #   states are used are for each example of the batch.",
      "            assert initial_states.shape == (batch, nheads, dim)",
      "",
      "    if seq_idx is not None:",
      "        seqlen = seq_idx.shape[-1]",
      "        assert seq_idx.shape == (batch, seqlen)",
      "    out_dtype = states.dtype if out_dtype is None else out_dtype",
      "    out = torch.empty((batch, nchunks, nheads, dim),",
      "                      device=states.device,",
      "                      dtype=out_dtype)",
      "    final_states = torch.empty((batch, nheads, dim),",
      "                               device=states.device,",
      "                               dtype=torch.float32)",
      "    grid = lambda META: (triton.cdiv(dim, META['BLOCK_SIZE']), batch, nheads)",
      "    with torch.cuda.device(states.device.index):",
      "        _state_passing_fwd_kernel[grid](",
      "            states,",
      "            out,",
      "            final_states,",
      "            dA_cumsum,",
      "            initial_states,",
      "            seq_idx,",
      "            chunk_offsets,",
      "            len(chunk_offsets) if chunk_offsets is not None else 0,",
      "            dim,",
      "            nchunks,",
      "            seqlen if seq_idx is not None else 0,",
      "            chunk_size,",
      "            states.stride(0),",
      "            states.stride(1),",
      "            states.stride(2),",
      "            states.stride(3),",
      "            out.stride(0),",
      "            out.stride(1),",
      "            out.stride(2),",
      "            out.stride(3),",
      "            final_states.stride(0),",
      "            final_states.stride(1),",
      "            final_states.stride(2),",
      "            dA_cumsum.stride(0),",
      "            dA_cumsum.stride(2),",
      "            dA_cumsum.stride(1),",
      "            dA_cumsum.stride(3),",
      "            *((initial_states.stride(0), initial_states.stride(1),",
      "               initial_states.stride(2)) if initial_states is not None else",
      "              (0, 0, 0)),",
      "            *((seq_idx.stride(0),",
      "               seq_idx.stride(1)) if seq_idx is not None else (0, 0)),",
      "            HAS_INITSTATES=initial_states is not None,",
      "            HAS_SEQ_IDX=seq_idx is not None,",
      "            IS_CONT_BATCHED=is_cont_batched,",
      "        )",
      "    return out, final_states"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/mamba/ops/mamba_ssm.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "# Copyright (c) 2024, Tri Dao, Albert Gu.",
      "# Adapted from https://github.com/state-spaces/mamba/blob/v2.2.4/mamba_ssm/ops/triton/selective_state_update.py",
      "",
      "import torch",
      "from packaging import version",
      "",
      "from vllm import _custom_ops as ops",
      "from vllm.attention.backends.utils import PAD_SLOT_ID",
      "from vllm.triton_utils import HAS_TRITON, tl, triton",
      "",
      "TRITON3 = HAS_TRITON and (version.parse(triton.__version__)",
      "                          >= version.parse(\"3.0.0\"))",
      "",
      "if TRITON3:",
      "",
      "    @triton.jit",
      "    def softplus(dt):",
      "        dt = tl.where(dt <= 20.0, tl.math.log(tl.math.exp(dt) + 1), dt)",
      "        return dt",
      "else:",
      "",
      "    @triton.jit",
      "    def softplus(dt):",
      "        dt = tl.where(dt <= 20.0, tl.math.log1p(tl.exp(dt)), dt)",
      "        return dt",
      "",
      "",
      "@triton.heuristics(",
      "    {\"HAS_DT_BIAS\": lambda args: args[\"dt_bias_ptr\"] is not None})",
      "@triton.heuristics({\"HAS_D\": lambda args: args[\"D_ptr\"] is not None})",
      "@triton.heuristics({\"HAS_Z\": lambda args: args[\"z_ptr\"] is not None})",
      "@triton.heuristics({",
      "    \"HAS_STATE_BATCH_INDICES\":",
      "    lambda args: args[\"state_batch_indices_ptr\"] is not None",
      "})",
      "@triton.heuristics(",
      "    {\"BLOCK_SIZE_DSTATE\": lambda args: triton.next_power_of_2(args[\"dstate\"])})",
      "@triton.jit",
      "def _selective_scan_update_kernel(",
      "    # Pointers to matrices",
      "    state_ptr,",
      "    x_ptr,",
      "    dt_ptr,",
      "    dt_bias_ptr,",
      "    A_ptr,",
      "    B_ptr,",
      "    C_ptr,",
      "    D_ptr,",
      "    z_ptr,",
      "    out_ptr,",
      "    state_batch_indices_ptr,",
      "    pad_slot_id,",
      "    # Matrix dimensions",
      "    batch,",
      "    nheads,",
      "    dim,",
      "    dstate,",
      "    nheads_ngroups_ratio,",
      "    # Strides",
      "    stride_state_batch,",
      "    stride_state_head,",
      "    stride_state_dim,",
      "    stride_state_dstate,",
      "    stride_x_batch,",
      "    stride_x_head,",
      "    stride_x_dim,",
      "    stride_dt_batch,",
      "    stride_dt_head,",
      "    stride_dt_dim,",
      "    stride_dt_bias_head,",
      "    stride_dt_bias_dim,",
      "    stride_A_head,",
      "    stride_A_dim,",
      "    stride_A_dstate,",
      "    stride_B_batch,",
      "    stride_B_group,",
      "    stride_B_dstate,",
      "    stride_C_batch,",
      "    stride_C_group,",
      "    stride_C_dstate,",
      "    stride_D_head,",
      "    stride_D_dim,",
      "    stride_z_batch,",
      "    stride_z_head,",
      "    stride_z_dim,",
      "    stride_out_batch,",
      "    stride_out_head,",
      "    stride_out_dim,",
      "    # Meta-parameters",
      "    DT_SOFTPLUS: tl.constexpr,",
      "    TIE_HDIM: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    HAS_DT_BIAS: tl.constexpr,",
      "    HAS_D: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    HAS_STATE_BATCH_INDICES: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(axis=0)",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "",
      "    # If HAS_STATE_BATCH_INDICES is true, then the ssm state's batch coordinate",
      "    # is taken from the state_batch_indices_ptr Otherwise, the state coordinate",
      "    # is the same as the batch id.",
      "    if HAS_STATE_BATCH_INDICES:",
      "        state_batch_indices_ptr += pid_b",
      "        state_batch_idx = tl.load(state_batch_indices_ptr).to(tl.int64)",
      "        state_ptr += (state_batch_idx * stride_state_batch +",
      "                      pid_h * stride_state_head)",
      "    else:",
      "        state_ptr += pid_b * stride_state_batch + pid_h * stride_state_head",
      "",
      "    x_ptr += pid_b * stride_x_batch + pid_h * stride_x_head",
      "    dt_ptr += pid_b * stride_dt_batch + pid_h * stride_dt_head",
      "    if HAS_DT_BIAS:",
      "        dt_bias_ptr += pid_h * stride_dt_bias_head",
      "    A_ptr += pid_h * stride_A_head",
      "    B_ptr += pid_b * stride_B_batch + (pid_h //",
      "                                       nheads_ngroups_ratio) * stride_B_group",
      "    C_ptr += pid_b * stride_C_batch + (pid_h //",
      "                                       nheads_ngroups_ratio) * stride_C_group",
      "    if HAS_Z:",
      "        z_ptr += pid_b * stride_z_batch + pid_h * stride_z_head",
      "    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)",
      "    state_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim +",
      "                              offs_n[None, :] * stride_state_dstate)",
      "    x_ptrs = x_ptr + offs_m * stride_x_dim",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_dim",
      "    if HAS_DT_BIAS:",
      "        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim",
      "    if HAS_D:",
      "        D_ptr += pid_h * stride_D_head",
      "    A_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim +",
      "                      offs_n[None, :] * stride_A_dstate)",
      "    B_ptrs = B_ptr + offs_n * stride_B_dstate",
      "    C_ptrs = C_ptr + offs_n * stride_C_dstate",
      "    if HAS_D:",
      "        D_ptrs = D_ptr + offs_m * stride_D_dim",
      "    if HAS_Z:",
      "        z_ptrs = z_ptr + offs_m * stride_z_dim",
      "    out_ptrs = out_ptr + offs_m * stride_out_dim",
      "    mask = (offs_m[:, None] < dim) & (offs_n[None, :] < dstate)",
      "    if HAS_STATE_BATCH_INDICES:",
      "        mask &= (state_batch_idx != pad_slot_id)",
      "    state = tl.load(state_ptrs, mask=mask, other=0.0)",
      "",
      "    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "    if not TIE_HDIM:",
      "        dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        if HAS_DT_BIAS:",
      "            dt += tl.load(dt_bias_ptrs, mask=offs_m < dim,",
      "                          other=0.0).to(tl.float32)",
      "        if DT_SOFTPLUS:",
      "            dt = softplus(dt)",
      "        A = tl.load(A_ptrs,",
      "                    mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate),",
      "                    other=0.0).to(tl.float32)",
      "        dA = tl.exp(A * dt[:, None])",
      "    else:",
      "        dt = tl.load(dt_ptr).to(tl.float32)",
      "        if HAS_DT_BIAS:",
      "            dt += tl.load(dt_bias_ptr).to(tl.float32)",
      "        if DT_SOFTPLUS:",
      "            dt = softplus(dt)",
      "        A = tl.load(A_ptr).to(tl.float32)",
      "        dA = tl.exp(A * dt)  # scalar, not a matrix",
      "",
      "    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)",
      "    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)",
      "    if HAS_D:",
      "        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "    if HAS_Z:",
      "        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "",
      "    dB = B[None, :] * dt[:, None] if not TIE_HDIM else B * dt",
      "    state = state * dA + dB * x[:, None]",
      "",
      "    mask = (offs_m[:, None] < dim) & (offs_n[None, :] < dstate)",
      "    if HAS_STATE_BATCH_INDICES:",
      "        mask &= (state_batch_idx != pad_slot_id)",
      "    tl.store(state_ptrs, state, mask=mask)",
      "    out = tl.sum(state * C[None, :], axis=1)",
      "    if HAS_D:",
      "        out += x * D",
      "    if HAS_Z:",
      "        out *= z * tl.sigmoid(z)",
      "    tl.store(out_ptrs, out, mask=offs_m < dim)",
      "",
      "",
      "def selective_state_update(state,",
      "                           x,",
      "                           dt,",
      "                           A,",
      "                           B,",
      "                           C,",
      "                           D=None,",
      "                           z=None,",
      "                           dt_bias=None,",
      "                           dt_softplus=False,",
      "                           state_batch_indices=None,",
      "                           pad_slot_id=PAD_SLOT_ID,",
      "                           out=None):",
      "    \"\"\"",
      "    Argument:",
      "        state: (batch, dim, dstate) or (batch, nheads, dim, dstate)",
      "        x: (batch, dim) or (batch, nheads, dim)",
      "        dt: (batch, dim) or (batch, nheads, dim)",
      "        A: (dim, dstate) or (nheads, dim, dstate)",
      "        B: (batch, dstate) or (batch, ngroups, dstate)",
      "        C: (batch, dstate) or (batch, ngroups, dstate)",
      "        D: (dim,) or (nheads, dim)",
      "        z: (batch, dim) or (batch, nheads, dim)",
      "        dt_bias: (dim,) or (nheads, dim)",
      "        pad_slot_id: int",
      "            if cache_indices is passed, lets the kernel identify padded ",
      "            entries that will not be processed, ",
      "            for example: cache_indices = [pad_slot_id, 1, 20, pad_slot_id] ",
      "            in this case, the kernel will not process entries at ",
      "            indices 0 and 3",
      "        out: Preallocated ssm output tensor. Assume same shape as x. ",
      "             In-place updated.",
      "    \"\"\"",
      "    if state.dim() == 3:",
      "        state = state.unsqueeze(1)",
      "    if x.dim() == 2:",
      "        x = x.unsqueeze(1)",
      "    if dt.dim() == 2:",
      "        dt = dt.unsqueeze(1)",
      "    if A.dim() == 2:",
      "        A = A.unsqueeze(0)",
      "    if B.dim() == 2:",
      "        B = B.unsqueeze(1)",
      "    if C.dim() == 2:",
      "        C = C.unsqueeze(1)",
      "    if D is not None and D.dim() == 1:",
      "        D = D.unsqueeze(0)",
      "    if z is not None and z.dim() == 2:",
      "        z = z.unsqueeze(1)",
      "    if dt_bias is not None and dt_bias.dim() == 1:",
      "        dt_bias = dt_bias.unsqueeze(0)",
      "    if out.dim() == 2:",
      "        out = out.unsqueeze(1)",
      "",
      "    _, nheads, dim, dstate = state.shape",
      "    batch = x.shape[0]",
      "",
      "    assert x.shape == (batch, nheads, dim)",
      "    assert dt.shape == x.shape",
      "    assert A.shape == (nheads, dim, dstate)",
      "    ngroups = B.shape[1]",
      "    assert nheads % ngroups == 0, \"nheads must be divisible by ngroups\"",
      "    assert B.shape == (batch, ngroups, dstate)",
      "    assert C.shape == B.shape",
      "    if D is not None:",
      "        assert D.shape == (nheads, dim)",
      "    if z is not None:",
      "        assert z.shape == x.shape",
      "    if dt_bias is not None:",
      "        assert dt_bias.shape == (nheads, dim)",
      "    if state_batch_indices is not None:",
      "        assert state_batch_indices.shape == (batch, )",
      "    assert out.shape == x.shape",
      "",
      "    grid = lambda META: (triton.cdiv(dim, META['BLOCK_SIZE_M']), batch, nheads)",
      "    z_strides = ((z.stride(0), z.stride(1), z.stride(2)) if z is not None else",
      "                 (0, 0, 0))",
      "    # We don't want autotune since it will overwrite the state",
      "    # We instead tune by hand.",
      "    BLOCK_SIZE_M, num_warps = ((32, 4) if dstate <= 16 else",
      "                               ((16, 4) if dstate <= 32 else",
      "                                ((8, 4) if dstate <= 64 else",
      "                                 ((4, 4) if dstate <= 128 else ((4, 8))))))",
      "    tie_hdim = A.stride(-1) == 0 and A.stride(-2) == 0 and dt.stride(",
      "        -1) == 0 and dt_bias.stride(-1) == 0",
      "    with torch.cuda.device(x.device.index):",
      "        _selective_scan_update_kernel[grid](",
      "            state,",
      "            x,",
      "            dt,",
      "            dt_bias,",
      "            A,",
      "            B,",
      "            C,",
      "            D,",
      "            z,",
      "            out,",
      "            state_batch_indices,",
      "            pad_slot_id,",
      "            batch,",
      "            nheads,",
      "            dim,",
      "            dstate,",
      "            nheads // ngroups,",
      "            state.stride(0),",
      "            state.stride(1),",
      "            state.stride(2),",
      "            state.stride(3),",
      "            x.stride(0),",
      "            x.stride(1),",
      "            x.stride(2),",
      "            dt.stride(0),",
      "            dt.stride(1),",
      "            dt.stride(2),",
      "            *(dt_bias.stride(0),",
      "              dt_bias.stride(1)) if dt_bias is not None else 0,",
      "            A.stride(0),",
      "            A.stride(1),",
      "            A.stride(2),",
      "            B.stride(0),",
      "            B.stride(1),",
      "            B.stride(2),",
      "            C.stride(0),",
      "            C.stride(1),",
      "            C.stride(2),",
      "            *(D.stride(0), D.stride(1)) if D is not None else 0,",
      "            z_strides[0],",
      "            z_strides[1],",
      "            z_strides[2],",
      "            out.stride(0),",
      "            out.stride(1),",
      "            out.stride(2),",
      "            dt_softplus,",
      "            tie_hdim,",
      "            BLOCK_SIZE_M,",
      "            num_warps=num_warps,",
      "        )",
      "",
      "",
      "def selective_scan_fn(u,",
      "                      ssm_states,",
      "                      delta,",
      "                      A,",
      "                      B,",
      "                      C,",
      "                      D=None,",
      "                      z=None,",
      "                      delta_bias=None,",
      "                      delta_softplus=False,",
      "                      query_start_loc=None,",
      "                      cache_indices=None,",
      "                      has_initial_state=None,",
      "                      pad_slot_id=PAD_SLOT_ID) -> torch.Tensor:",
      "    \"\"\"",
      "    u: (dim, total_length) for varlen or (batch, dim, seqlen) ",
      "        applies changes in place.",
      "    ssm_states: (batch, dim, dstate) or (batch, nheads, dim, dstate)",
      "        applies changes in place.",
      "    delta: (dim, total_length) for varlen or (batch, dim, seqlen)",
      "    A: (dim, dstate) ",
      "    B: (ngroups, dstate, total_length) for varlen or ",
      "                                        (batch,ngroups,dstate,seqlen)",
      "    C: (ngroups, dstate, total_length) for varlen or ",
      "                                        (batch,ngroups,dstate,seqlen)",
      "    D: (dim,) ",
      "    z: (dim, total_length) for varlen or (batch, dim, seqlen) ",
      "    dt_bias: (dim,) or (dim)",
      "    query_start_loc: (batch + 1) int32",
      "        The cumulative sequence lengths of the sequences in",
      "        the batch, used to index into sequence. prepended with 0.",
      "        for example: query_start_loc = torch.Tensor([0,10,16,17]), ",
      "        x.shape=(dim,17)",
      "    cache_indices: (batch) int32",
      "        A tensor with each cell is a correspondent ",
      "        input and output ssm_state index",
      "    has_initial_state: (batch) bool",
      "        A tensor populated with ones and zeros, ",
      "        indicate if the ssm_state at the corresponding index should be ",
      "        used as initial state. Not providing argument assumes ",
      "        there's no initial state",
      "    pad_slot_id: int",
      "        if cache_indices is passed, lets the kernel identify padding entries ",
      "        that will not be processed, ",
      "        for example: cache_indices = [pad_slot_id, 1 ,20 ,pad_slot_id] ",
      "        in this case, the kernel will not process entries at indices 0 and 3",
      "    returns",
      "        output: (dim, total_length) for varlen or (batch, dim, seqlen) ",
      "                supports inplace replacement",
      "    \"\"\"",
      "    if u.stride(-1) != 1:",
      "        u = u.contiguous()",
      "    if delta.stride(-1) != 1:",
      "        delta = delta.contiguous()",
      "    if D is not None:",
      "        D = D.contiguous()",
      "    if B.stride(-1) != 1:",
      "        B = B.contiguous()",
      "    if C.stride(-1) != 1:",
      "        C = C.contiguous()",
      "    if z is not None and z.stride(-1) != 1:",
      "        z = z.contiguous()",
      "    if B.dim() == 3 and query_start_loc is None:",
      "        B = B.unsqueeze(1)",
      "    if B.dim() == 2 and query_start_loc is not None:",
      "        B = B.unsqueeze(0)",
      "    if C.dim() == 3 and query_start_loc is None:",
      "        C = C.unsqueeze(1)",
      "    if C.dim() == 2 and query_start_loc is not None:",
      "        C = C.unsqueeze(0)",
      "",
      "    ops.selective_scan_fwd(u, delta, A, B, C, D, z, delta_bias, delta_softplus,",
      "                           query_start_loc, cache_indices, has_initial_state,",
      "                           ssm_states, pad_slot_id)",
      "",
      "    if z is None:",
      "        return delta  # output written inplace to delta",
      "    else:",
      "        return z  # output written inplace to z"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "# Copyright (c) 2024, Tri Dao, Albert Gu.",
      "# Adapted from https://github.com/state-spaces/mamba/blob/v2.2.4/mamba_ssm/ops/triton/ssd_chunk_scan.py",
      "",
      "# ruff: noqa: E501,SIM102",
      "",
      "import torch",
      "from packaging import version",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "TRITON_22 = version.parse(triton.__version__) >= version.parse('2.2.0')",
      "",
      "",
      "@triton.autotune(",
      "    configs=[",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 256,",
      "                'BLOCK_SIZE_K': 64",
      "            },",
      "            num_stages=3,",
      "            num_warps=8),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 256,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 128,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 64,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 128,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 64,",
      "                'BLOCK_SIZE_K': 64",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 128,",
      "                'BLOCK_SIZE_K': 64",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 32,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 32,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=5,",
      "            num_warps=2),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 32,",
      "                'BLOCK_SIZE_N': 64,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=5,",
      "            num_warps=2),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 64,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=2),",
      "    ],",
      "    key=['chunk_size', 'hdim', 'dstate', 'IS_CAUSAL'],",
      ")",
      "@triton.jit",
      "def _chunk_scan_fwd_kernel(",
      "    # Pointers to matrices",
      "    cb_ptr,",
      "    x_ptr,",
      "    z_ptr,",
      "    out_ptr,",
      "    out_x_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    C_ptr,",
      "    states_ptr,",
      "    D_ptr,",
      "    initstates_ptr,",
      "    chunk_indices_ptr,",
      "    chunk_offsets_ptr,",
      "    chunk_meta_num,",
      "    # Matrix dimensions",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    # Strides",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_z_batch,",
      "    stride_z_seqlen,",
      "    stride_z_head,",
      "    stride_z_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_states_batch,",
      "    stride_states_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_init_states_batch,",
      "    stride_init_states_head,",
      "    stride_init_states_hdim,",
      "    stride_init_states_dstate,",
      "    stride_D_head,",
      "    # Meta-parameters",
      "    IS_CAUSAL: tl.constexpr,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "    IS_TRITON_22: tl.constexpr,",
      "    HAS_INITSTATES: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1).to(tl.int64)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    if not HAS_INITSTATES:",
      "        c_idx = pid_c",
      "        c_off = 0",
      "    else:",
      "        c_idx = tl.load(chunk_indices_ptr + pid_c, mask=pid_c > -1, other=0)",
      "        c_off = tl.load(chunk_offsets_ptr + pid_c, mask=pid_c > -1, other=0)",
      "",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    cb_ptr += pid_b * stride_cb_batch + c_idx * stride_cb_chunk + (",
      "        pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    x_ptr += pid_b * stride_x_batch + c_idx * chunk_size * stride_x_seqlen + pid_h * stride_x_head",
      "    dt_ptr += pid_b * stride_dt_batch + c_idx * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += pid_b * stride_dA_cs_batch + c_idx * stride_dA_cs_chunk + pid_h * stride_dA_cs_head",
      "    C_ptr += pid_b * stride_C_batch + c_idx * chunk_size * stride_C_seqlen + (",
      "        pid_h // nheads_ngroups_ratio) * stride_C_head",
      "",
      "    # M-block offsets and prev states",
      "    #  - logic in next block may override these if there is an active offset",
      "    offs_m = pid_m * BLOCK_SIZE_M + c_off + tl.arange(0, BLOCK_SIZE_M)",
      "    prev_states_ptr = states_ptr + pid_b * stride_states_batch + c_idx * stride_states_chunk + pid_h * stride_states_head",
      "    prev_states_hdim = stride_states_hdim",
      "    prev_states_dstate = stride_states_dstate",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - c_idx * chunk_size)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += pid_b * stride_seq_idx_batch + c_idx * chunk_size * stride_seq_idx_seqlen",
      "",
      "        # - we only need seq_idx_prev to be aligned to chunk boundary",
      "        seq_idx_prev = tl.load(seq_idx_ptr - stride_seq_idx_seqlen,",
      "                               mask=c_idx >= 1,",
      "                               other=0)",
      "",
      "        if HAS_INITSTATES:",
      "            # if there are init states, we only need seq_idx_m to point",
      "            # what is the current seq_idx",
      "",
      "            # get current seq idx",
      "            if (pid_m * BLOCK_SIZE_M + c_off) < chunk_size_limit:",
      "                seq_idx_m = tl.load(",
      "                    seq_idx_ptr +",
      "                    (pid_m * BLOCK_SIZE_M + c_off) * stride_seq_idx_seqlen, )",
      "",
      "                # - recall that in ssd_state_passing, for the case c_off == 0",
      "                # i.e., the very first sequence, we made states_ptr hold its initial state",
      "                # so this edge case is taken care of",
      "                if ((c_off == 0) and",
      "                    (seq_idx_prev != seq_idx_m",
      "                     )  # if a seq is changed exactly on boundary",
      "                        or (c_off > 0)  # implies a new example (pseudo chunk)",
      "                    ):",
      "",
      "                    # - replace prev_states_ptr with init_states",
      "                    prev_states_ptr = initstates_ptr + seq_idx_m * stride_init_states_batch + pid_h * stride_init_states_head",
      "                    prev_states_hdim = stride_init_states_hdim  # override strides",
      "                    prev_states_dstate = stride_init_states_dstate",
      "",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize,",
      "                      mask=offs_m < chunk_size,",
      "                      other=0.0).to(tl.float32)",
      "",
      "    # - handle chunk state limit",
      "    if HAS_INITSTATES:",
      "",
      "        # have to split this if otherwise compilation will have problems",
      "        dA_cs_m_boundary = 0.0",
      "",
      "        # get the c_idx for the next (logica) chunk",
      "        c_idx_n = tl.load(",
      "            chunk_indices_ptr + (pid_c + 1),",
      "            mask=pid_c > -1 and (pid_c + 1) < chunk_meta_num,",
      "            other=-1  # to trigger different chunk",
      "        )",
      "",
      "        # - there are things to consider",
      "        # A. if c_off > 0 then we need to move the dA_cs boundary to ensure correct",
      "        #    contribution of past states",
      "        # B. if c_off_n < chunk_size_limit, then we need to adjust this so as not to",
      "        #    encroach into the next sequence, where c_off_n is the offset of the next",
      "        #    (logical) chunk.",
      "        # An equivalent check for B is c_idx == c_idx_n, where there is repetition in",
      "        # (logical) chunk indices.",
      "",
      "        if (c_idx == c_idx_n) or c_off > 0:",
      "",
      "            # get the next offset",
      "            c_off_n = tl.load(chunk_offsets_ptr + (pid_c + 1),",
      "                              mask=pid_c > -1 and (pid_c + 1) < chunk_meta_num,",
      "                              other=chunk_size)",
      "",
      "            # in this case, adjust down the chunk_size_limit",
      "            if c_idx == c_idx_n:",
      "                chunk_size_limit = min(c_off_n, chunk_size_limit)",
      "",
      "            # get the cs at the offset boundary",
      "            # - c_off == 0 is a passthrough",
      "            # - We need dA_cs at the boundary, defined by c_off - no need",
      "            #   to increase pointer by pid_m (it is a constant offset,",
      "            #   i.e. the same for all blocks)",
      "            dA_cs_m_boundary = tl.load(",
      "                dA_cumsum_ptr + (c_off - 1) * stride_dA_cs_csize,",
      "                mask=(((c_off - 1) > -1) and ((c_off) < chunk_size)),",
      "                other=0.0).to(tl.float32)",
      "",
      "    if HAS_SEQ_IDX:",
      "        # - handle seq idx when HAS_INITSTATES==False",
      "        if not HAS_INITSTATES:",
      "            seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "                                mask=offs_m < chunk_size_limit,",
      "                                other=-1)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    # Without the if (pid_c > -1), with Triton 2.1.0, I get",
      "    # Assertion `!(srcMmaLayout && dstMmaLayout) && \"Unexpected mma -> mm a layout conversion\"' failed.",
      "    # With Triton 2.2.0, this works",
      "    if IS_TRITON_22 or c_idx > -1:",
      "        # Faster to just do 1 iteration with larger BLOCK_SIZE_K, up to block size 128",
      "        offs_k_dstate = tl.arange(",
      "            0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K)",
      "        C_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen +",
      "                          offs_k_dstate[None, :] * stride_C_dstate)",
      "",
      "        prev_states_ptrs = prev_states_ptr + (",
      "            offs_n[None, :] * prev_states_hdim +",
      "            offs_k_dstate[:, None] * prev_states_dstate)",
      "        if HAS_SEQ_IDX:",
      "",
      "            if not HAS_INITSTATES:",
      "                # - this is for continuous batching where there is no init states",
      "                scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m),",
      "                                   0.0)",
      "            else:",
      "                # - if there is initstates, we will rely on prev_states, no zeroing",
      "                #   required.",
      "                scale_m = tl.exp(dA_cs_m - dA_cs_m_boundary)",
      "        else:",
      "            scale_m = tl.exp(dA_cs_m)",
      "        if BLOCK_SIZE_DSTATE <= 128:",
      "            C = tl.load(C_ptrs,",
      "                        mask=(offs_m[:, None] < chunk_size_limit) &",
      "                        (offs_k_dstate[None, :] < dstate),",
      "                        other=0.0)",
      "",
      "            prev_states = tl.load(prev_states_ptrs,",
      "                                  mask=(offs_k_dstate[:, None] < dstate) &",
      "                                  (offs_n[None, :] < hdim),",
      "                                  other=0.0)",
      "            prev_states = prev_states.to(C_ptr.dtype.element_ty)",
      "            acc = tl.dot(C, prev_states) * scale_m[:, None]",
      "        else:",
      "            for k in range(0, dstate, BLOCK_SIZE_K):",
      "                C = tl.load(C_ptrs,",
      "                            mask=(offs_m[:, None] < chunk_size_limit) &",
      "                            (offs_k_dstate[None, :] < dstate - k),",
      "                            other=0.0)",
      "                # C = (C * scale_m[:, None]).to(C_ptr.dtype.element_ty)",
      "                prev_states = tl.load(",
      "                    prev_states_ptrs,",
      "                    mask=(offs_k_dstate[:, None] < dstate - k) &",
      "                    (offs_n[None, :] < hdim),",
      "                    other=0.0)",
      "                prev_states = prev_states.to(C_ptr.dtype.element_ty)",
      "                acc += tl.dot(C, prev_states)",
      "                C_ptrs += BLOCK_SIZE_K",
      "                prev_states_ptrs += BLOCK_SIZE_K",
      "            acc *= scale_m[:, None]",
      "",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K) + c_off",
      "    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m +",
      "                        offs_k[None, :] * stride_cb_csize_k)",
      "    x_ptrs = x_ptr + (offs_k[:, None] * stride_x_seqlen +",
      "                      offs_n[None, :] * stride_x_hdim)",
      "    dt_ptrs = dt_ptr + offs_k * stride_dt_csize",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "    K_MAX = chunk_size_limit if not IS_CAUSAL else min(",
      "        (pid_m + 1) * BLOCK_SIZE_M, chunk_size_limit)",
      "    for k in range(0, K_MAX, BLOCK_SIZE_K):",
      "        cb = tl.load(cb_ptrs,",
      "                     mask=(offs_m[:, None] < chunk_size) &",
      "                     (offs_k[None, :] < chunk_size - k),",
      "                     other=0.0).to(tl.float32)",
      "        dA_cs_k = tl.load(dA_cumsum_ptrs,",
      "                          mask=offs_k < chunk_size - k,",
      "                          other=0.0).to(tl.float32)",
      "        # If there's seq_idx, we already set cb[i, j] = 0 for seq_idx[i] != seq_idx[j].",
      "        # So we don't need masking wrt seq_idx here.",
      "        cb *= tl.exp(dA_cs_m[:, None] - dA_cs_k[None, :])",
      "        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size - k,",
      "                       other=0.0).to(tl.float32)",
      "        cb *= dt_k",
      "        if IS_CAUSAL:",
      "            mask = offs_m[:, None] >= k + offs_k[None, :]",
      "            cb = tl.where(mask, cb, 0.0)",
      "        cb = cb.to(x_ptr.dtype.element_ty)",
      "        x = tl.load(x_ptrs,",
      "                    mask=(offs_k[:, None] < chunk_size_limit - k) &",
      "                    (offs_n[None, :] < hdim),",
      "                    other=0.0)",
      "        acc += tl.dot(cb, x)",
      "        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k",
      "        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen",
      "        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "",
      "    offs_out_m = pid_m * BLOCK_SIZE_M + c_off + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_out_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    if HAS_D:",
      "        if D_HAS_HDIM:",
      "            D = tl.load(D_ptr + pid_h * stride_D_head + offs_n,",
      "                        mask=offs_n < hdim,",
      "                        other=0.0).to(tl.float32)",
      "        else:",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        x_residual = tl.load(x_ptr + (offs_m[:, None] * stride_x_seqlen +",
      "                                      offs_n[None, :] * stride_x_hdim),",
      "                             mask=(offs_m[:, None] < chunk_size_limit) &",
      "                             (offs_n[None, :] < hdim),",
      "                             other=0.0).to(tl.float32)",
      "        acc += x_residual * D",
      "",
      "    if HAS_Z:",
      "        out_x_ptr += pid_b * stride_out_batch + c_idx * chunk_size * stride_out_seqlen + pid_h * stride_out_head",
      "        out_x_ptrs = out_x_ptr + (stride_out_seqlen * offs_out_m[:, None] +",
      "                                  offs_out_n[None, :])",
      "        tl.store(out_x_ptrs,",
      "                 acc,",
      "                 mask=(offs_out_m[:, None] < chunk_size_limit) &",
      "                 (offs_out_n[None, :] < hdim))",
      "",
      "        z_ptr += pid_b * stride_z_batch + c_idx * chunk_size * stride_z_seqlen + pid_h * stride_z_head",
      "        z_ptrs = z_ptr + (stride_z_seqlen * offs_out_m[:, None] +",
      "                          stride_z_hdim * offs_out_n[None, :])",
      "        z = tl.load(z_ptrs,",
      "                    mask=(offs_out_m[:, None] < chunk_size_limit) &",
      "                    (offs_out_n[None, :] < hdim),",
      "                    other=0.0).to(tl.float32)",
      "        acc *= z * tl.sigmoid(z)",
      "",
      "    out_ptr += pid_b * stride_out_batch + c_idx * chunk_size * stride_out_seqlen + pid_h * stride_out_head",
      "    out_ptrs = out_ptr + (stride_out_seqlen * offs_out_m[:, None] +",
      "                          offs_out_n[None, :] * stride_out_hdim)",
      "    tl.store(out_ptrs,",
      "             acc,",
      "             mask=(offs_out_m[:, None] < chunk_size_limit) &",
      "             (offs_out_n[None, :] < hdim))",
      "",
      "",
      "def _chunk_scan_fwd(",
      "    cb,",
      "    x,",
      "    dt,",
      "    dA_cumsum,",
      "    C,",
      "    states,",
      "    D=None,",
      "    z=None,",
      "    seq_idx=None,",
      "    chunk_indices=None,",
      "    chunk_offsets=None,",
      "    initial_states=None,",
      "    out=None,",
      "):",
      "    batch, seqlen, nheads, headdim = x.shape",
      "    _, _, nchunks, chunk_size = dt.shape",
      "    _, _, ngroups, dstate = C.shape",
      "    assert nheads % ngroups == 0",
      "    assert C.shape == (batch, seqlen, ngroups, dstate)",
      "    assert cb.shape == (batch, nchunks, ngroups, chunk_size, chunk_size)",
      "    if z is not None:",
      "        assert z.shape == x.shape",
      "    if D is not None:",
      "        assert D.shape == (nheads, headdim) or D.shape == (nheads, )",
      "    assert dt.shape == (batch, nheads, nchunks, chunk_size)",
      "    assert dA_cumsum.shape == (batch, nheads, nchunks, chunk_size)",
      "    assert states.shape == (batch, nchunks, nheads, headdim, dstate)",
      "",
      "    if seq_idx is not None:",
      "        assert seq_idx.shape == (batch, seqlen)",
      "",
      "        if initial_states is not None:",
      "            # with initial states, we need to take care of how",
      "            # seq_idx crosses the boundaries",
      "            assert batch == 1, \"chunk scan only supports initial states with batch 1\"",
      "            assert chunk_indices is not None and chunk_offsets is not None, \\",
      "                \"chunk_indices and chunk_offsets should have been set\"",
      "        else:",
      "            chunk_indices, chunk_offsets = None, None",
      "    else:",
      "        chunk_indices, chunk_offsets = None, None",
      "",
      "    assert out.shape == x.shape",
      "",
      "    if z is not None:",
      "        out_x = torch.empty_like(x)",
      "        assert out_x.stride() == out.stride()",
      "    else:",
      "        out_x = None",
      "",
      "    grid = lambda META: (",
      "        triton.cdiv(chunk_size, META['BLOCK_SIZE_M']) * triton.cdiv(",
      "            headdim, META['BLOCK_SIZE_N']), batch * nchunks",
      "        if chunk_offsets is None else len(chunk_offsets), nheads)",
      "    z_strides = ((z.stride(0), z.stride(1), z.stride(2),",
      "                  z.stride(3)) if z is not None else (0, 0, 0, 0))",
      "    _chunk_scan_fwd_kernel[grid](",
      "        cb,",
      "        x,",
      "        z,",
      "        out,",
      "        out_x,",
      "        dt,",
      "        dA_cumsum,",
      "        seq_idx,",
      "        C,",
      "        states,",
      "        D,",
      "        initial_states,",
      "        chunk_indices,",
      "        chunk_offsets,",
      "        len(chunk_indices) if chunk_indices is not None else 0,",
      "        chunk_size,",
      "        headdim,",
      "        dstate,",
      "        batch,",
      "        seqlen,",
      "        nheads // ngroups,",
      "        cb.stride(0),",
      "        cb.stride(1),",
      "        cb.stride(2),",
      "        cb.stride(3),",
      "        cb.stride(4),",
      "        x.stride(0),",
      "        x.stride(1),",
      "        x.stride(2),",
      "        x.stride(3),",
      "        z_strides[0],",
      "        z_strides[1],",
      "        z_strides[2],",
      "        z_strides[3],",
      "        out.stride(0),",
      "        out.stride(1),",
      "        out.stride(2),",
      "        out.stride(3),",
      "        dt.stride(0),",
      "        dt.stride(2),",
      "        dt.stride(1),",
      "        dt.stride(3),",
      "        dA_cumsum.stride(0),",
      "        dA_cumsum.stride(2),",
      "        dA_cumsum.stride(1),",
      "        dA_cumsum.stride(3),",
      "        *((seq_idx.stride(0), seq_idx.stride(1)) if seq_idx is not None else",
      "          (0, 0)),",
      "        C.stride(0),",
      "        C.stride(1),",
      "        C.stride(2),",
      "        C.stride(3),",
      "        states.stride(0),",
      "        states.stride(1),",
      "        states.stride(2),",
      "        states.stride(3),",
      "        states.stride(4),",
      "        *((initial_states.stride(0), initial_states.stride(1),",
      "           initial_states.stride(2),",
      "           initial_states.stride(3)) if initial_states is not None else",
      "          (0, 0, 0, 0)),",
      "        D.stride(0) if D is not None else 0,",
      "        True,",
      "        D is not None,",
      "        D.dim() == 2 if D is not None else True,",
      "        BLOCK_SIZE_DSTATE=max(triton.next_power_of_2(dstate), 16),",
      "        HAS_Z=z is not None,",
      "        HAS_SEQ_IDX=seq_idx is not None,",
      "        IS_TRITON_22=TRITON_22,",
      "        HAS_INITSTATES=initial_states is not None,",
      "    )",
      "    return out_x"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/mamba/ops/ssd_bmm.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "# Copyright (c) 2024, Tri Dao, Albert Gu.",
      "# Adapted from https://github.com/state-spaces/mamba/blob/v2.2.4/mamba_ssm/ops/triton/ssd_bmm.py",
      "",
      "# ruff: noqa: E501,SIM102",
      "",
      "import math",
      "",
      "import torch",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "",
      "@triton.autotune(",
      "    configs=[",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 256,",
      "                'BLOCK_SIZE_K': 64",
      "            },",
      "            num_stages=3,",
      "            num_warps=8),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 256,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 128,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 64,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 128,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 32,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 32,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=5,",
      "            num_warps=2),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 32,",
      "                'BLOCK_SIZE_N': 64,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=5,",
      "            num_warps=2),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 64,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=2),",
      "    ],",
      "    key=['chunk_size', 'K', 'IS_CAUSAL'],",
      ")",
      "@triton.jit",
      "def _bmm_chunk_fwd_kernel(",
      "    # Pointers to matrices",
      "    a_ptr,",
      "    b_ptr,",
      "    out_ptr,",
      "    seq_idx_ptr,",
      "    # Matrix dimensions",
      "    seqlen,",
      "    chunk_size,",
      "    K,",
      "    ngroups,",
      "    stride_a_batch,",
      "    stride_a_seqlen,",
      "    stride_a_head,",
      "    stride_ak,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_bk,",
      "    stride_out_batch,",
      "    stride_out_chunk,",
      "    stride_out_head,",
      "    stride_outm,",
      "    stride_outn,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    # Meta-parameters",
      "    IS_CAUSAL: tl.constexpr,",
      "    dot_dtype: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_ch = tl.program_id(axis=2).to(tl.int64)",
      "    pid_c = pid_ch // ngroups",
      "    pid_h = pid_ch - pid_c * ngroups",
      "    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    if IS_CAUSAL:",
      "        if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:",
      "            return",
      "    a_ptr += pid_b * stride_a_batch + pid_c * chunk_size * stride_a_seqlen + pid_h * stride_a_head",
      "    b_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + pid_h * stride_b_head",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_m[:, None] * stride_a_seqlen +",
      "                      offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk +",
      "                      offs_n[None, :] * stride_b_seqlen)",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(a_ptrs,",
      "                    mask=(offs_m[:, None] < chunk_size_limit) &",
      "                    (offs_k[None, :] < K - k * BLOCK_SIZE_K),",
      "                    other=0.0).to(dot_dtype)",
      "        b = tl.load(b_ptrs,",
      "                    mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K) &",
      "                    (offs_n[None, :] < chunk_size_limit),",
      "                    other=0.0).to(dot_dtype)",
      "        acc += tl.dot(a, b)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    if HAS_SEQ_IDX:",
      "        chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "                            mask=offs_m < chunk_size_limit,",
      "                            other=-1)",
      "        seq_idx_n = tl.load(seq_idx_ptr + offs_n * stride_seq_idx_seqlen,",
      "                            mask=offs_n < chunk_size_limit,",
      "                            other=-2)",
      "        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)",
      "    out = acc.to(out_ptr.dtype.element_ty)",
      "",
      "    out_ptr += pid_b * stride_out_batch + pid_c * stride_out_chunk + pid_h * stride_out_head",
      "    out_ptrs = out_ptr + (stride_outm * offs_m[:, None] +",
      "                          offs_n[None, :] * stride_outn)",
      "    tl.store(out_ptrs,",
      "             out,",
      "             mask=(offs_m[:, None] < chunk_size) &",
      "             (offs_n[None, :] < chunk_size))",
      "",
      "",
      "def _bmm_chunk_fwd(a,",
      "                   b,",
      "                   chunk_size,",
      "                   seq_idx=None,",
      "                   causal=False,",
      "                   output_dtype=None):",
      "    \"\"\"",
      "    Argument:",
      "        a: (batch, seqlen, k) or (batch, seqlen, ngroups, k)",
      "        b: (batch, seqlen, k) or (batch, seqlen, ngroups, k)",
      "        seq_idx: (batch, seqlen) or None. out[i, j] for seq_idx[i] != seq_idx[j] will be zeroed out.",
      "        causal: if True, then out[i, j] for i > j will be arbitrary, only out[i, j] for i <= j are",
      "            guaranteed to be correct.",
      "    Return:",
      "        out: (batch, nchunks, chunk_size, chunk_size) or (batch, nchunks, ngroups, chunk_size, chunk_size)",
      "    \"\"\"",
      "    # Check constraints.",
      "    has_groups = a.dim() == 4",
      "    if not has_groups:",
      "        batch, seqlen, k = a.shape",
      "    else:",
      "        batch, seqlen, ngroups, k = a.shape",
      "    assert b.shape == a.shape",
      "    if seq_idx is not None:",
      "        assert seq_idx.shape == (batch, seqlen)",
      "    if a.stride(-1) != 1 and a.stride(1) != 1:",
      "        a = a.contiguous()",
      "    if b.stride(-1) != 1 and b.stride(1) != 1:",
      "        b = b.contiguous()",
      "    nchunks = math.ceil(seqlen / chunk_size)",
      "    # Allocates output.",
      "    out_dtype = a.dtype if output_dtype is None else output_dtype",
      "    out = torch.empty(",
      "        (batch, nchunks, chunk_size, chunk_size) if not has_groups else",
      "        (batch, nchunks, ngroups, chunk_size, chunk_size),",
      "        device=a.device,",
      "        dtype=out_dtype)",
      "    dot_dtype = (tl.bfloat16",
      "                 if a.dtype == torch.bfloat16 or b.dtype == torch.bfloat16 else",
      "                 (tl.float16 if a.dtype == torch.float16",
      "                  or b.dtype == torch.float16 else tl.float32))",
      "    grid = lambda META: (triton.cdiv(",
      "        chunk_size, META['BLOCK_SIZE_M']) * triton.cdiv(",
      "            chunk_size, META['BLOCK_SIZE_N']), batch, nchunks",
      "                         if not has_groups else nchunks * ngroups)",
      "    with torch.cuda.device(a.device.index):",
      "        _bmm_chunk_fwd_kernel[grid](",
      "            a,",
      "            b,",
      "            out,",
      "            seq_idx,",
      "            seqlen,",
      "            chunk_size,",
      "            k,",
      "            ngroups if has_groups else 1,",
      "            a.stride(0),",
      "            a.stride(1),",
      "            0 if not has_groups else a.stride(2),",
      "            a.stride(-1),",
      "            b.stride(0),",
      "            b.stride(1),",
      "            0 if not has_groups else b.stride(2),",
      "            b.stride(-1),",
      "            out.stride(0),",
      "            out.stride(1),",
      "            0 if not has_groups else out.stride(2),",
      "            out.stride(-2),",
      "            out.stride(-1),",
      "            *((seq_idx.stride(0),",
      "               seq_idx.stride(1)) if seq_idx is not None else (0, 0)),",
      "            causal,",
      "            dot_dtype,",
      "            HAS_SEQ_IDX=seq_idx is not None,",
      "        )",
      "    return out"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/mamba/ops/ssd_chunk_state.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "# Copyright (c) 2024, Tri Dao, Albert Gu.",
      "# Adapted from https://github.com/state-spaces/mamba/blob/v2.2.4/mamba_ssm/ops/triton/ssd_chunk_state.py",
      "",
      "# ruff: noqa: E501",
      "",
      "import math",
      "",
      "import torch",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "from .mamba_ssm import softplus",
      "",
      "",
      "@triton.autotune(",
      "    configs=[",
      "        triton.Config({'BLOCK_SIZE_H': 1}),",
      "        triton.Config({'BLOCK_SIZE_H': 2}),",
      "        triton.Config({'BLOCK_SIZE_H': 4}),",
      "        triton.Config({'BLOCK_SIZE_H': 8}),",
      "        triton.Config({'BLOCK_SIZE_H': 16}),",
      "        triton.Config({'BLOCK_SIZE_H': 32}),",
      "        triton.Config({'BLOCK_SIZE_H': 64}),",
      "    ],",
      "    key=['chunk_size', 'nheads'],",
      ")",
      "@triton.jit",
      "def _chunk_cumsum_fwd_kernel(",
      "    # Pointers to matrices",
      "    dt_ptr,",
      "    A_ptr,",
      "    dt_bias_ptr,",
      "    dt_out_ptr,",
      "    dA_cumsum_ptr,",
      "    # Matrix dimension",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    chunk_size,",
      "    dt_min,",
      "    dt_max,",
      "    # Strides",
      "    stride_dt_batch,",
      "    stride_dt_seqlen,",
      "    stride_dt_head,",
      "    stride_A_head,",
      "    stride_dt_bias_head,",
      "    stride_dt_out_batch,",
      "    stride_dt_out_chunk,",
      "    stride_dt_out_head,",
      "    stride_dt_out_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    # Meta-parameters",
      "    DT_SOFTPLUS: tl.constexpr,",
      "    HAS_DT_BIAS: tl.constexpr,",
      "    BLOCK_SIZE_H: tl.constexpr,",
      "    BLOCK_SIZE_CHUNK: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=0)",
      "",
      "    # if dt is long, may cause problems, so use 64 bit",
      "    # https://github.com/triton-lang/triton/issues/1058",
      "    pid_c = tl.program_id(axis=1).to(tl.int64)",
      "    pid_h = tl.program_id(axis=2)",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen",
      "    dt_out_ptr += pid_b * stride_dt_out_batch + pid_c * stride_dt_out_chunk",
      "    dA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk",
      "",
      "    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)",
      "    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)",
      "    dt_ptrs = dt_ptr + (offs_h[:, None] * stride_dt_head +",
      "                        offs_c[None, :] * stride_dt_seqlen)",
      "    A_ptrs = A_ptr + offs_h * stride_A_head",
      "    dt_out_ptrs = dt_out_ptr + (offs_h[:, None] * stride_dt_out_head +",
      "                                offs_c[None, :] * stride_dt_out_csize)",
      "    dA_cs_ptrs = dA_cumsum_ptr + (offs_h[:, None] * stride_dA_cs_head +",
      "                                  offs_c[None, :] * stride_dA_cs_csize)",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    dt = tl.load(dt_ptrs,",
      "                 mask=(offs_h[:, None] < nheads) &",
      "                 (offs_c[None, :] < chunk_size_limit),",
      "                 other=0.0).to(tl.float32)",
      "    if HAS_DT_BIAS:",
      "        dt_bias = tl.load(dt_bias_ptr + offs_h * stride_dt_bias_head,",
      "                          mask=offs_h < nheads,",
      "                          other=0.0).to(tl.float32)",
      "        dt += dt_bias[:, None]",
      "    if DT_SOFTPLUS:",
      "        dt = tl.where(dt <= 20.0, softplus(dt), dt)",
      "    # As of Triton 2.2.0, tl.clamp is not available yet",
      "    # dt = tl.clamp(dt, dt_min, dt_max)",
      "    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)",
      "    dt = tl.where(",
      "        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt,",
      "        0.0)",
      "    tl.store(dt_out_ptrs,",
      "             dt,",
      "             mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size))",
      "    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)",
      "    dA = dt * A[:, None]",
      "    dA_cs = tl.cumsum(dA, axis=1)",
      "    tl.store(dA_cs_ptrs,",
      "             dA_cs,",
      "             mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size))",
      "",
      "",
      "@triton.autotune(",
      "    configs=[",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 256,",
      "                'BLOCK_SIZE_K': 64",
      "            },",
      "            num_stages=3,",
      "            num_warps=8),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 256,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 128,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 64,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 128,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 32,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 32,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=5,",
      "            num_warps=2),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 32,",
      "                'BLOCK_SIZE_N': 64,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=5,",
      "            num_warps=2),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 64,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=2),",
      "    ],",
      "    key=['hdim', 'dstate', 'chunk_size'],",
      ")",
      "@triton.jit",
      "def _chunk_state_fwd_kernel(",
      "    # Pointers to matrices",
      "    x_ptr,",
      "    b_ptr,",
      "    states_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    # Matrix dimensions",
      "    hdim,",
      "    dstate,",
      "    chunk_size,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    # Strides",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_states_batch,",
      "    stride_states_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    # Meta-parameters",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1).to(tl.int64)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    b_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + (",
      "        pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    x_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_hdim +",
      "                      offs_k[None, :] * stride_x_seqlen)",
      "    b_ptrs = b_ptr + (offs_n[None, :] * stride_b_dstate +",
      "                      offs_k[:, None] * stride_b_seqlen)",
      "    dt_ptrs = dt_ptr + offs_k * stride_dt_csize",
      "    dA_cs_last = tl.load(dA_cumsum_ptr +",
      "                         (chunk_size - 1) * stride_dA_cs_csize).to(tl.float32)",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptrs = seq_idx_ptr + offs_k * stride_seq_idx_seqlen",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_last = tl.load(seq_idx_ptr +",
      "                               (chunk_size_limit - 1) * stride_seq_idx_seqlen)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        x = tl.load(x_ptrs,",
      "                    mask=(offs_m[:, None] < hdim) &",
      "                    (offs_k[None, :] < chunk_size_limit - k),",
      "                    other=0.0)",
      "        b = tl.load(b_ptrs,",
      "                    mask=(offs_k[:, None] < chunk_size_limit - k) &",
      "                    (offs_n[None, :] < dstate),",
      "                    other=0.0).to(tl.float32)",
      "        dA_cs_k = tl.load(dA_cumsum_ptrs,",
      "                          mask=offs_k < chunk_size_limit - k,",
      "                          other=0.0).to(tl.float32)",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_k = tl.load(seq_idx_ptrs,",
      "                                mask=offs_k < chunk_size_limit - k,",
      "                                other=-1)",
      "        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k,",
      "                       other=0.0).to(tl.float32)",
      "        if not HAS_SEQ_IDX:",
      "            scale = tl.exp(dA_cs_last - dA_cs_k) * dt_k",
      "        else:",
      "            scale = tl.where(seq_idx_k == seq_idx_last,",
      "                             tl.exp(dA_cs_last - dA_cs_k) * dt_k, 0.0)",
      "        b *= scale[:, None]",
      "        b = b.to(x_ptr.dtype.element_ty)",
      "        acc += tl.dot(x, b)",
      "        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen",
      "        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen",
      "        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_ptrs += BLOCK_SIZE_K * stride_seq_idx_seqlen",
      "    states = acc.to(states_ptr.dtype.element_ty)",
      "",
      "    states_ptr += pid_b * stride_states_batch + pid_c * stride_states_chunk + pid_h * stride_states_head",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    states_ptrs = states_ptr + (offs_m[:, None] * stride_states_hdim +",
      "                                offs_n[None, :] * stride_states_dstate)",
      "    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)",
      "    tl.store(states_ptrs, states, mask=c_mask)",
      "",
      "",
      "@triton.autotune(",
      "    configs=[",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 256,",
      "                'BLOCK_SIZE_K': 64",
      "            },",
      "            num_stages=3,",
      "            num_warps=8),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 256,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 128,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 64,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 128,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 128,",
      "                'BLOCK_SIZE_N': 32,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=4),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 32,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=5,",
      "            num_warps=2),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 32,",
      "                'BLOCK_SIZE_N': 64,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=5,",
      "            num_warps=2),",
      "        triton.Config(",
      "            {",
      "                'BLOCK_SIZE_M': 64,",
      "                'BLOCK_SIZE_N': 64,",
      "                'BLOCK_SIZE_K': 32",
      "            },",
      "            num_stages=4,",
      "            num_warps=2),",
      "    ],",
      "    key=['hdim', 'dstate', 'chunk_size'],",
      ")",
      "@triton.jit",
      "def _chunk_state_varlen_kernel(",
      "    # Pointers to matrices",
      "    x_ptr,",
      "    b_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    chunk_states_ptr,",
      "    cu_seqlens_ptr,",
      "    states_ptr,",
      "    initstates_ptr,",
      "    # Matrix dimensions",
      "    hdim,",
      "    dstate,",
      "    chunk_size,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    # Strides",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_chunk_states_chunk,",
      "    stride_chunk_states_head,",
      "    stride_chunk_states_hdim,",
      "    stride_chunk_states_dstate,",
      "    stride_states_batch,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_init_states_batch,",
      "    stride_init_states_head,",
      "    stride_init_states_hdim,",
      "    stride_init_states_dstate,",
      "    # Meta-parameters",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    HAS_INITSTATES: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    end_idx = tl.load(cu_seqlens_ptr + pid_b + 1)",
      "    pid_c = (end_idx - 1) // chunk_size",
      "    b_ptr += pid_c * chunk_size * stride_b_seqlen + (",
      "        pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    x_ptr += pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head",
      "    dt_ptr += pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head",
      "    chunk_states_ptr += pid_c * stride_chunk_states_chunk + pid_h * stride_chunk_states_head",
      "",
      "    if HAS_INITSTATES:",
      "        # if there are init states provided, we differentiate between states (which",
      "        # are boundary conditions at a chunk boundary) and initstates (which are boundary",
      "        # conditions when a new example in a cont batch starts)",
      "        initstates_ptr += pid_h * stride_init_states_head",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_hdim +",
      "                      offs_k[None, :] * stride_x_seqlen)",
      "    b_ptrs = b_ptr + (offs_n[None, :] * stride_b_dstate +",
      "                      offs_k[:, None] * stride_b_seqlen)",
      "    dt_ptrs = dt_ptr + offs_k * stride_dt_csize",
      "    dA_cs_last = tl.load(dA_cumsum_ptr + (end_idx - pid_c * chunk_size - 1) *",
      "                         stride_dA_cs_csize).to(tl.float32)",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "",
      "    chunk_size_limit = end_idx - pid_c * chunk_size",
      "    start_idx = tl.load(cu_seqlens_ptr + pid_b)",
      "    start_idx_cur = tl.maximum(start_idx - pid_c * chunk_size, 0)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        x = tl.load(x_ptrs,",
      "                    mask=(offs_m[:, None] < hdim) &",
      "                    (offs_k[None, :] < chunk_size_limit - k) &",
      "                    (offs_k[None, :] >= start_idx_cur - k),",
      "                    other=0.0)",
      "        b = tl.load(b_ptrs,",
      "                    mask=(offs_k[:, None] < chunk_size_limit - k) &",
      "                    (offs_n[None, :] < dstate) &",
      "                    (offs_k[:, None] >= start_idx_cur - k),",
      "                    other=0.0).to(tl.float32)",
      "        dA_cs_k = tl.load(dA_cumsum_ptrs,",
      "                          mask=offs_k < chunk_size_limit - k,",
      "                          other=0.0).to(tl.float32)",
      "        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k,",
      "                       other=0.0).to(tl.float32)",
      "        scale = tl.where(",
      "            (offs_k >= start_idx_cur - k) & (offs_k < chunk_size_limit - k),",
      "            tl.exp(dA_cs_last - dA_cs_k) * dt_k, 0.0)",
      "        b *= scale[:, None]",
      "        b = b.to(x_ptr.dtype.element_ty)",
      "        acc += tl.dot(x, b)",
      "        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen",
      "        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen",
      "        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "",
      "    # If the sequence starts after the last chunk idx, we don't need to add the contribution from the last chunk",
      "    # If HAS_INITSTATES==True need to consider two possiblties",
      "    # - if start_idx < pid_c * chunk_size, then we need to take the past_states_ptrs",
      "    # - if state_idx >= pid * chunk_size, then we need to insert initstates",
      "    if ((start_idx < pid_c * chunk_size)  # first chunk",
      "            or (HAS_INITSTATES)):",
      "",
      "        dA_cs_boundary = 0.0  # default",
      "",
      "        if not HAS_INITSTATES:",
      "            past_states_ptrs = chunk_states_ptr + (",
      "                offs_m[:, None] * stride_chunk_states_hdim +",
      "                offs_n[None, :] * stride_chunk_states_dstate)",
      "        else:",
      "",
      "            # - this seems repetitive, buts its to help the compiler",
      "            if start_idx < pid_c * chunk_size:",
      "                past_states_ptrs = chunk_states_ptr + (",
      "                    offs_m[:, None] * stride_chunk_states_hdim +",
      "                    offs_n[None, :] * stride_chunk_states_dstate)",
      "            else:",
      "                past_states_ptrs = initstates_ptr + (",
      "                    pid_b * stride_init_states_batch +",
      "                    offs_m[:, None] * stride_init_states_hdim +",
      "                    offs_n[None, :] * stride_init_states_dstate)",
      "",
      "                # need to adjust the boundary",
      "                if start_idx > pid_c * chunk_size:",
      "                    dA_cs_boundary = tl.load(dA_cumsum_ptr +",
      "                                             (start_idx - pid_c * chunk_size -",
      "                                              1) * stride_dA_cs_csize).to(",
      "                                                  tl.float32)",
      "",
      "        past_states = tl.load(past_states_ptrs,",
      "                              mask=(offs_m[:, None] < hdim) &",
      "                              (offs_n[None, :] < dstate),",
      "                              other=0.0).to(tl.float32)",
      "",
      "        scale = tl.exp(dA_cs_last - dA_cs_boundary)",
      "        acc += past_states * scale",
      "",
      "    states = acc.to(states_ptr.dtype.element_ty)",
      "",
      "    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    states_ptrs = states_ptr + (offs_m[:, None] * stride_states_hdim +",
      "                                offs_n[None, :] * stride_states_dstate)",
      "    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)",
      "    tl.store(states_ptrs, states, mask=c_mask)",
      "",
      "",
      "def _chunk_cumsum_fwd(dt,",
      "                      A,",
      "                      chunk_size,",
      "                      dt_bias=None,",
      "                      dt_softplus=False,",
      "                      dt_limit=(0.0, float(\"inf\"))):",
      "    batch, seqlen, nheads = dt.shape",
      "    assert A.shape == (nheads, )",
      "    if dt_bias is not None:",
      "        assert dt_bias.shape == (nheads, )",
      "    nchunks = math.ceil(seqlen / chunk_size)",
      "    dt_out = torch.empty(batch,",
      "                         nheads,",
      "                         nchunks,",
      "                         chunk_size,",
      "                         device=dt.device,",
      "                         dtype=torch.float32)",
      "    dA_cumsum = torch.empty(batch,",
      "                            nheads,",
      "                            nchunks,",
      "                            chunk_size,",
      "                            device=dt.device,",
      "                            dtype=torch.float32)",
      "    grid_chunk_cs = lambda META: (batch, nchunks,",
      "                                  triton.cdiv(nheads, META['BLOCK_SIZE_H']))",
      "    with torch.cuda.device(dt.device.index):",
      "        _chunk_cumsum_fwd_kernel[grid_chunk_cs](",
      "            dt,",
      "            A,",
      "            dt_bias,",
      "            dt_out,",
      "            dA_cumsum,",
      "            batch,",
      "            seqlen,",
      "            nheads,",
      "            chunk_size,",
      "            dt_limit[0],",
      "            dt_limit[1],",
      "            dt.stride(0),",
      "            dt.stride(1),",
      "            dt.stride(2),",
      "            A.stride(0),",
      "            dt_bias.stride(0) if dt_bias is not None else 0,",
      "            dt_out.stride(0),",
      "            dt_out.stride(2),",
      "            dt_out.stride(1),",
      "            dt_out.stride(3),",
      "            dA_cumsum.stride(0),",
      "            dA_cumsum.stride(2),",
      "            dA_cumsum.stride(1),",
      "            dA_cumsum.stride(3),",
      "            dt_softplus,",
      "            HAS_DT_BIAS=dt_bias is not None,",
      "            BLOCK_SIZE_CHUNK=triton.next_power_of_2(chunk_size),",
      "        )",
      "    return dA_cumsum, dt_out",
      "",
      "",
      "def _chunk_state_fwd(B,",
      "                     x,",
      "                     dt,",
      "                     dA_cumsum,",
      "                     seq_idx=None,",
      "                     states=None,",
      "                     states_in_fp32=True):",
      "    batch, seqlen, nheads, headdim = x.shape",
      "    _, _, nchunks, chunk_size = dt.shape",
      "    _, _, ngroups, dstate = B.shape",
      "    assert nheads % ngroups == 0",
      "    assert B.shape == (batch, seqlen, ngroups, dstate)",
      "    assert dt.shape == (batch, nheads, nchunks, chunk_size)",
      "    assert dA_cumsum.shape == dt.shape",
      "    if seq_idx is not None:",
      "        assert seq_idx.shape == (batch, seqlen)",
      "    if states is not None:",
      "        assert states.shape == (batch, nchunks, nheads, headdim, dstate)",
      "    else:",
      "        states_dtype = torch.float32 if states_in_fp32 else B.dtype",
      "        states = torch.empty((batch, nchunks, nheads, headdim, dstate),",
      "                             device=x.device,",
      "                             dtype=states_dtype)",
      "    grid = lambda META: (",
      "        triton.cdiv(headdim, META['BLOCK_SIZE_M']) * triton.cdiv(",
      "            dstate, META['BLOCK_SIZE_N']), batch * nchunks, nheads)",
      "    with torch.cuda.device(x.device.index):",
      "        _chunk_state_fwd_kernel[grid](",
      "            x,",
      "            B,",
      "            states,",
      "            dt,",
      "            dA_cumsum,",
      "            seq_idx,",
      "            headdim,",
      "            dstate,",
      "            chunk_size,",
      "            batch,",
      "            seqlen,",
      "            nheads // ngroups,",
      "            x.stride(0),",
      "            x.stride(1),",
      "            x.stride(2),",
      "            x.stride(3),",
      "            B.stride(0),",
      "            B.stride(1),",
      "            B.stride(2),",
      "            B.stride(-1),",
      "            states.stride(0),",
      "            states.stride(1),",
      "            states.stride(2),",
      "            states.stride(3),",
      "            states.stride(4),",
      "            dt.stride(0),",
      "            dt.stride(2),",
      "            dt.stride(1),",
      "            dt.stride(3),",
      "            dA_cumsum.stride(0),",
      "            dA_cumsum.stride(2),",
      "            dA_cumsum.stride(1),",
      "            dA_cumsum.stride(3),",
      "            *((seq_idx.stride(0),",
      "               seq_idx.stride(1)) if seq_idx is not None else (0, 0)),",
      "            HAS_SEQ_IDX=seq_idx is not None,",
      "        )",
      "    return states",
      "",
      "",
      "def chunk_state_varlen(B,",
      "                       x,",
      "                       dt,",
      "                       dA_cumsum,",
      "                       cu_seqlens,",
      "                       chunk_states,",
      "                       initial_states=None):",
      "    total_seqlen, nheads, headdim = x.shape",
      "    _, nchunks, chunk_size = dt.shape",
      "    _, ngroups, dstate = B.shape",
      "    batch = cu_seqlens.shape[0] - 1",
      "    cu_seqlens = cu_seqlens.contiguous()",
      "    assert nheads % ngroups == 0",
      "    assert B.shape == (total_seqlen, ngroups, dstate)",
      "    assert dt.shape == (nheads, nchunks, chunk_size)",
      "    assert dA_cumsum.shape == dt.shape",
      "    assert chunk_states.shape == (nchunks, nheads, headdim, dstate)",
      "",
      "    if initial_states is not None:",
      "        assert initial_states.shape == (batch, nheads, headdim, dstate)",
      "",
      "    states = torch.empty(batch,",
      "                         nheads,",
      "                         headdim,",
      "                         dstate,",
      "                         dtype=chunk_states.dtype,",
      "                         device=chunk_states.device)",
      "    grid = lambda META: (triton.cdiv(headdim, META['BLOCK_SIZE_M']) * triton.",
      "                         cdiv(dstate, META['BLOCK_SIZE_N']), batch, nheads)",
      "    with torch.cuda.device(x.device.index):",
      "        _chunk_state_varlen_kernel[grid](",
      "            x,",
      "            B,",
      "            dt,",
      "            dA_cumsum,",
      "            chunk_states,",
      "            cu_seqlens,",
      "            states,",
      "            initial_states,",
      "            headdim,",
      "            dstate,",
      "            chunk_size,",
      "            total_seqlen,",
      "            nheads // ngroups,",
      "            x.stride(0),",
      "            x.stride(1),",
      "            x.stride(2),",
      "            B.stride(0),",
      "            B.stride(1),",
      "            B.stride(2),",
      "            dt.stride(1),",
      "            dt.stride(0),",
      "            dt.stride(2),",
      "            dA_cumsum.stride(1),",
      "            dA_cumsum.stride(0),",
      "            dA_cumsum.stride(2),",
      "            chunk_states.stride(0),",
      "            chunk_states.stride(1),",
      "            chunk_states.stride(2),",
      "            chunk_states.stride(3),",
      "            states.stride(0),",
      "            states.stride(1),",
      "            states.stride(2),",
      "            states.stride(3),",
      "            *((initial_states.stride(0), initial_states.stride(1),",
      "               initial_states.stride(2),",
      "               initial_states.stride(3)) if initial_states is not None else",
      "              (0, 0, 0, 0)),",
      "            HAS_INITSTATES=initial_states is not None)",
      "    return states"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/mamba/ops/causal_conv1d.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "# Copyright (c) 2024, Tri Dao.",
      "# Adapted from https://github.com/Dao-AILab/causal-conv1d/blob/main/causal_conv1d/causal_conv1d_interface.py",
      "",
      "from typing import Optional, Union",
      "",
      "import numpy as np",
      "import torch",
      "",
      "from vllm.attention.backends.utils import PAD_SLOT_ID",
      "from vllm.triton_utils import tl, triton",
      "",
      "",
      "@triton.jit()",
      "def _causal_conv1d_fwd_kernel(  # continuous batching",
      "    # Pointers to matrices",
      "    x_ptr,  # (dim, cu_seqlen) holding `batch` of actual sequences + padded sequences",
      "    w_ptr,  # (dim, width)",
      "    bias_ptr,",
      "    initial_states_ptr,  # conv_states_ptr",
      "    cache_indices_ptr,  # conv_state_indices_ptr",
      "    has_initial_states_ptr,",
      "    query_start_loc_ptr,",
      "    batch_ptr,",
      "    token_chunk_offset_ptr,",
      "    o_ptr,  # (dim, seqlen) - actually pointing to x_ptr",
      "    # Matrix dimensions",
      "    batch: tl.int32,  # actually padded_batch",
      "    dim: tl.constexpr,",
      "    seqlen: tl.int32,  # cu_seqlen",
      "    num_cache_lines: tl.constexpr,  # added to support vLLM larger cache lines",
      "    # Strides",
      "    stride_x_seq: tl.constexpr,  # stride to get to next sequence,",
      "    stride_x_dim: tl.constexpr,  # stride to get to next feature-value,",
      "    stride_x_token: tl.",
      "    constexpr,  # stride to get to next token (same feature-index, same sequence-index)",
      "    stride_w_dim: tl.constexpr,  # stride to get to next dim-axis value",
      "    stride_w_width: tl.constexpr,  # stride to get to next width-axis value",
      "    stride_istate_seq: tl.constexpr,",
      "    stride_istate_dim: tl.constexpr,",
      "    stride_istate_token: tl.constexpr,",
      "    stride_o_seq: tl.constexpr,",
      "    stride_o_dim: tl.constexpr,",
      "    stride_o_token: tl.constexpr,",
      "    # others",
      "    pad_slot_id: tl.constexpr,",
      "    # Meta-parameters",
      "    HAS_BIAS: tl.constexpr,",
      "    KERNEL_WIDTH: tl.constexpr,",
      "    SILU_ACTIVATION: tl.constexpr,",
      "    HAS_INITIAL_STATES: tl.constexpr,",
      "    HAS_CACHE: tl.constexpr,",
      "    IS_CONTINUOUS_BATCHING: tl.constexpr,",
      "    USE_PAD_SLOT: tl.constexpr,",
      "    NP2_STATELEN: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    conv_states_ptr = initial_states_ptr",
      "    conv_state_indices_ptr = cache_indices_ptr",
      "    stride_conv_state_seq = stride_istate_seq",
      "    stride_conv_state_dim = stride_istate_dim",
      "    stride_conv_state_tok = stride_istate_token",
      "    state_len = KERNEL_WIDTH - 1  # can be passed via argument if it's not the same as this value",
      "",
      "    # one program handles one chunk in a single sequence",
      "    # rather than mixing sequences - to make updating initial_states across sequences efficiently",
      "",
      "    # single-sequence id",
      "    idx_seq = tl.load(batch_ptr + tl.program_id(0))",
      "    chunk_offset = tl.load(token_chunk_offset_ptr + tl.program_id(0))",
      "",
      "    # BLOCK_N elements along the feature-dimension (channel)",
      "    idx_feats = tl.program_id(1) * BLOCK_N + tl.arange(0, BLOCK_N)",
      "",
      "    if idx_seq == pad_slot_id:",
      "        return",
      "",
      "    sequence_start_index = tl.load(query_start_loc_ptr + idx_seq)",
      "    sequence_end_index = tl.load(query_start_loc_ptr + idx_seq + 1)",
      "    # find the actual sequence length",
      "    seqlen = sequence_end_index - sequence_start_index",
      "",
      "    token_offset = BLOCK_M * chunk_offset",
      "    segment_len = min(BLOCK_M, seqlen - token_offset)",
      "",
      "    # base of the sequence",
      "    x_base = x_ptr + sequence_start_index * stride_x_token + idx_feats * stride_x_dim  # [BLOCK_N,]",
      "",
      "    if IS_CONTINUOUS_BATCHING:",
      "        # cache_idx",
      "        conv_state_batch_coord = tl.load(conv_state_indices_ptr + idx_seq).to(",
      "            tl.int64)",
      "    else:",
      "        # cache_idx",
      "        conv_state_batch_coord = idx_seq",
      "    if USE_PAD_SLOT:  # noqa",
      "        if conv_state_batch_coord == pad_slot_id:",
      "            # not processing as this is not the actual sequence",
      "            return",
      "    conv_states_base = (conv_states_ptr +",
      "                        (conv_state_batch_coord * stride_conv_state_seq) +",
      "                        (idx_feats * stride_conv_state_dim))  # [BLOCK_N,]",
      "",
      "    w_base = w_ptr + (idx_feats * stride_w_dim)  # [BLOCK_N,]",
      "",
      "    # Does 2 things:",
      "    # 1. READ prior-block init-state data - [done by every Triton programs]",
      "    # 2. update conv_state with new data [only by the Triton program handles chunk_offset=0]",
      "    if chunk_offset == 0:",
      "        # read from conv_states",
      "        load_init_state = False",
      "        if HAS_INITIAL_STATES:  # the new HAS_INITIAL_STATES",
      "            load_init_state = tl.load(has_initial_states_ptr + idx_seq).to(",
      "                tl.int1)",
      "        if load_init_state:",
      "            # load from conv_states",
      "            prior_tokens = conv_states_base + (state_len -",
      "                                               1) * stride_conv_state_tok",
      "            mask_w = idx_feats < dim",
      "            if KERNEL_WIDTH == 2:",
      "                conv_states_ptrs = prior_tokens  # [BLOCK_N]",
      "                col0 = tl.load(conv_states_ptrs, mask_w, 0.0)",
      "            if KERNEL_WIDTH == 3:",
      "                conv_states_ptrs = prior_tokens  # [BLOCK_N]",
      "                col1 = tl.load(conv_states_ptrs, mask_w, 0.0)",
      "                conv_states_ptrs = prior_tokens - 1 * stride_conv_state_tok  # [BLOCK_N]",
      "                col0 = tl.load(conv_states_ptrs, mask_w, 0.0)",
      "            if KERNEL_WIDTH == 4:",
      "                conv_states_ptrs = prior_tokens  # [BLOCK_N]",
      "                col2 = tl.load(conv_states_ptrs, mask_w, 0.0)",
      "                conv_states_ptrs = prior_tokens - 1 * stride_conv_state_tok  # [BLOCK_N]",
      "                col1 = tl.load(conv_states_ptrs, mask_w, 0.0)",
      "                conv_states_ptrs = prior_tokens - 2 * stride_conv_state_tok  # [BLOCK_N]",
      "                col0 = tl.load(conv_states_ptrs, mask_w, 0.0)",
      "            if KERNEL_WIDTH == 5:",
      "                conv_states_ptrs = prior_tokens  # [BLOCK_N]",
      "                col3 = tl.load(conv_states_ptrs, mask_w, 0.0)",
      "                conv_states_ptrs = prior_tokens - 1 * stride_conv_state_tok  # [BLOCK_N]",
      "                col2 = tl.load(conv_states_ptrs, mask_w, 0.0)",
      "                conv_states_ptrs = prior_tokens - 2 * stride_conv_state_tok  # [BLOCK_N]",
      "                col1 = tl.load(conv_states_ptrs, mask_w, 0.0)",
      "                conv_states_ptrs = prior_tokens - 3 * stride_conv_state_tok  # [BLOCK_N]",
      "                col0 = tl.load(conv_states_ptrs, mask_w, 0.0)",
      "        else:",
      "            # prior-tokens are zeros",
      "            if KERNEL_WIDTH >= 2:  # STRATEGY1",
      "                # first chunk and does not have prior-token, so just set to 0",
      "                col0 = tl.zeros((BLOCK_N, ), dtype=x_ptr.dtype.element_ty)",
      "            if KERNEL_WIDTH >= 3:  # STRATEGY1",
      "                col1 = tl.zeros((BLOCK_N, ), dtype=x_ptr.dtype.element_ty)",
      "            if KERNEL_WIDTH >= 4:  # STRATEGY1",
      "                col2 = tl.zeros((BLOCK_N, ), dtype=x_ptr.dtype.element_ty)",
      "            if KERNEL_WIDTH >= 5:  # STRATEGY1",
      "                col3 = tl.zeros((BLOCK_N, ), dtype=x_ptr.dtype.element_ty)",
      "",
      "        # STEP 2:",
      "        # here prepare data for updating conv_state",
      "        if state_len <= seqlen:  # SMALL_CACHE=True (only move part of 'x' into conv_state cache)",
      "            # just read from 'x'",
      "            # copy 'x' data to conv_state",
      "            # load only 'x' data (and set 0 before 'x' if seqlen < state_len)",
      "            idx_tokens_last = (seqlen - state_len) + tl.arange(",
      "                0, NP2_STATELEN)  # [BLOCK_M]",
      "            x_ptrs = x_ptr + (",
      "                (sequence_start_index + idx_tokens_last) *",
      "                stride_x_token)[:, None] + (",
      "                    idx_feats * stride_x_dim)[None, :]  # [BLOCK_M,BLOCK_N,]",
      "            mask_x = ((idx_tokens_last >= 0)[:, None] &",
      "                      (idx_tokens_last < seqlen)[:, None] &",
      "                      (idx_feats < dim)[None, :]",
      "                      )  # token-index  # token-index  # feature-index",
      "            loaded_x = tl.load(x_ptrs, mask_x, 0.0)",
      "            new_conv_state = tl.load(x_ptrs, mask_x, 0.0)",
      "            idx_tokens_conv = tl.arange(0, NP2_STATELEN)  # [BLOCK_M]",
      "            conv_states_ptrs_target = conv_states_base[None, :] + (",
      "                idx_tokens_conv * stride_conv_state_tok)[:, None]",
      "",
      "            mask = (idx_tokens_conv < state_len)[:, None] & (idx_feats",
      "                                                             < dim)[None, :]",
      "            tl.debug_barrier()  #  NOTE: use this due to bug in Triton compiler",
      "            tl.store(conv_states_ptrs_target, new_conv_state, mask)",
      "",
      "        else:",
      "            if load_init_state:",
      "                # update conv_state by shifting left, i.e. take last few cols from conv_state + cols from 'x'",
      "                idx_tokens_conv = tl.arange(0, NP2_STATELEN)  # [BLOCK_M]",
      "",
      "                conv_states_ptrs_source = (",
      "                    conv_states_ptr +",
      "                    (conv_state_batch_coord * stride_conv_state_seq) +",
      "                    (idx_feats * stride_conv_state_dim)[None, :] +",
      "                    ((idx_tokens_conv + seqlen) * stride_conv_state_tok)[:,",
      "                                                                         None]",
      "                )  # [BLOCK_M, BLOCK_N]",
      "                mask = ((conv_state_batch_coord < num_cache_lines)",
      "                        & ((idx_tokens_conv + seqlen) < state_len)[:, None]",
      "                        & (idx_feats < dim)[None, :])",
      "                conv_state = tl.load(conv_states_ptrs_source, mask, other=0.0)",
      "",
      "                VAL = state_len - seqlen",
      "",
      "                x_ptrs = x_base[None, :] + (",
      "                    (idx_tokens_conv - VAL) *",
      "                    stride_x_token)[:, None]  # [BLOCK_M, BLOCK_N]",
      "",
      "                mask_x = ((idx_tokens_conv - VAL >= 0)[:, None] &",
      "                          (idx_tokens_conv - VAL < seqlen)[:, None] &",
      "                          (idx_feats < dim)[None, :]",
      "                          )  # token-index  # token-index  # feature-index",
      "                loaded_x = tl.load(x_ptrs, mask_x, 0.0)",
      "",
      "                tl.debug_barrier(",
      "                )  # need this due to the bug in tl.where not enforcing this when data is the result of another tl.load",
      "                new_conv_state = tl.where(",
      "                    mask, conv_state, loaded_x",
      "                )  # BUG in 'tl.where'  which requires a barrier before this",
      "                conv_states_ptrs_target = conv_states_base + (",
      "                    idx_tokens_conv *",
      "                    stride_conv_state_tok)[:, None]  # [BLOCK_M, BLOCK_N]",
      "                mask = (idx_tokens_conv",
      "                        < state_len)[:, None] & (idx_feats < dim)[None, :]",
      "                tl.store(conv_states_ptrs_target, new_conv_state, mask)",
      "            else:  # load_init_state == False",
      "                # update conv_state by shifting left, BUT",
      "                # set cols prior to 'x' as zeros + cols from 'x'",
      "                idx_tokens_conv = tl.arange(0, NP2_STATELEN)  # [BLOCK_M]",
      "",
      "                VAL = state_len - seqlen",
      "",
      "                x_ptrs = x_base[None, :] + (",
      "                    (idx_tokens_conv - VAL) *",
      "                    stride_x_token)[:, None]  # [BLOCK_M, BLOCK_N]",
      "",
      "                mask_x = ((idx_tokens_conv - VAL >= 0)[:, None] &",
      "                          (idx_tokens_conv - VAL < seqlen)[:, None] &",
      "                          (idx_feats < dim)[None, :]",
      "                          )  # token-index  # token-index  # feature-index",
      "                new_conv_state = tl.load(x_ptrs, mask_x, 0.0)",
      "",
      "                conv_states_ptrs_target = conv_states_base + (",
      "                    idx_tokens_conv *",
      "                    stride_conv_state_tok)[:, None]  # [BLOCK_M, BLOCK_N]",
      "                mask = (idx_tokens_conv",
      "                        < state_len)[:, None] & (idx_feats < dim)[None, :]",
      "                tl.store(conv_states_ptrs_target, new_conv_state, mask)",
      "",
      "    else:  # chunk_offset > 0",
      "        # read prior-token data from `x`",
      "        load_init_state = True",
      "        prior_tokens = x_base + (token_offset - 1) * stride_x_token",
      "        mask_w = idx_feats < dim",
      "        if KERNEL_WIDTH == 2:",
      "            conv_states_ptrs = prior_tokens  # [BLOCK_N]",
      "            col0 = tl.load(conv_states_ptrs, mask_w, 0.0, cache_modifier='.ca')",
      "        if KERNEL_WIDTH == 3:",
      "            conv_states_ptrs = prior_tokens  # [BLOCK_N]",
      "            col1 = tl.load(conv_states_ptrs, mask_w, 0.0, cache_modifier='.ca')",
      "            conv_states_ptrs = prior_tokens - 1 * stride_x_token  # [BLOCK_N]",
      "            col0 = tl.load(conv_states_ptrs, mask_w, 0.0, cache_modifier='.ca')",
      "        if KERNEL_WIDTH == 4:",
      "            conv_states_ptrs = prior_tokens  # [BLOCK_N]",
      "            col2 = tl.load(conv_states_ptrs, mask_w, 0.0, cache_modifier='.ca')",
      "            conv_states_ptrs = prior_tokens - 1 * stride_x_token  # [BLOCK_N]",
      "            col1 = tl.load(conv_states_ptrs, mask_w, 0.0, cache_modifier='.ca')",
      "            conv_states_ptrs = prior_tokens - 2 * stride_x_token  # [BLOCK_N]",
      "            col0 = tl.load(conv_states_ptrs, mask_w, 0.0, cache_modifier='.ca')",
      "        if KERNEL_WIDTH == 5:",
      "            # ruff: noqa: F841",
      "            conv_states_ptrs = prior_tokens  # [BLOCK_N]",
      "            col3 = tl.load(conv_states_ptrs, mask_w, 0.0, cache_modifier='.ca')",
      "            conv_states_ptrs = prior_tokens - 1 * stride_x_token  # [BLOCK_N]",
      "            col2 = tl.load(conv_states_ptrs, mask_w, 0.0, cache_modifier='.ca')",
      "            conv_states_ptrs = prior_tokens - 2 * stride_x_token  # [BLOCK_N]",
      "            col1 = tl.load(conv_states_ptrs, mask_w, 0.0, cache_modifier='.ca')",
      "            conv_states_ptrs = prior_tokens - 3 * stride_x_token  # [BLOCK_N]",
      "            col0 = tl.load(conv_states_ptrs, mask_w, 0.0, cache_modifier='.ca')",
      "",
      "    if HAS_BIAS:",
      "        bias = bias_ptr + idx_feats",
      "        mask_bias = idx_feats < dim",
      "        acc_preload = tl.load(bias, mask=mask_bias,",
      "                              other=0.0).to(tl.float32)  # [BLOCK_N]",
      "    else:",
      "        acc_preload = tl.zeros((BLOCK_N, ), dtype=tl.float32)",
      "",
      "    x_base_1d = x_base + token_offset * stride_x_token  # starting of chunk",
      "",
      "    # PRE-LOAD WEIGHTS",
      "    mask_w = idx_feats < dim",
      "    if KERNEL_WIDTH >= 2:",
      "        w_ptrs = w_base + (0 * stride_w_width)  # [BLOCK_N] tensor",
      "        w_col0 = tl.load(w_ptrs, mask_w, other=0.0)",
      "        w_ptrs = w_base + (1 * stride_w_width)  # [BLOCK_N] tensor",
      "        w_col1 = tl.load(w_ptrs, mask_w, other=0.0)",
      "    if KERNEL_WIDTH >= 3:",
      "        w_ptrs = w_base + (2 * stride_w_width)  # [BLOCK_N] tensor",
      "        w_col2 = tl.load(w_ptrs, mask_w, other=0.0)",
      "    if KERNEL_WIDTH >= 4:",
      "        w_ptrs = w_base + (3 * stride_w_width)  # [BLOCK_N] tensor",
      "        w_col3 = tl.load(w_ptrs, mask_w, other=0.0)",
      "    mask_x_1d = idx_feats < dim",
      "    for idx_token in range(segment_len):",
      "        acc = acc_preload",
      "",
      "        matrix_w = w_col0",
      "        matrix_x = col0",
      "        for j in tl.static_range(KERNEL_WIDTH):",
      "",
      "            if KERNEL_WIDTH == 2:",
      "                if j == 1:  # KERNEL_WIDTH-1:",
      "                    matrix_w = w_col1",
      "                    x_ptrs_1d = x_base_1d + idx_token * stride_x_token  # [BLOCK_N]",
      "                    matrix_x = tl.load(x_ptrs_1d, mask=mask_x_1d)",
      "            elif KERNEL_WIDTH == 3:",
      "                if j == 1:",
      "                    matrix_w = w_col1",
      "                    matrix_x = col1",
      "                elif j == 2:",
      "                    matrix_w = w_col2",
      "                    x_ptrs_1d = x_base_1d + idx_token * stride_x_token  # [BLOCK_N]",
      "                    matrix_x = tl.load(x_ptrs_1d, mask=mask_x_1d)",
      "            elif KERNEL_WIDTH == 4:",
      "                if j == 1:",
      "                    matrix_w = w_col1",
      "                    matrix_x = col1",
      "                elif j == 2:",
      "                    matrix_w = w_col2",
      "                    matrix_x = col2",
      "                elif j == 3:",
      "                    matrix_w = w_col3",
      "                    x_ptrs_1d = x_base_1d + idx_token * stride_x_token  # [BLOCK_N]",
      "                    matrix_x = tl.load(x_ptrs_1d, mask=mask_x_1d)",
      "",
      "            acc += matrix_x * matrix_w  # [BLOCK_N]",
      "",
      "        if KERNEL_WIDTH == 2:",
      "            col0 = matrix_x",
      "        elif KERNEL_WIDTH == 3:",
      "            col0 = col1",
      "            col1 = matrix_x",
      "        elif KERNEL_WIDTH == 4:",
      "            col0 = col1",
      "            col1 = col2",
      "            col2 = matrix_x",
      "",
      "        if SILU_ACTIVATION:",
      "            acc = acc / (1 + tl.exp(-acc))",
      "        mask_1d = (idx_token < segment_len) & (",
      "            idx_feats < dim)  # token-index  # feature-index",
      "        o_ptrs = o_ptr + (sequence_start_index + token_offset + idx_token",
      "                          ) * stride_o_token + (idx_feats * stride_o_dim)",
      "",
      "        tl.store(o_ptrs, acc, mask=mask_1d)",
      "",
      "",
      "def causal_conv1d_fn(",
      "    x: torch.Tensor,",
      "    weight: torch.Tensor,",
      "    bias: Union[torch.Tensor, None],",
      "    conv_states: torch.Tensor,",
      "    query_start_loc: torch.Tensor,",
      "    cache_indices: Optional[torch.Tensor] = None,",
      "    has_initial_state: Optional[torch.Tensor] = None,",
      "    activation: Optional[str] = \"silu\",",
      "    pad_slot_id: int = PAD_SLOT_ID,",
      "    metadata=None,",
      "    validate_data=False,",
      "):",
      "    \"\"\"support varlen + continuous batching when x is 2D tensor",
      "",
      "    x: (dim,cu_seq_len)",
      "        cu_seq_len = total tokens of all seqs in that batch",
      "        sequences are concatenated from left to right for varlen",
      "    weight: (dim, width)",
      "    conv_states: (...,dim,width - 1) itype",
      "        updated inplace if provided",
      "        [it use `cache_indices` to get the index to the cache of conv_state for that sequence",
      "",
      "        conv_state[cache_indices[i]] for seq-i - to be used as initial_state when has_initial_state[i] = True",
      "             and after that conv_state[cache_indices[i]] need to be shift-left and updated with values from 'x'",
      "        ]",
      "    query_start_loc: (batch + 1) int32",
      "        The cumulative sequence lengths of the sequences in",
      "        the batch, used to index into sequence. prepended by 0.",
      "        if",
      "        x = [5, 1, 1, 1] <- continuous batching (batch=4)",
      "        then",
      "        query_start_loc = [0, 5, 6, 7, 8] <- the starting index of the next sequence; while the last value is",
      "           the ending index of the last sequence",
      "        [length(query_start_loc)-1 == batch]",
      "        for example: query_start_loc = torch.Tensor([0,10,16,17]),",
      "        x.shape=(dim,17)",
      "    cache_indices: (batch)  int32",
      "        indicates the corresponding state index,",
      "        like so: conv_state = conv_states[cache_indices[batch_id]]",
      "    has_initial_state: (batch) bool",
      "        indicates whether should the kernel take the current state as initial",
      "        state for the calculations",
      "        [single boolean for each sequence in the batch: True or False]",
      "    bias: (dim,)",
      "    activation: either None or \"silu\" or \"swish\" or True",
      "    pad_slot_id: int",
      "        if cache_indices is passed, lets the kernel identify padded",
      "        entries that will not be processed,",
      "        for example: cache_indices = [pad_slot_id, 1, 20, pad_slot_id]",
      "        in this case, the kernel will not process entries at",
      "        indices 0 and 3",
      "",
      "    out: same shape as `x`",
      "    \"\"\"",
      "    if isinstance(activation, bool) and activation:",
      "        activation = \"silu\"",
      "",
      "    args = None",
      "    out = torch.empty_like(x)",
      "    if metadata is not None:",
      "        cu_seqlen = metadata.cu_seqlen",
      "        nums_dict = metadata.nums_dict",
      "        #x = metadata.x",
      "        args = nums_dict",
      "        batch_ptr = metadata.batch_ptr",
      "        token_chunk_offset_ptr = metadata.token_chunk_offset_ptr",
      "    else:",
      "        seqlens = np.diff(query_start_loc.to('cpu'))",
      "        args = seqlens",
      "        MAX_NUM_PROGRAMS = 1024",
      "",
      "        batch_ptr = torch.full(",
      "            (MAX_NUM_PROGRAMS, ),",
      "            PAD_SLOT_ID,",
      "            dtype=torch.int32,",
      "            device=x.device",
      "        )  # tracking which seq-idx the Triton program is handling",
      "        token_chunk_offset_ptr = torch.full(",
      "            (MAX_NUM_PROGRAMS, ),",
      "            PAD_SLOT_ID,",
      "            dtype=torch.int32,",
      "            device=x.device",
      "        )  # tracking BLOCK_M-based index in the sequence the Triton program is handling",
      "",
      "    is_channel_last = (x.stride(0) == 1) & (x.stride(1) > 1)",
      "    dim, cu_seqlen = x.shape",
      "    _, width = weight.shape",
      "    state_len = width - 1",
      "    np2_statelen = triton.next_power_of_2(state_len)",
      "",
      "    padded_batch = query_start_loc.size(0) - 1",
      "    stride_x_seq = 0",
      "    stride_x_dim = x.stride(0)",
      "    stride_x_token = x.stride(1)",
      "    stride_w_dim = weight.stride(0)",
      "    stride_w_width = weight.stride(1)",
      "    stride_istate_seq = 0",
      "    stride_istate_dim = 0",
      "    stride_istate_token = 0",
      "    num_cache_lines = 0",
      "    if conv_states is not None:",
      "        # extensions to support vLLM:",
      "        # 1. conv_states is used to replaced initial_states",
      "        # 2. conv_states serve as a cache with num cache lines can be larger than batch size",
      "        # 3. mapping from sequence x[idx] to a cache line at index as specified via cache_indices[idx]",
      "        # 4. computation can be skipped if cache_indices[idx] == pad_slot_id",
      "        num_cache_lines = conv_states.size(0)",
      "        assert (num_cache_lines, dim, width - 1) == conv_states.shape",
      "        stride_istate_seq = conv_states.stride(0)",
      "        stride_istate_dim = conv_states.stride(1)",
      "        stride_istate_token = conv_states.stride(2)",
      "        assert stride_istate_dim == 1",
      "    if out.dim() == 2:",
      "        stride_o_seq = 0",
      "        stride_o_dim = out.stride(0)",
      "        stride_o_token = out.stride(1)",
      "    else:",
      "        stride_o_seq = out.stride(0)",
      "        stride_o_dim = out.stride(1)",
      "        stride_o_token = out.stride(2)",
      "",
      "    if validate_data:",
      "        assert x.dim() == 2",
      "        assert query_start_loc is not None",
      "        assert query_start_loc.dim() == 1",
      "        assert x.stride(0) == 1 or x.stride(1) == 1",
      "        if bias is not None:",
      "            assert bias.dim() == 1",
      "            assert dim == bias.size(0)",
      "        if cache_indices is not None:",
      "            assert cache_indices.dim() == 1",
      "            assert padded_batch == cache_indices.size(0)",
      "        if has_initial_state is not None:",
      "            assert has_initial_state.size() == (padded_batch, )",
      "            assert conv_states is not None, \"ERROR: `has_initial_state` is used, which needs also `conv_states`\"",
      "        assert weight.stride(1) == 1",
      "        assert (dim, width) == weight.shape",
      "        assert is_channel_last, \"Need to run in channel-last layout\"",
      "",
      "    if metadata is None:",
      "",
      "        def num_program(META, seqlens):",
      "            tot = 0",
      "",
      "            mlist = []",
      "            offsetlist = []  # type: ignore",
      "",
      "            nums = -(-seqlens // META[\"BLOCK_M\"])",
      "",
      "            tot = nums.sum().item()",
      "            mlist = np.repeat(np.arange(len(nums)), nums)",
      "            for idx, num in enumerate(nums):",
      "                offsetlist.extend(",
      "                    range(num)",
      "                )  # chunk-idx if a sequence is split into multiple chunks",
      "",
      "            if META[\"batch_ptr\"].nelement() < len(mlist):",
      "                newlen = len(mlist) + 1",
      "                META[\"batch_ptr\"].resize_(newlen).fill_(PAD_SLOT_ID)",
      "                META[\"token_chunk_offset_ptr\"].resize_(newlen).fill_(",
      "                    PAD_SLOT_ID)",
      "",
      "            if META[\"batch_ptr\"].nelement() >= len(mlist):",
      "                META[\"batch_ptr\"][0:len(mlist)].copy_(",
      "                    torch.from_numpy(np.array(mlist)))",
      "                META[\"token_chunk_offset_ptr\"][0:len(mlist)].copy_(",
      "                    torch.from_numpy(np.array(offsetlist)))",
      "",
      "            META[\"batch_ptr\"] = META[\"batch_ptr\"].to(META[\"x_ptr\"].device)",
      "            META[\"token_chunk_offset_ptr\"] = META[\"token_chunk_offset_ptr\"].to(",
      "                META[\"x_ptr\"].device)",
      "            return tot",
      "    else:",
      "",
      "        def num_program(META, nums_dict):",
      "            tot = nums_dict[META[\"BLOCK_M\"]]['tot']",
      "",
      "            mlist = nums_dict[META[\"BLOCK_M\"]]['mlist']",
      "            mlist_len = nums_dict[META[\"BLOCK_M\"]]['mlist_len']",
      "",
      "            offsetlist = nums_dict[META[\"BLOCK_M\"]]['offsetlist']",
      "",
      "            if nums_dict[META[\"BLOCK_M\"]][\"batch_ptr\"] is not None:",
      "                META[\"batch_ptr\"] = nums_dict[META[\"BLOCK_M\"]][\"batch_ptr\"]",
      "                META[\"token_chunk_offset_ptr\"] = nums_dict[",
      "                    META[\"BLOCK_M\"]][\"token_chunk_offset_ptr\"]",
      "            else:",
      "                if META[\"batch_ptr\"].nelement() < mlist_len:",
      "                    newlen = mlist_len + 1",
      "                    META[\"batch_ptr\"].resize_(newlen).fill_(PAD_SLOT_ID)",
      "                    META[\"token_chunk_offset_ptr\"].resize_(newlen).fill_(",
      "                        PAD_SLOT_ID)",
      "",
      "                if META[\"batch_ptr\"].nelement() >= mlist_len:",
      "                    META[\"batch_ptr\"][0:mlist_len].copy_(mlist)",
      "                    META[\"token_chunk_offset_ptr\"][0:mlist_len].copy_(",
      "                        offsetlist)",
      "            return tot",
      "",
      "    def grid(META):",
      "        return (",
      "            num_program(META, args),",
      "            triton.cdiv(dim, META[\"BLOCK_N\"]),",
      "        )",
      "",
      "    if batch_ptr.device != x.device:",
      "        batch_ptr = batch_ptr.to(x.device)",
      "        token_chunk_offset_ptr = token_chunk_offset_ptr.to(x.device)",
      "",
      "    _causal_conv1d_fwd_kernel[grid](",
      "        # Pointers to matrices",
      "        x,",
      "        weight,",
      "        bias,",
      "        conv_states,",
      "        cache_indices,",
      "        has_initial_state,",
      "        query_start_loc,",
      "        batch_ptr,",
      "        token_chunk_offset_ptr,",
      "        out,",
      "        # Matrix dimensions",
      "        padded_batch,",
      "        dim,",
      "        cu_seqlen,",
      "        num_cache_lines,",
      "        # stride",
      "        stride_x_seq,",
      "        stride_x_dim,",
      "        stride_x_token,",
      "        stride_w_dim,",
      "        stride_w_width,",
      "        stride_istate_seq,",
      "        stride_istate_dim,",
      "        stride_istate_token,",
      "        stride_o_seq,",
      "        stride_o_dim,",
      "        stride_o_token,",
      "        # others",
      "        pad_slot_id,",
      "        # META",
      "        HAS_BIAS=bias is not None,",
      "        KERNEL_WIDTH=width,",
      "        SILU_ACTIVATION=activation in [\"silu\", \"swish\"],",
      "        HAS_INITIAL_STATES=has_initial_state is not None,",
      "        HAS_CACHE=conv_states is not None,",
      "        IS_CONTINUOUS_BATCHING=cache_indices is not None,",
      "        USE_PAD_SLOT=pad_slot_id is not None,",
      "        NP2_STATELEN=np2_statelen,",
      "        #launch_cooperative_grid=True",
      "        BLOCK_M=8,",
      "        BLOCK_N=256,",
      "        num_stages=2,",
      "    )",
      "    return out",
      "",
      "",
      "@triton.jit()",
      "def _causal_conv1d_update_kernel(",
      "    # Pointers to matrices",
      "    x_ptr,  # (batch, dim, seqlen)",
      "    w_ptr,  # (dim, width)",
      "    bias_ptr,",
      "    conv_state_ptr,",
      "    cache_seqlens_ptr,  # circular buffer",
      "    conv_state_indices_ptr,",
      "    o_ptr,  # (batch, dim, seqlen)",
      "    # Matrix dimensions",
      "    batch: int,",
      "    dim: tl.constexpr,",
      "    seqlen: tl.constexpr,",
      "    state_len: tl.constexpr,",
      "    num_cache_lines: tl.constexpr,  # added to support vLLM larger cache lines",
      "    # Strides",
      "    stride_x_seq: tl.constexpr,",
      "    stride_x_dim: tl.constexpr,",
      "    stride_x_token: tl.constexpr,",
      "    stride_w_dim: tl.constexpr,",
      "    stride_w_width: tl.constexpr,",
      "    stride_conv_state_seq: tl.constexpr,",
      "    stride_conv_state_dim: tl.constexpr,",
      "    stride_conv_state_tok: tl.constexpr,",
      "    stride_o_seq: tl.constexpr,",
      "    stride_o_dim: tl.constexpr,",
      "    stride_o_token: tl.constexpr,",
      "    # others",
      "    pad_slot_id: tl.constexpr,",
      "    # Meta-parameters",
      "    HAS_BIAS: tl.constexpr,",
      "    KERNEL_WIDTH: tl.constexpr,",
      "    SILU_ACTIVATION: tl.constexpr,",
      "    IS_CONTINUOUS_BATCHING: tl.constexpr,",
      "    NP2_STATELEN: tl.constexpr,",
      "    USE_PAD_SLOT: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    # ruff: noqa: E501",
      "    idx_seq = tl.program_id(0)",
      "    if idx_seq >= batch:",
      "        return",
      "",
      "    # [BLOCK_N,] elements along the feature-dimension (channel)",
      "    idx_feats = tl.program_id(1) * BLOCK_N + tl.arange(0, BLOCK_N)",
      "",
      "    if IS_CONTINUOUS_BATCHING:",
      "        # mask = idx_seq < batch",
      "        conv_state_batch_coord = tl.load(conv_state_indices_ptr + idx_seq).to(",
      "            tl.int64)",
      "    else:",
      "        conv_state_batch_coord = idx_seq",
      "    if USE_PAD_SLOT:  # noqa",
      "        if conv_state_batch_coord == pad_slot_id:",
      "            # not processing as this is not the actual sequence",
      "            return",
      "",
      "    # STEP 1: READ init_state data",
      "    conv_states_base = (conv_state_ptr +",
      "                        (conv_state_batch_coord * stride_conv_state_seq) +",
      "                        (idx_feats * stride_conv_state_dim))",
      "    mask_w = idx_feats < dim",
      "",
      "    prior_tokens = conv_states_base",
      "    if KERNEL_WIDTH >= 2:",
      "        conv_states_ptrs = prior_tokens  # [BLOCK_N]",
      "        col0 = tl.load(conv_states_ptrs, mask_w, 0.0)",
      "    if KERNEL_WIDTH >= 3:",
      "        conv_states_ptrs = prior_tokens + 1 * stride_conv_state_tok  # [BLOCK_N]",
      "        col1 = tl.load(conv_states_ptrs, mask_w, 0.0)",
      "    if KERNEL_WIDTH >= 4:",
      "        conv_states_ptrs = prior_tokens + 2 * stride_conv_state_tok  # [BLOCK_N]",
      "        col2 = tl.load(conv_states_ptrs, mask_w, 0.0)",
      "    if KERNEL_WIDTH == 5:",
      "        conv_states_ptrs = prior_tokens + 3 * stride_conv_state_tok  # [BLOCK_N]",
      "        col3 = tl.load(conv_states_ptrs, mask_w, 0.0)",
      "",
      "    # STEP 2: assume state_len > seqlen",
      "    idx_tokens = tl.arange(0, NP2_STATELEN)  # [BLOCK_M]",
      "",
      "    conv_state_ptrs_source = (",
      "        conv_state_ptr + (conv_state_batch_coord * stride_conv_state_seq) +",
      "        (idx_feats * stride_conv_state_dim)[None, :] +",
      "        ((idx_tokens + seqlen) * stride_conv_state_tok)[:, None]",
      "    )  # [BLOCK_M, BLOCK_N]",
      "    mask = ((conv_state_batch_coord < num_cache_lines)",
      "            & ((idx_tokens + seqlen) < state_len)[:, None]",
      "            & (idx_feats < dim)[None, :])",
      "    conv_state = tl.load(conv_state_ptrs_source, mask, other=0.0)",
      "",
      "    VAL = state_len - seqlen",
      "    x_base = x_ptr + (idx_seq * stride_x_seq) + (idx_feats * stride_x_dim",
      "                                                 )  # [BLOCK_N]",
      "",
      "    x_ptrs = x_base[None, :] + (",
      "        (idx_tokens - VAL) * stride_x_token)[:, None]  # [BLOCK_M, BLOCK_N]",
      "",
      "    mask_x = ((idx_tokens - VAL >= 0)[:, None] &",
      "              (idx_tokens - VAL < seqlen)[:, None] & (idx_feats < dim)[None, :]",
      "              )  # token-index  # token-index  # feature-index",
      "    loaded_x = tl.load(x_ptrs, mask_x, 0.0)",
      "    tl.debug_barrier()",
      "",
      "    new_conv_state = tl.where(mask, conv_state, loaded_x)",
      "",
      "    conv_state_base = (conv_state_ptr +",
      "                       (conv_state_batch_coord * stride_conv_state_seq) +",
      "                       (idx_feats * stride_conv_state_dim))  # [BLOCK_N,]",
      "    conv_state_ptrs_target = conv_state_base + (",
      "        idx_tokens * stride_conv_state_tok)[:, None]  # [BLOCK_M, BLOCK_N]",
      "    mask = (idx_tokens < state_len)[:, None] & (idx_feats < dim)[None, :]",
      "    tl.store(conv_state_ptrs_target, new_conv_state, mask)",
      "",
      "    # STEP 3: init accumulator",
      "    if HAS_BIAS:",
      "        bias = bias_ptr + idx_feats",
      "        mask_bias = idx_feats < dim",
      "        acc_preload = tl.load(bias, mask=mask_bias,",
      "                              other=0.0).to(tl.float32)  # [BLOCK_N]",
      "    else:",
      "        acc_preload = tl.zeros((BLOCK_N, ), dtype=tl.float32)",
      "",
      "    # STEP 4:",
      "    # PRE-LOAD WEIGHTS",
      "    # first kernel column, configured for weights to handle BLOCK_N features in range",
      "    w_base = w_ptr + (idx_feats * stride_w_dim)  # [BLOCK_N,]",
      "    mask_w = idx_feats < dim",
      "    if KERNEL_WIDTH >= 2:",
      "        w_ptrs = w_base + (0 * stride_w_width)  # [BLOCK_N] tensor",
      "        w_col0 = tl.load(w_ptrs, mask_w, other=0.0)",
      "        w_ptrs = w_base + (1 * stride_w_width)  # [BLOCK_N] tensor",
      "        w_col1 = tl.load(w_ptrs, mask_w, other=0.0)",
      "    if KERNEL_WIDTH >= 3:",
      "        w_ptrs = w_base + (2 * stride_w_width)  # [BLOCK_N] tensor",
      "        w_col2 = tl.load(w_ptrs, mask_w, other=0.0)",
      "    if KERNEL_WIDTH >= 4:",
      "        w_ptrs = w_base + (3 * stride_w_width)  # [BLOCK_N] tensor",
      "        w_col3 = tl.load(w_ptrs, mask_w, other=0.0)",
      "",
      "    x_base_1d = x_base  # starting of chunk [BLOCK_N]",
      "    mask_x_1d = idx_feats < dim",
      "",
      "    # STEP 5: compute each token",
      "    for idx_token in tl.static_range(seqlen):",
      "        acc = acc_preload",
      "",
      "        matrix_w = w_col0",
      "        matrix_x = col0",
      "        for j in tl.static_range(KERNEL_WIDTH):",
      "            if KERNEL_WIDTH == 2:",
      "                if j == 1:  # KERNEL_WIDTH-1:",
      "                    matrix_w = w_col1",
      "                    x_ptrs_1d = x_base_1d + idx_token * stride_x_token  # [BLOCK_N]",
      "                    matrix_x = tl.load(x_ptrs_1d, mask=mask_x_1d)",
      "            elif KERNEL_WIDTH == 3:",
      "                if j == 1:",
      "                    matrix_w = w_col1",
      "                    matrix_x = col1",
      "                elif j == 2:",
      "                    matrix_w = w_col2",
      "                    x_ptrs_1d = x_base_1d + idx_token * stride_x_token  # [BLOCK_N]",
      "                    matrix_x = tl.load(x_ptrs_1d, mask=mask_x_1d)",
      "            elif KERNEL_WIDTH == 4:",
      "                if j == 1:",
      "                    matrix_w = w_col1",
      "                    matrix_x = col1",
      "                elif j == 2:",
      "                    matrix_w = w_col2",
      "                    matrix_x = col2",
      "                elif j == 3:",
      "                    matrix_w = w_col3",
      "                    x_ptrs_1d = x_base_1d + idx_token * stride_x_token  # [BLOCK_N]",
      "                    matrix_x = tl.load(x_ptrs_1d, mask=mask_x_1d)",
      "",
      "            acc += matrix_x * matrix_w  # [BLOCK_N]",
      "",
      "        if KERNEL_WIDTH == 2:",
      "            col0 = matrix_x",
      "        elif KERNEL_WIDTH == 3:",
      "            col0 = col1",
      "            col1 = matrix_x",
      "        elif KERNEL_WIDTH == 4:",
      "            col0 = col1",
      "            col1 = col2",
      "            col2 = matrix_x",
      "",
      "        if SILU_ACTIVATION:",
      "            acc = acc / (1 + tl.exp(-acc))",
      "        mask_1d = (idx_token < seqlen) & (idx_feats < dim",
      "                                          )  # token-index  # feature-index",
      "        o_ptrs = o_ptr + (",
      "            idx_seq) * stride_o_seq + idx_token * stride_o_token + (",
      "                idx_feats * stride_o_dim)",
      "",
      "        tl.store(o_ptrs, acc, mask=mask_1d)",
      "",
      "",
      "def causal_conv1d_update(",
      "    x: torch.Tensor,",
      "    conv_state: torch.Tensor,",
      "    weight: torch.Tensor,",
      "    bias: Optional[torch.Tensor] = None,",
      "    activation: Union[bool, str, None] = None,",
      "    cache_seqlens: Optional[torch.Tensor] = None,",
      "    conv_state_indices: Optional[torch.Tensor] = None,",
      "    pad_slot_id: int = PAD_SLOT_ID,",
      "    metadata=None,",
      "    validate_data=False,",
      "):",
      "    \"\"\"",
      "    x: (batch, dim) or (batch, dim, seqlen)",
      "        [shape=2: single token prediction]",
      "        [shape=3: single or multiple tokens prediction]",
      "    conv_state: (..., dim, state_len), where state_len >= width - 1",
      "    weight: (dim, width)",
      "    bias: (dim,)",
      "    cache_seqlens: (batch,), dtype int32.",
      "        If not None, the conv_state is treated as a circular buffer.",
      "        The conv_state will be updated by copying x to the conv_state",
      "        starting at the index",
      "        @cache_seqlens % state_len.",
      "    conv_state_indices: (batch,), dtype int32",
      "        If not None, the conv_state is a larger tensor along the batch dim,",
      "        and we are selecting the batch coords specified by conv_state_indices.",
      "        Useful for a continuous batching scenario.",
      "    pad_slot_id: int",
      "            if cache_indices is passed, lets the kernel identify padded",
      "            entries that will not be processed,",
      "            for example: cache_indices = [pad_slot_id, 1 ,20 ,pad_slot_id]",
      "            in this case, the kernel will not process entries at",
      "            indices 0 and 3",
      "    out: (batch, dim) or (batch, dim, seqlen)",
      "    \"\"\"",
      "    if validate_data:",
      "        assert cache_seqlens is None  # not implemented yet - ok for vLLM",
      "        assert pad_slot_id is not None",
      "        assert x.stride(1) == 1",
      "    if isinstance(activation, bool):",
      "        activation = \"silu\" if activation is True else None",
      "    elif activation is not None:",
      "        assert activation in [\"silu\", \"swish\"]",
      "    unsqueeze = x.dim() == 2",
      "    if unsqueeze:",
      "        # make it (batch, dim, seqlen) with seqlen == 1",
      "        x = x.unsqueeze(-1)",
      "    batch, dim, seqlen = x.shape",
      "    _, width = weight.shape",
      "    # conv_state: (..., dim, state_len), where state_len >= width - 1",
      "    num_cache_lines, _, state_len = conv_state.size()",
      "",
      "    if validate_data:",
      "        assert dim == weight.size(0)",
      "        assert conv_state.stride(",
      "            -2",
      "        ) == 1, f\"ERROR: expect contiguous along feat-dim of conv_state (currently stride={conv_state.stride()})\"",
      "        assert state_len >= width - 1",
      "        # when above happens, we don't shift-left to keep any records in conv_state",
      "        assert dim == conv_state.size(1)",
      "        if conv_state_indices is None:",
      "            assert conv_state.size(0) >= batch",
      "        else:",
      "            assert (batch, ) == conv_state_indices.shape",
      "",
      "        assert num_cache_lines >= batch",
      "        assert weight.stride(1) == 1  # Need this",
      "        assert cache_seqlens is None  # not needed for vLLM - circular buffer",
      "",
      "    # adopt the strategy in vLLM that overwrite on 'x' directly, rather than creating a new tensor 'o'",
      "    out = x",
      "    stride_w_dim, stride_w_width = weight.stride()",
      "",
      "    stride_x_seq, stride_x_dim, stride_x_token = x.stride(",
      "    )  # X (batch, dim, seqlen)",
      "",
      "    stride_o_seq, stride_o_dim, stride_o_token = out.stride()",
      "",
      "    stride_istate_seq, stride_istate_dim, stride_istate_token = conv_state.stride(",
      "    )",
      "    state_len = width - 1",
      "    np2_statelen = triton.next_power_of_2(state_len)",
      "",
      "    def grid(META):",
      "        return (",
      "            batch,",
      "            triton.cdiv(dim, META[\"BLOCK_N\"]),",
      "        )",
      "",
      "    _causal_conv1d_update_kernel[grid](",
      "        # Pointers to matrices",
      "        x,",
      "        weight,",
      "        bias,",
      "        conv_state,",
      "        cache_seqlens,",
      "        conv_state_indices,",
      "        out,",
      "        # Matrix dimensions",
      "        batch,",
      "        dim,",
      "        seqlen,",
      "        state_len,",
      "        num_cache_lines,",
      "        # stride",
      "        stride_x_seq,",
      "        stride_x_dim,",
      "        stride_x_token,",
      "        stride_w_dim,",
      "        stride_w_width,",
      "        stride_istate_seq,",
      "        stride_istate_dim,",
      "        stride_istate_token,",
      "        stride_o_seq,",
      "        stride_o_dim,",
      "        stride_o_token,",
      "        # others",
      "        pad_slot_id,",
      "        # META",
      "        HAS_BIAS=bias is not None,",
      "        KERNEL_WIDTH=width,",
      "        SILU_ACTIVATION=activation in [\"silu\", \"swish\"],",
      "        IS_CONTINUOUS_BATCHING=conv_state_indices is not None,",
      "        NP2_STATELEN=np2_statelen,",
      "        USE_PAD_SLOT=pad_slot_id is not None,",
      "        BLOCK_N=256,",
      "    )",
      "    if unsqueeze:",
      "        out = out.squeeze(-1)",
      "    return out"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/mamba/ops/layernorm_gated.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "# Copyright (c) 2024, Tri Dao.",
      "# Adapted from https://github.com/state-spaces/mamba/blob/60dadf2e0ee730ac337035d5533de10bc26e4847/mamba_ssm/ops/triton/layernorm_gated.py",
      "",
      "import torch",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "",
      "@triton.heuristics({\"HAS_BIAS\": lambda args: args[\"B\"] is not None})",
      "@triton.heuristics({\"HAS_Z\": lambda args: args[\"Z\"] is not None})",
      "@triton.jit",
      "def _layer_norm_fwd_1pass_kernel(",
      "    X,  # pointer to the input",
      "    Y,  # pointer to the output",
      "    W,  # pointer to the weights",
      "    B,  # pointer to the biases",
      "    Z,  # pointer to the other branch",
      "    Mean,  # pointer to the mean",
      "    Rstd,  # pointer to the 1/std",
      "    stride_x_row: tl.int64,",
      "    stride_y_row: tl.int64,",
      "    stride_z_row: tl.int64,",
      "    M: tl.int64,  # number of rows in X",
      "    N: tl.int64,  # number of columns in X",
      "    eps,  # epsilon to avoid division by zero",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    NORM_BEFORE_GATE: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "):",
      "    # Map the program id to the row of X and Y it should compute.",
      "    row = tl.program_id(0)",
      "    group = tl.program_id(1)",
      "    X += row * stride_x_row + group * N",
      "    Y += row * stride_y_row + group * N",
      "    if HAS_Z:",
      "        Z += row * stride_z_row + group * N",
      "    if not IS_RMS_NORM:",
      "        Mean += group * M",
      "    Rstd += group * M",
      "    W += group * N",
      "    if HAS_BIAS:",
      "        B += group * N",
      "    # Compute mean and variance",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)",
      "    if HAS_Z and not NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=cols < N).to(tl.float32)",
      "        x *= z * tl.sigmoid(z)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "    # Normalize and apply linear transformation",
      "    mask = cols < N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "    y = x_hat * w + b if HAS_BIAS else x_hat * w",
      "    if HAS_Z and NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=mask).to(tl.float32)",
      "        y *= z * tl.sigmoid(z)",
      "    # Write output",
      "    tl.store(Y + cols, y, mask=mask)",
      "",
      "",
      "def _layer_norm_fwd(x,",
      "                    weight,",
      "                    bias,",
      "                    eps,",
      "                    z=None,",
      "                    out=None,",
      "                    group_size=None,",
      "                    norm_before_gate=True,",
      "                    is_rms_norm=False):",
      "    M, N = x.shape",
      "    if group_size is None:",
      "        group_size = N",
      "    assert N % group_size == 0",
      "    ngroups = N // group_size",
      "    assert x.stride(-1) == 1",
      "    if z is not None:",
      "        assert z.stride(-1) == 1",
      "        assert z.shape == (M, N)",
      "    assert weight.shape == (N, )",
      "    assert weight.stride(-1) == 1",
      "    if bias is not None:",
      "        assert bias.stride(-1) == 1",
      "        assert bias.shape == (N, )",
      "    # allocate output",
      "    if out is not None:",
      "        assert out.shape == x.shape",
      "    else:",
      "        out = torch.empty_like(x)",
      "    assert out.stride(-1) == 1",
      "    mean = torch.empty((ngroups * M, ), dtype=torch.float32,",
      "                       device=x.device) if not is_rms_norm else None",
      "    rstd = torch.empty((ngroups * M, ), dtype=torch.float32, device=x.device)",
      "    # Less than 64KB per feature: enqueue fused kernel",
      "    MAX_FUSED_SIZE = 65536 // x.element_size()",
      "    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(group_size))",
      "    if group_size > BLOCK_N:",
      "        raise RuntimeError(",
      "            \"This layer norm doesn't support feature dim >= 64KB.\")",
      "    # heuristics for number of warps",
      "    num_warps = min(max(BLOCK_N // 256, 1), 8)",
      "    grid = (M, ngroups)",
      "    with torch.cuda.device(x.device.index):",
      "        _layer_norm_fwd_1pass_kernel[grid](x,",
      "                                           out,",
      "                                           weight,",
      "                                           bias,",
      "                                           z,",
      "                                           mean,",
      "                                           rstd,",
      "                                           x.stride(0),",
      "                                           out.stride(0),",
      "                                           z.stride(0) if z is not None else 0,",
      "                                           M,",
      "                                           group_size,",
      "                                           eps,",
      "                                           BLOCK_N=BLOCK_N,",
      "                                           NORM_BEFORE_GATE=norm_before_gate,",
      "                                           IS_RMS_NORM=is_rms_norm,",
      "                                           num_warps=num_warps)",
      "    return out, mean, rstd",
      "",
      "",
      "def rms_norm_gated(x,",
      "                   weight,",
      "                   bias,",
      "                   z=None,",
      "                   eps=1e-6,",
      "                   group_size=None,",
      "                   norm_before_gate=True):",
      "    x_shape_og = x.shape",
      "    # reshape input data into 2D tensor",
      "    x = x.reshape(-1, x.shape[-1])",
      "    if x.stride(-1) != 1:",
      "        x = x.contiguous()",
      "    if z is not None:",
      "        assert z.shape == x_shape_og",
      "        z = z.reshape(-1, z.shape[-1])",
      "        if z.stride(-1) != 1:",
      "            z = z.contiguous()",
      "    weight = weight.contiguous()",
      "    if bias is not None:",
      "        bias = bias.contiguous()",
      "    y, _, _ = _layer_norm_fwd(x,",
      "                              weight,",
      "                              bias,",
      "                              eps,",
      "                              z=z,",
      "                              group_size=group_size,",
      "                              norm_before_gate=norm_before_gate,",
      "                              is_rms_norm=True)",
      "",
      "    return y.reshape(x_shape_og)"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/fla/ops/op.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "# SPDX-FileCopyrightText: Songlin Yang, Yu Zhang",
      "#",
      "# This file contains code copied from the flash-linear-attention project.",
      "# The original source code was licensed under the MIT license and included",
      "# the following copyright notice:",
      "# Copyright (c) 2023-2025, Songlin Yang, Yu Zhang",
      "",
      "import os",
      "",
      "from vllm.triton_utils import tl, tldevice, triton",
      "",
      "if os.environ.get('FLA_USE_FAST_OPS', '0') == '1':",
      "    div = tldevice.fast_dividef",
      "    exp = tldevice.fast_expf",
      "    log = tldevice.fast_logf",
      "    log2 = tldevice.fast_log2f",
      "else:",
      "",
      "    @triton.jit",
      "    def div_normal(x, y):",
      "        return x / y",
      "",
      "    div = div_normal",
      "    exp = tl.exp",
      "    log = tl.log",
      "    log2 = tl.log2",
      "",
      "",
      "@triton.jit",
      "def safe_exp(x):",
      "    return exp(tl.where(x <= 0, x, float('-inf')))",
      "",
      "",
      "if not hasattr(tl, 'gather'):",
      "",
      "    @triton.jit",
      "    def gather(src, index, axis, _builder=None):",
      "        # This is a fallback implementation when tl.gather is not supported",
      "        # In order to pass triton compiler, there is no actual gather operation",
      "        return src",
      "else:",
      "    gather = tl.gather"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/fla/ops/solve_tril.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "# SPDX-FileCopyrightText: Songlin Yang, Yu Zhang",
      "#",
      "# This file contains code copied from the flash-linear-attention project.",
      "# The original source code was licensed under the MIT license and included",
      "# the following copyright notice:",
      "# Copyright (c) 2023-2025, Songlin Yang, Yu Zhang",
      "# ruff: noqa: E501",
      "from typing import Optional",
      "",
      "import torch",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "from .index import prepare_chunk_indices",
      "from .utils import input_guard",
      "",
      "",
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(",
      "    configs=[",
      "        triton.Config({}, num_warps=num_warps, num_stages=num_stages)",
      "        for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4, 5]",
      "    ],",
      "    key=['BT'],",
      ")",
      "@triton.jit(do_not_specialize=['T'])",
      "def solve_tril_16x16_kernel(",
      "    A,",
      "    Ad,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(",
      "            tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(",
      "            tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    A = A + (bos * H + i_h) * BT",
      "    Ad = Ad + (bos * H + i_h) * 16",
      "",
      "    offset = (i_t * 16) % BT",
      "    p_A = tl.make_block_ptr(A, (T, BT), (H * BT, 1), (i_t * 16, offset),",
      "                            (16, 16), (1, 0))",
      "    p_Ai = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 16, 0), (16, 16),",
      "                             (1, 0))",
      "    b_A = tl.load(p_A, boundary_check=(0, 1)).to(tl.float32)",
      "    b_A = -tl.where(",
      "        tl.arange(0, 16)[:, None] > tl.arange(0, 16)[None, :], b_A, 0)",
      "",
      "    o_i = tl.arange(0, 16)",
      "    for i in range(1, min(16, T - i_t * 16)):",
      "        b_a = -tl.load(A + (i_t * 16 + i) * H * BT + o_i + offset)",
      "        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0)",
      "        mask = o_i == i",
      "        b_A = tl.where(mask[:, None], b_a, b_A)",
      "    b_A += o_i[:, None] == o_i[None, :]",
      "    tl.store(p_Ai,",
      "             b_A.to(p_Ai.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "",
      "",
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(",
      "    configs=[",
      "        triton.Config({}, num_warps=num_warps, num_stages=num_stages)",
      "        for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4, 5]",
      "    ],",
      "    key=['H', 'BT', 'IS_VARLEN'],",
      ")",
      "@triton.jit(do_not_specialize=['T'])",
      "def merge_16x16_to_32x32_inverse_kernel(A, Ad, Ai, cu_seqlens, chunk_indices,",
      "                                        T, H: tl.constexpr, BT: tl.constexpr,",
      "                                        IS_VARLEN: tl.constexpr):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(",
      "            tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(",
      "            tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    A += (bos * H + i_h) * 32",
      "    Ad += (bos * H + i_h) * 16",
      "    Ai += (bos * H + i_h) * 32",
      "",
      "    p_A_21 = tl.make_block_ptr(A, (T, 32), (H * 32, 1), (i_t * 32 + 16, 0),",
      "                               (16, 16), (1, 0))",
      "    p_Ad_11 = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 32, 0),",
      "                                (16, 16), (1, 0))",
      "    p_Ad_22 = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 32 + 16, 0),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_11 = tl.make_block_ptr(Ai, (T, 32), (H * 32, 1), (i_t * 32, 0),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_22 = tl.make_block_ptr(Ai, (T, 32), (H * 32, 1), (i_t * 32 + 16, 16),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_21 = tl.make_block_ptr(Ai, (T, 32), (H * 32, 1), (i_t * 32 + 16, 0),",
      "                                (16, 16), (1, 0))",
      "",
      "    A_21 = tl.load(p_A_21, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_11 = tl.load(p_Ad_11, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_22 = tl.load(p_Ad_22, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_21 = -tl.dot(tl.dot(Ai_22, A_21, input_precision='ieee'),",
      "                    Ai_11,",
      "                    input_precision='ieee')",
      "    tl.store(p_Ai_11,",
      "             Ai_11.to(p_Ai_11.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_22,",
      "             Ai_22.to(p_Ai_22.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_21,",
      "             Ai_21.to(p_Ai_21.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "",
      "",
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(",
      "    configs=[",
      "        triton.Config({}, num_warps=num_warps, num_stages=num_stages)",
      "        for num_warps in [2, 4, 8] for num_stages in [2, 3, 4, 5]",
      "    ],",
      "    key=['H', 'BT', 'IS_VARLEN'],",
      ")",
      "@triton.jit(do_not_specialize=['T'])",
      "def merge_16x16_to_64x64_inverse_kernel(A, Ad, Ai, cu_seqlens, chunk_indices,",
      "                                        T, H: tl.constexpr, BT: tl.constexpr,",
      "                                        IS_VARLEN: tl.constexpr):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(",
      "            tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(",
      "            tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    A += (bos * H + i_h) * 64",
      "    Ad += (bos * H + i_h) * 16",
      "    Ai += (bos * H + i_h) * 64",
      "",
      "    p_A_21 = tl.make_block_ptr(A, (T, 64), (H * 64, 1), (i_t * 64 + 16, 0),",
      "                               (16, 16), (1, 0))",
      "    p_A_32 = tl.make_block_ptr(A, (T, 64), (H * 64, 1), (i_t * 64 + 32, 16),",
      "                               (16, 16), (1, 0))",
      "    p_A_31 = tl.make_block_ptr(A, (T, 64), (H * 64, 1), (i_t * 64 + 32, 0),",
      "                               (16, 16), (1, 0))",
      "    p_A_43 = tl.make_block_ptr(A, (T, 64), (H * 64, 1), (i_t * 64 + 48, 32),",
      "                               (16, 16), (1, 0))",
      "    p_A_42 = tl.make_block_ptr(A, (T, 64), (H * 64, 1), (i_t * 64 + 48, 16),",
      "                               (16, 16), (1, 0))",
      "    p_A_41 = tl.make_block_ptr(A, (T, 64), (H * 64, 1), (i_t * 64 + 48, 0),",
      "                               (16, 16), (1, 0))",
      "    p_Ad_11 = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 64, 0),",
      "                                (16, 16), (1, 0))",
      "    p_Ad_22 = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 64 + 16, 0),",
      "                                (16, 16), (1, 0))",
      "    p_Ad_33 = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 64 + 32, 0),",
      "                                (16, 16), (1, 0))",
      "    p_Ad_44 = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 64 + 48, 0),",
      "                                (16, 16), (1, 0))",
      "",
      "    A_21 = tl.load(p_A_21, boundary_check=(0, 1)).to(tl.float32)",
      "    A_32 = tl.load(p_A_32, boundary_check=(0, 1)).to(tl.float32)",
      "    A_31 = tl.load(p_A_31, boundary_check=(0, 1)).to(tl.float32)",
      "    A_43 = tl.load(p_A_43, boundary_check=(0, 1)).to(tl.float32)",
      "    A_42 = tl.load(p_A_42, boundary_check=(0, 1)).to(tl.float32)",
      "    A_41 = tl.load(p_A_41, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    Ai_11 = tl.load(p_Ad_11, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_22 = tl.load(p_Ad_22, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_33 = tl.load(p_Ad_33, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_44 = tl.load(p_Ad_44, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    Ai_21 = -tl.dot(tl.dot(Ai_22, A_21, input_precision='ieee'),",
      "                    Ai_11,",
      "                    input_precision='ieee')",
      "    Ai_32 = -tl.dot(tl.dot(Ai_33, A_32, input_precision='ieee'),",
      "                    Ai_22,",
      "                    input_precision='ieee')",
      "    Ai_43 = -tl.dot(tl.dot(Ai_44, A_43, input_precision='ieee'),",
      "                    Ai_33,",
      "                    input_precision='ieee')",
      "",
      "    Ai_31 = -tl.dot(Ai_33,",
      "                    tl.dot(A_31, Ai_11, input_precision='ieee') +",
      "                    tl.dot(A_32, Ai_21, input_precision='ieee'),",
      "                    input_precision='ieee')",
      "    Ai_42 = -tl.dot(Ai_44,",
      "                    tl.dot(A_42, Ai_22, input_precision='ieee') +",
      "                    tl.dot(A_43, Ai_32, input_precision='ieee'),",
      "                    input_precision='ieee')",
      "    Ai_41 = -tl.dot(Ai_44,",
      "                    tl.dot(A_41, Ai_11, input_precision='ieee') +",
      "                    tl.dot(A_42, Ai_21, input_precision='ieee') +",
      "                    tl.dot(A_43, Ai_31, input_precision='ieee'),",
      "                    input_precision='ieee')",
      "",
      "    p_Ai_11 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64, 0),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_22 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 16, 16),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_33 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 32),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_44 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 48),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_21 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 16, 0),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_31 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 0),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_32 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 16),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_41 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 0),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_42 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 16),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_43 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 32),",
      "                                (16, 16), (1, 0))",
      "    tl.store(p_Ai_11,",
      "             Ai_11.to(p_Ai_11.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_22,",
      "             Ai_22.to(p_Ai_22.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_33,",
      "             Ai_33.to(p_Ai_33.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_44,",
      "             Ai_44.to(p_Ai_44.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_21,",
      "             Ai_21.to(p_Ai_21.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_31,",
      "             Ai_31.to(p_Ai_31.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_32,",
      "             Ai_32.to(p_Ai_32.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_41,",
      "             Ai_41.to(p_Ai_41.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_42,",
      "             Ai_42.to(p_Ai_42.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_43,",
      "             Ai_43.to(p_Ai_43.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "",
      "    fill_zeros = tl.zeros((16, 16), dtype=tl.float32)",
      "    p_Ai_12 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64, 16),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_13 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64, 32),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_14 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64, 48),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_23 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 16, 32),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_24 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 16, 48),",
      "                                (16, 16), (1, 0))",
      "    p_Ai_34 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 48),",
      "                                (16, 16), (1, 0))",
      "    tl.store(p_Ai_12,",
      "             fill_zeros.to(p_Ai_12.dtype.element_ty,",
      "                           fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_13,",
      "             fill_zeros.to(p_Ai_13.dtype.element_ty,",
      "                           fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_14,",
      "             fill_zeros.to(p_Ai_14.dtype.element_ty,",
      "                           fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_23,",
      "             fill_zeros.to(p_Ai_23.dtype.element_ty,",
      "                           fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_24,",
      "             fill_zeros.to(p_Ai_24.dtype.element_ty,",
      "                           fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "    tl.store(p_Ai_34,",
      "             fill_zeros.to(p_Ai_34.dtype.element_ty,",
      "                           fp_downcast_rounding=\"rtne\"),",
      "             boundary_check=(0, 1))",
      "",
      "",
      "@input_guard",
      "def solve_tril(A: torch.Tensor,",
      "               cu_seqlens: Optional[torch.Tensor] = None,",
      "               output_dtype: torch.dtype = torch.float) -> torch.Tensor:",
      "    \"\"\"",
      "    Compute the inverse of the lower triangular matrix",
      "    A should be strictly lower triangular, i.e., A.triu() == 0.",
      "",
      "    Args:",
      "        A (torch.Tensor):",
      "            [B, T, H, K]",
      "        cu_seqlens (torch.Tensor):",
      "            The cumulative sequence lengths of the input tensor.",
      "            Default: None.",
      "        output_dtype (torch.dtype):",
      "            The dtype of the output tensor. Default: `torch.float`",
      "",
      "    Returns:",
      "        (I + A)^-1 with the same shape as A",
      "    \"\"\"",
      "    assert A.shape[-1] in [16, 32, 64]",
      "",
      "    B, T, H, BT = A.shape",
      "    Ad = torch.empty(B,",
      "                     T,",
      "                     H,",
      "                     16,",
      "                     device=A.device,",
      "                     dtype=torch.float if BT != 16 else output_dtype)",
      "",
      "    chunk_indices = prepare_chunk_indices(",
      "        cu_seqlens, 16) if cu_seqlens is not None else None",
      "    NT = len(chunk_indices) if cu_seqlens is not None else triton.cdiv(T, 16)",
      "    solve_tril_16x16_kernel[NT, B * H](",
      "        A=A,",
      "        Ad=Ad,",
      "        cu_seqlens=cu_seqlens,",
      "        chunk_indices=chunk_indices,",
      "        T=T,",
      "        H=H,",
      "        BT=BT,",
      "    )",
      "    if BT == 16:",
      "        return Ad",
      "",
      "    Ai = torch.empty(B, T, H, BT, device=A.device, dtype=output_dtype)",
      "    merge_fn = merge_16x16_to_32x32_inverse_kernel if BT == 32 else merge_16x16_to_64x64_inverse_kernel",
      "    chunk_indices = prepare_chunk_indices(",
      "        cu_seqlens, BT) if cu_seqlens is not None else None",
      "    NT = len(chunk_indices) if cu_seqlens is not None else triton.cdiv(T, BT)",
      "    merge_fn[NT, B * H](",
      "        A=A,",
      "        Ad=Ad,",
      "        Ai=Ai,",
      "        cu_seqlens=cu_seqlens,",
      "        chunk_indices=chunk_indices,",
      "        T=T,",
      "        H=H,",
      "        BT=BT,",
      "    )",
      "    return Ai"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "# SPDX-FileCopyrightText: Songlin Yang, Yu Zhang",
      "#",
      "# This file contains code copied from the flash-linear-attention project.",
      "# The original source code was licensed under the MIT license and included",
      "# the following copyright notice:",
      "# Copyright (c) 2023-2025, Songlin Yang, Yu Zhang",
      "# ruff: noqa: E501",
      "from typing import Optional",
      "",
      "import torch",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "from .index import prepare_chunk_indices",
      "from .op import safe_exp",
      "",
      "",
      "@triton.heuristics({",
      "    'IS_VARLEN': lambda args: args['cu_seqlens'] is not None,",
      "    'USE_G': lambda args: args['g_cumsum'] is not None",
      "})",
      "@triton.autotune(",
      "    configs=[",
      "        triton.Config({'BK': BK}, num_warps=num_warps, num_stages=num_stages)",
      "        for BK in [32, 64, 128] for num_warps in [2, 4, 8]",
      "        for num_stages in [2, 3, 4]",
      "    ],",
      "    key=['H', 'K', 'BT', 'IS_VARLEN'],",
      ")",
      "@triton.jit(do_not_specialize=['T'])",
      "def chunk_scaled_dot_kkt_fwd_kernel(",
      "    k,",
      "    beta,",
      "    g_cumsum,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    Hg: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(",
      "            tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(",
      "            tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    o_t = tl.arange(0, BT)",
      "",
      "    p_beta = tl.make_block_ptr(beta + bos * H + i_h, (T, ), (H, ),",
      "                               (i_t * BT, ), (BT, ), (0, ))",
      "    b_beta = tl.load(p_beta, boundary_check=(0, ))",
      "",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(k + (bos * Hg + i_h // (H // Hg)) * K, (T, K),",
      "                                (Hg * K, 1), (i_t * BT, i_k * BK), (BT, BK),",
      "                                (1, 0))",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_kb = b_k * b_beta[:, None]",
      "        b_A += tl.dot(b_kb.to(b_k.dtype), tl.trans(b_k))",
      "",
      "    if USE_G:",
      "        p_g = tl.make_block_ptr(g_cumsum + bos * H + i_h, (T, ), (H, ),",
      "                                (i_t * BT, ), (BT, ), (0, ))",
      "        b_g = tl.load(p_g, boundary_check=(0, ))",
      "        b_g_diff = b_g[:, None] - b_g[None, :]",
      "        b_A = b_A * safe_exp(b_g_diff)",
      "",
      "    b_A = tl.where(o_t[:, None] > o_t[None, :], b_A, 0)",
      "    p_A = tl.make_block_ptr(A + (bos * H + i_h) * BT, (T, BT), (BT * H, 1),",
      "                            (i_t * BT, 0), (BT, BT), (1, 0))",
      "    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "",
      "def chunk_scaled_dot_kkt_fwd(",
      "        k: torch.Tensor,",
      "        beta: torch.Tensor,",
      "        g_cumsum: Optional[torch.Tensor] = None,",
      "        cu_seqlens: Optional[torch.LongTensor] = None,",
      "        chunk_size: int = 64,",
      "        output_dtype: torch.dtype = torch.float32) -> torch.Tensor:",
      "    r\"\"\"",
      "    Compute beta * K * K^T.",
      "",
      "    Args:",
      "        k (torch.Tensor):",
      "            The key tensor of shape `[B, T, H, K]`.",
      "        beta (torch.Tensor):",
      "            The beta tensor of shape `[B, T, H]`.",
      "        g_cumsum (torch.Tensor):",
      "            The cumulative sum of the gate tensor of shape `[B, T, H]`.",
      "            Default: None",
      "        cu_seqlens (torch.LongTensor):",
      "            The cumulative sequence lengths of the input tensor.",
      "            Default: None",
      "        chunk_size (int):",
      "            The chunk size. Default: 64.",
      "        output_dtype (torch.dtype):",
      "            The dtype of the output tensor. Default: `torch.float32`",
      "",
      "    Returns:",
      "        beta * K * K^T of shape `[B, T, H, BT]` where `BT` is the chunk size.",
      "    \"\"\"",
      "",
      "    B, T, Hg, K = k.shape",
      "",
      "    H = beta.shape[-1]",
      "    BT = chunk_size",
      "    chunk_indices = prepare_chunk_indices(",
      "        cu_seqlens, BT) if cu_seqlens is not None else None",
      "    NT = triton.cdiv(T, BT) if cu_seqlens is None else len(chunk_indices)",
      "    A = torch.empty(B, T, H, BT, device=k.device, dtype=output_dtype)",
      "    chunk_scaled_dot_kkt_fwd_kernel[(NT, B * H)](",
      "        k=k,",
      "        beta=beta,",
      "        g_cumsum=g_cumsum,",
      "        A=A,",
      "        cu_seqlens=cu_seqlens,",
      "        chunk_indices=chunk_indices,",
      "        T=T,",
      "        H=H,",
      "        Hg=Hg,",
      "        K=K,",
      "        BT=BT,",
      "    )",
      "    return A"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/fla/ops/fused_recurrent.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "# SPDX-FileCopyrightText: Songlin Yang, Yu Zhang",
      "#",
      "# This file contains code copied from the flash-linear-attention project.",
      "# The original source code was licensed under the MIT license and included",
      "# the following copyright notice:",
      "# Copyright (c) 2023-2025, Songlin Yang, Yu Zhang",
      "# ruff: noqa: E501",
      "from typing import Optional",
      "",
      "import torch",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "from .op import exp",
      "",
      "",
      "@triton.heuristics({",
      "    'USE_INITIAL_STATE':",
      "    lambda args: args['h0'] is not None,",
      "    'IS_VARLEN':",
      "    lambda args: args['cu_seqlens'] is not None,",
      "    \"IS_CONTINUOUS_BATCHING\":",
      "    lambda args: args['ssm_state_indices'] is not None,",
      "    \"IS_SPEC_DECODING\":",
      "    lambda args: args['num_accepted_tokens'] is not None,",
      "})",
      "@triton.jit(do_not_specialize=['N', 'T'])",
      "def fused_recurrent_gated_delta_rule_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    beta,",
      "    o,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    ssm_state_indices,",
      "    num_accepted_tokens,",
      "    scale,",
      "    N: tl.constexpr,  # num of sequences",
      "    T: tl.constexpr,  # num of tokens",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HV: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    stride_init_state_token: tl.constexpr,",
      "    stride_final_state_token: tl.constexpr,",
      "    stride_indices_seq: tl.constexpr,",
      "    stride_indices_tok: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,  # whether to use initial state",
      "    INPLACE_FINAL_STATE: tl.constexpr,  # whether to store final state inplace",
      "    IS_BETA_HEADWISE: tl.",
      "    constexpr,  # whether beta is headwise vector or scalar,",
      "    USE_QK_L2NORM_IN_KERNEL: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    IS_CONTINUOUS_BATCHING: tl.constexpr,",
      "    IS_SPEC_DECODING: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_hv = i_nh // HV, i_nh % HV",
      "    i_h = i_hv // (HV // H)",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(",
      "            tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "",
      "    if T == 0:",
      "        # no tokens to process for this sequence",
      "        return",
      "",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "",
      "    p_q = q + (bos * H + i_h) * K + o_k",
      "    p_k = k + (bos * H + i_h) * K + o_k",
      "    p_v = v + (bos * HV + i_hv) * V + o_v",
      "    if IS_BETA_HEADWISE:",
      "        p_beta = beta + (bos * HV + i_hv) * V + o_v",
      "    else:",
      "        p_beta = beta + bos * HV + i_hv",
      "    p_g = g + bos * HV + i_hv",
      "    p_o = o + ((i_k * all + bos) * HV + i_hv) * V + o_v",
      "",
      "    mask_k = o_k < K",
      "    mask_v = o_v < V",
      "    mask_h = mask_k[:, None] & mask_v[None, :]",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        if IS_CONTINUOUS_BATCHING:",
      "            if IS_SPEC_DECODING:",
      "                i_t = tl.load(num_accepted_tokens + i_n).to(tl.int64) - 1",
      "            else:",
      "                i_t = 0",
      "            p_h0 = h0 + tl.load(ssm_state_indices + i_n * stride_indices_seq +",
      "                                i_t).to(tl.int64) * stride_init_state_token",
      "        else:",
      "            p_h0 = h0 + bos * HV * K * V",
      "        p_h0 = p_h0 + i_hv * K * V + o_k[:, None] * V + o_v[None, :]",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for i_t in range(0, T):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32)",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_g = tl.load(p_g).to(tl.float32)",
      "",
      "        if USE_QK_L2NORM_IN_KERNEL:",
      "            b_q = b_q / (tl.sqrt(tl.sum(b_q * b_q)) + 1e-6)",
      "            b_k = b_k / (tl.sqrt(tl.sum(b_k * b_k)) + 1e-6)",
      "        b_q = b_q * scale",
      "        # [BK, BV]",
      "        b_h *= exp(b_g)",
      "        # [BV]",
      "        b_v -= tl.sum(b_h * b_k[:, None], 0)",
      "        if IS_BETA_HEADWISE:",
      "            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)",
      "        else:",
      "            b_beta = tl.load(p_beta).to(tl.float32)",
      "        b_v *= b_beta",
      "        # [BK, BV]",
      "        b_h += b_k[:, None] * b_v[None, :]",
      "        # [BV]",
      "        b_o = tl.sum(b_h * b_q[:, None], 0)",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "",
      "        # keep the states for multi-query tokens",
      "        if INPLACE_FINAL_STATE:",
      "            p_ht = ht + tl.load(ssm_state_indices + i_n * stride_indices_seq +",
      "                                i_t).to(tl.int64) * stride_final_state_token",
      "        else:",
      "            p_ht = ht + (bos + i_t) * stride_final_state_token",
      "        p_ht = p_ht + i_hv * K * V + o_k[:, None] * V + o_v[None, :]",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)",
      "",
      "        p_q += H * K",
      "        p_k += H * K",
      "        p_o += HV * V",
      "        p_v += HV * V",
      "        p_g += HV",
      "        p_beta += HV * (V if IS_BETA_HEADWISE else 1)",
      "",
      "",
      "def fused_recurrent_gated_delta_rule_fwd(",
      "    q: torch.Tensor,",
      "    k: torch.Tensor,",
      "    v: torch.Tensor,",
      "    g: torch.Tensor,",
      "    beta: torch.Tensor,",
      "    scale: float,",
      "    initial_state: torch.Tensor,",
      "    inplace_final_state: bool = True,",
      "    cu_seqlens: Optional[torch.LongTensor] = None,",
      "    ssm_state_indices: Optional[torch.Tensor] = None,",
      "    num_accepted_tokens: Optional[torch.Tensor] = None,",
      "    use_qk_l2norm_in_kernel: bool = False,",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    B, T, H, K, V = *k.shape, v.shape[-1]",
      "    HV = v.shape[2]",
      "    N = B if cu_seqlens is None else len(cu_seqlens) - 1",
      "    BK, BV = triton.next_power_of_2(K), min(triton.next_power_of_2(V), 8)",
      "    NK, NV = triton.cdiv(K, BK), triton.cdiv(V, BV)",
      "    assert NK == 1, \"NK > 1 is not supported yet\"",
      "    num_stages = 3",
      "    num_warps = 1",
      "",
      "    o = q.new_empty(NK, *v.shape)",
      "    if inplace_final_state:",
      "        final_state = initial_state",
      "    else:",
      "        final_state = q.new_empty(T, HV, K, V, dtype=initial_state.dtype)",
      "",
      "    stride_init_state_token = initial_state.stride(0)",
      "    stride_final_state_token = final_state.stride(0)",
      "",
      "    if ssm_state_indices is None:",
      "        stride_indices_seq, stride_indices_tok = 1, 1",
      "    elif ssm_state_indices.ndim == 1:",
      "        stride_indices_seq, stride_indices_tok = ssm_state_indices.stride(0), 1",
      "    else:",
      "        stride_indices_seq, stride_indices_tok = ssm_state_indices.stride()",
      "",
      "    grid = (NK, NV, N * HV)",
      "    fused_recurrent_gated_delta_rule_fwd_kernel[grid](",
      "        q=q,",
      "        k=k,",
      "        v=v,",
      "        g=g,",
      "        beta=beta,",
      "        o=o,",
      "        h0=initial_state,",
      "        ht=final_state,",
      "        cu_seqlens=cu_seqlens,",
      "        ssm_state_indices=ssm_state_indices,",
      "        num_accepted_tokens=num_accepted_tokens,",
      "        scale=scale,",
      "        N=N,",
      "        T=T,",
      "        B=B,",
      "        H=H,",
      "        HV=HV,",
      "        K=K,",
      "        V=V,",
      "        BK=BK,",
      "        BV=BV,",
      "        stride_init_state_token=stride_init_state_token,",
      "        stride_final_state_token=stride_final_state_token,",
      "        stride_indices_seq=stride_indices_seq,",
      "        stride_indices_tok=stride_indices_tok,",
      "        IS_BETA_HEADWISE=beta.ndim == v.ndim,",
      "        USE_QK_L2NORM_IN_KERNEL=use_qk_l2norm_in_kernel,",
      "        INPLACE_FINAL_STATE=inplace_final_state,",
      "        num_warps=num_warps,",
      "        num_stages=num_stages,",
      "    )",
      "    o = o.squeeze(0)",
      "    return o, final_state",
      "",
      "",
      "class FusedRecurrentFunction(torch.autograd.Function):",
      "",
      "    @staticmethod",
      "    def forward(ctx,",
      "                q: torch.Tensor,",
      "                k: torch.Tensor,",
      "                v: torch.Tensor,",
      "                g: torch.Tensor,",
      "                beta: torch.Tensor,",
      "                scale: float,",
      "                initial_state: torch.Tensor,",
      "                inplace_final_state: bool = True,",
      "                cu_seqlens: Optional[torch.LongTensor] = None,",
      "                ssm_state_indices: Optional[torch.Tensor] = None,",
      "                num_accepted_tokens: Optional[torch.Tensor] = None,",
      "                use_qk_l2norm_in_kernel: bool = False):",
      "        o, final_state = fused_recurrent_gated_delta_rule_fwd(",
      "            q=q.contiguous(),",
      "            k=k.contiguous(),",
      "            v=v.contiguous(),",
      "            g=g.contiguous(),",
      "            beta=beta.contiguous(),",
      "            scale=scale,",
      "            initial_state=initial_state,",
      "            inplace_final_state=inplace_final_state,",
      "            cu_seqlens=cu_seqlens,",
      "            ssm_state_indices=ssm_state_indices,",
      "            num_accepted_tokens=num_accepted_tokens,",
      "            use_qk_l2norm_in_kernel=use_qk_l2norm_in_kernel,",
      "        )",
      "",
      "        return o, final_state",
      "",
      "",
      "def fused_recurrent_gated_delta_rule(",
      "    q: torch.Tensor,",
      "    k: torch.Tensor,",
      "    v: torch.Tensor,",
      "    g: torch.Tensor,",
      "    beta: torch.Tensor = None,",
      "    scale: float = None,",
      "    initial_state: torch.Tensor = None,",
      "    inplace_final_state: bool = True,",
      "    cu_seqlens: Optional[torch.LongTensor] = None,",
      "    ssm_state_indices: Optional[torch.Tensor] = None,",
      "    num_accepted_tokens: Optional[torch.Tensor] = None,",
      "    use_qk_l2norm_in_kernel: bool = False,",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    r\"\"\"",
      "    Args:",
      "        q (torch.Tensor):",
      "            queries of shape `[B, T, H, K]`.",
      "        k (torch.Tensor):",
      "            keys of shape `[B, T, H, K]`.",
      "        v (torch.Tensor):",
      "            values of shape `[B, T, HV, V]`.",
      "            GVA is applied if `HV > H`.",
      "        g (torch.Tensor):",
      "            g (decays) of shape `[B, T, HV]`.",
      "        beta (torch.Tensor):",
      "            betas of shape `[B, T, HV]`.",
      "        scale (Optional[int]):",
      "            Scale factor for the RetNet attention scores.",
      "            If not provided, it will default to `1 / sqrt(K)`. Default: `None`.",
      "        initial_state (Optional[torch.Tensor]):",
      "            Initial state of shape `[N, HV, K, V]` for `N` input sequences.",
      "            For equal-length input sequences, `N` equals the batch size `B`.",
      "            Default: `None`.",
      "        inplace_final_state: bool:",
      "            Whether to store the final state in-place to save memory.",
      "            Default: `True`.",
      "        cu_seqlens (torch.LongTensor):",
      "            Cumulative sequence lengths of shape `[N+1]` used for variable-length training,",
      "            consistent with the FlashAttention API.",
      "        ssm_state_indices (Optional[torch.Tensor]):",
      "            Indices to map the input sequences to the initial/final states.",
      "        num_accepted_tokens (Optional[torch.Tensor]):",
      "            Number of accepted tokens for each sequence during decoding.",
      "",
      "    Returns:",
      "        o (torch.Tensor):",
      "            Outputs of shape `[B, T, HV, V]`.",
      "        final_state (torch.Tensor):",
      "            Final state of shape `[N, HV, K, V]`.",
      "",
      "    Examples::",
      "        >>> import torch",
      "        >>> import torch.nn.functional as F",
      "        >>> from einops import rearrange",
      "        >>> from fla.ops.gated_delta_rule import fused_recurrent_gated_delta_rule",
      "        # inputs with equal lengths",
      "        >>> B, T, H, HV, K, V = 4, 2048, 4, 8, 512, 512",
      "        >>> q = torch.randn(B, T, H, K, device='cuda')",
      "        >>> k = F.normalize(torch.randn(B, T, H, K, device='cuda'), p=2, dim=-1)",
      "        >>> v = torch.randn(B, T, HV, V, device='cuda')",
      "        >>> g = F.logsigmoid(torch.rand(B, T, HV, device='cuda'))",
      "        >>> beta = torch.rand(B, T, HV, device='cuda').sigmoid()",
      "        >>> h0 = torch.randn(B, HV, K, V, device='cuda')",
      "        >>> o, ht = fused_gated_recurrent_delta_rule(",
      "            q, k, v, g, beta,",
      "            initial_state=h0,",
      "        )",
      "        # for variable-length inputs, the batch size `B` is expected to be 1 and `cu_seqlens` is required",
      "        >>> q, k, v, g, beta = map(lambda x: rearrange(x, 'b t ... -> 1 (b t) ...'), (q, k, v, g, beta))",
      "        # for a batch with 4 sequences, `cu_seqlens` with 5 start/end positions are expected",
      "        >>> cu_seqlens = q.new_tensor([0, 2048, 4096, 6144, 8192], dtype=torch.long)",
      "        >>> o_var, ht_var = fused_gated_recurrent_delta_rule(",
      "            q, k, v, g, beta,",
      "            initial_state=h0,",
      "            cu_seqlens=cu_seqlens",
      "        )",
      "    \"\"\"",
      "    if cu_seqlens is not None and q.shape[0] != 1:",
      "        raise ValueError(",
      "            f\"The batch size is expected to be 1 rather than {q.shape[0]} when using `cu_seqlens`.\"",
      "            f\"Please flatten variable-length inputs before processing.\")",
      "    if scale is None:",
      "        scale = k.shape[-1]**-0.5",
      "    else:",
      "        assert scale > 0, \"scale must be positive\"",
      "    if beta is None:",
      "        beta = torch.ones_like(q[..., 0])",
      "    o, final_state = FusedRecurrentFunction.apply(",
      "        q,",
      "        k,",
      "        v,",
      "        g,",
      "        beta,",
      "        scale,",
      "        initial_state,",
      "        inplace_final_state,",
      "        cu_seqlens,",
      "        ssm_state_indices,",
      "        num_accepted_tokens,",
      "        use_qk_l2norm_in_kernel,",
      "    )",
      "    return o, final_state"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/fla/ops/l2norm.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "# SPDX-FileCopyrightText: Songlin Yang, Yu Zhang",
      "#",
      "# This file contains code copied from the flash-linear-attention project.",
      "# The original source code was licensed under the MIT license and included",
      "# the following copyright notice:",
      "# Copyright (c) 2023-2025, Songlin Yang, Yu Zhang",
      "",
      "import os",
      "from typing import Optional",
      "",
      "import torch",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "BT_LIST = [8, 16, 32, 64, 128]",
      "",
      "USE_DEFAULT_FLA_NORM = int(os.getenv(\"USE_DEFAULT_FLA_NORM\", \"0\"))",
      "",
      "",
      "@triton.autotune(configs=[",
      "    triton.Config({}, num_warps=num_warps)",
      "    for num_warps in [1, 2, 4, 8, 16, 32]",
      "],",
      "                 key=['D'])",
      "@triton.jit",
      "def l2norm_fwd_kernel1(",
      "    x,",
      "    y,",
      "    D,",
      "    BD: tl.constexpr,",
      "    eps,",
      "):",
      "    i_t = tl.program_id(0)",
      "    x += i_t * D",
      "    y += i_t * D",
      "    # Compute mean and variance",
      "    cols = tl.arange(0, BD)",
      "    mask = cols < D",
      "    b_x = tl.load(x + cols, mask=mask, other=0.0).to(tl.float32)",
      "    b_var = tl.sum(b_x * b_x, axis=0)",
      "    b_rstd = 1 / tl.sqrt(b_var + eps)",
      "    # tl.store(Rstd + i_t, rstd)",
      "    # Normalize and apply linear transformation",
      "    b_y = b_x * b_rstd",
      "    tl.store(y + cols, b_y, mask=mask)",
      "",
      "",
      "@triton.autotune(configs=[",
      "    triton.Config({'BT': BT}, num_warps=num_warps)",
      "    for num_warps in [1, 2, 4, 8, 16] for BT in BT_LIST",
      "],",
      "                 key=['D'])",
      "@triton.jit(do_not_specialize=[\"NB\"])",
      "def l2norm_fwd_kernel(",
      "    x,",
      "    y,",
      "    eps,",
      "    NB,",
      "    T,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "):",
      "    i_t = tl.program_id(0)",
      "    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)",
      "    b_var = tl.sum(b_x * b_x, axis=1)",
      "    b_y = b_x / tl.sqrt(b_var + eps)[:, None]",
      "    p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "",
      "@triton.jit",
      "def l2norm_fwd_kernel2(X, Y, eps, M, N: tl.constexpr, MBLOCK: tl.constexpr):",
      "    xoffset = tl.program_id(0) * MBLOCK",
      "    row_idx = xoffset + tl.arange(0, MBLOCK)[:, None]",
      "    xmask = row_idx < M",
      "    rindex = tl.arange(0, N)[None, :]",
      "    xs = tl.load(X + (rindex + N * row_idx), None).to(tl.float32)",
      "    square = tl.broadcast_to(xs * xs, [MBLOCK, N])",
      "    square_sum = tl.sum(tl.where(xmask, square, 0), 1)[:, None]",
      "    rsqrt = tl.rsqrt(square_sum + eps)",
      "    tl.store(Y + (rindex + N * row_idx), xs * rsqrt, xmask)",
      "",
      "",
      "def l2norm_fwd(x: torch.Tensor,",
      "               eps: float = 1e-6,",
      "               output_dtype: Optional[torch.dtype] = None):",
      "    x_shape_og = x.shape",
      "    x = x.view(-1, x.shape[-1])",
      "    # allocate output",
      "    if output_dtype is None:",
      "        y = torch.empty_like(x)",
      "    else:",
      "        y = torch.empty_like(x, dtype=output_dtype)",
      "    assert y.stride(-1) == 1",
      "    T, D = x.shape[0], x.shape[-1]",
      "    # rstd = torch.empty((T,), dtype=torch.float32, device=x.device)",
      "    # Less than 64KB per feature: enqueue fused kernel",
      "    MAX_FUSED_SIZE = 65536 // x.element_size()",
      "    BD = min(MAX_FUSED_SIZE, triton.next_power_of_2(D))",
      "    if D > BD:",
      "        raise RuntimeError(\"This layer doesn't support feature dim >= 64KB.\")",
      "",
      "    if not USE_DEFAULT_FLA_NORM:",
      "        MBLOCK = 32",
      "        # M, N = x.shape",
      "        l2norm_fwd_kernel2[(triton.cdiv(T, MBLOCK), )](",
      "            x,",
      "            y,",
      "            eps,",
      "            T,",
      "            D,",
      "            MBLOCK,",
      "        )",
      "    else:",
      "        if D <= 512:",
      "            NB = triton.cdiv(T, 2048)",
      "",
      "            def grid(meta):",
      "                return (triton.cdiv(T, meta['BT']), )",
      "",
      "            l2norm_fwd_kernel[grid](",
      "                x,",
      "                y,",
      "                eps,",
      "                NB=NB,",
      "                T=T,",
      "                D=D,",
      "                BD=BD,",
      "            )",
      "        else:",
      "            l2norm_fwd_kernel1[(T, )](",
      "                x,",
      "                y,",
      "                eps=eps,",
      "                D=D,",
      "                BD=BD,",
      "            )",
      "",
      "    return y.view(x_shape_og)"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/fla/ops/chunk_delta_h.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "# SPDX-FileCopyrightText: Songlin Yang, Yu Zhang",
      "#",
      "# This file contains code copied from the flash-linear-attention project.",
      "# The original source code was licensed under the MIT license and included",
      "# the following copyright notice:",
      "# Copyright (c) 2023-2025, Songlin Yang, Yu Zhang",
      "# ruff: noqa: E501",
      "from typing import Optional",
      "",
      "import torch",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "from .index import prepare_chunk_indices, prepare_chunk_offsets",
      "from .op import exp, safe_exp",
      "from .utils import is_nvidia_hopper, use_cuda_graph",
      "",
      "NUM_WARPS = [2, 4] if is_nvidia_hopper else [2, 4, 8, 16]",
      "",
      "",
      "@triton.heuristics({",
      "    'USE_G': lambda args: args['g'] is not None,",
      "    'USE_INITIAL_STATE': lambda args: args['h0'] is not None,",
      "    'STORE_FINAL_STATE': lambda args: args['ht'] is not None,",
      "    'SAVE_NEW_VALUE': lambda args: args['v_new'] is not None,",
      "    'IS_VARLEN': lambda args: args['cu_seqlens'] is not None,",
      "})",
      "@triton.autotune(",
      "    configs=[",
      "        triton.Config({'BV': BV}, num_warps=num_warps, num_stages=num_stages)",
      "        for num_warps in [2, 4] for num_stages in [2, 3, 4] for BV in [32, 64]",
      "    ],",
      "    key=['H', 'K', 'V', 'BT', 'USE_G'],",
      "    use_cuda_graph=use_cuda_graph,",
      ")",
      "@triton.jit(do_not_specialize=['T'])",
      "def chunk_gated_delta_rule_fwd_kernel_h_blockdim64(",
      "    k,",
      "    v,",
      "    w,",
      "    v_new,",
      "    g,",
      "    h,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    Hg: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    SAVE_NEW_VALUE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(",
      "            tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    # [BK, BV]",
      "    b_h1 = tl.zeros([64, BV], dtype=tl.float32)",
      "    if K > 64:",
      "        b_h2 = tl.zeros([64, BV], dtype=tl.float32)",
      "    if K > 128:",
      "        b_h3 = tl.zeros([64, BV], dtype=tl.float32)",
      "    if K > 192:",
      "        b_h4 = tl.zeros([64, BV], dtype=tl.float32)",
      "",
      "    # calculate offset",
      "    h += (boh * H + i_h) * K * V",
      "    v += (bos * H + i_h) * V",
      "    k += (bos * Hg + i_h // (H // Hg)) * K",
      "    w += (bos * H + i_h) * K",
      "    if SAVE_NEW_VALUE:",
      "        v_new += (bos * H + i_h) * V",
      "    stride_v = H * V",
      "    stride_h = H * K * V",
      "    stride_k = Hg * K",
      "    stride_w = H * K",
      "    if USE_INITIAL_STATE:",
      "        h0 = h0 + i_nh * K * V",
      "    if STORE_FINAL_STATE:",
      "        ht = ht + i_nh * K * V",
      "",
      "    # load initial state",
      "    if USE_INITIAL_STATE:",
      "        p_h0_1 = tl.make_block_ptr(h0, (K, V), (V, 1), (0, i_v * BV), (64, BV),",
      "                                   (1, 0))",
      "        b_h1 += tl.load(p_h0_1, boundary_check=(0, 1)).to(tl.float32)",
      "        if K > 64:",
      "            p_h0_2 = tl.make_block_ptr(h0, (K, V), (V, 1), (64, i_v * BV),",
      "                                       (64, BV), (1, 0))",
      "            b_h2 += tl.load(p_h0_2, boundary_check=(0, 1)).to(tl.float32)",
      "        if K > 128:",
      "            p_h0_3 = tl.make_block_ptr(h0, (K, V), (V, 1), (128, i_v * BV),",
      "                                       (64, BV), (1, 0))",
      "            b_h3 += tl.load(p_h0_3, boundary_check=(0, 1)).to(tl.float32)",
      "        if K > 192:",
      "            p_h0_4 = tl.make_block_ptr(h0, (K, V), (V, 1), (192, i_v * BV),",
      "                                       (64, BV), (1, 0))",
      "            b_h4 += tl.load(p_h0_4, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    # main recurrence",
      "    for i_t in range(NT):",
      "        p_h1 = tl.make_block_ptr(h + i_t * stride_h, (K, V), (V, 1),",
      "                                 (0, i_v * BV), (64, BV), (1, 0))",
      "        tl.store(p_h1, b_h1.to(p_h1.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 64:",
      "            p_h2 = tl.make_block_ptr(h + i_t * stride_h, (K, V), (V, 1),",
      "                                     (64, i_v * BV), (64, BV), (1, 0))",
      "            tl.store(p_h2,",
      "                     b_h2.to(p_h2.dtype.element_ty),",
      "                     boundary_check=(0, 1))",
      "        if K > 128:",
      "            p_h3 = tl.make_block_ptr(h + i_t * stride_h, (K, V), (V, 1),",
      "                                     (128, i_v * BV), (64, BV), (1, 0))",
      "            tl.store(p_h3,",
      "                     b_h3.to(p_h3.dtype.element_ty),",
      "                     boundary_check=(0, 1))",
      "        if K > 192:",
      "            p_h4 = tl.make_block_ptr(h + i_t * stride_h, (K, V), (V, 1),",
      "                                     (192, i_v * BV), (64, BV), (1, 0))",
      "            tl.store(p_h4,",
      "                     b_h4.to(p_h4.dtype.element_ty),",
      "                     boundary_check=(0, 1))",
      "",
      "        p_v = tl.make_block_ptr(v, (T, V), (stride_v, 1), (i_t * BT, i_v * BV),",
      "                                (BT, BV), (1, 0))",
      "        p_v_new = tl.make_block_ptr(v_new, (T, V), (stride_v, 1),",
      "                                    (i_t * BT, i_v * BV), (BT, BV),",
      "                                    (1, 0)) if SAVE_NEW_VALUE else None",
      "        b_v_new = tl.zeros([BT, BV], dtype=tl.float32)",
      "        p_w = tl.make_block_ptr(w, (T, K), (stride_w, 1), (i_t * BT, 0),",
      "                                (BT, 64), (1, 0))",
      "        b_w = tl.load(p_w, boundary_check=(0, 1))",
      "        b_v_new += tl.dot(b_w, b_h1.to(b_w.dtype))",
      "        if K > 64:",
      "            p_w = tl.make_block_ptr(w, (T, K), (stride_w, 1), (i_t * BT, 64),",
      "                                    (BT, 64), (1, 0))",
      "            b_w = tl.load(p_w, boundary_check=(0, 1))",
      "            b_v_new += tl.dot(b_w, b_h2.to(b_w.dtype))",
      "        if K > 128:",
      "            p_w = tl.make_block_ptr(w, (T, K), (stride_w, 1), (i_t * BT, 128),",
      "                                    (BT, 64), (1, 0))",
      "            b_w = tl.load(p_w, boundary_check=(0, 1))",
      "            b_v_new += tl.dot(b_w, b_h3.to(b_w.dtype))",
      "        if K > 192:",
      "            p_w = tl.make_block_ptr(w, (T, K), (stride_w, 1), (i_t * BT, 192),",
      "                                    (BT, 64), (1, 0))",
      "            b_w = tl.load(p_w, boundary_check=(0, 1))",
      "            b_v_new += tl.dot(b_w, b_h4.to(b_w.dtype))",
      "        b_v_new = -b_v_new + tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        if SAVE_NEW_VALUE:",
      "            p_v_new = tl.make_block_ptr(v_new, (T, V), (stride_v, 1),",
      "                                        (i_t * BT, i_v * BV), (BT, BV), (1, 0))",
      "            tl.store(p_v_new,",
      "                     b_v_new.to(p_v_new.dtype.element_ty),",
      "                     boundary_check=(0, 1))",
      "",
      "        if USE_G:",
      "            last_idx = min((i_t + 1) * BT, T) - 1",
      "            b_g_last = tl.load(g + bos * H + last_idx * H + i_h)",
      "            p_g = tl.make_block_ptr(g + bos * H + i_h, (T, ), (H, ),",
      "                                    (i_t * BT, ), (BT, ), (0, ))",
      "            b_g = tl.load(p_g, boundary_check=(0, ))",
      "            b_v_new = b_v_new * safe_exp(b_g_last - b_g)[:, None]",
      "            b_g_last = exp(b_g_last)",
      "            b_h1 = b_h1 * b_g_last",
      "            if K > 64:",
      "                b_h2 = b_h2 * b_g_last",
      "            if K > 128:",
      "                b_h3 = b_h3 * b_g_last",
      "            if K > 192:",
      "                b_h4 = b_h4 * b_g_last",
      "        b_v_new = b_v_new.to(k.dtype.element_ty)",
      "        p_k = tl.make_block_ptr(k, (K, T), (1, stride_k), (0, i_t * BT),",
      "                                (64, BT), (0, 1))",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_h1 += tl.dot(b_k, b_v_new)",
      "        if K > 64:",
      "            p_k = tl.make_block_ptr(k, (K, T), (1, stride_k), (64, i_t * BT),",
      "                                    (64, BT), (0, 1))",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_h2 += tl.dot(b_k, b_v_new)",
      "        if K > 128:",
      "            p_k = tl.make_block_ptr(k, (K, T), (1, stride_k), (128, i_t * BT),",
      "                                    (64, BT), (0, 1))",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_h3 += tl.dot(b_k, b_v_new)",
      "        if K > 192:",
      "            p_k = tl.make_block_ptr(k, (K, T), (1, stride_k), (192, i_t * BT),",
      "                                    (64, BT), (0, 1))",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_h4 += tl.dot(b_k, b_v_new)",
      "",
      "    # epilogue",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(ht, (K, V), (V, 1), (0, i_v * BV), (64, BV),",
      "                                 (1, 0))",
      "        tl.store(p_ht, b_h1.to(p_ht.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 64:",
      "            p_ht = tl.make_block_ptr(ht, (K, V), (V, 1), (64, i_v * BV),",
      "                                     (64, BV), (1, 0))",
      "            tl.store(p_ht,",
      "                     b_h2.to(p_ht.dtype.element_ty),",
      "                     boundary_check=(0, 1))",
      "        if K > 128:",
      "            p_ht = tl.make_block_ptr(ht, (K, V), (V, 1), (128, i_v * BV),",
      "                                     (64, BV), (1, 0))",
      "            tl.store(p_ht,",
      "                     b_h3.to(p_ht.dtype.element_ty),",
      "                     boundary_check=(0, 1))",
      "        if K > 192:",
      "            p_ht = tl.make_block_ptr(ht, (K, V), (V, 1), (192, i_v * BV),",
      "                                     (64, BV), (1, 0))",
      "            tl.store(p_ht,",
      "                     b_h4.to(p_ht.dtype.element_ty),",
      "                     boundary_check=(0, 1))",
      "",
      "",
      "def chunk_gated_delta_rule_fwd_h(",
      "    k: torch.Tensor,",
      "    w: torch.Tensor,",
      "    u: torch.Tensor,",
      "    g: Optional[torch.Tensor] = None,",
      "    initial_state: Optional[torch.Tensor] = None,",
      "    output_final_state: bool = False,",
      "    chunk_size: int = 64,  # SY: remove this argument and force chunk size 64?",
      "    save_new_value: bool = True,",
      "    cu_seqlens: Optional[torch.LongTensor] = None,",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    B, T, Hg, K, V = *k.shape, u.shape[-1]",
      "    H = u.shape[-2]",
      "    BT = chunk_size",
      "",
      "    chunk_indices = prepare_chunk_indices(",
      "        cu_seqlens, chunk_size) if cu_seqlens is not None else None",
      "    # N: the actual number of sequences in the batch with either equal or variable lengths",
      "    if cu_seqlens is None:",
      "        N, NT, chunk_offsets = B, triton.cdiv(T, BT), None",
      "    else:",
      "        N, NT, chunk_offsets = len(cu_seqlens) - 1, len(",
      "            chunk_indices), prepare_chunk_offsets(cu_seqlens, BT)",
      "    assert K <= 256, \"current kernel does not support head dimension larger than 256.\"",
      "",
      "    h = k.new_empty(B, NT, H, K, V)",
      "    final_state = k.new_empty(",
      "        N, H, K, V, dtype=torch.float32) if output_final_state else None",
      "",
      "    v_new = torch.empty_like(u) if save_new_value else None",
      "",
      "    def grid(meta):",
      "        return (triton.cdiv(V, meta['BV']), N * H)",
      "",
      "    chunk_gated_delta_rule_fwd_kernel_h_blockdim64[grid](",
      "        k=k,",
      "        v=u,",
      "        w=w,",
      "        v_new=v_new,",
      "        g=g,",
      "        h=h,",
      "        h0=initial_state,",
      "        ht=final_state,",
      "        cu_seqlens=cu_seqlens,",
      "        chunk_offsets=chunk_offsets,",
      "        T=T,",
      "        H=H,",
      "        Hg=Hg,",
      "        K=K,",
      "        V=V,",
      "        BT=BT)",
      "    return h, v_new, final_state"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/fla/ops/cumsum.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "# SPDX-FileCopyrightText: Songlin Yang, Yu Zhang",
      "#",
      "# This file contains code copied from the flash-linear-attention project.",
      "# The original source code was licensed under the MIT license and included",
      "# the following copyright notice:",
      "# Copyright (c) 2023-2025, Songlin Yang, Yu Zhang",
      "# ruff: noqa: E501",
      "import warnings",
      "from typing import Optional",
      "",
      "import torch",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "from .index import prepare_chunk_indices",
      "from .utils import check_shared_mem, input_guard",
      "",
      "BS_LIST = [32, 64] if check_shared_mem() else [16, 32]",
      "",
      "",
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[",
      "    triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8]",
      "],",
      "                 key=['B', 'H', 'BT', 'IS_VARLEN', 'REVERSE'])",
      "@triton.jit(do_not_specialize=['T'])",
      "def chunk_local_cumsum_scalar_kernel(",
      "    s,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    HEAD_FIRST: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(",
      "            tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(",
      "            tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    if HEAD_FIRST:",
      "        p_s = tl.make_block_ptr(s + bos * H + i_h * T, (T, ), (1, ),",
      "                                (i_t * BT, ), (BT, ), (0, ))",
      "        p_o = tl.make_block_ptr(o + bos * H + i_h * T, (T, ), (1, ),",
      "                                (i_t * BT, ), (BT, ), (0, ))",
      "    else:",
      "        p_s = tl.make_block_ptr(s + bos * H + i_h, (T, ), (H, ), (i_t * BT, ),",
      "                                (BT, ), (0, ))",
      "        p_o = tl.make_block_ptr(o + bos * H + i_h, (T, ), (H, ), (i_t * BT, ),",
      "                                (BT, ), (0, ))",
      "    # [BT]",
      "    b_s = tl.load(p_s, boundary_check=(0, )).to(tl.float32)",
      "    b_o = tl.cumsum(b_s, axis=0)",
      "    if REVERSE:",
      "        b_z = tl.sum(b_s, axis=0)",
      "        b_o = -b_o + b_z[None] + b_s",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, ))",
      "",
      "",
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[",
      "    triton.Config({'BS': BS}, num_warps=num_warps) for BS in BS_LIST",
      "    for num_warps in [2, 4, 8]",
      "],",
      "                 key=['B', 'H', 'S', 'BT', 'IS_VARLEN', 'REVERSE'])",
      "@triton.jit(do_not_specialize=['T'])",
      "def chunk_local_cumsum_vector_kernel(",
      "    s,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    HEAD_FIRST: tl.constexpr,",
      "):",
      "    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(",
      "            tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(",
      "            tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    o_i = tl.arange(0, BT)",
      "    if REVERSE:",
      "        m_s = tl.where(o_i[:, None] <= o_i[None, :], 1., 0.)",
      "    else:",
      "        m_s = tl.where(o_i[:, None] >= o_i[None, :], 1., 0.)",
      "",
      "    if HEAD_FIRST:",
      "        p_s = tl.make_block_ptr(s + (bos * H + i_h * T) * S, (T, S), (S, 1),",
      "                                (i_t * BT, i_s * BS), (BT, BS), (1, 0))",
      "        p_o = tl.make_block_ptr(o + (bos * H + i_h * T) * S, (T, S), (S, 1),",
      "                                (i_t * BT, i_s * BS), (BT, BS), (1, 0))",
      "    else:",
      "        p_s = tl.make_block_ptr(s + (bos * H + i_h) * S, (T, S), (H * S, 1),",
      "                                (i_t * BT, i_s * BS), (BT, BS), (1, 0))",
      "        p_o = tl.make_block_ptr(o + (bos * H + i_h) * S, (T, S), (H * S, 1),",
      "                                (i_t * BT, i_s * BS), (BT, BS), (1, 0))",
      "    # [BT, BS]",
      "    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)",
      "    b_o = tl.dot(m_s, b_s, allow_tf32=False)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "",
      "def chunk_local_cumsum_scalar(",
      "        g: torch.Tensor,",
      "        chunk_size: int,",
      "        reverse: bool = False,",
      "        cu_seqlens: Optional[torch.Tensor] = None,",
      "        head_first: bool = False,",
      "        output_dtype: Optional[torch.dtype] = torch.float) -> torch.Tensor:",
      "    if head_first:",
      "        B, H, T = g.shape",
      "    else:",
      "        B, T, H = g.shape",
      "    assert chunk_size == 2**(chunk_size.bit_length() -",
      "                             1), \"chunk_size must be a power of 2\"",
      "    BT = chunk_size",
      "    chunk_indices = prepare_chunk_indices(",
      "        cu_seqlens, BT) if cu_seqlens is not None else None",
      "    NT = triton.cdiv(T, BT) if cu_seqlens is None else len(chunk_indices)",
      "    g_org, g = g, torch.empty_like(g, dtype=output_dtype or g.dtype)",
      "    grid = (NT, B * H)",
      "    chunk_local_cumsum_scalar_kernel[grid](g_org,",
      "                                           g,",
      "                                           cu_seqlens,",
      "                                           chunk_indices,",
      "                                           T=T,",
      "                                           B=B,",
      "                                           H=H,",
      "                                           BT=BT,",
      "                                           HEAD_FIRST=head_first,",
      "                                           REVERSE=reverse)",
      "    return g",
      "",
      "",
      "def chunk_local_cumsum_vector(",
      "        g: torch.Tensor,",
      "        chunk_size: int,",
      "        reverse: bool = False,",
      "        cu_seqlens: Optional[torch.Tensor] = None,",
      "        head_first: bool = False,",
      "        output_dtype: Optional[torch.dtype] = torch.float) -> torch.Tensor:",
      "    if head_first:",
      "        B, H, T, S = g.shape",
      "    else:",
      "        B, T, H, S = g.shape",
      "    BT = chunk_size",
      "    chunk_indices = prepare_chunk_indices(",
      "        cu_seqlens, chunk_size) if cu_seqlens is not None else None",
      "    NT = triton.cdiv(T, BT) if cu_seqlens is None else len(chunk_indices)",
      "    assert chunk_size == 2**(chunk_size.bit_length() -",
      "                             1), \"chunk_size must be a power of 2\"",
      "",
      "    g_org, g = g, torch.empty_like(g, dtype=output_dtype or g.dtype)",
      "",
      "    def grid(meta):",
      "        return (triton.cdiv(meta['S'], meta['BS']), NT, B * H)",
      "",
      "    # keep cummulative normalizer in fp32",
      "    # this kernel is equivalent to",
      "    # g = g.view(B, H, NT, BT, -1).cumsum(-2).view(B, H, T, -1)",
      "    chunk_local_cumsum_vector_kernel[grid](g_org,",
      "                                           g,",
      "                                           cu_seqlens,",
      "                                           chunk_indices,",
      "                                           T=T,",
      "                                           B=B,",
      "                                           H=H,",
      "                                           S=S,",
      "                                           BT=BT,",
      "                                           HEAD_FIRST=head_first,",
      "                                           REVERSE=reverse)",
      "    return g",
      "",
      "",
      "@input_guard",
      "def chunk_local_cumsum(g: torch.Tensor,",
      "                       chunk_size: int,",
      "                       reverse: bool = False,",
      "                       cu_seqlens: Optional[torch.Tensor] = None,",
      "                       head_first: bool = False,",
      "                       output_dtype: Optional[torch.dtype] = torch.float,",
      "                       **kwargs) -> torch.Tensor:",
      "    if not head_first and g.shape[1] < g.shape[2]:",
      "        warnings.warn(",
      "            f\"Input tensor shape suggests potential format mismatch: seq_len ({g.shape[1]}) < num_heads ({g.shape[2]}). \"",
      "            \"This may indicate the inputs were passed in head-first format [B, H, T, ...] \"",
      "            \"when head_first=False was specified. \"",
      "            \"Please verify your input tensor format matches the expected shape [B, T, H, ...].\",",
      "            stacklevel=2)",
      "    if cu_seqlens is not None:",
      "        assert g.shape[",
      "            0] == 1, \"Only batch size 1 is supported when cu_seqlens are provided\"",
      "    if len(g.shape) == 3:",
      "        return chunk_local_cumsum_scalar(g, chunk_size, reverse, cu_seqlens,",
      "                                         head_first, output_dtype)",
      "    elif len(g.shape) == 4:",
      "        return chunk_local_cumsum_vector(g, chunk_size, reverse, cu_seqlens,",
      "                                         head_first, output_dtype)",
      "    else:",
      "        raise ValueError(f\"Unsupported input shape {g.shape}. \"",
      "                         f\"which should be (B, T, H, D) if `head_first=False` \"",
      "                         f\"or (B, H, T, D) otherwise\")"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/fla/ops/chunk_o.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "# SPDX-FileCopyrightText: Songlin Yang, Yu Zhang",
      "#",
      "# This file contains code copied from the flash-linear-attention project.",
      "# The original source code was licensed under the MIT license and included",
      "# the following copyright notice:",
      "# Copyright (c) 2023-2025, Songlin Yang, Yu Zhang",
      "",
      "# ruff: noqa: E501",
      "",
      "from typing import Optional",
      "",
      "import torch",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "from .index import prepare_chunk_indices",
      "from .op import exp, safe_exp",
      "from .utils import FLA_GDN_FIX_BT, check_shared_mem, is_nvidia_hopper",
      "",
      "BKV_LIST = [64, 128] if check_shared_mem() else [32, 64]",
      "NUM_WARPS = [2, 4] if is_nvidia_hopper else [2, 4, 8]",
      "",
      "",
      "@triton.heuristics({",
      "    'USE_G': lambda args: args['g'] is not None,",
      "    'IS_VARLEN': lambda args: args['cu_seqlens'] is not None",
      "})",
      "@triton.autotune(",
      "    configs=[",
      "        triton.Config({",
      "            'BK': BK,",
      "            'BV': BV",
      "        },",
      "                      num_warps=num_warps,",
      "                      num_stages=num_stages) for BK in BKV_LIST",
      "        for BV in BKV_LIST for num_warps in NUM_WARPS",
      "        for num_stages in [2, 3, 4]",
      "    ],",
      "    key=['H', 'K', 'V', 'BT'],",
      ")",
      "@triton.jit(do_not_specialize=['T'])",
      "def chunk_fwd_kernel_o(",
      "    q,",
      "    k,",
      "    v,",
      "    h,",
      "    g,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    Hg: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(",
      "            tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(",
      "            tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    # offset calculation",
      "    q += (bos * Hg + i_h // (H // Hg)) * K",
      "    k += (bos * Hg + i_h // (H // Hg)) * K",
      "    v += (bos * H + i_h) * V",
      "    o += (bos * H + i_h) * V",
      "    h += (i_tg * H + i_h).to(tl.int64) * K * V",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(q, (T, K), (Hg * K, 1), (i_t * BT, i_k * BK),",
      "                                (BT, BK), (1, 0))",
      "        p_k = tl.make_block_ptr(k, (K, T), (1, Hg * K), (i_k * BK, i_t * BT),",
      "                                (BK, BT), (0, 1))",
      "        p_h = tl.make_block_ptr(h, (K, V), (V, 1), (i_k * BK, i_v * BV),",
      "                                (BK, BV), (1, 0))",
      "        # [BT, BK]",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        # [BK, BT]",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        # [BK, BV]",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        # [BT, BK] @ [BK, BV] -> [BT, BV]",
      "        b_o += tl.dot(b_q, b_h)",
      "        # [BT, BK] @ [BK, BT] -> [BT, BT]",
      "        b_A += tl.dot(b_q, b_k)",
      "",
      "    if USE_G:",
      "        g += bos * H + i_h",
      "        p_g = tl.make_block_ptr(g, (T, ), (H, ), (i_t * BT, ), (BT, ), (0, ))",
      "        b_g = tl.load(p_g, boundary_check=(0, ))",
      "        b_o = b_o * exp(b_g)[:, None]",
      "        b_A = b_A * safe_exp(b_g[:, None] - b_g[None, :])",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_A = o_i[:, None] >= o_i[None, :]",
      "    b_A = tl.where(m_A, b_A, 0)",
      "",
      "    p_v = tl.make_block_ptr(v, (T, V), (H * V, 1), (i_t * BT, i_v * BV),",
      "                            (BT, BV), (1, 0))",
      "    p_o = tl.make_block_ptr(o, (T, V), (H * V, 1), (i_t * BT, i_v * BV),",
      "                            (BT, BV), (1, 0))",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "    # to fix mma -> mma layout conversion",
      "    # already solved by triton v3.2 or higher",
      "    b_o = b_o * scale + tl.dot(b_A.to(b_v.dtype), b_v) * scale",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "",
      "def chunk_fwd_o(",
      "        q: torch.Tensor,",
      "        k: torch.Tensor,",
      "        v: torch.Tensor,",
      "        h: torch.Tensor,",
      "        g: Optional[torch.Tensor] = None,  # cumsum of log decay",
      "        scale: Optional[float] = None,",
      "        cu_seqlens: Optional[torch.LongTensor] = None,",
      "        chunk_size: int = 64) -> torch.Tensor:",
      "    B, T, Hg, K, V = *q.shape, v.shape[-1]",
      "    H = v.shape[-2]",
      "    if FLA_GDN_FIX_BT:",
      "        BT = 64",
      "    else:",
      "        BT = min(chunk_size, max(16, triton.next_power_of_2(T)))",
      "    chunk_indices = prepare_chunk_indices(",
      "        cu_seqlens, BT) if cu_seqlens is not None else None",
      "    NT = triton.cdiv(T, BT) if cu_seqlens is None else len(chunk_indices)",
      "    if scale is None:",
      "        scale = k.shape[-1]**-0.5",
      "",
      "    o = torch.empty_like(v)",
      "",
      "    def grid(meta):",
      "        return (triton.cdiv(V, meta['BV']), NT, B * H)",
      "",
      "    chunk_fwd_kernel_o[grid](",
      "        q,",
      "        k,",
      "        v,",
      "        h,",
      "        g,",
      "        o,",
      "        cu_seqlens,",
      "        chunk_indices,",
      "        scale,",
      "        T=T,",
      "        H=H,",
      "        Hg=Hg,",
      "        K=K,",
      "        V=V,",
      "        BT=BT,",
      "    )",
      "    return o"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/fla/ops/layernorm_guard.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "# SPDX-FileCopyrightText: Tri Dao",
      "#",
      "# This file contains code copied from the flash-linear-attention project.",
      "# The original source code was licensed under the MIT license and included",
      "# the following copyright notice:",
      "# Copyright (c) 2024, Tri Dao.",
      "",
      "# ruff: noqa: E501",
      "# Based on the Triton LayerNorm tutorial: https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html",
      "# For the backward pass, we keep weight_grad and bias_grad in registers and accumulate.",
      "# This backward pass is faster for dimensions up to 8k, but after that it's much slower due to register spilling.",
      "# The models we train have hidden dim up to 8k anyway (e.g. Llama 70B), so this is fine.",
      "",
      "from typing import Optional",
      "",
      "import torch",
      "import torch.nn as nn",
      "import torch.nn.functional as F",
      "from einops import rearrange",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "from .utils import input_guard",
      "",
      "",
      "def rms_norm_ref(x,",
      "                 weight,",
      "                 bias,",
      "                 z=None,",
      "                 eps=1e-6,",
      "                 group_size=None,",
      "                 norm_before_gate=True,",
      "                 upcast=True):",
      "    dtype = x.dtype",
      "    weight = weight.float()",
      "    bias = bias.float() if bias is not None else None",
      "    if upcast:",
      "        x = x.float()",
      "        z = z.float() if z is not None else z",
      "    if z is not None and not norm_before_gate:",
      "        x = x * F.silu(z)",
      "    if group_size is None:",
      "        rstd = 1 / torch.sqrt((x.square()).mean(dim=-1, keepdim=True) + eps)",
      "        out = (x * rstd * weight) + bias if bias is not None else (x * rstd *",
      "                                                                   weight)",
      "    else:",
      "        x_group = rearrange(x, \"... (g d) -> ... g d\", d=group_size)",
      "        rstd = 1 / torch.sqrt((x_group.square()).mean(dim=-1, keepdim=True) +",
      "                              eps)",
      "        out = rearrange(x_group * rstd, \"... g d -> ... (g d)\") * weight",
      "        if bias is not None:",
      "            out = out + bias",
      "    if z is not None and norm_before_gate:",
      "        out *= F.silu(z)",
      "    return out.to(dtype)",
      "",
      "",
      "@triton.heuristics({",
      "    \"HAS_BIAS\": lambda args: args[\"B\"] is not None,",
      "    \"HAS_Z\": lambda args: args[\"Z\"] is not None,",
      "})",
      "@triton.jit",
      "def layer_norm_fwd_kernel(",
      "    X,  # pointer to the input",
      "    Y,  # pointer to the output",
      "    W,  # pointer to the weights",
      "    B,  # pointer to the biases",
      "    Z,  # pointer to the other branch",
      "    Mean,  # pointer to the mean",
      "    Rstd,  # pointer to the 1/std",
      "    stride_x_row,  # how much to increase the pointer when moving by 1 row",
      "    stride_y_row,",
      "    stride_z_row,",
      "    M,  # number of rows in X",
      "    N,  # number of columns in X",
      "    eps,  # epsilon to avoid division by zero",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    NORM_BEFORE_GATE: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "):",
      "    # Map the program id to the row of X and Y it should compute.",
      "    row = tl.program_id(0)",
      "    group = tl.program_id(1)",
      "    X += row * stride_x_row + group * N",
      "    Y += row * stride_y_row + group * N",
      "    if HAS_Z:",
      "        Z += row * stride_z_row + group * N",
      "    if not IS_RMS_NORM:",
      "        Mean += group * M",
      "    Rstd += group * M",
      "    W += group * N",
      "    if HAS_BIAS:",
      "        B += group * N",
      "    # Compute mean and variance",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)",
      "    if HAS_Z and not NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=cols < N).to(tl.float32)",
      "        x *= z * tl.sigmoid(z)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "    # Normalize and apply linear transformation",
      "    mask = cols < N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "    y = x_hat * w + b if HAS_BIAS else x_hat * w",
      "    if HAS_Z and NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=mask).to(tl.float32)",
      "        y *= z * tl.sigmoid(z)",
      "    # Write output",
      "    tl.store(Y + cols, y, mask=mask)",
      "",
      "",
      "def layer_norm_fwd(",
      "    x: torch.Tensor,",
      "    weight: torch.Tensor,",
      "    bias: torch.Tensor,",
      "    eps: float,",
      "    z: torch.Tensor = None,",
      "    out: torch.Tensor = None,",
      "    group_size: int = None,",
      "    norm_before_gate: bool = True,",
      "    is_rms_norm: bool = False,",
      "):",
      "    M, N = x.shape",
      "    if group_size is None:",
      "        group_size = N",
      "    assert N % group_size == 0",
      "    ngroups = N // group_size",
      "    assert x.stride(-1) == 1",
      "    if z is not None:",
      "        assert z.stride(-1) == 1",
      "        assert z.shape == (M, N)",
      "    assert weight.shape == (N, )",
      "    assert weight.stride(-1) == 1",
      "    if bias is not None:",
      "        assert bias.stride(-1) == 1",
      "        assert bias.shape == (N, )",
      "    # allocate output",
      "    if out is not None:",
      "        assert out.shape == x.shape",
      "    else:",
      "        out = torch.empty_like(x)",
      "    assert out.stride(-1) == 1",
      "    mean = torch.empty((ngroups * M, ), dtype=torch.float32,",
      "                       device=x.device) if not is_rms_norm else None",
      "    rstd = torch.empty((ngroups * M, ), dtype=torch.float32, device=x.device)",
      "    # Less than 64KB per feature: enqueue fused kernel",
      "    MAX_FUSED_SIZE = 65536 // x.element_size()",
      "    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(group_size))",
      "    if group_size > BLOCK_N:",
      "        raise RuntimeError(",
      "            \"This layer norm doesn't support feature dim >= 64KB.\")",
      "    # heuristics for number of warps",
      "    num_warps = min(max(BLOCK_N // 256, 1), 8)",
      "    grid = (M, ngroups)",
      "    layer_norm_fwd_kernel[grid](x,",
      "                                out,",
      "                                weight,",
      "                                bias,",
      "                                z,",
      "                                mean,",
      "                                rstd,",
      "                                x.stride(0),",
      "                                out.stride(0),",
      "                                z.stride(0) if z is not None else 0,",
      "                                M,",
      "                                group_size,",
      "                                eps,",
      "                                BLOCK_N=BLOCK_N,",
      "                                NORM_BEFORE_GATE=norm_before_gate,",
      "                                IS_RMS_NORM=is_rms_norm,",
      "                                num_warps=num_warps)",
      "    return out, mean, rstd",
      "",
      "",
      "class LayerNormFn(torch.autograd.Function):",
      "",
      "    @input_guard",
      "    @staticmethod",
      "    def forward(ctx,",
      "                x,",
      "                weight,",
      "                bias,",
      "                z=None,",
      "                eps=1e-6,",
      "                group_size=None,",
      "                norm_before_gate=True,",
      "                is_rms_norm=False):",
      "        \"\"\"If z is not None, we do norm(x) * silu(z) if norm_before_gate, else norm(x * silu(z))",
      "        \"\"\"",
      "",
      "        x_shape_og = x.shape",
      "        # reshape input data into 2D tensor",
      "        x = x.reshape(-1, x.shape[-1])",
      "        if x.stride(-1) != 1:",
      "            x = x.contiguous()",
      "        if z is not None:",
      "            assert z.shape == x_shape_og",
      "            z = z.reshape(-1, z.shape[-1])",
      "            if z.stride(-1) != 1:",
      "                z = z.contiguous()",
      "        weight = weight.contiguous()",
      "        if bias is not None:",
      "            bias = bias.contiguous()",
      "        y, mean, rstd = layer_norm_fwd(",
      "            x,",
      "            weight,",
      "            bias,",
      "            eps,",
      "            z=z,",
      "            group_size=group_size,",
      "            norm_before_gate=norm_before_gate,",
      "            is_rms_norm=is_rms_norm,",
      "        )",
      "        ctx.save_for_backward(x, weight, bias, mean, rstd, z)",
      "        ctx.x_shape_og = x_shape_og",
      "        ctx.eps = eps",
      "        ctx.group_size = group_size",
      "        ctx.norm_before_gate = norm_before_gate",
      "        ctx.is_rms_norm = is_rms_norm",
      "        return y.reshape(x_shape_og)",
      "",
      "",
      "def layernorm_fn(x,",
      "                 weight,",
      "                 bias,",
      "                 z=None,",
      "                 eps=1e-6,",
      "                 group_size=None,",
      "                 norm_before_gate=True,",
      "                 is_rms_norm=False):",
      "    return LayerNormFn.apply(x, weight, bias, z, eps, group_size,",
      "                             norm_before_gate, is_rms_norm)",
      "",
      "",
      "def rmsnorm_fn(x,",
      "               weight,",
      "               bias,",
      "               z=None,",
      "               eps=1e-6,",
      "               group_size=None,",
      "               norm_before_gate=True):",
      "    return LayerNormFn.apply(x, weight, bias, z, eps, group_size,",
      "                             norm_before_gate, True)",
      "",
      "",
      "class LayerNormGated(nn.Module):",
      "",
      "    def __init__(",
      "        self,",
      "        hidden_size,",
      "        eps: float = 1e-5,",
      "        group_size: Optional[int] = None,",
      "        norm_before_gate: bool = True,",
      "        device: Optional[torch.device] = None,",
      "        dtype: Optional[torch.dtype] = None,",
      "    ):",
      "        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.",
      "        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).",
      "        \"\"\"",
      "",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype}",
      "        super().__init__()",
      "        self.eps = eps",
      "        self.weight = nn.Parameter(torch.empty(hidden_size, **factory_kwargs))",
      "        self.bias = nn.Parameter(torch.empty(hidden_size, **factory_kwargs))",
      "        self.group_size = group_size",
      "        self.norm_before_gate = norm_before_gate",
      "        self.reset_parameters()",
      "",
      "    def reset_parameters(self):",
      "        torch.nn.init.ones_(self.weight)",
      "        torch.nn.init.zeros_(self.bias)",
      "",
      "    def forward(self, x, z=None):",
      "        \"\"\"If z is not None, we do norm(x) * silu(z) if norm_before_gate, else norm(x * silu(z))",
      "        \"\"\"",
      "        return layernorm_fn(x,",
      "                            self.weight,",
      "                            self.bias,",
      "                            z=z,",
      "                            group_size=self.group_size,",
      "                            eps=self.eps,",
      "                            norm_before_gate=self.norm_before_gate)",
      "",
      "",
      "class RMSNormGated(nn.Module):",
      "",
      "    def __init__(",
      "        self,",
      "        hidden_size,",
      "        eps: float = 1e-5,",
      "        group_size: Optional[int] = None,",
      "        norm_before_gate: bool = False,",
      "        device: Optional[torch.device] = None,",
      "        dtype: Optional[torch.dtype] = None,",
      "    ):",
      "        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.",
      "        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).",
      "        \"\"\"",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype}",
      "        super().__init__()",
      "        self.eps = eps",
      "        self.weight = nn.Parameter(torch.empty(hidden_size, **factory_kwargs))",
      "        self.register_parameter(\"bias\", None)",
      "        self.group_size = group_size",
      "        self.norm_before_gate = norm_before_gate",
      "        self.reset_parameters()",
      "",
      "    def reset_parameters(self):",
      "        torch.nn.init.ones_(self.weight)",
      "",
      "    def forward(self, x, z=None):",
      "        \"\"\"If z is not None, we do norm(x) * silu(z) if norm_before_gate, else norm(x * silu(z))",
      "        \"\"\"",
      "        return rmsnorm_fn(x,",
      "                          self.weight,",
      "                          self.bias,",
      "                          z=z,",
      "                          eps=self.eps,",
      "                          group_size=self.group_size,",
      "                          norm_before_gate=self.norm_before_gate)"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/fla/ops/wy_fast.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "# SPDX-FileCopyrightText: Songlin Yang, Yu Zhang",
      "#",
      "# This file contains code copied from the flash-linear-attention project.",
      "# The original source code was licensed under the MIT license and included",
      "# the following copyright notice:",
      "# Copyright (c) 2023-2025, Songlin Yang, Yu Zhang",
      "",
      "# ruff: noqa: E501",
      "from typing import Optional",
      "",
      "import torch",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "from .index import prepare_chunk_indices",
      "",
      "",
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(",
      "    configs=[",
      "        triton.Config({}, num_warps=num_warps, num_stages=num_stages)",
      "        for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]",
      "    ],",
      "    key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'IS_VARLEN'],",
      ")",
      "@triton.jit(do_not_specialize=['T'])",
      "def recompute_w_u_fwd_kernel(k, v, beta, w, u, A, g, cu_seqlens, chunk_indices,",
      "                             T, H: tl.constexpr, Hg: tl.constexpr,",
      "                             K: tl.constexpr, V: tl.constexpr,",
      "                             BT: tl.constexpr, BK: tl.constexpr,",
      "                             BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(",
      "            tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(",
      "            tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    p_beta = tl.make_block_ptr(beta + bos * H + i_h, (T, ), (H, ),",
      "                               (i_t * BT, ), (BT, ), (0, ))",
      "    p_g = tl.make_block_ptr(g + (bos * H + i_h), (T, ), (H, ), (i_t * BT, ),",
      "                            (BT, ), (0, ))",
      "    p_A = tl.make_block_ptr(A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1),",
      "                            (i_t * BT, 0), (BT, BT), (1, 0))",
      "    b_beta = tl.load(p_beta, boundary_check=(0, ))",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "    b_g = tl.exp(tl.load(p_g, boundary_check=(0, )))",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1),",
      "                                (i_t * BT, i_v * BV), (BT, BV), (1, 0))",
      "        p_u = tl.make_block_ptr(u + (bos * H + i_h) * V, (T, V), (H * V, 1),",
      "                                (i_t * BT, i_v * BV), (BT, BV), (1, 0))",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_vb = (b_v * b_beta[:, None]).to(b_v.dtype)",
      "        b_u = tl.dot(b_A, b_vb, allow_tf32=False)",
      "        tl.store(p_u, b_u.to(p_u.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(k + (bos * Hg + i_h // (H // Hg)) * K, (T, K),",
      "                                (Hg * K, 1), (i_t * BT, i_k * BK), (BT, BK),",
      "                                (1, 0))",
      "        p_w = tl.make_block_ptr(w + (bos * H + i_h) * K, (T, K), (H * K, 1),",
      "                                (i_t * BT, i_k * BK), (BT, BK), (1, 0))",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_kb = (b_k * b_beta[:, None] * b_g[:, None]).to(b_k.dtype)",
      "        b_w = tl.dot(b_A, b_kb)",
      "        tl.store(p_w, b_w.to(p_w.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "",
      "def recompute_w_u_fwd(",
      "    k: torch.Tensor,",
      "    v: torch.Tensor,",
      "    beta: torch.Tensor,",
      "    g_cumsum: torch.Tensor,",
      "    A: torch.Tensor,",
      "    cu_seqlens: Optional[torch.LongTensor],",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    B, T, Hg, K, V = *k.shape, v.shape[-1]",
      "    H = v.shape[-2]",
      "    BT = A.shape[-1]",
      "",
      "    chunk_indices = prepare_chunk_indices(",
      "        cu_seqlens, BT) if cu_seqlens is not None else None",
      "    NT = triton.cdiv(T, BT) if cu_seqlens is None else len(chunk_indices)",
      "    BK = 64",
      "    BV = 64",
      "    u = torch.empty_like(v)",
      "    w = k.new_empty(B, T, H, K)",
      "    recompute_w_u_fwd_kernel[(NT, B * H)](",
      "        k=k,",
      "        v=v,",
      "        beta=beta,",
      "        w=w,",
      "        u=u,",
      "        A=A,",
      "        g=g_cumsum,",
      "        cu_seqlens=cu_seqlens,",
      "        chunk_indices=chunk_indices,",
      "        T=T,",
      "        H=H,",
      "        Hg=Hg,",
      "        K=K,",
      "        V=V,",
      "        BT=BT,",
      "        BK=BK,",
      "        BV=BV,",
      "    )",
      "    return w, u"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/rotary_embedding/mrope.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "import itertools",
      "from typing import Optional, Union",
      "",
      "import numpy as np",
      "import torch",
      "from transformers import PretrainedConfig",
      "",
      "from vllm.platforms import current_platform",
      "from vllm.triton_utils import tl, triton",
      "",
      "from .base import RotaryEmbedding",
      "from .common import apply_rotary_emb_dispatch",
      "",
      "",
      "@triton.jit",
      "def _triton_qwen2vl_mrope_forward(",
      "    q_ptr,",
      "    k_ptr,",
      "    cos,",
      "    sin,",
      "    num_tokens,",
      "    n_qh: tl.constexpr,",
      "    n_kh: tl.constexpr,",
      "    hd: tl.constexpr,",
      "    rd: tl.constexpr,",
      "    pad_n_qh: tl.constexpr,",
      "    pad_n_kh: tl.constexpr,",
      "    pad_hd: tl.constexpr,",
      "    mrope_section_t: tl.constexpr,",
      "    mrope_section_h: tl.constexpr,",
      "):",
      "    # Adapted from",
      "    # https://github.com/linkedin/Liger-Kernel/blob/main/src/liger_kernel/ops/qwen2vl_mrope.py",
      "    # This version supports flatten input tensors from vllm",
      "    # and supports cos and sin cache with shape (3, num_tokens, head_dim // 2)",
      "    # instead of (3, bsz, seq_len, head_dim)",
      "    pid = tl.program_id(0)",
      "    # locate start address",
      "    q_ptr = q_ptr + pid * (n_qh * hd)",
      "    k_ptr = k_ptr + pid * (n_kh * hd)",
      "",
      "    # ####################################################################",
      "    # get the cos(m\u03b8_{i...d/2}) and sin(m\u03b8_{i...d/2}) for token position",
      "    # m of this program instance",
      "    # ####################################################################",
      "    # Note: cos and sin now have shape (3, num_tokens, head_dim // 2)",
      "",
      "    t_end = mrope_section_t",
      "    h_end = t_end + mrope_section_h",
      "",
      "    # Updated stride calculation for half head_dim",
      "    half_rd = rd // 2",
      "    t_cos = cos + pid * half_rd",
      "    h_cos = t_cos + num_tokens * half_rd",
      "    w_cos = h_cos + num_tokens * half_rd",
      "    t_sin = sin + pid * half_rd",
      "    h_sin = t_sin + num_tokens * half_rd",
      "    w_sin = h_sin + num_tokens * half_rd",
      "",
      "    # Updated offsets for half head_dim",
      "    cos_offsets = tl.arange(0, pad_hd // 2)",
      "    t_mask = cos_offsets < t_end",
      "    h_mask = (t_end <= cos_offsets) & (cos_offsets < h_end)",
      "    w_mask = (h_end <= cos_offsets) & (cos_offsets < half_rd)",
      "",
      "    t_cos_row = tl.load(t_cos + cos_offsets, mask=t_mask, other=0)",
      "    h_cos_row = tl.load(h_cos + cos_offsets, mask=h_mask, other=0)",
      "    w_cos_row = tl.load(w_cos + cos_offsets, mask=w_mask, other=0)",
      "    t_sin_row = tl.load(t_sin + cos_offsets, mask=t_mask, other=0)",
      "    h_sin_row = tl.load(h_sin + cos_offsets, mask=h_mask, other=0)",
      "    w_sin_row = tl.load(w_sin + cos_offsets, mask=w_mask, other=0)",
      "",
      "    cos_row = t_cos_row + h_cos_row + w_cos_row",
      "    sin_row = t_sin_row + h_sin_row + w_sin_row",
      "",
      "    # ####################################################################",
      "    # Load the left and right half of q and k for the current",
      "    # program instance (i.e. for the current token) separately",
      "    # ####################################################################",
      "    # left half of the head",
      "    first_half_q_offsets = tl.arange(0, pad_n_qh)[:, None] * hd + tl.arange(",
      "        0, pad_hd // 2)[None, :]",
      "    first_half_k_offsets = tl.arange(0, pad_n_kh)[:, None] * hd + tl.arange(",
      "        0, pad_hd // 2)[None, :]",
      "    first_q_mask = (tl.arange(0, pad_n_qh)[:, None] < n_qh) & (tl.arange(",
      "        0, pad_hd // 2)[None, :] < rd // 2)",
      "    first_k_mask = (tl.arange(0, pad_n_kh)[:, None] < n_kh) & (tl.arange(",
      "        0, pad_hd // 2)[None, :] < rd // 2)",
      "",
      "    q_tile_1 = tl.load(q_ptr + first_half_q_offsets,",
      "                       mask=first_q_mask,",
      "                       other=0).to(sin_row.dtype)",
      "    k_tile_1 = tl.load(k_ptr + first_half_k_offsets,",
      "                       mask=first_k_mask,",
      "                       other=0).to(sin_row.dtype)",
      "",
      "    # right half of the head",
      "    second_half_q_offsets = first_half_q_offsets + (rd // 2)",
      "    second_half_k_offsets = first_half_k_offsets + (rd // 2)",
      "    second_q_mask = first_q_mask",
      "    second_k_mask = first_k_mask",
      "",
      "    q_tile_2 = tl.load(q_ptr + second_half_q_offsets,",
      "                       mask=second_q_mask,",
      "                       other=0).to(sin_row.dtype)",
      "    k_tile_2 = tl.load(k_ptr + second_half_k_offsets,",
      "                       mask=second_k_mask,",
      "                       other=0).to(sin_row.dtype)",
      "",
      "    # y = [x1, x2] * [cos, cos] + [-x2, x1] * [sin, sin]",
      "    # Since cos and sin are now half-size,",
      "    # we use the same cos_row and sin_row for both halves",
      "    new_q_tile_1 = q_tile_1 * cos_row - q_tile_2 * sin_row",
      "    tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)",
      "    new_q_tile_2 = q_tile_2 * cos_row + q_tile_1 * sin_row",
      "    tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=second_q_mask)",
      "",
      "    new_k_tile_1 = k_tile_1 * cos_row - k_tile_2 * sin_row",
      "    tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)",
      "    new_k_tile_2 = k_tile_2 * cos_row + k_tile_1 * sin_row",
      "    tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=second_k_mask)",
      "",
      "",
      "def triton_mrope(",
      "    q: torch.Tensor,",
      "    k: torch.Tensor,",
      "    cos: torch.Tensor,",
      "    sin: torch.Tensor,",
      "    mrope_section: list[int],",
      "    head_size: int,",
      "    rotary_dim: int,",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    \"\"\"Qwen2VL mrope kernel.",
      "",
      "    Args:",
      "        query: [num_tokens, num_heads * head_size]",
      "        key: [num_tokens, num_kv_heads * head_size]",
      "        cos: [3, num_tokens, head_size //2 ]",
      "            (T/H/W positions with multimodal inputs)",
      "        sin: [3, num_tokens, head_size //2 ]",
      "            (T/H/W positions with multimodal inputs)",
      "        mrope_section: [t, h, w]",
      "        head_size: int",
      "    \"\"\"",
      "    n_row, n_q_head_head_dim = q.shape",
      "    n_q_head = n_q_head_head_dim // head_size",
      "    n_kv_head = k.shape[1] // head_size",
      "    pad_hd = triton.next_power_of_2(head_size)",
      "    pad_n_q_head = triton.next_power_of_2(n_q_head)",
      "    pad_n_kv_head = triton.next_power_of_2(n_kv_head)",
      "",
      "    # ensure tensors passed into the kernel are contiguous.",
      "    # It will be no-op if they are already contiguous",
      "    q = q.contiguous()",
      "    k = k.contiguous()",
      "    cos = cos.contiguous()",
      "    sin = sin.contiguous()",
      "",
      "    _triton_qwen2vl_mrope_forward[(n_row, )](",
      "        q,",
      "        k,",
      "        cos,",
      "        sin,",
      "        n_row,",
      "        n_q_head,",
      "        n_kv_head,",
      "        head_size,",
      "        rotary_dim,",
      "        pad_n_q_head,",
      "        pad_n_kv_head,",
      "        pad_hd,",
      "        mrope_section[0],",
      "        mrope_section[1],",
      "    )",
      "    return q, k",
      "",
      "",
      "class MRotaryEmbedding(RotaryEmbedding):",
      "    \"\"\"Rotary Embedding with Multimodal Sections.\"\"\"",
      "",
      "    def __init__(",
      "        self,",
      "        head_size: int,",
      "        rotary_dim: int,",
      "        max_position_embeddings: int,",
      "        base: float,",
      "        is_neox_style: bool,",
      "        dtype: torch.dtype,",
      "        mrope_section: Optional[list[int]] = None,",
      "    ) -> None:",
      "        # In Qwen2.5-VL, the maximum index value is related to the duration of",
      "        # the input video. We enlarge max_position_embeddings to 4 times to get",
      "        # a larger the cos and sin cache.",
      "        self.cache_max_position_num = max_position_embeddings * 4",
      "        super().__init__(head_size, rotary_dim, self.cache_max_position_num,",
      "                         base, is_neox_style, dtype)",
      "",
      "        self.mrope_section = mrope_section",
      "        if self.mrope_section:",
      "            assert sum(self.mrope_section) == rotary_dim // 2",
      "",
      "        self.use_triton = current_platform.is_cuda_alike()",
      "",
      "    def forward(",
      "        self,",
      "        positions: torch.Tensor,",
      "        query: torch.Tensor,",
      "        key: Optional[torch.Tensor] = None,",
      "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:",
      "        \"\"\"MRope forward.",
      "",
      "        Args:",
      "            positions:",
      "                [num_tokens,] (text only) or",
      "                [3, num_tokens] (T/H/W positions with multimodal inputs)",
      "            query: [num_tokens, num_heads * head_size]",
      "            key: [num_tokens, num_kv_heads * head_size]",
      "        \"\"\"",
      "        if self.use_triton:",
      "            return self.forward_cuda(positions, query, key)",
      "        else:",
      "            return self.forward_native(positions, query, key)",
      "",
      "    def forward_native(",
      "        self,",
      "        positions: torch.Tensor,",
      "        query: torch.Tensor,",
      "        key: Optional[torch.Tensor] = None,",
      "        offsets: Optional[torch.Tensor] = None,",
      "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:",
      "        \"\"\"PyTorch-native implementation equivalent to forward().",
      "",
      "        Args:",
      "            positions:",
      "                [num_tokens,] (text only) or",
      "                [3, num_tokens] (T/H/W positions with multimodal inputs)",
      "            query: [num_tokens, num_heads * head_size]",
      "            key: [num_tokens, num_kv_heads * head_size]",
      "        \"\"\"",
      "        assert positions.ndim == 1 or positions.ndim == 2",
      "        assert key is not None",
      "",
      "        num_tokens = positions.shape[-1]",
      "        cos_sin = self.cos_sin_cache[positions]",
      "        cos, sin = cos_sin.chunk(2, dim=-1)",
      "        if positions.ndim == 2:",
      "            assert self.mrope_section",
      "",
      "            cos = torch.cat([",
      "                m[i]",
      "                for i, m in enumerate(cos.split(self.mrope_section, dim=-1))",
      "            ],",
      "                            dim=-1)",
      "            sin = torch.cat([",
      "                m[i]",
      "                for i, m in enumerate(sin.split(self.mrope_section, dim=-1))",
      "            ],",
      "                            dim=-1)",
      "",
      "        query_shape = query.shape",
      "        query = query.view(num_tokens, -1, self.head_size)",
      "        query_rot = query[..., :self.rotary_dim]",
      "        query_pass = query[..., self.rotary_dim:]",
      "        query_rot = apply_rotary_emb_dispatch(query_rot, cos, sin,",
      "                                              self.is_neox_style)",
      "        query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)",
      "",
      "        key_shape = key.shape",
      "        key = key.view(num_tokens, -1, self.head_size)",
      "        key_rot = key[..., :self.rotary_dim]",
      "        key_pass = key[..., self.rotary_dim:]",
      "        key_rot = apply_rotary_emb_dispatch(key_rot, cos, sin,",
      "                                            self.is_neox_style)",
      "        key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)",
      "        return query, key",
      "",
      "    def forward_cuda(",
      "        self,",
      "        positions: torch.Tensor,",
      "        query: torch.Tensor,",
      "        key: Optional[torch.Tensor] = None,",
      "        offsets: Optional[torch.Tensor] = None,",
      "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:",
      "",
      "        assert positions.ndim == 1 or positions.ndim == 2",
      "        assert key is not None",
      "",
      "        num_tokens = positions.shape[-1]",
      "        cos_sin = self.cos_sin_cache[positions]",
      "        cos, sin = cos_sin.chunk(2, dim=-1)",
      "        query_shape = query.shape",
      "        key_shape = key.shape",
      "        if positions.ndim == 2:",
      "            assert self.mrope_section",
      "",
      "            q, k = triton_mrope(",
      "                query,",
      "                key,",
      "                cos,",
      "                sin,",
      "                self.mrope_section,",
      "                self.head_size,",
      "                self.rotary_dim,",
      "            )",
      "",
      "            return q.reshape(query_shape), k.reshape(key_shape)",
      "",
      "        query = query.view(num_tokens, -1, self.head_size)",
      "        query_rot = query[..., :self.rotary_dim]",
      "        query_pass = query[..., self.rotary_dim:]",
      "        query_rot = apply_rotary_emb_dispatch(query_rot, cos, sin,",
      "                                              self.is_neox_style)",
      "        query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)",
      "",
      "        key = key.view(num_tokens, -1, self.head_size)",
      "        key_rot = key[..., :self.rotary_dim]",
      "        key_pass = key[..., self.rotary_dim:]",
      "        key_rot = apply_rotary_emb_dispatch(key_rot, cos, sin,",
      "                                            self.is_neox_style)",
      "        key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)",
      "        return query, key",
      "",
      "    @classmethod",
      "    def get_input_positions(",
      "        cls,",
      "        input_tokens: list[int],",
      "        hf_config: PretrainedConfig,",
      "        image_grid_thw: Optional[Union[list[list[int]], torch.Tensor]],",
      "        video_grid_thw: Optional[Union[list[list[int]], torch.Tensor]],",
      "        second_per_grid_ts: Optional[list[float]],",
      "        context_len: int = 0,",
      "        seq_len: Optional[int] = None,",
      "        audio_feature_lengths: Optional[torch.Tensor] = None,",
      "        use_audio_in_video: bool = False,",
      "    ) -> tuple[list[list[int]], int]:",
      "        \"\"\"Get mrope input positions and delta value.\"\"\"",
      "",
      "        image_grid_thw = [] if image_grid_thw is None else image_grid_thw",
      "        video_grid_thw = [] if video_grid_thw is None else video_grid_thw",
      "        second_per_grid_ts = [] if second_per_grid_ts is None else \\",
      "            second_per_grid_ts",
      "",
      "        llm_positions, mrope_position_delta = \\",
      "            cls.get_input_positions_tensor(",
      "                input_tokens=input_tokens,",
      "                hf_config=hf_config,",
      "                image_grid_thw=image_grid_thw,",
      "                video_grid_thw=video_grid_thw,",
      "                second_per_grid_ts=second_per_grid_ts,",
      "                context_len=context_len,",
      "                seq_len=seq_len,",
      "                audio_feature_lengths=audio_feature_lengths,",
      "                use_audio_in_video=use_audio_in_video,",
      "            )",
      "",
      "        return llm_positions.tolist(), mrope_position_delta",
      "",
      "    @classmethod",
      "    def get_input_positions_tensor(",
      "        cls,",
      "        input_tokens: list[int],",
      "        hf_config: PretrainedConfig,",
      "        image_grid_thw: Union[list[list[int]], torch.Tensor],",
      "        video_grid_thw: Union[list[list[int]], torch.Tensor],",
      "        second_per_grid_ts: list[float],",
      "        context_len: int = 0,",
      "        seq_len: Optional[int] = None,",
      "        audio_feature_lengths: Optional[torch.Tensor] = None,",
      "        use_audio_in_video: bool = False,",
      "    ) -> tuple[torch.Tensor, int]:",
      "        from vllm.transformers_utils.config import thinker_uses_mrope",
      "        if thinker_uses_mrope(hf_config):",
      "            return cls._omni_get_input_positions_tensor(",
      "                input_tokens=input_tokens,",
      "                hf_config=hf_config,",
      "                image_grid_thw=image_grid_thw,",
      "                video_grid_thw=video_grid_thw,",
      "                second_per_grid_ts=second_per_grid_ts,",
      "                context_len=context_len,",
      "                seq_len=seq_len,",
      "                audio_feature_lengths=audio_feature_lengths,",
      "                use_audio_in_video=use_audio_in_video,",
      "            )",
      "        elif hf_config.model_type in [\"glm4v\", \"glm4v_moe\"]:",
      "            return cls._glm4v_get_input_positions_tensor(",
      "                input_tokens=input_tokens,",
      "                hf_config=hf_config,",
      "                image_grid_thw=image_grid_thw,",
      "                video_grid_thw=video_grid_thw,",
      "                context_len=context_len,",
      "                seq_len=seq_len,",
      "            )",
      "        elif hf_config.model_type in [\"ernie4_5_moe_vl\", \"ernie4_5_vl\"]:",
      "            return cls._ernie_get_input_positions_tensor(",
      "                input_tokens=input_tokens,",
      "                hf_config=hf_config,",
      "                image_grid_thw=image_grid_thw,",
      "                video_grid_thw=video_grid_thw,",
      "                context_len=context_len,",
      "                seq_len=seq_len,",
      "            )",
      "        elif \"KeyeVL1_5\" in hf_config.model_type:",
      "            return cls._keye_get_input_positions_tensor(",
      "                input_tokens=input_tokens,",
      "                hf_config=hf_config,",
      "                image_grid_thw=image_grid_thw,",
      "                video_grid_thw=video_grid_thw,",
      "                context_len=context_len,",
      "                seq_len=seq_len,",
      "            )",
      "        else:",
      "            return cls._vl_get_input_positions_tensor(",
      "                input_tokens=input_tokens,",
      "                hf_config=hf_config,",
      "                image_grid_thw=image_grid_thw,",
      "                video_grid_thw=video_grid_thw,",
      "                second_per_grid_ts=second_per_grid_ts,",
      "                context_len=context_len,",
      "                seq_len=seq_len,",
      "            )",
      "",
      "    @classmethod",
      "    def _glm4v_get_input_positions_tensor(",
      "        cls,",
      "        input_tokens: list[int],",
      "        hf_config: PretrainedConfig,",
      "        image_grid_thw: Union[list[list[int]], torch.Tensor],",
      "        video_grid_thw: Union[list[list[int]], torch.Tensor],",
      "        context_len: int = 0,",
      "        seq_len: Optional[int] = None,",
      "    ) -> tuple[torch.Tensor, int]:",
      "        \"\"\"Get mrope input positions and delta value for GLM4V.\"\"\"",
      "",
      "        image_token_id = hf_config.image_token_id",
      "        video_start_token_id = hf_config.video_start_token_id",
      "        video_end_token_id = hf_config.video_end_token_id",
      "        spatial_merge_size = hf_config.vision_config.spatial_merge_size",
      "        llm_pos_ids_list: list = []",
      "",
      "        if not (image_grid_thw is None and video_grid_thw is None):",
      "            if isinstance(image_grid_thw, torch.Tensor):",
      "                image_grid_thw = image_grid_thw.tolist()",
      "",
      "            input_token_type: list[str] = []",
      "            video_check_flg = False",
      "            for token in input_tokens:",
      "                if token == video_start_token_id:",
      "                    video_check_flg = True",
      "                elif token == video_end_token_id:",
      "                    video_check_flg = False",
      "",
      "                if (token == image_token_id) and (video_check_flg is False):",
      "                    input_token_type.append(\"image\")",
      "                elif (token == image_token_id) and (video_check_flg is True):",
      "                    input_token_type.append(\"video\")",
      "                else:",
      "                    input_token_type.append(\"text\")",
      "",
      "            input_type_group: list[tuple[str, int, int]] = []",
      "            for key, group_iter in itertools.groupby(",
      "                    enumerate(input_token_type), lambda x: x[1]):",
      "                group_list = list(group_iter)",
      "                start_index = group_list[0][0]",
      "                end_index = group_list[-1][0] + 1",
      "                input_type_group.append((key, start_index, end_index))",
      "",
      "            video_frame_num = 1",
      "            mm_data_idx = 0",
      "            for modality_type, start_idx, end_idx in input_type_group:",
      "                st_idx = llm_pos_ids_list[-1].max() + 1 if len(",
      "                    llm_pos_ids_list) > 0 else 0",
      "                if modality_type == \"image\":",
      "                    t, h, w = (",
      "                        image_grid_thw[mm_data_idx][0],",
      "                        image_grid_thw[mm_data_idx][1],",
      "                        image_grid_thw[mm_data_idx][2],",
      "                    )",
      "                    llm_grid_t, llm_grid_h, llm_grid_w = \\",
      "                        t, h // spatial_merge_size, w // spatial_merge_size",
      "",
      "                    t_index = torch.arange(llm_grid_t).view(-1, 1).expand(",
      "                        -1, llm_grid_h * llm_grid_w).flatten()",
      "                    h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(",
      "                        llm_grid_t, -1, llm_grid_w).flatten()",
      "                    w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(",
      "                        llm_grid_t, llm_grid_h, -1).flatten()",
      "                    llm_pos_ids_list.append(",
      "                        torch.stack([t_index, h_index, w_index]) + st_idx)",
      "                    mm_data_idx += 1",
      "",
      "                elif modality_type == \"video\":",
      "                    t, h, w = (",
      "                        video_frame_num,",
      "                        image_grid_thw[mm_data_idx][1],",
      "                        image_grid_thw[mm_data_idx][2],",
      "                    )",
      "                    llm_grid_t, llm_grid_h, llm_grid_w = \\",
      "                        t, h // spatial_merge_size, w // spatial_merge_size",
      "",
      "                    for t_idx in range(llm_grid_t):",
      "                        t_index = torch.tensor(t_idx).view(-1, 1).expand(",
      "                            -1, llm_grid_h * llm_grid_w).flatten()",
      "                        h_index = torch.arange(llm_grid_h).view(",
      "                            1, -1, 1).expand(1, -1, llm_grid_w).flatten()",
      "                        w_index = torch.arange(llm_grid_w).view(",
      "                            1, 1, -1).expand(1, llm_grid_h, -1).flatten()",
      "                        llm_pos_ids_list.append(",
      "                            torch.stack([t_index, h_index, w_index]) + st_idx)",
      "",
      "                    mm_data_idx += 1",
      "                    video_frame_num += 1",
      "",
      "                else:",
      "                    text_len = end_idx - start_idx",
      "                    llm_pos_ids_list.append(",
      "                        torch.arange(text_len).view(1, -1).expand(3, -1) +",
      "                        st_idx)",
      "                    video_frame_num = 1",
      "",
      "        else:",
      "            text_len = len(input_tokens)",
      "            llm_pos_ids_list.append(",
      "                torch.arange(text_len).view(1, -1).expand(3, -1))",
      "",
      "        llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)",
      "        llm_positions = llm_positions[:, context_len:seq_len]",
      "        mrope_position_delta = (llm_positions.max() + 1 -",
      "                                len(input_tokens)).item()",
      "        return llm_positions, mrope_position_delta",
      "",
      "    @classmethod",
      "    def _ernie_get_input_positions_tensor(",
      "        cls,",
      "        input_tokens: list[int],",
      "        hf_config: PretrainedConfig,",
      "        image_grid_thw: Union[list[list[int]], torch.Tensor],",
      "        video_grid_thw: Union[list[list[int]], torch.Tensor],",
      "        context_len: int = 0,",
      "        seq_len: Optional[int] = None,",
      "    ) -> tuple[torch.Tensor, int]:",
      "        \"\"\"Get mrope input positions and delta value for Ernie VL.\"\"\"",
      "",
      "        image_token_id = hf_config.im_patch_id",
      "        video_start_token_id = hf_config.video_start_token_id",
      "        video_end_token_id = hf_config.video_end_token_id",
      "        spatial_conv_size = hf_config.spatial_conv_size",
      "        temporal_conv_size = hf_config.temporal_conv_size",
      "        llm_pos_ids_list: list = []",
      "",
      "        if not (image_grid_thw is None and video_grid_thw is None):",
      "            if isinstance(image_grid_thw, torch.Tensor):",
      "                image_grid_thw = image_grid_thw.tolist()",
      "",
      "            input_token_type: list[str] = []",
      "            video_check_flg = False",
      "            for token in input_tokens:",
      "                if token == video_start_token_id:",
      "                    video_check_flg = True",
      "                elif token == video_end_token_id:",
      "                    video_check_flg = False",
      "",
      "                if (token == image_token_id) and (video_check_flg is False):",
      "                    input_token_type.append(\"image\")",
      "                elif (token == image_token_id) and (video_check_flg is True):",
      "                    input_token_type.append(\"video\")",
      "                else:",
      "                    input_token_type.append(\"text\")",
      "",
      "            input_type_group: list[tuple[str, int, int]] = []",
      "            for key, group_iter in itertools.groupby(",
      "                    enumerate(input_token_type), lambda x: x[1]):",
      "                group_list = list(group_iter)",
      "                start_index = group_list[0][0]",
      "                end_index = group_list[-1][0] + 1",
      "                input_type_group.append((key, start_index, end_index))",
      "",
      "            video_frame_num = 1",
      "            mm_data_idx = 0",
      "            for modality_type, start_idx, end_idx in input_type_group:",
      "                st_idx = llm_pos_ids_list[-1].max() + 1 if len(",
      "                    llm_pos_ids_list) > 0 else 0",
      "                if modality_type == \"image\":",
      "                    t, h, w = (",
      "                        image_grid_thw[mm_data_idx][0],",
      "                        image_grid_thw[mm_data_idx][1],",
      "                        image_grid_thw[mm_data_idx][2],",
      "                    )",
      "                    llm_grid_t, llm_grid_h, llm_grid_w = \\",
      "                        t, h // spatial_conv_size, w // spatial_conv_size",
      "",
      "                    t_index = torch.arange(llm_grid_t).view(-1, 1).expand(",
      "                        -1, llm_grid_h * llm_grid_w).flatten()",
      "                    h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(",
      "                        llm_grid_t, -1, llm_grid_w).flatten()",
      "                    w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(",
      "                        llm_grid_t, llm_grid_h, -1).flatten()",
      "                    llm_pos_ids_list.append(",
      "                        torch.stack([t_index, h_index, w_index]) + st_idx)",
      "                    mm_data_idx += 1",
      "",
      "                elif modality_type == \"video\":",
      "                    t, h, w = (",
      "                        video_grid_thw[mm_data_idx][0],",
      "                        video_grid_thw[mm_data_idx][1],",
      "                        video_grid_thw[mm_data_idx][2],",
      "                    )",
      "                    llm_grid_t, llm_grid_h, llm_grid_w = (t //",
      "                                                          temporal_conv_size,",
      "                                                          h //",
      "                                                          spatial_conv_size,",
      "                                                          w //",
      "                                                          spatial_conv_size)",
      "",
      "                    for t_idx in range(llm_grid_t):",
      "                        t_index = torch.tensor(t_idx).view(-1, 1).expand(",
      "                            -1, llm_grid_h * llm_grid_w).flatten()",
      "                        h_index = torch.arange(llm_grid_h).view(",
      "                            1, -1, 1).expand(1, -1, llm_grid_w).flatten()",
      "                        w_index = torch.arange(llm_grid_w).view(",
      "                            1, 1, -1).expand(1, llm_grid_h, -1).flatten()",
      "                        llm_pos_ids_list.append(",
      "                            torch.stack([t_index, h_index, w_index]) + st_idx)",
      "",
      "                    mm_data_idx += 1",
      "                    video_frame_num += 1",
      "",
      "                else:",
      "                    text_len = end_idx - start_idx",
      "                    llm_pos_ids_list.append(",
      "                        torch.arange(text_len).view(1, -1).expand(3, -1) +",
      "                        st_idx)",
      "                    video_frame_num = 1",
      "",
      "        else:",
      "            text_len = len(input_tokens)",
      "            llm_pos_ids_list.append(",
      "                torch.arange(text_len).view(1, -1).expand(3, -1))",
      "",
      "        llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)",
      "        llm_positions = llm_positions[:, context_len:seq_len]",
      "        mrope_position_delta = (llm_positions.max() + 1 -",
      "                                len(input_tokens)).item()",
      "        return llm_positions, mrope_position_delta",
      "",
      "    @classmethod",
      "    def _keye_get_input_positions_tensor(",
      "        cls,",
      "        input_tokens: list[int],",
      "        hf_config: PretrainedConfig,",
      "        image_grid_thw: Union[list[list[int]], torch.Tensor],",
      "        video_grid_thw: Union[list[list[int]], torch.Tensor],",
      "        context_len: int = 0,",
      "        seq_len: Optional[int] = None,",
      "    ) -> tuple[torch.Tensor, int]:",
      "        if isinstance(video_grid_thw, list) and len(video_grid_thw) > 0:",
      "            video_grid_thw = video_grid_thw[0]",
      "        \"\"\"Get mrope input positions and delta value (Keye series).\"\"\"",
      "",
      "        def split_thw(",
      "                grid_thw: Union[torch.Tensor, list[int]]) -> list[list[int]]:",
      "            \"\"\"",
      "            Split grid_thw along the t dimension.",
      "",
      "            Args:",
      "                grid_thw: shape [N, 3] tensor or nested list of [t, h, w].",
      "",
      "            Returns:",
      "                List of [1, h, w] rows, repeated t times for each original row.",
      "            \"\"\"",
      "",
      "            if isinstance(grid_thw, list):",
      "                grid_thw = torch.tensor(grid_thw, dtype=torch.long)",
      "",
      "            if grid_thw.numel() == 0:",
      "                return []",
      "",
      "            t, hw = grid_thw[:, 0], grid_thw[:, 1:]",
      "            ones = torch.ones_like(hw[:, :1])  # [N,1]",
      "            out = torch.cat([ones, hw], dim=1).repeat_interleave(t, dim=0)",
      "            return out.tolist()",
      "",
      "        video_grid_thw = split_thw(video_grid_thw)",
      "",
      "        image_token_id = hf_config.image_token_id",
      "        video_token_id = hf_config.video_token_id",
      "        spatial_merge_size = hf_config.vision_config.spatial_merge_size",
      "",
      "        image_nums = len(image_grid_thw)",
      "        frame_nums = len(video_grid_thw)",
      "        llm_pos_ids_list: list = []",
      "",
      "        st = 0",
      "        remain_images, remain_frames = image_nums, frame_nums",
      "",
      "        image_index, video_index = 0, 0",
      "        for _ in range(image_nums + frame_nums):",
      "            if remain_images > 0:",
      "                try:",
      "                    ed_image = input_tokens.index(image_token_id, st)",
      "                except ValueError:",
      "                    ed_image = len(input_tokens) + 1",
      "            else:",
      "                ed_image = len(input_tokens) + 1",
      "            if remain_frames > 0:",
      "                try:",
      "                    ed_video = input_tokens.index(video_token_id, st)",
      "                except ValueError:",
      "                    ed_video = len(input_tokens) + 1",
      "            else:",
      "                ed_video = len(input_tokens) + 1",
      "",
      "            if ed_image < ed_video:",
      "                t, h, w = (",
      "                    image_grid_thw[image_index][0],",
      "                    image_grid_thw[image_index][1],",
      "                    image_grid_thw[image_index][2],",
      "                )",
      "                image_index += 1",
      "                remain_images -= 1",
      "                ed = ed_image",
      "            else:",
      "                t, h, w = (",
      "                    video_grid_thw[video_index][0],",
      "                    video_grid_thw[video_index][1],",
      "                    video_grid_thw[video_index][2],",
      "                )",
      "                video_index += 1",
      "                remain_frames -= 1",
      "                ed = ed_video",
      "",
      "            llm_grid_t, llm_grid_h, llm_grid_w = \\",
      "                t, h // spatial_merge_size, w // spatial_merge_size",
      "            text_len = ed - st",
      "",
      "            st_idx = llm_pos_ids_list[-1].max() + 1 if len(",
      "                llm_pos_ids_list) > 0 else 0",
      "            llm_pos_ids_list.append(",
      "                torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)",
      "",
      "            t_index = (torch.arange(llm_grid_t).view(-1, 1).expand(",
      "                -1, llm_grid_h * llm_grid_w)).long().flatten()",
      "",
      "            h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(",
      "                llm_grid_t, -1, llm_grid_w).flatten()",
      "            w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(",
      "                llm_grid_t, llm_grid_h, -1).flatten()",
      "            llm_pos_ids_list.append(",
      "                torch.stack([t_index, h_index, w_index]) + text_len + st_idx)",
      "            st = ed + llm_grid_t * llm_grid_h * llm_grid_w",
      "",
      "        if st < len(input_tokens):",
      "            st_idx = llm_pos_ids_list[-1].max() + 1 if len(",
      "                llm_pos_ids_list) > 0 else 0",
      "            text_len = len(input_tokens) - st",
      "            llm_pos_ids_list.append(",
      "                torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)",
      "",
      "        llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)",
      "        mrope_position_delta = (llm_positions.max() + 1 -",
      "                                len(input_tokens)).item()",
      "        llm_positions = llm_positions[:, context_len:seq_len]",
      "",
      "        return llm_positions, mrope_position_delta",
      "",
      "    @classmethod",
      "    def _vl_get_input_positions_tensor(",
      "        cls,",
      "        input_tokens: list[int],",
      "        hf_config: PretrainedConfig,",
      "        image_grid_thw: Union[list[list[int]], torch.Tensor],",
      "        video_grid_thw: Union[list[list[int]], torch.Tensor],",
      "        second_per_grid_ts: list[float],",
      "        context_len: int = 0,",
      "        seq_len: Optional[int] = None,",
      "    ) -> tuple[torch.Tensor, int]:",
      "        \"\"\"Get mrope input positions and delta value.\"\"\"",
      "",
      "        image_token_id = hf_config.image_token_id",
      "        video_token_id = hf_config.video_token_id",
      "        vision_start_token_id = hf_config.vision_start_token_id",
      "        spatial_merge_size = hf_config.vision_config.spatial_merge_size",
      "        tokens_per_second = getattr(hf_config.vision_config,",
      "                                    \"tokens_per_second\", 1.0)",
      "",
      "        input_tokens_tensor = torch.tensor(input_tokens)",
      "        vision_start_indices = torch.argwhere(",
      "            input_tokens_tensor == vision_start_token_id).squeeze(1)",
      "        vision_tokens = input_tokens_tensor[vision_start_indices + 1]",
      "        image_nums = (vision_tokens == image_token_id).sum()",
      "        video_nums = (vision_tokens == video_token_id).sum()",
      "        llm_pos_ids_list: list = []",
      "",
      "        st = 0",
      "        remain_images, remain_videos = image_nums, video_nums",
      "",
      "        image_index, video_index = 0, 0",
      "        for _ in range(image_nums + video_nums):",
      "            video_second_per_grid_t = 0.0",
      "            if remain_images > 0:",
      "                try:",
      "                    ed_image = input_tokens.index(image_token_id, st)",
      "                except ValueError:",
      "                    ed_image = len(input_tokens) + 1",
      "            else:",
      "                ed_image = len(input_tokens) + 1",
      "            if remain_videos > 0:",
      "                try:",
      "                    ed_video = input_tokens.index(video_token_id, st)",
      "                except ValueError:",
      "                    ed_video = len(input_tokens) + 1",
      "            else:",
      "                ed_video = len(input_tokens) + 1",
      "            if ed_image < ed_video:",
      "                t, h, w = (",
      "                    image_grid_thw[image_index][0],",
      "                    image_grid_thw[image_index][1],",
      "                    image_grid_thw[image_index][2],",
      "                )",
      "                image_index += 1",
      "                remain_images -= 1",
      "                ed = ed_image",
      "            else:",
      "                t, h, w = (",
      "                    video_grid_thw[video_index][0],",
      "                    video_grid_thw[video_index][1],",
      "                    video_grid_thw[video_index][2],",
      "                )",
      "                video_second_per_grid_t = 1.0",
      "                if second_per_grid_ts:",
      "                    video_second_per_grid_t = second_per_grid_ts[video_index]",
      "                video_index += 1",
      "                remain_videos -= 1",
      "                ed = ed_video",
      "",
      "            llm_grid_t, llm_grid_h, llm_grid_w = \\",
      "                t, h // spatial_merge_size, w // spatial_merge_size",
      "            text_len = ed - st",
      "",
      "            st_idx = llm_pos_ids_list[-1].max() + 1 if len(",
      "                llm_pos_ids_list) > 0 else 0",
      "            llm_pos_ids_list.append(",
      "                torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)",
      "",
      "            t_index = (torch.arange(llm_grid_t).view(-1, 1).expand(",
      "                -1, llm_grid_h * llm_grid_w) * video_second_per_grid_t *",
      "                       tokens_per_second).long().flatten()",
      "",
      "            h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(",
      "                llm_grid_t, -1, llm_grid_w).flatten()",
      "            w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(",
      "                llm_grid_t, llm_grid_h, -1).flatten()",
      "            llm_pos_ids_list.append(",
      "                torch.stack([t_index, h_index, w_index]) + text_len + st_idx)",
      "            st = ed + llm_grid_t * llm_grid_h * llm_grid_w",
      "",
      "        if st < len(input_tokens):",
      "            st_idx = llm_pos_ids_list[-1].max() + 1 if len(",
      "                llm_pos_ids_list) > 0 else 0",
      "            text_len = len(input_tokens) - st",
      "            llm_pos_ids_list.append(",
      "                torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)",
      "",
      "        llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)",
      "        mrope_position_delta = (llm_positions.max() + 1 -",
      "                                len(input_tokens)).item()",
      "        llm_positions = llm_positions[:, context_len:seq_len]",
      "",
      "        return llm_positions, mrope_position_delta",
      "",
      "    @classmethod",
      "    def _omni_get_input_positions_tensor(",
      "        cls,",
      "        input_tokens: list[int],",
      "        hf_config: PretrainedConfig,",
      "        image_grid_thw: Union[list[list[int]], torch.Tensor],",
      "        video_grid_thw: Union[list[list[int]], torch.Tensor],",
      "        second_per_grid_ts: Optional[list[float]] = None,",
      "        context_len: int = 0,",
      "        seq_len: Optional[int] = None,",
      "        audio_feature_lengths: Optional[torch.Tensor] = None,",
      "        use_audio_in_video: bool = False,",
      "    ) -> tuple[torch.Tensor, int]:",
      "        \"\"\"Get mrope input positions and delta value (Qwen2.5-Omni version).",
      "",
      "        Differences from MRotaryEmbedding:",
      "            1. Add audio support (and related `audio_feature_lengths`).",
      "            2. Add `use_audio_in_video` option to read audio from video inputs.",
      "                In this case, audio and vision position ids will be split into",
      "                chunks and interleaved.",
      "",
      "        Example:",
      "",
      "            (V_i are vision position ids, A_i are audio position ids)",
      "",
      "            |V_1 ...    V_n|A_1 ...   A_n|V_n+1 ... V_2n|A_n+1 ... A_2n|...",
      "            |vision chunk 1|audio chunk 1|vision chunk 2|audio chunk 2 |...",
      "        \"\"\"",
      "",
      "        # TODO(fyabc): refactor and share more code with",
      "        #  _vl_get_input_positions_tensor.",
      "",
      "        thinker_config = hf_config.thinker_config",
      "        audio_token_id = thinker_config.audio_token_index",
      "        image_token_id = thinker_config.image_token_index",
      "        video_token_id = thinker_config.video_token_index",
      "        audio_start_token_id = thinker_config.audio_start_token_id",
      "        audio_end_token_id = thinker_config.audio_end_token_id",
      "        vision_start_token_id = thinker_config.vision_start_token_id",
      "        vision_end_token_id = thinker_config.vision_end_token_id",
      "        seconds_per_chunk = thinker_config.seconds_per_chunk",
      "        spatial_merge_size = thinker_config.vision_config.spatial_merge_size",
      "        tokens_per_second = getattr(thinker_config.vision_config,",
      "                                    \"tokens_per_second\", 25)",
      "",
      "        if isinstance(image_grid_thw, list):",
      "            image_grid_thw = torch.tensor(image_grid_thw)",
      "        if isinstance(video_grid_thw, list):",
      "            video_grid_thw = torch.tensor(video_grid_thw)",
      "",
      "        src_item = input_tokens",
      "        audio_seqlens = audio_feature_lengths",
      "        if not second_per_grid_ts:",
      "            second_per_grid_ts = [1] * video_grid_thw.shape[0]",
      "        audio_idx = 0",
      "        video_idx = 0",
      "        image_idx = 0",
      "        new_src_item: list[int] = []",
      "        llm_pos_ids_list: list[torch.Tensor] = []",
      "",
      "        idx = 0",
      "        while idx < len(src_item):",
      "            new_src_item_len = len(new_src_item)",
      "            start_idx = llm_pos_ids_list[-1].max() + 1 if len(",
      "                llm_pos_ids_list) > 0 else 0",
      "            if src_item[idx] not in [",
      "                    audio_token_id, video_token_id, image_token_id",
      "            ]:",
      "                if use_audio_in_video and idx > 0:",
      "                    if src_item[idx] == vision_end_token_id and \\",
      "                        src_item[idx - 1] == audio_end_token_id:",
      "                        # processing the <|audio_eos|> before <|vision_eos|>",
      "                        start_idx -= 1",
      "                    elif src_item[idx] == audio_start_token_id and \\",
      "                        src_item[idx - 1] == vision_start_token_id:",
      "                        # processing the <|audio_bos|> after <|vision_eos|>",
      "                        start_idx -= 1",
      "                new_src_item.append(src_item[idx])",
      "                llm_pos_ids = torch.tensor([start_idx],",
      "                                           dtype=torch.long).expand(3, -1)",
      "                llm_pos_ids_list.append(llm_pos_ids)",
      "            elif src_item[idx] == audio_token_id:",
      "                assert audio_seqlens is not None",
      "                audio_seqlen = audio_seqlens[audio_idx]",
      "                place_num = (((audio_seqlen - 1) // 2 + 1 - 2) // 2 + 1)",
      "                new_src_item.extend([audio_token_id] * place_num)",
      "                llm_pos_ids = torch.arange(place_num).expand(3, -1) + start_idx",
      "                llm_pos_ids_list.append(llm_pos_ids)",
      "                audio_idx += 1",
      "            elif src_item[idx] == image_token_id:",
      "                grid_t = image_grid_thw[image_idx][0]",
      "                grid_hs = image_grid_thw[:, 1]",
      "                grid_ws = image_grid_thw[:, 2]",
      "                t_index = (torch.arange(grid_t) * 1 * tokens_per_second).long()",
      "                llm_pos_ids = cls._get_llm_pos_ids_for_vision(",
      "                    start_idx, image_idx, spatial_merge_size, t_index, grid_hs,",
      "                    grid_ws)",
      "                llm_pos_ids_list.append(llm_pos_ids)",
      "                vision_seqlen = image_grid_thw[image_idx].prod() // (",
      "                    spatial_merge_size**2)",
      "                new_src_item.extend([image_token_id] * vision_seqlen)",
      "                image_idx += 1",
      "            elif src_item[idx] == video_token_id and not use_audio_in_video:",
      "                grid_t = video_grid_thw[video_idx][0]",
      "                grid_hs = video_grid_thw[:, 1]",
      "                grid_ws = video_grid_thw[:, 2]",
      "                t_index = (torch.arange(grid_t) *",
      "                           second_per_grid_ts[video_idx] *",
      "                           tokens_per_second).long()",
      "                llm_pos_ids = cls._get_llm_pos_ids_for_vision(",
      "                    start_idx, video_idx, spatial_merge_size, t_index, grid_hs,",
      "                    grid_ws)",
      "                llm_pos_ids_list.append(llm_pos_ids)",
      "                vision_seqlen = video_grid_thw[video_idx].prod() // (",
      "                    spatial_merge_size**2)",
      "                new_src_item.extend([video_token_id] * vision_seqlen)",
      "                video_idx += 1",
      "            else:",
      "                # read audio from video",
      "                assert audio_seqlens is not None",
      "                audio_seqlen = audio_seqlens[audio_idx]",
      "                vision_seqlen = video_grid_thw[video_idx].prod() // (",
      "                    spatial_merge_size**2)",
      "                grid_t = video_grid_thw[video_idx][0]",
      "                grid_h = video_grid_thw[video_idx][1]",
      "                grid_w = video_grid_thw[video_idx][2]",
      "                grid_hs = video_grid_thw[:, 1]",
      "                grid_ws = video_grid_thw[:, 2]",
      "                t_ntoken_per_chunk = int(tokens_per_second * seconds_per_chunk)",
      "                t_index = (torch.arange(grid_t) *",
      "                           second_per_grid_ts[video_idx] *",
      "                           tokens_per_second).long()",
      "                t_index_split_chunk = cls._split_list_into_ranges(",
      "                    t_index, t_ntoken_per_chunk)",
      "                place_num = (((audio_seqlen - 1) // 2 + 1 - 2) // 2 + 1) + 2",
      "                pure_audio_len = place_num - 2",
      "                added_audio_len = 0",
      "                audio_llm_pos_ids_list: list[torch.Tensor] = []",
      "                for t_chunk in t_index_split_chunk:",
      "                    vision_ntoken_per_chunk = len(",
      "                        t_chunk) * grid_h * grid_w // (spatial_merge_size**2)",
      "                    new_src_item.extend([video_token_id] *",
      "                                        vision_ntoken_per_chunk)",
      "                    vision_llm_pos_ids_list = cls._get_llm_pos_ids_for_vision(",
      "                        start_idx, video_idx, spatial_merge_size, t_chunk,",
      "                        grid_hs, grid_ws).split(1, dim=1)",
      "                    llm_pos_ids_list.extend(vision_llm_pos_ids_list)",
      "                    new_src_item.extend(",
      "                        min(t_ntoken_per_chunk, pure_audio_len -",
      "                            added_audio_len) * [audio_token_id])",
      "                    audio_start_idx = start_idx if len(",
      "                        audio_llm_pos_ids_list",
      "                    ) == 0 else audio_llm_pos_ids_list[-1][0].item() + 1",
      "                    if min(t_ntoken_per_chunk,",
      "                           pure_audio_len - added_audio_len) > 0:",
      "                        audio_llm_pos_ids_list = (torch.arange(",
      "                            min(t_ntoken_per_chunk, pure_audio_len -",
      "                                added_audio_len)).expand(3, -1) +",
      "                                                  audio_start_idx).split(1,",
      "                                                                         dim=1)",
      "                    else:",
      "                        audio_llm_pos_ids_list = []",
      "                    added_audio_len += min(t_ntoken_per_chunk,",
      "                                           pure_audio_len - added_audio_len)",
      "                    llm_pos_ids_list.extend(audio_llm_pos_ids_list)",
      "                if added_audio_len < pure_audio_len:",
      "                    new_src_item.extend(",
      "                        (pure_audio_len - added_audio_len) * [audio_token_id])",
      "                    audio_llm_pos_ids_list = (",
      "                        torch.arange(pure_audio_len - added_audio_len).expand(",
      "                            3, -1) + llm_pos_ids_list[-1].max() + 1).split(",
      "                                1, dim=1)",
      "                    llm_pos_ids_list.extend(audio_llm_pos_ids_list)",
      "                audio_idx += 1",
      "                video_idx += 1",
      "            # move to the next token",
      "            idx += len(new_src_item) - new_src_item_len",
      "",
      "        llm_positions = torch.cat(llm_pos_ids_list, dim=1)",
      "        mrope_position_delta = torch.cat(llm_pos_ids_list,",
      "                                         dim=1).max() + 1 - len(src_item)",
      "        llm_positions = llm_positions[:, context_len:seq_len]",
      "",
      "        return llm_positions, mrope_position_delta",
      "",
      "    @staticmethod",
      "    def _get_llm_pos_ids_for_vision(",
      "        start_idx: int,",
      "        vision_idx: int,",
      "        spatial_merge_size: int,",
      "        t_index: list[int],",
      "        grid_hs: torch.Tensor,",
      "        grid_ws: torch.Tensor,",
      "    ) -> torch.Tensor:",
      "        llm_pos_ids_list = []",
      "        llm_grid_h = grid_hs[vision_idx] // spatial_merge_size",
      "        llm_grid_w = grid_ws[vision_idx] // spatial_merge_size",
      "        h_index = (torch.arange(llm_grid_h).view(1, -1, 1).expand(",
      "            len(t_index), -1, llm_grid_w).flatten())",
      "        w_index = (torch.arange(llm_grid_w).view(1, 1, -1).expand(",
      "            len(t_index), llm_grid_h, -1).flatten())",
      "        t_index_tensor = torch.Tensor(t_index).to(llm_grid_h.device).view(",
      "            -1, 1).expand(-1, llm_grid_h * llm_grid_w).long().flatten()",
      "        _llm_pos_ids = torch.stack([t_index_tensor, h_index, w_index])",
      "        llm_pos_ids_list.append(_llm_pos_ids + start_idx)",
      "        llm_pos_ids = torch.cat(llm_pos_ids_list, dim=1)",
      "        return llm_pos_ids",
      "",
      "    @staticmethod",
      "    def _split_list_into_ranges(lst: torch.Tensor,",
      "                                interval: int) -> list[list[int]]:",
      "        ranges: list[list[int]] = [[]",
      "                                   for _ in range((max(lst) // interval) + 1)]",
      "        for num in lst:",
      "            index = num // interval",
      "            ranges[index].append(num)",
      "        return ranges",
      "",
      "    @staticmethod",
      "    def get_next_input_positions(",
      "        mrope_position_delta: int,",
      "        context_len: int,",
      "        seq_len: int,",
      "    ) -> list[list[int]]:",
      "        return [",
      "            list(",
      "                range(context_len + mrope_position_delta,",
      "                      seq_len + mrope_position_delta)) for _ in range(3)",
      "        ]",
      "",
      "    @staticmethod",
      "    def get_next_input_positions_tensor(out: np.ndarray, out_offset: int,",
      "                                        mrope_position_delta: int,",
      "                                        context_len: int, num_new_tokens: int):",
      "",
      "        values = np.arange(mrope_position_delta + context_len,",
      "                           mrope_position_delta + context_len + num_new_tokens,",
      "                           dtype=out.dtype)",
      "        out[:, out_offset:out_offset + num_new_tokens] = values",
      "",
      "    @classmethod",
      "    def omni_get_updates_use_audio_in_video(",
      "        cls,",
      "        thinker_config: PretrainedConfig,",
      "        audio_len: int,",
      "        video_grid_thw: Union[list[int], torch.Tensor],",
      "        video_second_per_grid_t: float,",
      "    ) -> list[int]:",
      "        \"\"\"Get video prompt updates when `use_audio_in_video` is True.",
      "",
      "        In this case, audio and vision update ids will be split into",
      "        chunks and interleaved (details in `_omni_get_input_positions_tensor`).",
      "",
      "        <|video_bos|><|VIDEO|><|video_eos|> =>",
      "        <|video_bos|><|audio_bos|>(... chunks ...)<|audio_eos|><|video_eos|>",
      "        \"\"\"",
      "",
      "        audio_token_id = thinker_config.audio_token_index",
      "        video_token_id = thinker_config.video_token_index",
      "        audio_start_token_id = thinker_config.audio_start_token_id",
      "        audio_end_token_id = thinker_config.audio_end_token_id",
      "        seconds_per_chunk = thinker_config.seconds_per_chunk",
      "        spatial_merge_size = thinker_config.vision_config.spatial_merge_size",
      "        tokens_per_second = getattr(thinker_config.vision_config,",
      "                                    \"tokens_per_second\", 25)",
      "",
      "        grid_t = video_grid_thw[0]",
      "        grid_h = video_grid_thw[1]",
      "        grid_w = video_grid_thw[2]",
      "        t_ntoken_per_chunk = int(tokens_per_second * seconds_per_chunk)",
      "        t_index = (torch.arange(grid_t) * video_second_per_grid_t *",
      "                   tokens_per_second).long()",
      "        t_index_split_chunk = cls._split_list_into_ranges(",
      "            t_index, t_ntoken_per_chunk)",
      "",
      "        updates = [audio_start_token_id]",
      "        added_audio_len = 0",
      "        for t_chunk in t_index_split_chunk:",
      "            vision_ntoken_per_chunk = len(t_chunk) * grid_h * grid_w // (",
      "                spatial_merge_size**2)",
      "            updates.extend([video_token_id] * vision_ntoken_per_chunk)",
      "",
      "            audio_chunk_size = min(t_ntoken_per_chunk,",
      "                                   audio_len - added_audio_len)",
      "            updates.extend(audio_chunk_size * [audio_token_id])",
      "            added_audio_len += audio_chunk_size",
      "        if added_audio_len < audio_len:",
      "            updates.extend((audio_len - added_audio_len) * [audio_token_id])",
      "        updates.extend([audio_end_token_id])",
      "",
      "        return updates"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/quantization/awq_triton.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "import torch",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "AWQ_TRITON_SUPPORTED_GROUP_SIZES = [-1, 32, 64, 128]",
      "",
      "",
      "@triton.jit",
      "def awq_dequantize_kernel(",
      "        qweight_ptr,  # quantized matrix",
      "        scales_ptr,  # scales, per group",
      "        zeros_ptr,  # zeros, per group",
      "        group_size,  # Should always be one of the supported group sizes",
      "        result_ptr,  # Output matrix",
      "        num_cols,  # input num cols in qweight",
      "        num_rows,  # input num rows in qweight",
      "        BLOCK_SIZE_X: tl.constexpr,",
      "        BLOCK_SIZE_Y: tl.constexpr):",
      "    # Set up the pids.",
      "    pid_x = tl.program_id(axis=0)",
      "    pid_y = tl.program_id(axis=1)",
      "",
      "    # Compute offsets and masks for qweight_ptr.",
      "    offsets_y = pid_y * BLOCK_SIZE_Y + tl.arange(0, BLOCK_SIZE_Y)",
      "    offsets_x = pid_x * BLOCK_SIZE_X + tl.arange(0, BLOCK_SIZE_X)",
      "    offsets = num_cols * offsets_y[:, None] + offsets_x[None, :]",
      "",
      "    masks_y = offsets_y < num_rows",
      "    masks_x = offsets_x < num_cols",
      "",
      "    masks = masks_y[:, None] & masks_x[None, :]",
      "",
      "    # Compute offsets and masks for result output ptr.",
      "    result_offsets_y = pid_y * BLOCK_SIZE_Y + tl.arange(0, BLOCK_SIZE_Y)",
      "    result_offsets_x = pid_x * BLOCK_SIZE_X * 8 + tl.arange(",
      "        0, BLOCK_SIZE_X * 8)",
      "    result_offsets = (8 * num_cols * result_offsets_y[:, None] +",
      "                      result_offsets_x[None, :])",
      "",
      "    result_masks_y = result_offsets_y < num_rows",
      "    result_masks_x = result_offsets_x < num_cols * 8",
      "    result_masks = result_masks_y[:, None] & result_masks_x[None, :]",
      "",
      "    # Load the weights.",
      "    iweights = tl.load(qweight_ptr + offsets, masks, 0.0)",
      "    iweights = tl.interleave(iweights, iweights)",
      "    iweights = tl.interleave(iweights, iweights)",
      "    iweights = tl.interleave(iweights, iweights)",
      "",
      "    # Create reverse AWQ order as tensor: [0, 4, 1, 5, 2, 6, 3, 7]",
      "    # that will map given indices to the correct order.",
      "    reverse_awq_order_tensor = ((tl.arange(0, 2) * 4)[None, :] +",
      "                                tl.arange(0, 4)[:, None]).reshape(8)",
      "",
      "    # Use this to compute a set of shifts that can be used to unpack and",
      "    # reorder the values in iweights and zeros.",
      "    shifts = reverse_awq_order_tensor * 4",
      "    shifts = tl.broadcast_to(shifts[None, :], (BLOCK_SIZE_Y * BLOCK_SIZE_X, 8))",
      "    shifts = tl.reshape(shifts, (BLOCK_SIZE_Y, BLOCK_SIZE_X * 8))",
      "",
      "    # Unpack and reorder: shift out the correct 4-bit value and mask.",
      "    iweights = (iweights >> shifts) & 0xF",
      "",
      "    # Compute zero offsets and masks.",
      "    zero_offsets_y = pid_y * BLOCK_SIZE_Y // group_size + tl.arange(0, 1)",
      "    zero_offsets_x = pid_x * BLOCK_SIZE_X + tl.arange(0, BLOCK_SIZE_X)",
      "    zero_offsets = num_cols * zero_offsets_y[:, None] + zero_offsets_x[None, :]",
      "",
      "    zero_masks_y = zero_offsets_y < num_rows // group_size",
      "    zero_masks_x = zero_offsets_x < num_cols",
      "    zero_masks = zero_masks_y[:, None] & zero_masks_x[None, :]",
      "",
      "    # Load the zeros.",
      "    zeros = tl.load(zeros_ptr + zero_offsets, zero_masks, 0.0)",
      "    zeros = tl.interleave(zeros, zeros)",
      "    zeros = tl.interleave(zeros, zeros)",
      "    zeros = tl.interleave(zeros, zeros)",
      "    zeros = tl.broadcast_to(zeros, (BLOCK_SIZE_Y, BLOCK_SIZE_X * 8))",
      "",
      "    # Unpack and reorder: shift out the correct 4-bit value and mask.",
      "    zeros = (zeros >> shifts) & 0xF",
      "",
      "    # Compute scale offsets and masks.",
      "    scale_offsets_y = pid_y * BLOCK_SIZE_Y // group_size + tl.arange(0, 1)",
      "    scale_offsets_x = (pid_x * BLOCK_SIZE_X * 8 +",
      "                       tl.arange(0, BLOCK_SIZE_X * 8))",
      "    scale_offsets = (num_cols * 8 * scale_offsets_y[:, None] +",
      "                     scale_offsets_x[None, :])",
      "    scale_masks_y = scale_offsets_y < num_rows // group_size",
      "    scale_masks_x = scale_offsets_x < num_cols * 8",
      "    scale_masks = scale_masks_y[:, None] & scale_masks_x[None, :]",
      "",
      "    # Load the scales.",
      "    scales = tl.load(scales_ptr + scale_offsets, scale_masks, 0.0)",
      "    scales = tl.broadcast_to(scales, (BLOCK_SIZE_Y, BLOCK_SIZE_X * 8))",
      "",
      "    # Dequantize.",
      "    iweights = (iweights - zeros) * scales",
      "    iweights = iweights.to(result_ptr.type.element_ty)",
      "",
      "    # Finally, store.",
      "    tl.store(result_ptr + result_offsets, iweights, result_masks)",
      "",
      "",
      "@triton.jit",
      "def awq_gemm_kernel(a_ptr, b_ptr, c_ptr, zeros_ptr, scales_ptr, M, N, K,",
      "                    group_size, BLOCK_SIZE_M: tl.constexpr,",
      "                    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,",
      "                    SPLIT_K: tl.constexpr):",
      "    pid = tl.program_id(axis=0)",
      "    pid_z = tl.program_id(1)",
      "",
      "    # NOTE: This doesn't work in TRITON_INTERPRET=1 mode.  Use below instead.",
      "    # num_pid_n = (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "",
      "    pid_m = pid // num_pid_n",
      "    pid_n = pid % num_pid_n",
      "",
      "    accumulator_dtype = c_ptr.type.element_ty",
      "",
      "    # NOTE: This doesn't work in TRITON_INTERPRET=1 mode.  Use below instead.",
      "    # accumulator = tl.arange(0, BLOCK_SIZE_N)",
      "    # accumulator = tl.broadcast_to(accumulator[None, :],",
      "    # (BLOCK_SIZE_M, BLOCK_SIZE_N))",
      "    # accumulator = accumulator & 0x0",
      "    # accumulator = accumulator.to(accumulator_dtype)",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N),",
      "                           dtype=accumulator_dtype)",
      "",
      "    # Create reverse AWQ order as tensor: [0, 4, 1, 5, 2, 6, 3, 7]",
      "    # that will map given indices to the correct order.",
      "    reverse_awq_order_tensor = ((tl.arange(0, 2) * 4)[None, :] +",
      "                                tl.arange(0, 4)[:, None]).reshape(8)",
      "",
      "    # Create the necessary shifts to use to unpack.",
      "    shifts = reverse_awq_order_tensor * 4",
      "    shifts = tl.broadcast_to(shifts[None, :],",
      "                             (BLOCK_SIZE_K * (BLOCK_SIZE_N // 8), 8))",
      "    shifts = tl.reshape(shifts, (BLOCK_SIZE_K, BLOCK_SIZE_N))",
      "",
      "    # Offsets and masks.",
      "    offsets_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    masks_am = offsets_am < M",
      "",
      "    offsets_bn = pid_n * (BLOCK_SIZE_N // 8) + tl.arange(0, BLOCK_SIZE_N // 8)",
      "    masks_bn = offsets_bn < N // 8",
      "",
      "    offsets_zn = pid_n * (BLOCK_SIZE_N // 8) + tl.arange(0, BLOCK_SIZE_N // 8)",
      "    masks_zn = offsets_zn < N // 8",
      "",
      "    offsets_sn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    masks_sn = offsets_sn < N",
      "",
      "    offsets_k = pid_z * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)",
      "    offsets_a = K * offsets_am[:, None] + offsets_k[None, :]",
      "    offsets_b = (N // 8) * offsets_k[:, None] + offsets_bn[None, :]",
      "",
      "    a_ptrs = a_ptr + offsets_a",
      "    b_ptrs = b_ptr + offsets_b",
      "",
      "    # NOTE: Use this in TRITON_INTERPRET=1 mode instead of tl.cdiv",
      "    # block_offset = BLOCK_SIZE_K * SPLIT_K",
      "    # for k in range(0, (K + block_offset - 1) // (block_offset)):",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):",
      "        masks_k = offsets_k < K",
      "        masks_a = masks_am[:, None] & masks_k[None, :]",
      "        a = tl.load(a_ptrs, mask=masks_a, other=0.0)",
      "",
      "        masks_b = masks_k[:, None] & masks_bn[None, :]",
      "        b = tl.load(b_ptrs, mask=masks_b, other=0.0)",
      "        b = tl.interleave(b, b)",
      "        b = tl.interleave(b, b)",
      "        b = tl.interleave(b, b)",
      "",
      "        # Dequantize b.",
      "        offsets_szk = (",
      "            (BLOCK_SIZE_K * SPLIT_K * k + pid_z * BLOCK_SIZE_K) // group_size +",
      "            tl.arange(0, 1))",
      "        offsets_z = (N // 8) * offsets_szk[:, None] + offsets_zn[None, :]",
      "        masks_zk = offsets_szk < K // group_size",
      "        masks_z = masks_zk[:, None] & masks_zn[None, :]",
      "        zeros_ptrs = zeros_ptr + offsets_z",
      "        zeros = tl.load(zeros_ptrs, mask=masks_z, other=0.0)",
      "        zeros = tl.interleave(zeros, zeros)",
      "        zeros = tl.interleave(zeros, zeros)",
      "        zeros = tl.interleave(zeros, zeros)",
      "        zeros = tl.broadcast_to(zeros, (BLOCK_SIZE_K, BLOCK_SIZE_N))",
      "",
      "        offsets_s = N * offsets_szk[:, None] + offsets_sn[None, :]",
      "        masks_sk = offsets_szk < K // group_size",
      "        masks_s = masks_sk[:, None] & masks_sn[None, :]",
      "        scales_ptrs = scales_ptr + offsets_s",
      "        scales = tl.load(scales_ptrs, mask=masks_s, other=0.0)",
      "        scales = tl.broadcast_to(scales, (BLOCK_SIZE_K, BLOCK_SIZE_N))",
      "",
      "        b = (b >> shifts) & 0xF",
      "        zeros = (zeros >> shifts) & 0xF",
      "        b = (b - zeros) * scales",
      "        b = b.to(c_ptr.type.element_ty)",
      "",
      "        # Accumulate results.",
      "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)",
      "",
      "        offsets_k += BLOCK_SIZE_K * SPLIT_K",
      "        a_ptrs += BLOCK_SIZE_K * SPLIT_K",
      "        b_ptrs += BLOCK_SIZE_K * SPLIT_K * (N // 8)",
      "",
      "    c = accumulator.to(c_ptr.type.element_ty)",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + pid_z * N * M + N * offs_cm[:, None] + offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)",
      "",
      "",
      "# qweights - [K     , M // 8], int32",
      "# scales   - [K // G, M     ], float16",
      "# zeros    - [K // G, M // 8], int32",
      "def awq_dequantize_triton(qweight: torch.Tensor,",
      "                          scales: torch.Tensor,",
      "                          zeros: torch.Tensor,",
      "                          block_size_x: int = 32,",
      "                          block_size_y: int = 32) -> torch.Tensor:",
      "    K = qweight.shape[0]",
      "    M = scales.shape[1]",
      "    group_size = qweight.shape[0] // scales.shape[0]",
      "",
      "    assert K > 0 and M > 0",
      "    assert scales.shape[0] == K // group_size and scales.shape[1] == M",
      "    assert zeros.shape[0] == K // group_size and zeros.shape[1] == M // 8",
      "    assert group_size <= K",
      "    assert group_size in AWQ_TRITON_SUPPORTED_GROUP_SIZES or group_size == K",
      "",
      "    # Result tensor:",
      "    # number of rows = same as input tensor",
      "    # number of cols = 8 x input tensor num cols",
      "    result = torch.empty(qweight.shape[0],",
      "                         qweight.shape[1] * 8,",
      "                         device=qweight.device,",
      "                         dtype=scales.dtype)",
      "",
      "    Y = qweight.shape[0]  # num rows",
      "    X = qweight.shape[1]  # num cols",
      "",
      "    grid = lambda META: (",
      "        triton.cdiv(X, META['BLOCK_SIZE_X']),",
      "        triton.cdiv(Y, META['BLOCK_SIZE_Y']),",
      "    )",
      "    awq_dequantize_kernel[grid](qweight,",
      "                                scales,",
      "                                zeros,",
      "                                group_size,",
      "                                result,",
      "                                X,",
      "                                Y,",
      "                                BLOCK_SIZE_X=block_size_x,",
      "                                BLOCK_SIZE_Y=block_size_y)",
      "",
      "    return result",
      "",
      "",
      "# input   - [M, K]",
      "# qweight - [K, N // 8]",
      "# qzeros  - [K // G, N // 8]",
      "# scales  - [K // G, N]",
      "# split_k_iters - parallelism along K-dimension, int, power of 2.",
      "def awq_gemm_triton(input: torch.Tensor,",
      "                    qweight: torch.Tensor,",
      "                    scales: torch.Tensor,",
      "                    qzeros: torch.Tensor,",
      "                    split_k_iters: int,",
      "                    block_size_m: int = 32,",
      "                    block_size_n: int = 32,",
      "                    block_size_k: int = 32) -> torch.Tensor:",
      "    M, K = input.shape",
      "    N = qweight.shape[1] * 8",
      "    group_size = qweight.shape[0] // qzeros.shape[0]",
      "",
      "    assert N > 0 and K > 0 and M > 0",
      "    assert qweight.shape[0] == K and qweight.shape[1] == N // 8",
      "    assert qzeros.shape[0] == K // group_size and qzeros.shape[1] == N // 8",
      "    assert scales.shape[0] == K // group_size and scales.shape[1] == N",
      "    assert split_k_iters & (split_k_iters - 1) == 0 and split_k_iters != 0",
      "    assert split_k_iters <= 32",
      "    assert group_size <= K",
      "    assert group_size in AWQ_TRITON_SUPPORTED_GROUP_SIZES or group_size == K",
      "",
      "    grid = lambda META: (",
      "        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(",
      "            N, META['BLOCK_SIZE_N']),",
      "        split_k_iters,",
      "    )",
      "",
      "    result = torch.zeros((split_k_iters, M, N),",
      "                         dtype=scales.dtype,",
      "                         device=input.device)",
      "",
      "    # A = input, B = qweight, C = result",
      "    # A = M x K, B = K x N, C = M x N",
      "    awq_gemm_kernel[grid](input,",
      "                          qweight,",
      "                          result,",
      "                          qzeros,",
      "                          scales,",
      "                          M,",
      "                          N,",
      "                          K,",
      "                          group_size,",
      "                          BLOCK_SIZE_M=block_size_m,",
      "                          BLOCK_SIZE_N=block_size_n,",
      "                          BLOCK_SIZE_K=block_size_k,",
      "                          SPLIT_K=split_k_iters)",
      "",
      "    result = result.sum(0)",
      "",
      "    return result"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "from typing import Optional",
      "",
      "import torch",
      "",
      "from vllm.triton_utils import tl, triton",
      "",
      "",
      "def is_weak_contiguous(x: torch.Tensor):",
      "    strides = x.stride()",
      "    sizes = x.shape",
      "    is_not_transpose = strides[0] == 1 and (strides[1] >= max(1, sizes[0]))",
      "    is_transpose = strides[1] == 1 and (strides[0] >= max(1, sizes[1]))",
      "    return is_transpose or is_not_transpose",
      "",
      "",
      "@triton.jit",
      "def scaled_mm_kernel(a_ptr, b_ptr, scale_a_ptr, scale_b_ptr, c_ptr, bias_ptr,",
      "                     M, N, K, stride_am, stride_ak, stride_bk, stride_bn,",
      "                     stride_cm, stride_cn, ACCUMULATOR_DTYPE: tl.constexpr,",
      "                     BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,",
      "                     BLOCK_SIZE_K: tl.constexpr,",
      "                     BLOCK_SIZE_SCALE_A: tl.constexpr,",
      "                     BLOCK_SIZE_SCALE_B: tl.constexpr):",
      "    pid = tl.program_id(axis=0)",
      "",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "",
      "    pid_m = pid // num_pid_n",
      "    pid_n = pid % num_pid_n",
      "",
      "    accumulator_dtype = ACCUMULATOR_DTYPE",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N),",
      "                           dtype=accumulator_dtype)",
      "",
      "    # NOTE: Some tensor inputs are so large, they will cause int32 overflow",
      "    # so it is necessary to use tl.int64 for all the offsets, else SEGV will",
      "    # eventually occur.",
      "",
      "    # Offsets and masks.",
      "    offsets_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)",
      "    masks_am = offsets_am < M",
      "",
      "    offsets_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)",
      "    masks_bn = offsets_bn < N",
      "",
      "    offsets_k = tl.arange(0, BLOCK_SIZE_K).to(tl.int64)",
      "    offsets_a = (stride_am * offsets_am[:, None] +",
      "                 stride_ak * offsets_k[None, :])",
      "    offsets_b = (stride_bk * offsets_k[:, None] +",
      "                 stride_bn * offsets_bn[None, :])",
      "",
      "    # NOTE: BLOCK_SIZE_SCALE_A could be 1 or BLOCK_SIZE_M, so need to create",
      "    # appropriate offsets and masks for each case. Same goes for",
      "    # BLOCK_SIZE_SCALE_B.",
      "    offsets_scale_am = (tl.arange(0, BLOCK_SIZE_SCALE_A) +",
      "                        (BLOCK_SIZE_SCALE_A > 1) * pid_m * BLOCK_SIZE_M)",
      "    masks_scale_am = offsets_scale_am < M",
      "",
      "    offsets_scale_bn = (tl.arange(0, BLOCK_SIZE_SCALE_B) +",
      "                        (BLOCK_SIZE_SCALE_B > 1) * pid_n * BLOCK_SIZE_N)",
      "    masks_scale_bn = offsets_scale_bn < N",
      "",
      "    a_ptrs = a_ptr + offsets_a",
      "    b_ptrs = b_ptr + offsets_b",
      "",
      "    scale_a_ptrs = scale_a_ptr + offsets_scale_am",
      "    scale_b_ptrs = scale_b_ptr + offsets_scale_bn",
      "",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        masks_k = offsets_k < K",
      "        masks_a = masks_am[:, None] & masks_k[None, :]",
      "        a = tl.load(a_ptrs, mask=masks_a)",
      "",
      "        masks_b = masks_k[:, None] & masks_bn[None, :]",
      "        b = tl.load(b_ptrs, mask=masks_b)",
      "",
      "        # Accumulate results.",
      "        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)",
      "",
      "        offsets_k += BLOCK_SIZE_K",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    # Apply scale at end.",
      "    masks_scale_a = masks_scale_am[:, None] & (tl.arange(0, 1) < 1)[:, None]",
      "    scale_a = tl.load(scale_a_ptrs[:, None], masks_scale_a)",
      "    # Need to broadcast to the appropriate size, if scale_a is already",
      "    # (BLOCK_SIZE_M, 1) then it will broadcast to its own shape. Same goes",
      "    # for scale_b below.",
      "    scale_a = scale_a.broadcast_to((BLOCK_SIZE_M, 1))",
      "    accumulator = scale_a * accumulator.to(tl.float32)",
      "",
      "    masks_scale_b = masks_scale_bn[:, None] & (tl.arange(0, 1) < 1)[None, :]",
      "    scale_b = tl.load(scale_b_ptrs[:, None], masks_scale_b)",
      "    scale_b = scale_b.broadcast_to((BLOCK_SIZE_N, 1))",
      "    accumulator = scale_b.T * accumulator.to(tl.float32)",
      "",
      "    # Convert to output format.",
      "    c = accumulator.to(c_ptr.type.element_ty)",
      "",
      "    # Add bias, it's already in output format, so add it after conversion.",
      "    if bias_ptr:",
      "        offsets_bias = offsets_bn",
      "        bias_ptrs = bias_ptr + offsets_bias",
      "        bias_mask = offsets_bias < N",
      "        bias = tl.load(bias_ptrs, bias_mask)",
      "        c += bias",
      "",
      "    # Save output",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)",
      "    offs_cm = offs_cm.to(tl.int64)",
      "    offs_cn = offs_cn.to(tl.int64)",
      "    c_ptrs = (c_ptr + stride_cm * offs_cm[:, None] +",
      "              stride_cn * offs_cn[None, :])",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "",
      "    tl.store(c_ptrs, c, mask=c_mask)",
      "",
      "",
      "# input   - [M, K]",
      "# weight - [K, N]",
      "def triton_scaled_mm(input: torch.Tensor,",
      "                     weight: torch.Tensor,",
      "                     scale_a: torch.Tensor,",
      "                     scale_b: torch.Tensor,",
      "                     out_dtype: type[torch.dtype],",
      "                     bias: Optional[torch.Tensor] = None,",
      "                     block_size_m: int = 32,",
      "                     block_size_n: int = 32,",
      "                     block_size_k: int = 32,",
      "                     use_heuristic=True) -> torch.Tensor:",
      "    M, K = input.shape",
      "    N = weight.shape[1]",
      "",
      "    assert N > 0 and K > 0 and M > 0",
      "    assert weight.shape[0] == K",
      "    assert input.dtype == weight.dtype",
      "",
      "    scale_a = scale_a.reshape(-1, 1) if scale_a.dim() <= 1 else scale_a",
      "    scale_b = scale_b.reshape(-1, 1) if scale_b.dim() <= 1 else scale_b",
      "",
      "    assert scale_a.dtype == scale_b.dtype and scale_a.is_floating_point()",
      "    assert scale_a.shape[1] == 1 and (scale_a.shape[0] == 1",
      "                                      or scale_a.shape[0] == M)",
      "    assert scale_b.shape[1] == 1 and (scale_b.shape[0] == 1",
      "                                      or scale_b.shape[0] == N)",
      "    assert out_dtype.is_floating_point",
      "    assert bias is None or bias.is_floating_point()",
      "    assert is_weak_contiguous(input)",
      "    assert is_weak_contiguous(weight)",
      "",
      "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(",
      "        N, META['BLOCK_SIZE_N']), )",
      "",
      "    result = torch.empty((M, N), dtype=out_dtype, device=input.device)",
      "",
      "    has_scalar = lambda x: x.shape[0] == 1 and x.shape[1] == 1",
      "",
      "    if use_heuristic:",
      "        is_small_N = N < 8192",
      "        next_power_of_2_M = max(32, triton.next_power_of_2(M))",
      "        if next_power_of_2_M <= 32:",
      "            tile_shape = (64, 64, 256) if is_small_N else (64, 128, 256)",
      "        elif next_power_of_2_M <= 64:",
      "            tile_shape = (64, 64, 256)",
      "        elif next_power_of_2_M <= 128:",
      "            tile_shape = (64, 128, 128)",
      "        else:",
      "            tile_shape = (128, 128, 128)",
      "",
      "    block_size_m, block_size_n, block_size_k = tile_shape",
      "",
      "    block_size_sa = 1 if has_scalar(scale_a) else block_size_m",
      "    block_size_sb = 1 if has_scalar(scale_b) else block_size_n",
      "",
      "    accumulator_dtype = tl.float32 if input.is_floating_point() else tl.int32",
      "",
      "    # A = input, B = weight, C = result",
      "    # A = M x K, B = K x N, C = M x N",
      "    scaled_mm_kernel[grid](input,",
      "                           weight,",
      "                           scale_a,",
      "                           scale_b,",
      "                           result,",
      "                           bias,",
      "                           M,",
      "                           N,",
      "                           K,",
      "                           input.stride(0),",
      "                           input.stride(1),",
      "                           weight.stride(0),",
      "                           weight.stride(1),",
      "                           result.stride(0),",
      "                           result.stride(1),",
      "                           accumulator_dtype,",
      "                           BLOCK_SIZE_M=block_size_m,",
      "                           BLOCK_SIZE_N=block_size_n,",
      "                           BLOCK_SIZE_K=block_size_k,",
      "                           BLOCK_SIZE_SCALE_A=block_size_sa,",
      "                           BLOCK_SIZE_SCALE_B=block_size_sb)",
      "",
      "    return result.to(out_dtype)"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/quantization/utils/int8_utils.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "# Adapted from https://github.com/sgl-project/sglang/blob/4cb53ecd0cffceb6dee5c011a58f65997a86f151/python/sglang/srt/layers/quantization/int8_kernel.py",
      "import functools",
      "import json",
      "import logging",
      "import os",
      "from typing import Any, Optional",
      "",
      "import torch",
      "",
      "from vllm.platforms import current_platform",
      "from vllm.triton_utils import tl, triton",
      "",
      "logger = logging.getLogger(__name__)",
      "",
      "",
      "def apply_w8a8_block_int8_linear(",
      "    input: torch.Tensor,",
      "    weight: torch.Tensor,",
      "    block_size: list[int],",
      "    weight_scale: torch.Tensor,",
      "    input_scale: Optional[torch.Tensor] = None,",
      "    bias: Optional[torch.Tensor] = None,",
      ") -> torch.Tensor:",
      "    assert input_scale is None",
      "    # View input as 2D matrix for fp8 methods",
      "    input_2d = input.view(-1, input.shape[-1])",
      "    output_shape = [*input.shape[:-1], weight.shape[0]]",
      "",
      "    q_input, x_scale = per_token_group_quant_int8(input_2d, block_size[1])",
      "    output = w8a8_block_int8_matmul(q_input,",
      "                                    weight,",
      "                                    x_scale,",
      "                                    weight_scale,",
      "                                    block_size,",
      "                                    output_dtype=input.dtype)",
      "",
      "    if bias is not None:",
      "        output = output + bias",
      "    return output.to(dtype=input.dtype).view(*output_shape)",
      "",
      "",
      "def input_to_int8(",
      "        x: torch.Tensor,",
      "        dtype: torch.dtype = torch.int8) -> tuple[torch.Tensor, torch.Tensor]:",
      "    \"\"\"This function quantizes input values to int8 values with",
      "    tensor-wise quantization.\"\"\"",
      "    iinfo = torch.iinfo(dtype)",
      "    min_val, max_val = x.aminmax()",
      "    amax = torch.maximum(min_val.abs(), max_val.abs()).clamp(min=1e-12)",
      "    int8_min, int8_max = iinfo.min, iinfo.max",
      "    scale = int8_max / amax",
      "    x_scl_sat = (x * scale).clamp(min=int8_min, max=int8_max)",
      "    return x_scl_sat.to(dtype).contiguous(), scale.float().reciprocal()",
      "",
      "",
      "def block_dequant(",
      "    x_q_block: torch.Tensor,",
      "    x_s: torch.Tensor,",
      "    block_size: list[int],",
      ") -> torch.Tensor:",
      "    \"\"\"This function conducts block-wise dequantization.",
      "    The inputs are block-wise quantization tensor `x_q_block`,",
      "    block-wise quantization scale and the block size.",
      "    The outputs are dequantized tensor.",
      "    \"\"\"",
      "    block_n, block_k = block_size[0], block_size[1]",
      "    n, k = x_q_block.shape",
      "    n_tiles = (n + block_n - 1) // block_n",
      "    k_tiles = (k + block_k - 1) // block_k",
      "    assert n_tiles == x_s.shape[0]",
      "    assert k_tiles == x_s.shape[1]",
      "",
      "    x_dq_block = x_q_block.to(torch.float32)",
      "",
      "    for i in range(k_tiles):",
      "        for j in range(n_tiles):",
      "            x_dq_block[",
      "                j * block_n:min((j + 1) * block_n, n),",
      "                i * block_k:min((i + 1) * block_k, k),",
      "            ] *= x_s[j][i]",
      "",
      "    return x_dq_block",
      "",
      "",
      "if current_platform.is_rocm():",
      "    from triton.language import core",
      "",
      "    # NOTE: This can be removed when hip.libdevice.round() is available.",
      "    @core.extern",
      "    def round_f32(arg0, _builder=None):",
      "        return core.extern_elementwise(\"\",",
      "                                       \"\", [arg0], {",
      "                                           (core.dtype(\"fp32\"), ):",
      "                                           (\"llvm.round\", core.dtype(\"fp32\")),",
      "                                           (core.dtype(\"fp64\"), ):",
      "                                           (\"llvm.round\", core.dtype(\"fp64\")),",
      "                                       },",
      "                                       is_pure=True,",
      "                                       _builder=_builder)",
      "",
      "    @triton.jit",
      "    def round_int8(x):",
      "        return round_f32(x).to(tl.int8)",
      "else:",
      "",
      "    @triton.jit",
      "    def round_int8(x):",
      "        return tl.extra.cuda.libdevice.round(x).to(tl.int8)",
      "",
      "",
      "@triton.jit",
      "def _per_token_quant_int8(",
      "    x_ptr,",
      "    xq_ptr,",
      "    scale_ptr,",
      "    stride_x,",
      "    stride_xq,",
      "    N,",
      "    BLOCK: tl.constexpr,",
      "):",
      "    # Adapted from https://github.com/InternLM/lmdeploy/blob/086481ed84b59bee3b8e4274e5fc69620040c048/lmdeploy/pytorch/kernels/cuda/w8a8_triton_kernels.py#L282",
      "    row_id = tl.program_id(0)",
      "",
      "    cols = tl.arange(0, BLOCK)",
      "    mask = cols < N",
      "",
      "    x = tl.load(x_ptr + row_id * stride_x + cols, mask=mask,",
      "                other=0.0).to(tl.float32)",
      "    absmax = tl.maximum(tl.max(tl.abs(x)), 1e-10)",
      "    scale_x = absmax / 127",
      "    x_q = x * (127 / absmax)",
      "    x_q = round_int8(x_q)",
      "",
      "    tl.store(xq_ptr + row_id * stride_xq + cols, x_q, mask=mask)",
      "    tl.store(scale_ptr + row_id, scale_x)",
      "",
      "",
      "def per_token_quant_int8(x):",
      "    M = x.numel() // x.shape[-1]",
      "    N = x.shape[-1]",
      "    x_q = torch.empty_like(x, device=x.device, dtype=torch.int8)",
      "    scales = torch.empty(x.shape[:-1] + (1, ),",
      "                         device=x.device,",
      "                         dtype=torch.float32)",
      "    BLOCK = triton.next_power_of_2(N)",
      "    # heuristics for number of warps",
      "    num_warps = min(max(BLOCK // 256, 1), 8)",
      "",
      "    assert x.is_contiguous()",
      "    _per_token_quant_int8[(M, )](",
      "        x,",
      "        x_q,",
      "        scales,",
      "        stride_x=x.stride(-2),",
      "        stride_xq=x_q.stride(-2),",
      "        N=N,",
      "        BLOCK=BLOCK,",
      "        num_warps=num_warps,",
      "        num_stages=1,",
      "    )",
      "",
      "    return x_q, scales",
      "",
      "",
      "@triton.jit",
      "def _per_token_group_quant_int8(",
      "    # Pointers to inputs and output",
      "    y_ptr,",
      "    y_q_ptr,",
      "    y_s_ptr,",
      "    # Stride of input",
      "    y_stride,",
      "    # Columns of input",
      "    N,",
      "    # Avoid to divide zero",
      "    eps,",
      "    # Information for int8",
      "    int8_min,",
      "    int8_max,",
      "    # Meta-parameters",
      "    BLOCK: tl.constexpr,",
      "):",
      "    \"\"\"A Triton-accelerated function to perform per-token-group",
      "    quantization on a tensor.",
      "",
      "    This function converts the tensor values into int8 values.",
      "    \"\"\"",
      "    # Map the program id to the row of X and Y it should compute.",
      "    g_id = tl.program_id(0)",
      "    y_ptr += g_id * y_stride",
      "    y_q_ptr += g_id * y_stride",
      "    y_s_ptr += g_id",
      "",
      "    cols = tl.arange(0, BLOCK)  # N <= BLOCK",
      "    mask = cols < N",
      "",
      "    y = tl.load(y_ptr + cols, mask=mask, other=0.0).to(tl.float32)",
      "    # Quant",
      "    _absmax = tl.maximum(tl.max(tl.abs(y)), eps)",
      "    y_s = _absmax / int8_max",
      "    y_q = tl.clamp(y / y_s, int8_min, int8_max).to(y_q_ptr.dtype.element_ty)",
      "",
      "    tl.store(y_q_ptr + cols, y_q, mask=mask)",
      "    tl.store(y_s_ptr, y_s)",
      "",
      "",
      "def per_token_group_quant_int8(",
      "    x: torch.Tensor,",
      "    group_size: int,",
      "    eps: float = 1e-10,",
      "    dtype: torch.dtype = torch.int8,",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    \"\"\"Function to perform per-token-group quantization on an input tensor `x`.",
      "",
      "    It converts the tensor values into signed int8 values and returns the",
      "    quantized tensor along with the scaling factor used for quantization.",
      "",
      "    Args:",
      "        x: The input tensor with ndim >= 2.",
      "        group_size: The group size used for quantization.",
      "        eps: The minimum to avoid dividing zero.",
      "        dtype: The dype of output tensor. Note that only `torch.int8`",
      "            is supported for now.",
      "",
      "    Returns:",
      "        tuple[torch.Tensor, torch.Tensor]: The quantized tensor and the",
      "            scaling factor for quantization.",
      "    \"\"\"",
      "    assert (x.shape[-1] % group_size == 0",
      "            ), \"the last dimension of `x` cannot be divisible by `group_size`\"",
      "    assert x.is_contiguous(), \"`x` is not contiguous\"",
      "",
      "    iinfo = torch.iinfo(dtype)",
      "    int8_max = iinfo.max",
      "    int8_min = iinfo.min",
      "",
      "    x_q = torch.empty_like(x, device=x.device, dtype=dtype)",
      "    x_s = torch.empty(",
      "        x.shape[:-1] + (x.shape[-1] // group_size, ),",
      "        device=x.device,",
      "        dtype=torch.float32,",
      "    )",
      "    # prefer CUDA kernel if available",
      "    if current_platform.is_cuda():",
      "        torch.ops._C.per_token_group_quant_int8(x, x_q, x_s, group_size, eps,",
      "                                                float(int8_min),",
      "                                                float(int8_max))",
      "        return x_q, x_s",
      "",
      "    M = x.numel() // group_size",
      "    N = group_size",
      "",
      "    BLOCK = triton.next_power_of_2(N)",
      "    # heuristics for number of warps",
      "    num_warps = min(max(BLOCK // 256, 1), 8)",
      "    num_stages = 1",
      "    _per_token_group_quant_int8[(M, )](",
      "        x,",
      "        x_q,",
      "        x_s,",
      "        group_size,",
      "        N,",
      "        eps,",
      "        int8_min=int8_min,",
      "        int8_max=int8_max,",
      "        BLOCK=BLOCK,",
      "        num_warps=num_warps,",
      "        num_stages=num_stages,",
      "    )",
      "",
      "    return x_q, x_s",
      "",
      "",
      "@triton.jit",
      "def _w8a8_block_int8_matmul(",
      "    # Pointers to inputs and output",
      "    A,",
      "    B,",
      "    C,",
      "    As,",
      "    Bs,",
      "    # Shape for matmul",
      "    M,",
      "    N,",
      "    K,",
      "    # Block size for block-wise quantization",
      "    group_n,",
      "    group_k,",
      "    # Stride for inputs and output",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    stride_As_m,",
      "    stride_As_k,",
      "    stride_Bs_k,",
      "    stride_Bs_n,",
      "    # Meta-parameters",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "    \"\"\"Triton-accelerated function used to perform linear operations (dot",
      "    product) on input tensors `A` and `B` with block-wise quantization, and",
      "    store the result in output tensor `C`.",
      "    \"\"\"",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = A + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = B + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    As_ptrs = As + offs_am * stride_As_m",
      "    offs_bsn = offs_bn // group_n",
      "    Bs_ptrs = Bs + offs_bsn * stride_Bs_n",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(a_ptrs,",
      "                    mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,",
      "                    other=0.0)",
      "        b = tl.load(b_ptrs,",
      "                    mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,",
      "                    other=0.0)",
      "",
      "        k_start = k * BLOCK_SIZE_K",
      "        offs_ks = k_start // group_k",
      "        a_s = tl.load(As_ptrs + offs_ks * stride_As_k)",
      "        b_s = tl.load(Bs_ptrs + offs_ks * stride_Bs_k)",
      "",
      "        accumulator += tl.dot(a, b).to(tl.float32) * a_s[:,",
      "                                                         None] * b_s[None, :]",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    if C.dtype.element_ty == tl.bfloat16:",
      "        c = accumulator.to(tl.bfloat16)",
      "    elif C.dtype.element_ty == tl.float16:",
      "        c = accumulator.to(tl.float16)",
      "    else:",
      "        c = accumulator.to(tl.float32)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = C + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)",
      "",
      "",
      "@functools.lru_cache",
      "def get_w8a8_block_int8_configs(N: int, K: int, block_n: int,",
      "                                block_k: int) -> Optional[dict[int, Any]]:",
      "    \"\"\"",
      "    Return optimized configurations for the w8a8 block fp8 kernel.",
      "",
      "    The return value will be a dictionary that maps an irregular grid of",
      "    batch sizes to configurations of the w8a8 block fp8 kernel. To evaluate the",
      "    kernel on a given batch size bs, the closest batch size in the grid should",
      "    be picked and the associated configuration chosen to invoke the kernel.",
      "    \"\"\"",
      "",
      "    # First look up if an optimized configuration is available in the configs",
      "    # directory",
      "    device_name = current_platform.get_device_name().replace(\" \", \"_\")",
      "    json_file_name = f\"N={N},K={K},device_name={device_name},dtype=int8_w8a8,block_shape=[{block_n}, {block_k}].json\"  # noqa: E501",
      "",
      "    config_file_path = os.path.join(",
      "        os.path.dirname(os.path.realpath(__file__)), \"configs\", json_file_name)",
      "    if os.path.exists(config_file_path):",
      "        with open(config_file_path) as f:",
      "            logger.info(",
      "                \"Using configuration from %s for W8A8 Block INT8 kernel.\",",
      "                config_file_path,",
      "            )",
      "            # If a configuration has been found, return it",
      "            return {int(key): val for key, val in json.load(f).items()}",
      "",
      "    # If no optimized configuration is available, we will use the default",
      "    # configuration",
      "    logger.warning(",
      "        (\"Using default W8A8 Block INT8 kernel config. Performance might \"",
      "         \"be sub-optimal! Config file not found at %s\"),",
      "        config_file_path,",
      "    )",
      "    return None",
      "",
      "",
      "def w8a8_block_int8_matmul(",
      "    A: torch.Tensor,",
      "    B: torch.Tensor,",
      "    As: torch.Tensor,",
      "    Bs: torch.Tensor,",
      "    block_size: list[int],",
      "    output_dtype: torch.dtype = torch.float16,",
      ") -> torch.Tensor:",
      "    \"\"\"This function performs matrix multiplication with block-wise",
      "    quantization.",
      "",
      "    It takes two input tensors `A` and `B` with scales `As` and `Bs`.",
      "    The output is returned in the specified `output_dtype`.",
      "",
      "    Args:",
      "        A: The input tensor, e.g., activation.",
      "        B: The input tensor, e.g., weight.",
      "        As: The per-token-group quantization scale for `A`.",
      "        Bs: The per-block quantization scale for `B`.",
      "        block_size: The block size for per-block quantization. It should be",
      "            2-dim, e.g., [128, 128].",
      "        output_dytpe: The dtype of the returned tensor.",
      "",
      "    Returns:",
      "        torch.Tensor: The result of matmul.",
      "    \"\"\"",
      "    assert len(block_size) == 2",
      "    block_n, block_k = block_size[0], block_size[1]",
      "",
      "    assert A.shape[-1] == B.shape[-1]",
      "    assert A.shape[:-1] == As.shape[:-1] and A.is_contiguous()",
      "    assert triton.cdiv(A.shape[-1], block_k) == As.shape[-1]",
      "    M = A.numel() // A.shape[-1]",
      "",
      "    assert B.ndim == 2 and B.is_contiguous() and Bs.ndim == 2",
      "    N, K = B.shape",
      "    assert triton.cdiv(N, block_n) == Bs.shape[0]",
      "    assert triton.cdiv(K, block_k) == Bs.shape[1]",
      "",
      "    C_shape = A.shape[:-1] + (N, )",
      "    C = A.new_empty(C_shape, dtype=output_dtype)",
      "",
      "    configs = get_w8a8_block_int8_configs(N, K, block_size[0], block_size[1])",
      "    if configs:",
      "        # If an optimal configuration map has been found, look up the",
      "        # optimal config",
      "        config = configs[min(configs.keys(), key=lambda x: abs(x - M))]",
      "    else:",
      "        # Default config",
      "        # Block-wise quant: BLOCK_SIZE_K must be divisible by block_size[1]",
      "        config = {",
      "            \"BLOCK_SIZE_M\": 64,",
      "            \"BLOCK_SIZE_N\": block_size[0],",
      "            \"BLOCK_SIZE_K\": block_size[1],",
      "            \"GROUP_SIZE_M\": 32,",
      "            \"num_warps\": 4,",
      "            \"num_stages\": 3,",
      "        }",
      "",
      "    def grid(META):",
      "        return (triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) *",
      "                triton.cdiv(N, META[\"BLOCK_SIZE_N\"]), )",
      "",
      "    _w8a8_block_int8_matmul[grid](",
      "        A,",
      "        B,",
      "        C,",
      "        As,",
      "        Bs,",
      "        M,",
      "        N,",
      "        K,",
      "        block_n,",
      "        block_k,",
      "        A.stride(-2),",
      "        A.stride(-1),",
      "        B.stride(1),",
      "        B.stride(0),",
      "        C.stride(-2),",
      "        C.stride(-1),",
      "        As.stride(-2),",
      "        As.stride(-1),",
      "        Bs.stride(1),",
      "        Bs.stride(0),",
      "        **config,",
      "    )",
      "",
      "    return C"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/quantization/utils/fp8_utils.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "# Adapted from https://github.com/sgl-project/sglang/pull/2575",
      "import functools",
      "import json",
      "import os",
      "from collections.abc import Sequence",
      "from typing import Any, Callable, Optional, Union",
      "",
      "import torch",
      "",
      "import vllm.envs as envs",
      "from vllm import _custom_ops as ops",
      "from vllm.logger import init_logger",
      "from vllm.model_executor.layers.quantization.utils.quant_utils import (",
      "    group_broadcast)",
      "from vllm.model_executor.layers.quantization.utils.w8a8_utils import (",
      "    CUTLASS_BLOCK_FP8_SUPPORTED)",
      "from vllm.platforms import current_platform",
      "from vllm.triton_utils import tl, triton",
      "from vllm.utils import cdiv, direct_register_custom_op",
      "from vllm.utils.deep_gemm import (is_deep_gemm_e8m0_used,",
      "                                  should_use_deepgemm_for_fp8_linear)",
      "",
      "logger = init_logger(__name__)",
      "",
      "",
      "def is_fp8(x: Union[torch.dtype, torch.Tensor]) -> bool:",
      "    if isinstance(x, torch.Tensor):",
      "        x = x.dtype",
      "    return x == torch.float8_e4m3fn or x == torch.float8_e4m3fnuz",
      "",
      "",
      "def cutlass_scaled_mm(",
      "    A: torch.Tensor,",
      "    B: torch.Tensor,",
      "    As: torch.Tensor,",
      "    Bs: torch.Tensor,",
      "    block_size: list[int],",
      "    output_dtype: torch.dtype = torch.float16,",
      ") -> torch.Tensor:",
      "    return ops.cutlass_scaled_mm(A,",
      "                                 B.T,",
      "                                 out_dtype=output_dtype,",
      "                                 scale_a=As,",
      "                                 scale_b=Bs.T)",
      "",
      "",
      "def rocm_aiter_gemm_w8a8_blockscale_impl(",
      "    A: torch.Tensor,",
      "    B: torch.Tensor,",
      "    As: torch.Tensor,",
      "    Bs: torch.Tensor,",
      "    block_size: list[int],",
      "    output_dtype: torch.dtype = torch.float16,",
      ") -> torch.Tensor:",
      "    import aiter as rocm_aiter",
      "",
      "    return rocm_aiter.gemm_a8w8_blockscale(A, B, As, Bs, dtype=output_dtype)",
      "",
      "",
      "def rocm_aiter_gemm_w8a8_blockscale_fake(",
      "    A: torch.Tensor,",
      "    B: torch.Tensor,",
      "    As: torch.Tensor,",
      "    Bs: torch.Tensor,",
      "    block_size: list[int],",
      "    output_dtype: torch.dtype = torch.float16,",
      ") -> torch.Tensor:",
      "",
      "    m = A.shape[0]",
      "    n = B.shape[0]",
      "    Y = torch.empty(m, n, dtype=output_dtype, device=A.device)",
      "    return Y",
      "",
      "",
      "if current_platform.is_rocm():",
      "    direct_register_custom_op(",
      "        op_name=\"rocm_aiter_gemm_w8a8_blockscale\",",
      "        op_func=rocm_aiter_gemm_w8a8_blockscale_impl,",
      "        mutates_args=[],",
      "        fake_impl=rocm_aiter_gemm_w8a8_blockscale_fake,",
      "        dispatch_key=current_platform.dispatch_key,",
      "    )",
      "    if (envs.VLLM_ROCM_USE_AITER and envs.VLLM_ROCM_USE_AITER_LINEAR",
      "            and current_platform.is_fp8_fnuz()):",
      "",
      "        import aiter as rocm_aiter",
      "        from aiter import get_hip_quant",
      "",
      "        aiter_per1x128_quant = get_hip_quant(rocm_aiter.QuantType.per_1x128)",
      "",
      "",
      "def dispatch_w8a8_blockscale_func(",
      "    use_cutlass: bool, use_aiter_and_is_supported: bool",
      ") -> Callable[[",
      "        torch.Tensor,",
      "        torch.Tensor,",
      "        torch.Tensor,",
      "        torch.Tensor,",
      "        list[int],",
      "        torch.dtype,",
      "], torch.Tensor]:",
      "    if use_cutlass:",
      "        return cutlass_scaled_mm",
      "    if (use_aiter_and_is_supported):",
      "        return torch.ops.vllm.rocm_aiter_gemm_w8a8_blockscale",
      "    return w8a8_block_fp8_matmul",
      "",
      "",
      "# TODO fix ROCm->Triton custom path:",
      "#  https://github.com/vllm-project/vllm/issues/14397",
      "def apply_w8a8_block_fp8_linear(",
      "    input: torch.Tensor,",
      "    weight: torch.Tensor,",
      "    block_size: list[int],",
      "    weight_scale: torch.Tensor,",
      "    input_scale: Optional[torch.Tensor] = None,",
      "    bias: Optional[torch.Tensor] = None,",
      "    cutlass_block_fp8_supported: bool = CUTLASS_BLOCK_FP8_SUPPORTED,",
      "    use_aiter_and_is_supported: bool = False,",
      ") -> torch.Tensor:",
      "    assert input_scale is None",
      "    # View input as 2D matrix for fp8 methods",
      "    input_2d = input.view(-1, input.shape[-1])",
      "    output_shape = [*input.shape[:-1], weight.shape[0]]",
      "    output_dtype = input.dtype",
      "",
      "    if should_use_deepgemm_for_fp8_linear(output_dtype, weight):",
      "",
      "        input_2d = input.view(-1, input.shape[-1])",
      "        output_shape = [*input.shape[:-1], weight.shape[0]]",
      "",
      "        q_input, x_scale = per_token_group_quant_fp8(",
      "            input_2d,",
      "            block_size[1],",
      "            column_major_scales=True,",
      "        )",
      "",
      "        # ensure DeepGEMM-backed custom op is registered before use",
      "        import vllm.model_executor.layers.quantization.deepgemm  # noqa: F401",
      "",
      "        output = torch.ops.vllm.w8a8_block_fp8_matmul_deepgemm(",
      "            q_input,",
      "            weight,",
      "            x_scale,",
      "            weight_scale,",
      "            block_size,",
      "            output_dtype=output_dtype)",
      "        if bias is not None:",
      "            output += bias",
      "        return output.to(dtype=output_dtype).view(*output_shape)",
      "",
      "    if current_platform.is_cuda():",
      "        if current_platform.has_device_capability(100):",
      "",
      "            use_cutlass = cutlass_block_fp8_supported and (",
      "                cdiv(weight.shape[0], 128) == weight_scale.shape[0]",
      "                and cdiv(weight.shape[1], 128) == weight_scale.shape[1])",
      "        else:",
      "            # TODO: update this after switching to public sm90 block scale gemm",
      "            # as it also supports weight.shape % 128 != 0",
      "            use_cutlass = cutlass_block_fp8_supported and (",
      "                weight.shape[0] % 128 == 0 and weight.shape[1] % 128 == 0)",
      "    else:",
      "        use_cutlass = False",
      "",
      "    w8a8_blockscale_func = dispatch_w8a8_blockscale_func(",
      "        use_cutlass, use_aiter_and_is_supported)",
      "    if use_cutlass:",
      "        q_input, x_scale = per_token_group_quant_fp8(",
      "            input_2d, block_size[1], column_major_scales=use_cutlass)",
      "        output = w8a8_blockscale_func(q_input, weight, x_scale, weight_scale,",
      "                                      block_size, input.dtype)",
      "",
      "    else:",
      "        if use_aiter_and_is_supported:",
      "            q_input, x_scale = aiter_per1x128_quant(",
      "                input_2d.contiguous(), quant_dtype=rocm_aiter.dtypes.fp8)",
      "        else:",
      "            q_input, x_scale = per_token_group_quant_fp8(",
      "                input_2d, block_size[1], column_major_scales=use_cutlass)",
      "",
      "        output = w8a8_blockscale_func(q_input, weight, x_scale, weight_scale,",
      "                                      block_size, input.dtype)",
      "",
      "    if bias is not None:",
      "        output = output + bias",
      "    return output.to(dtype=input.dtype).view(*output_shape)",
      "",
      "",
      "def apply_w8a8_block_fp8_linear_fake(",
      "    input: torch.Tensor,",
      "    weight: torch.Tensor,",
      "    block_size: list[int],",
      "    weight_scale: torch.Tensor,",
      "    input_scale: Optional[torch.Tensor] = None,",
      "    bias: Optional[torch.Tensor] = None,",
      "    cutlass_block_fp8_supported: bool = CUTLASS_BLOCK_FP8_SUPPORTED,",
      "    use_aiter_and_is_supported: bool = False,",
      ") -> torch.Tensor:",
      "    output_shape = [*input.shape[:-1], weight.shape[0]]",
      "    return torch.empty(output_shape, dtype=input.dtype, device=input.device)",
      "",
      "",
      "if not current_platform.is_cpu():",
      "    direct_register_custom_op(",
      "        op_name=\"apply_w8a8_block_fp8_linear\",",
      "        op_func=apply_w8a8_block_fp8_linear,",
      "        mutates_args=[],",
      "        fake_impl=apply_w8a8_block_fp8_linear_fake,",
      "    )",
      "",
      "",
      "def input_to_float8(",
      "        x: torch.Tensor,",
      "        dtype: Optional[torch.dtype] = None",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    \"\"\"This function quantizes input values to float8 values \"",
      "    \"with tensor-wise quantization.\"\"\"",
      "    dtype = current_platform.fp8_dtype() if dtype is None else dtype",
      "    finfo = torch.finfo(dtype)",
      "    min_val, max_val = x.aminmax()",
      "    amax = torch.maximum(min_val.abs(), max_val.abs()).clamp(min=1e-12)",
      "    scale = finfo.max / amax",
      "    x_scl_sat = (x * scale).clamp(min=finfo.min, max=finfo.max)",
      "    return x_scl_sat.to(dtype).contiguous(), scale.float().reciprocal()",
      "",
      "",
      "def block_quant_to_tensor_quant(",
      "    x_q_block: torch.Tensor,",
      "    x_s: torch.Tensor,",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    \"\"\"This function converts block-wise quantization to tensor-wise",
      "    quantization. The inputs are block-wise quantization tensor `x_q_block`,",
      "    block-wise quantization scale and the block size.",
      "    The outputs are tensor-wise quantization tensor and tensor-wise",
      "    quantization scale. Note only float8 is supported for now.",
      "    \"\"\"",
      "    x_dq_block = group_broadcast(x_q_block, x_s)",
      "    x_q_tensor, scale = input_to_float8(x_dq_block, dtype=x_q_block.dtype)",
      "    return x_q_tensor, scale",
      "",
      "",
      "@triton.jit",
      "def _per_token_group_quant_fp8(",
      "    # Pointers to inputs and output",
      "    y_ptr,",
      "    y_q_ptr,",
      "    y_s_ptr,",
      "    group_size,",
      "    # Num columns of y",
      "    y_num_columns,",
      "    y_row_stride,",
      "    # Avoid to divide zero",
      "    eps,",
      "    # Information for float8",
      "    fp8_min,",
      "    fp8_max,",
      "    use_ue8m0: tl.constexpr,",
      "    # Meta-parameters",
      "    BLOCK: tl.constexpr,",
      "):",
      "    \"\"\"A Triton-accelerated function to perform per-token-group",
      "    quantization on a tensor.",
      "    This function converts the tensor values into float8 values.",
      "    \"\"\"",
      "    groups_per_row = y_num_columns // group_size",
      "",
      "    # Map the program id to the row of X and Y it should compute.",
      "    g_id = tl.program_id(0)",
      "    row = g_id // groups_per_row",
      "    row_g_id = g_id % groups_per_row",
      "",
      "    # Ensure offset calculations use int64 to prevent overflow",
      "    y_ptr_offset = (row.to(tl.int64) * y_row_stride) + (row_g_id.to(tl.int64) *",
      "                                                        group_size)",
      "    y_ptr += y_ptr_offset",
      "",
      "    y_q_ptr_offset = g_id.to(tl.int64) * group_size",
      "    y_q_ptr += y_q_ptr_offset",
      "    y_s_ptr += g_id",
      "",
      "    cols = tl.arange(0, BLOCK)  # N <= BLOCK",
      "    mask = cols < group_size",
      "",
      "    y = tl.load(y_ptr + cols, mask=mask, other=0.0).to(tl.float32)",
      "    # Quant",
      "    _absmax = tl.maximum(tl.max(tl.abs(y)), eps)",
      "    scale_raw = _absmax / fp8_max",
      "    y_s = tl.math.exp2(tl.ceil(tl.log2(scale_raw))) if use_ue8m0 else scale_raw",
      "    y_q = tl.clamp(y / y_s, fp8_min, fp8_max).to(y_q_ptr.dtype.element_ty)",
      "",
      "    tl.store(y_q_ptr + cols, y_q, mask=mask)",
      "    tl.store(y_s_ptr, y_s)",
      "",
      "",
      "@triton.jit",
      "def _per_token_group_quant_fp8_colmajor(",
      "    # Pointers to inputs and output",
      "    y_ptr,",
      "    y_q_ptr,",
      "    y_s_ptr,",
      "    group_size,",
      "    # Num columns of y",
      "    y_num_columns,",
      "    y_row_stride,",
      "    # Stride from one column to the next of y_s",
      "    y_s_col_stride,",
      "    # Avoid to divide zero",
      "    eps,",
      "    # Information for float8",
      "    fp8_min,",
      "    fp8_max,",
      "    use_ue8m0: tl.constexpr,",
      "    # Meta-parameters",
      "    BLOCK: tl.constexpr,",
      "):",
      "    \"\"\"A Triton-accelerated function to perform per-token-group",
      "    quantization on a tensor.",
      "    This function converts the tensor values into float8 values.",
      "    \"\"\"",
      "    groups_per_row = y_num_columns // group_size",
      "",
      "    # Map the program id to the row of X and Y it should compute.",
      "    g_id = tl.program_id(0)",
      "    row = g_id // groups_per_row",
      "    row_g_id = g_id % groups_per_row",
      "",
      "    # Ensure offset calculations use int64 to prevent overflow",
      "    y_ptr_offset = (row.to(tl.int64) * y_row_stride) + (row_g_id.to(tl.int64) *",
      "                                                        group_size)",
      "    y_ptr += y_ptr_offset",
      "",
      "    y_q_ptr_offset = g_id.to(tl.int64) * group_size",
      "    y_q_ptr += y_q_ptr_offset",
      "",
      "    # Convert g_id the flattened block coordinate to 2D so we can index",
      "    # into the output y_scales matrix",
      "    blocks_per_row = y_num_columns // group_size",
      "    scale_col = g_id % blocks_per_row",
      "    scale_row = g_id // blocks_per_row",
      "    # Ensure offset calculation uses int64 for y_s_ptr",
      "    y_s_ptr_offset = (scale_col.to(tl.int64) * y_s_col_stride) + scale_row.to(",
      "        tl.int64)",
      "    y_s_ptr += y_s_ptr_offset",
      "",
      "    cols = tl.arange(0, BLOCK)  # group_size <= BLOCK",
      "    mask = cols < group_size",
      "",
      "    y = tl.load(y_ptr + cols, mask=mask, other=0.0).to(tl.float32)",
      "    # Quant",
      "    _absmax = tl.maximum(tl.max(tl.abs(y)), eps)",
      "    scale_raw = _absmax / fp8_max",
      "    y_s = tl.math.exp2(tl.ceil(tl.log2(scale_raw))) if use_ue8m0 else scale_raw",
      "    y_q = tl.clamp(y / y_s, fp8_min, fp8_max).to(y_q_ptr.dtype.element_ty)",
      "",
      "    tl.store(y_q_ptr + cols, y_q, mask=mask)",
      "    tl.store(y_s_ptr, y_s)",
      "",
      "",
      "def per_token_group_quant_fp8(",
      "    x: torch.Tensor,",
      "    group_size: int,",
      "    eps: float = 1e-10,",
      "    dtype: Optional[torch.dtype] = None,",
      "    column_major_scales: bool = False,",
      "    out_q: Optional[torch.Tensor] = None,",
      "    use_ue8m0: Optional[bool] = None,",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    \"\"\"Function to perform per-token-group quantization on an input tensor `x`.",
      "    It converts the tensor values into signed float8 values and returns the",
      "    quantized tensor along with the scaling factor used for quantization.",
      "    Args:",
      "        x: The input tensor with ndim >= 2.",
      "        group_size: The group size used for quantization.",
      "        eps: The minimum to avoid dividing zero.",
      "        dtype: The dype of output tensor. Note that only `torch.float8_e4m3fn`",
      "        is supported for now.",
      "        column_major_scales: Outputs scales in column major.",
      "        out_q: Optional output tensor. If not provided, function will create.",
      "    Returns:",
      "        tuple[torch.Tensor, torch.Tensor]: The quantized tensor and the",
      "        scaling factor.",
      "    \"\"\"",
      "    if use_ue8m0 is None:",
      "        use_ue8m0 = is_deep_gemm_e8m0_used()",
      "    dtype = current_platform.fp8_dtype() if dtype is None else dtype",
      "    assert (x.shape[-1] % group_size == 0), (",
      "        f\"the last dimension of `x` {x.shape[-1]} must be divisible \"",
      "        f\"by `group_size` {group_size}\")",
      "    assert x.stride(-1) == 1, \"`x` groups must be contiguous\"",
      "",
      "    finfo = torch.finfo(dtype)",
      "    fp8_min = finfo.min",
      "    fp8_max = finfo.max",
      "",
      "    assert out_q is None or out_q.shape == x.shape",
      "    x_q = out_q",
      "    if x_q is None:",
      "        x_q = torch.empty_like(x, device=x.device, dtype=dtype)",
      "",
      "    # Allocate the scale tensor in either row- or column-major format.",
      "    if column_major_scales:",
      "        shape = (x.shape[-1] // group_size, ) + x.shape[:-1]",
      "        x_s = torch.empty(shape, device=x.device,",
      "                          dtype=torch.float32).permute(-1, -2)",
      "    else:",
      "        shape = x.shape[:-1] + (x.shape[-1] // group_size, )",
      "        x_s = torch.empty(shape, device=x.device, dtype=torch.float32)",
      "",
      "    # prefer CUDA kernel if available",
      "    if current_platform.is_cuda() and x.is_contiguous():",
      "        torch.ops._C.per_token_group_fp8_quant(x, x_q, x_s, group_size, eps,",
      "                                               fp8_min, fp8_max, use_ue8m0)",
      "        return x_q, x_s",
      "",
      "    # TRITON FALLBACK",
      "    M = x.numel() // group_size",
      "    N = group_size",
      "    BLOCK = triton.next_power_of_2(N)",
      "    # heuristics for number of warps",
      "    num_warps = min(max(BLOCK // 256, 1), 8)",
      "    num_stages = 1",
      "    if column_major_scales:",
      "        _per_token_group_quant_fp8_colmajor[(M, )](",
      "            x,",
      "            x_q,",
      "            x_s,",
      "            group_size,",
      "            x.shape[1],",
      "            x.stride(0),",
      "            x_s.stride(1),",
      "            eps,",
      "            fp8_min=fp8_min,",
      "            fp8_max=fp8_max,",
      "            use_ue8m0=use_ue8m0,",
      "            BLOCK=BLOCK,",
      "            num_warps=num_warps,",
      "            num_stages=num_stages,",
      "        )",
      "    else:",
      "        _per_token_group_quant_fp8[(M, )](",
      "            x,",
      "            x_q,",
      "            x_s,",
      "            group_size,",
      "            x.shape[1],",
      "            x.stride(0),",
      "            eps,",
      "            fp8_min=fp8_min,",
      "            fp8_max=fp8_max,",
      "            use_ue8m0=use_ue8m0,",
      "            BLOCK=BLOCK,",
      "            num_warps=num_warps,",
      "            num_stages=num_stages,",
      "        )",
      "",
      "    return x_q, x_s",
      "",
      "",
      "@triton.jit",
      "def _w8a8_block_fp8_matmul(",
      "    # Pointers to inputs and output",
      "    A,",
      "    B,",
      "    C,",
      "    As,",
      "    Bs,",
      "    # Shape for matmul",
      "    M,",
      "    N,",
      "    K,",
      "    # Block size for block-wise quantization",
      "    group_n,",
      "    group_k,",
      "    # Stride for inputs and output",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    stride_As_m,",
      "    stride_As_k,",
      "    stride_Bs_k,",
      "    stride_Bs_n,",
      "    # Meta-parameters",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "    \"\"\"Triton-accelerated function used to perform linear operations (dot",
      "    product) on input tensors `A` and `B` with block-wise quantization, and",
      "    store the result in output tensor `C`.",
      "    \"\"\"",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = A + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = B + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    As_ptrs = As + offs_am * stride_As_m",
      "    offs_bsn = offs_bn // group_n",
      "    Bs_ptrs = Bs + offs_bsn * stride_Bs_n",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(a_ptrs,",
      "                    mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,",
      "                    other=0.0)",
      "        b = tl.load(b_ptrs,",
      "                    mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,",
      "                    other=0.0)",
      "",
      "        k_start = k * BLOCK_SIZE_K",
      "        offs_ks = k_start // group_k",
      "        a_s = tl.load(As_ptrs + offs_ks * stride_As_k)",
      "        b_s = tl.load(Bs_ptrs + offs_ks * stride_Bs_k)",
      "",
      "        accumulator += tl.dot(a, b) * a_s[:, None] * b_s[None, :]",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    if C.dtype.element_ty == tl.bfloat16:",
      "        c = accumulator.to(tl.bfloat16)",
      "    elif C.dtype.element_ty == tl.float16:",
      "        c = accumulator.to(tl.float16)",
      "    else:",
      "        c = accumulator.to(tl.float32)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = C + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)",
      "",
      "",
      "@functools.lru_cache",
      "def get_w8a8_block_fp8_configs(N: int, K: int, block_n: int,",
      "                               block_k: int) -> Optional[dict[int, Any]]:",
      "    \"\"\"",
      "    Return optimized configurations for the w8a8 block fp8 kernel.",
      "    The return value will be a dictionary that maps an irregular grid of",
      "    batch sizes to configurations of the w8a8 block fp8 kernel. To evaluate the",
      "    kernel on a given batch size bs, the closest batch size in the grid should",
      "    be picked and the associated configuration chosen to invoke the kernel.",
      "    \"\"\"",
      "",
      "    # First look up if an optimized configuration is available in the configs",
      "    # directory",
      "    device_name = current_platform.get_device_name().replace(\" \", \"_\")",
      "    json_file_name = f\"N={N},K={K},device_name={device_name},dtype=fp8_w8a8,block_shape=[{block_n},{block_k}].json\"  # noqa: E501",
      "",
      "    config_file_path = os.path.join(",
      "        os.path.dirname(os.path.realpath(__file__)), \"configs\", json_file_name)",
      "    if os.path.exists(config_file_path):",
      "        with open(config_file_path) as f:",
      "            logger.info(",
      "                \"Using configuration from %s for W8A8 Block FP8 kernel.\",",
      "                config_file_path,",
      "            )",
      "            # If a configuration has been found, return it",
      "            return {int(key): val for key, val in json.load(f).items()}",
      "",
      "    # If no optimized configuration is available, we will use the default",
      "    # configuration",
      "    logger.warning(",
      "        \"Using default W8A8 Block FP8 kernel config. Performance might \"",
      "        \"be sub-optimal! Config file not found at %s\",",
      "        config_file_path,",
      "    )",
      "    return None",
      "",
      "",
      "def w8a8_block_fp8_matmul(",
      "    A: torch.Tensor,",
      "    B: torch.Tensor,",
      "    As: torch.Tensor,",
      "    Bs: torch.Tensor,",
      "    block_size: list[int],",
      "    output_dtype: torch.dtype = torch.float16,",
      ") -> torch.Tensor:",
      "    \"\"\"This function performs matrix multiplication with block-wise",
      "    quantization.",
      "    It takes two input tensors `A` and `B` with scales `As` and `Bs`.",
      "    The output is returned in the specified `output_dtype`.",
      "    Args:",
      "        A: The input tensor, e.g., activation.",
      "        B: The input tensor, e.g., weight.",
      "        As: The per-token-group quantization scale for `A`.",
      "        Bs: The per-block quantization scale for `B`.",
      "        block_size: The block size for per-block quantization. It should",
      "        be 2-dim, e.g., [128, 128].",
      "        output_dytpe: The dtype of the returned tensor.",
      "    Returns:",
      "        torch.Tensor: The result of matmul.",
      "    \"\"\"",
      "    assert len(block_size) == 2",
      "    block_n, block_k = block_size[0], block_size[1]",
      "",
      "    assert A.shape[-1] == B.shape[-1]",
      "    assert A.shape[:-1] == As.shape[:-1] and A.is_contiguous()",
      "    assert triton.cdiv(A.shape[-1], block_k) == As.shape[-1]",
      "    M = A.numel() // A.shape[-1]",
      "",
      "    assert B.ndim == 2 and Bs.ndim == 2",
      "    N, K = B.shape",
      "    assert triton.cdiv(N, block_n) == Bs.shape[0]",
      "    assert triton.cdiv(K, block_k) == Bs.shape[1]",
      "",
      "    C_shape = A.shape[:-1] + (N, )",
      "    C = A.new_empty(C_shape, dtype=output_dtype)",
      "",
      "    configs = get_w8a8_block_fp8_configs(N, K, block_size[0], block_size[1])",
      "    if configs:",
      "        # Get the optimal config if there is one",
      "        config = configs[min(configs.keys(), key=lambda x: abs(x - M))]",
      "    else:",
      "        # Default config",
      "        # Block-wise quant: BLOCK_SIZE_N must be divisible by block_size[0]",
      "        # BLOCK_SIZE_K must be divisible by block_size[1]",
      "        config = {",
      "            \"BLOCK_SIZE_M\": 64,",
      "            \"BLOCK_SIZE_N\": block_size[0],",
      "            \"BLOCK_SIZE_K\": block_size[1],",
      "            \"GROUP_SIZE_M\": 32,",
      "            \"num_warps\": 4,",
      "            \"num_stages\": 2,",
      "        }",
      "",
      "    def grid(META):",
      "        return (triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) *",
      "                triton.cdiv(N, META[\"BLOCK_SIZE_N\"]), )",
      "",
      "    _w8a8_block_fp8_matmul[grid](",
      "        A,",
      "        B,",
      "        C,",
      "        As,",
      "        Bs,",
      "        M,",
      "        N,",
      "        K,",
      "        block_n,",
      "        block_k,",
      "        A.stride(-2),",
      "        A.stride(-1),",
      "        B.stride(1),",
      "        B.stride(0),",
      "        C.stride(-2),",
      "        C.stride(-1),",
      "        As.stride(-2),",
      "        As.stride(-1),",
      "        Bs.stride(1),",
      "        Bs.stride(0),",
      "        **config,",
      "    )",
      "",
      "    return C",
      "",
      "",
      "# Taken from https://github.com/deepseek-ai/DeepGEMM/blob/0c88cd01392c1073c7049a97d6328c7bba9b3947",
      "# TODO(wentao): remove this function when DeepGEMM exposes this function",
      "def get_tma_aligned_size(x: int, element_size: int) -> int:",
      "    \"\"\"",
      "    Global memory address of TMA must be 16-byte aligned.",
      "    Since we use column-major layout for the LHS scaling tensor,",
      "        the M-axis of the LHS scaling tensor needs to be padded to a multiple of",
      "        16 bytes.",
      "",
      "    Arguments:",
      "        x: original M-axis shape of the LHS scaling tensor.",
      "        element_size: element size of the LHS scaling tensor.",
      "",
      "    Returns:",
      "        M-axis shape of the LHS scaling tensor after padding.",
      "    \"\"\"",
      "    tma_alignment_bytes = 16",
      "    assert tma_alignment_bytes % element_size == 0",
      "    alignment = tma_alignment_bytes // element_size",
      "    return cdiv(x, alignment) * alignment",
      "",
      "",
      "# Taken from https://github.com/deepseek-ai/DeepGEMM/blob/0c88cd01392c1073c7049a97d6328c7bba9b3947",
      "# TODO(wentao): remove this function when DeepGEMM exposes this function",
      "def get_col_major_tma_aligned_tensor(x: torch.Tensor) -> torch.Tensor:",
      "    \"\"\"",
      "    Returns TMA-aligned transposed format of the input tensor. `torch.transpose`",
      "        will be called if necessary.",
      "    If the input tensor is already column-major layout and 16-byte aligned along",
      "        the M axis (thus meets the requirement of LHS scaling tensor in",
      "        DeepGEMM), this function will do nothing.",
      "",
      "    Arguments:",
      "        x: usually the LHS scaling tensor in GEMM.",
      "",
      "    Returns:",
      "        The LHS scaling tensor of TMA-aligned transposed format.",
      "    \"\"\"",
      "    # NOTES: for the extreme performance, you may rewrite/fuse this function in",
      "    # CUDA",
      "    assert x.dim() in (2, 3)",
      "    remove_dim = False",
      "    m, n = x.shape[-2], x.shape[-1]",
      "    aligned_m = get_tma_aligned_size(m, x.element_size())",
      "    if x.dim() == 2:",
      "        if x.stride(0) == 1 and x.stride(1) == aligned_m:",
      "            return x",
      "        x, remove_dim = x.unsqueeze(0), True",
      "",
      "    b = x.shape[0]",
      "",
      "    # The last kernel gives a column-major TMA aligned layout",
      "    if x.stride(0) == aligned_m * n and x.stride(1) == 1 and x.stride(",
      "            2) == aligned_m:",
      "        return x.squeeze(0) if remove_dim else x",
      "",
      "    # Normal layout requires transposing",
      "    aligned_x = torch.transpose(",
      "        torch.empty((b, n, aligned_m), device=x.device, dtype=x.dtype), 1, 2)",
      "    aligned_x[:, :m, :] = x",
      "    aligned_x = aligned_x[:, :m, :]",
      "    return aligned_x.squeeze(0) if remove_dim else aligned_x",
      "",
      "",
      "def requant_weight_ue8m0_inplace(",
      "        weight: torch.Tensor,",
      "        weight_scale: torch.Tensor,",
      "        block_size: Sequence[int] = (128, 128),",
      ") -> None:",
      "    \"\"\"Re-quantise *weight* so that its per-block scaling factors are in the",
      "    UE8M0 (power-of-two) format expected by the new DeepGEMM kernels inplace.",
      "",
      "    Args:",
      "        weight: Block-quantised weight tensor stored in ``torch.float8_e4m3fn``.",
      "            Expected shape ``(..., M, K)``.",
      "        weight_scale: Corresponding per-block scale tensor (``torch.float32``)",
      "            with shape ``(..., M // block_size[0], K // block_size[1])``.",
      "        block_size: 2-element iterable ``[block_m, block_k]`` describing the",
      "            block quantisation granularity.",
      "    \"\"\"",
      "    if weight.numel() == 0:",
      "        return",
      "",
      "    if weight.dtype != torch.float8_e4m3fn:",
      "        raise ValueError(\"Expected *weight* to be torch.float8_e4m3fn, got \"",
      "                         f\"{weight.dtype} instead.\")",
      "",
      "    from vllm.utils.deep_gemm import per_block_cast_to_fp8",
      "",
      "    block_m, block_k = int(block_size[0]), int(block_size[1])",
      "",
      "    # Flatten leading dimensions so we can iterate over the last two dims.",
      "    leading_shape = weight.shape[:-2]",
      "    if len(leading_shape) == 0:",
      "        w_view = weight.unsqueeze(0)",
      "        s_view = weight_scale.unsqueeze(0)",
      "    else:",
      "        w_view = weight.reshape(-1, weight.shape[-2], weight.shape[-1])",
      "        s_view = weight_scale.reshape(-1, *weight_scale.shape[-2:])",
      "",
      "    num_mats = w_view.size(0)",
      "    for idx in range(num_mats):",
      "        w_q = w_view[idx]",
      "        s_old = s_view[idx]",
      "",
      "        # De-quantise with the *old* scaling factors (float32).",
      "        m_cur, k_cur = w_q.shape",
      "        s_float = s_old.to(torch.float32)",
      "        # Expand scales along rows and cols by block size, then crop.",
      "        s_exp_r = torch.repeat_interleave(s_float, block_m, dim=0)",
      "        s_exp = torch.repeat_interleave(s_exp_r, block_k, dim=1)",
      "        s_exp = s_exp[:m_cur, :k_cur]",
      "        w_dq = w_q.to(torch.float32) * s_exp",
      "        # Re-quantise using power-of-two scaling (UE8M0).",
      "        w_requant, s_requant = per_block_cast_to_fp8(w_dq, [block_m, block_k],",
      "                                                     use_ue8m0=True)",
      "",
      "        # Write back the results in-place.",
      "        w_q.copy_(w_requant)",
      "        s_old.copy_(s_requant)"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/fused_moe/utils.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "from math import prod",
      "from typing import Optional, Union",
      "",
      "import torch",
      "",
      "from vllm import _custom_ops as ops",
      "from vllm.model_executor.layers.quantization.utils.fp8_utils import (",
      "    per_token_group_quant_fp8)",
      "from vllm.model_executor.layers.quantization.utils.int8_utils import (",
      "    per_token_group_quant_int8, per_token_quant_int8)",
      "from vllm.model_executor.layers.quantization.utils.mxfp4_utils import (",
      "    quant_dequant_mxfp4)",
      "from vllm.model_executor.layers.quantization.utils.mxfp8_utils import (",
      "    mxfp8_quantize)",
      "from vllm.platforms import current_platform",
      "from vllm.triton_utils import tl, triton",
      "from vllm.utils import cdiv",
      "from vllm.utils.flashinfer import fp4_quantize",
      "",
      "",
      "@triton.jit",
      "def _count_expert_num_tokens(topk_ids_ptr, expert_num_tokens_ptr, num_experts,",
      "                             topk_numel, expert_map,",
      "                             HAS_EXPERT_MAP: tl.constexpr,",
      "                             BLOCK_SIZE: tl.constexpr):",
      "",
      "    curr_expert = tl.program_id(0)",
      "",
      "    offsets = tl.arange(0, BLOCK_SIZE)",
      "    topk_ids_ptrs = topk_ids_ptr + offsets",
      "",
      "    acc = tl.zeros((BLOCK_SIZE, ), dtype=tl.int32)",
      "    for x in range(tl.cdiv(topk_numel, BLOCK_SIZE)):",
      "        mask = offsets < (topk_numel - x * BLOCK_SIZE)",
      "        expert_ids = tl.load(topk_ids_ptrs, mask=mask, other=-1)",
      "        if HAS_EXPERT_MAP:",
      "            expert_map_ptrs = expert_map + expert_ids",
      "            expert_map_mask = expert_ids >= 0",
      "            expert_ids = tl.load(expert_map_ptrs,",
      "                                 mask=expert_map_mask,",
      "                                 other=-1)",
      "",
      "        has_curr_expert = tl.where(expert_ids == curr_expert, 1, 0)",
      "        acc = acc + has_curr_expert",
      "        topk_ids_ptrs += BLOCK_SIZE",
      "",
      "    if curr_expert < num_experts:",
      "        tl.store(expert_num_tokens_ptr + curr_expert, tl.sum(acc))",
      "",
      "",
      "def count_expert_num_tokens(",
      "        topk_ids: torch.Tensor, num_local_experts: int,",
      "        expert_map: Optional[torch.Tensor]) -> torch.Tensor:",
      "    \"\"\"",
      "    Count the number to tokens assigned to each expert.",
      "",
      "    Parameters:",
      "    - topk_ids (torch.Tensor): Tensor mapping each token to its",
      "    list of experts.",
      "    - num_local_experts (int): Number of experts in this rank.",
      "    - expert_map (Optional[torch.Tensor]):  A tensor mapping expert indices",
      "    from the global expert space to the local expert space of the expert",
      "    parallel shard.",
      "",
      "    Returns:",
      "    A tensor of size num_local_experts, where tensor[i] holds the number",
      "    of tokens assigned to the ith expert.",
      "    \"\"\"",
      "    assert topk_ids.dtype.is_signed, (",
      "        \"The kernel uses -1 to represent invalid topk_ids\")",
      "    expert_num_tokens = torch.empty((num_local_experts),",
      "                                    device=topk_ids.device,",
      "                                    dtype=torch.int32)",
      "",
      "    grid = num_local_experts",
      "    BLOCK_SIZE = min(topk_ids.numel(), 1024)",
      "    BLOCK_SIZE = triton.next_power_of_2(BLOCK_SIZE)",
      "",
      "    _count_expert_num_tokens[(grid, )](",
      "        topk_ids,",
      "        expert_num_tokens,",
      "        num_local_experts,",
      "        topk_ids.numel(),",
      "        expert_map,",
      "        HAS_EXPERT_MAP=expert_map is not None,",
      "        BLOCK_SIZE=BLOCK_SIZE,",
      "    )",
      "",
      "    return expert_num_tokens",
      "",
      "",
      "def _resize_cache(x: torch.Tensor, v: tuple[int, ...]) -> torch.Tensor:",
      "    \"\"\"",
      "    Shrink the given tensor and apply the given view to it.  This is",
      "    used to resize the intermediate fused_moe caches.",
      "    \"\"\"",
      "    assert prod(v) <= x.numel(",
      "    ), f\"{v} ({prod(v)}) <= {x.shape} ({x.numel()})\"  # CUDAGRAPH unfriendly?",
      "    return x.flatten()[:prod(v)].view(*v)",
      "",
      "",
      "def _fp4_quantize(",
      "    A: torch.Tensor,",
      "    A_scale: Optional[torch.Tensor],",
      "    is_sf_swizzled_layout: bool,",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    return fp4_quantize(A,",
      "                        A_scale,",
      "                        is_sf_swizzled_layout=is_sf_swizzled_layout)",
      "",
      "",
      "def _fp8_quantize(",
      "    A: torch.Tensor,",
      "    A_scale: Optional[torch.Tensor],",
      "    per_act_token: bool,",
      "    block_shape: Optional[list[int]] = None,",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    \"\"\"",
      "    Perform fp8 quantization on the inputs.  If a block_shape",
      "    is provided, the output will be blocked.",
      "    \"\"\"",
      "    if block_shape is None:",
      "        # TODO(luka): use QuantFP8 custom op",
      "        #  https://github.com/vllm-project/vllm/issues/20711",
      "        A, A_scale = ops.scaled_fp8_quant(",
      "            A, A_scale, use_per_token_if_dynamic=per_act_token)",
      "    else:",
      "        assert not per_act_token",
      "        assert len(block_shape) == 2",
      "        _, block_k = block_shape[0], block_shape[1]",
      "        A, A_scale = per_token_group_quant_fp8(A, block_k)",
      "        assert cdiv(A.size(-1), block_k) == A_scale.size(-1)",
      "",
      "    return A, A_scale",
      "",
      "",
      "def _int8_quantize(",
      "    A: torch.Tensor,",
      "    A_scale: Optional[torch.Tensor],",
      "    per_act_token: bool,",
      "    block_shape: Optional[list[int]] = None,",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    \"\"\"",
      "    Perform int8 quantization on the inputs.  If a block_shape",
      "    is provided, the output will be blocked.",
      "    \"\"\"",
      "",
      "    # If weights are per-channel (per_channel_quant=True), then",
      "    # activations apply per-token quantization. Otherwise, assume",
      "    # activation tensor-wise fp8/int8 quantization, dynamic or static",
      "    if block_shape is None:",
      "        assert per_act_token, \\",
      "            \"int8 quantization only supports block or channel-wise\"",
      "        A, A_scale = per_token_quant_int8(A)",
      "    else:",
      "        assert not per_act_token",
      "        assert len(block_shape) == 2",
      "        _, block_k = block_shape[0], block_shape[1]",
      "        A, A_scale = per_token_group_quant_int8(A, block_k)",
      "        assert cdiv(A.size(-1), block_k) == A_scale.size(-1)",
      "",
      "    return A, A_scale",
      "",
      "",
      "def _mxfp4_quantize(",
      "    A: torch.Tensor,",
      "    A_scale: Optional[torch.Tensor],",
      "    per_act_token_quant: bool,",
      "    block_shape: Optional[list[int]] = None,",
      ") -> tuple[torch.Tensor, None]:",
      "    assert block_shape is None",
      "    if not current_platform.supports_mx():",
      "        A = quant_dequant_mxfp4(A)",
      "    else:",
      "        raise NotImplementedError()",
      "",
      "    return A, None",
      "",
      "",
      "def _mxfp8_quantize(",
      "    A: torch.Tensor,",
      "    A_scale: Optional[torch.Tensor],",
      "    per_act_token_quant: bool,",
      "    block_shape: Optional[list[int]] = None,",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    assert A_scale is None",
      "    assert not per_act_token_quant",
      "    assert block_shape is None",
      "    return mxfp8_quantize(A)",
      "",
      "",
      "def moe_kernel_quantize_input(",
      "    A: torch.Tensor,",
      "    A_scale: Optional[torch.Tensor],",
      "    quant_dtype: Union[None, torch.dtype, str],",
      "    per_act_token_quant: bool,",
      "    block_shape: Optional[list[int]] = None,",
      "    is_fp4_scale_swizzled: bool = True,",
      ") -> tuple[torch.Tensor, Optional[torch.Tensor]]:",
      "    if quant_dtype == torch.float8_e4m3fn:",
      "        return _fp8_quantize(A, A_scale, per_act_token_quant, block_shape)",
      "    elif quant_dtype == torch.int8:",
      "        return _int8_quantize(A, A_scale, per_act_token_quant, block_shape)",
      "    elif quant_dtype == \"nvfp4\":",
      "        return _fp4_quantize(A,",
      "                             A_scale,",
      "                             is_sf_swizzled_layout=is_fp4_scale_swizzled)",
      "    elif quant_dtype == \"mxfp4\":",
      "        return _mxfp4_quantize(A, A_scale, per_act_token_quant, block_shape)",
      "    elif quant_dtype == \"mxfp8\":",
      "        return _mxfp8_quantize(A, A_scale, per_act_token_quant, block_shape)",
      "    else:",
      "        return A, A_scale",
      "",
      "",
      "def _fp8_perm(m: torch.Tensor, idx: torch.Tensor) -> torch.Tensor:",
      "    \"\"\"",
      "    A permutation routine that works on fp8 types.",
      "    \"\"\"",
      "    if torch.is_floating_point(m) and m.dtype.itemsize == 1:",
      "        return m.view(dtype=torch.uint8)[idx, ...].view(dtype=m.dtype)",
      "    else:",
      "        return m[idx, ...]",
      "",
      "",
      "def normalize_scales_shape(",
      "        scales: Optional[torch.Tensor]) -> Optional[torch.Tensor]:",
      "    if scales is not None:",
      "        if scales.numel() == 1:",
      "            scales = scales.view(1, 1)",
      "        else:",
      "            scales = scales.view(-1, scales.size(-1))",
      "    return scales",
      "",
      "",
      "def normalize_batched_scales_shape(",
      "    scales: Optional[torch.Tensor],",
      "    num_experts: int,",
      ") -> Optional[torch.Tensor]:",
      "    if scales is not None and scales.ndim < 3:",
      "        if scales.numel() == 1:",
      "            scales = scales.view(1)",
      "            scales = torch.repeat_interleave(scales, num_experts,",
      "                                             dim=0).view(num_experts, 1, 1)",
      "        else:",
      "            scales = scales.view(num_experts, -1, scales.size(-1))",
      "",
      "    return scales",
      "",
      "",
      "def _validate_scale_shape(",
      "    a: torch.Tensor,",
      "    a_scale: Optional[torch.Tensor],",
      "    per_act_token_quant: bool,",
      "    block_shape: Optional[list[int]],",
      ") -> None:",
      "    if a_scale is None:",
      "        return",
      "",
      "    if not per_act_token_quant and block_shape is None:",
      "        assert a_scale.numel() == 1, f\"{a_scale.shape}\"",
      "    elif per_act_token_quant:",
      "        assert a_scale.shape[0] == a.shape[0] and a_scale.shape[1] == 1, (",
      "            f\"{a_scale.shape[0]} == {a.shape[0]} and {a_scale.shape[1]} == 1\")",
      "    else:",
      "        assert block_shape is not None",
      "        expected = (a.shape[0], cdiv(a.shape[1], block_shape[1]))",
      "        assert a_scale.shape == expected, f\"{a_scale.shape} == {expected}\""
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/fused_moe/fused_moe.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "\"\"\"Fused MoE kernel.\"\"\"",
      "import functools",
      "import json",
      "import os",
      "# torch.compile needs typing.List. It will fail torch.library.infer_schema",
      "# otherwise",
      "from typing import List  # noqa: UP035",
      "from typing import Any, Callable, Optional",
      "",
      "import torch",
      "import torch.nn.functional as F",
      "",
      "import vllm.envs as envs",
      "import vllm.model_executor.layers.fused_moe.modular_kernel as mk",
      "from vllm import _custom_ops as ops",
      "from vllm.logger import init_logger",
      "# yapf: disable",
      "from vllm.model_executor.layers.fused_moe.config import (",
      "    FusedMoEQuantConfig, get_config_quant_dtype)",
      "from vllm.model_executor.layers.fused_moe.cutlass_moe import (",
      "    _valid_cutlass_block_scaled_grouped_gemm,",
      "    run_cutlass_block_scaled_fused_experts)",
      "# yapf: enable",
      "from vllm.model_executor.layers.fused_moe.deep_gemm_moe import (",
      "    _valid_deep_gemm, deep_gemm_moe_fp8)",
      "from vllm.model_executor.layers.fused_moe.moe_align_block_size import (",
      "    moe_align_block_size)",
      "from vllm.model_executor.layers.fused_moe.prepare_finalize import (",
      "    MoEPrepareAndFinalizeNoEP)",
      "from vllm.model_executor.layers.fused_moe.topk_weight_and_reduce import (",
      "    TopKWeightAndReduceNoOP)",
      "from vllm.model_executor.layers.fused_moe.utils import (",
      "    _resize_cache, moe_kernel_quantize_input, per_token_group_quant_fp8)",
      "from vllm.model_executor.layers.quantization.utils.flashinfer_utils import (",
      "    calculate_tile_tokens_dim)",
      "from vllm.model_executor.layers.quantization.utils.mxfp4_utils import (",
      "    dequant_mxfp4)",
      "from vllm.platforms import current_platform",
      "from vllm.triton_utils import tl, triton",
      "from vllm.utils import direct_register_custom_op, is_torch_equal_or_newer",
      "from vllm.utils.deep_gemm import is_deep_gemm_e8m0_used",
      "",
      "from .rocm_aiter_fused_moe import is_rocm_aiter_moe_enabled",
      "",
      "logger = init_logger(__name__)",
      "",
      "",
      "@triton.jit",
      "def write_zeros_to_output(c_ptr, stride_cm, stride_cn, pid_n, N, offs_token,",
      "                          token_mask, BLOCK_SIZE_M, BLOCK_SIZE_N,",
      "                          compute_type):",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=compute_type)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[",
      "        None, :]",
      "    c_mask = token_mask[:, None] & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, accumulator, mask=c_mask)",
      "",
      "",
      "@triton.jit",
      "def fused_moe_kernel_gptq_awq(",
      "        # Pointers to matrices",
      "        a_ptr,",
      "        b_ptr,",
      "        c_ptr,",
      "        b_scale_ptr,",
      "        b_zp_ptr,",
      "        topk_weights_ptr,",
      "        sorted_token_ids_ptr,",
      "        expert_ids_ptr,",
      "        num_tokens_post_padded_ptr,",
      "        # Matrix dimensions",
      "        N: tl.constexpr,",
      "        K: tl.constexpr,",
      "        EM,",
      "        num_valid_tokens,",
      "        # The stride variables represent how much to increase the ptr by when",
      "        # moving by 1 element in a particular dimension. E.g. `stride_am` is",
      "        # how much to increase `a_ptr` by to get the element one row down",
      "        # (A has M rows).",
      "        stride_am,",
      "        stride_ak,",
      "        stride_be,",
      "        stride_bk,",
      "        stride_bn,",
      "        stride_cm,",
      "        stride_cn,",
      "        stride_bse,",
      "        stride_bsk,",
      "        stride_bsn,",
      "        stride_bze,",
      "        stride_bzk,",
      "        stride_bzn,",
      "        block_k_diviable: tl.constexpr,",
      "        group_size: tl.constexpr,",
      "        # Meta-parameters",
      "        BLOCK_SIZE_M: tl.constexpr,",
      "        BLOCK_SIZE_N: tl.constexpr,",
      "        BLOCK_SIZE_K: tl.constexpr,",
      "        GROUP_SIZE_M: tl.constexpr,",
      "        MUL_ROUTED_WEIGHT: tl.constexpr,",
      "        top_k: tl.constexpr,",
      "        compute_type: tl.constexpr,",
      "        has_zp: tl.constexpr,",
      "        use_int4_w4a16: tl.constexpr,",
      "        use_int8_w8a16: tl.constexpr):",
      "    \"\"\"",
      "    Implements the fused computation for a Mixture of Experts (MOE) using",
      "    token and expert matrices.",
      "",
      "    Key Parameters:",
      "    - A: The input tensor representing tokens with shape (*, K), where '*' can",
      "        be any shape representing batches and K is the feature dimension of",
      "        each token.",
      "    - B: The stacked MOE weight tensor with shape (E, N, K), where E is",
      "        the number of experts, K is the input feature dimension, and N is",
      "        the output feature dimension.",
      "    - C: The output cache tensor with shape (M, topk, N), where M is the",
      "        total number of tokens post padding, topk is the number of times",
      "        each token is repeated, and N is the output feature dimension.",
      "    - sorted_token_ids: A tensor containing the sorted indices of tokens,",
      "        repeated topk times and arranged by the expert index they are",
      "        assigned to.",
      "    - expert_ids: A tensor containing the indices of the expert for each",
      "        block. It determines which expert matrix from B should be used for",
      "        each block in A.",
      "    This kernel performs the multiplication of a token by its corresponding",
      "    expert matrix as determined by `expert_ids`. The sorting of",
      "    `sorted_token_ids` by expert index and padding ensures divisibility by",
      "    BLOCK_SIZE_M, which is necessary to maintain consistency in block matrix",
      "    multiplication across different blocks processed by the same expert.",
      "    \"\"\"",
      "    # -----------------------------------------------------------",
      "    # Map program ids `pid` to the block of C it should compute.",
      "    # This is done in a grouped ordering to promote L2 data reuse.",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    # ----------------------------------------------------------",
      "    # Create pointers for the first blocks of A and B.",
      "    # We will advance this pointer as we move in the K direction",
      "    # and accumulate",
      "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers",
      "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers",
      "    num_tokens_post_padded = tl.load(num_tokens_post_padded_ptr)",
      "    if pid_m * BLOCK_SIZE_M >= num_tokens_post_padded:",
      "        return",
      "    offs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(",
      "        tl.int64)",
      "    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)",
      "    token_mask = offs_token < num_valid_tokens",
      "",
      "    off_experts = tl.load(expert_ids_ptr + pid_m).to(tl.int64)",
      "    if off_experts == -1:",
      "        # -----------------------------------------------------------",
      "        # Write back zeros to the output when the expert is not",
      "        # in the current expert parallel rank.",
      "        write_zeros_to_output(c_ptr, stride_cm, stride_cn, pid_n, N,",
      "                              offs_token, token_mask, BLOCK_SIZE_M,",
      "                              BLOCK_SIZE_N, compute_type)",
      "        return",
      "",
      "    offs_bn = (pid_n * BLOCK_SIZE_N +",
      "               tl.arange(0, BLOCK_SIZE_N).to(tl.int64)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_token[:, None] // top_k * stride_am +",
      "                      offs_k[None, :] * stride_ak)",
      "",
      "    if use_int4_w4a16:",
      "        b_ptrs = b_ptr + off_experts * stride_be + \\",
      "            (offs_k[:, None] // 2) * stride_bk + offs_bn[None, :] * \\",
      "                stride_bn",
      "        b_shifter = (offs_k[:, None] % 2) * 4",
      "    elif use_int8_w8a16:",
      "        b_ptrs = b_ptr + off_experts * stride_be + \\",
      "            offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn",
      "",
      "    if not has_zp and use_int4_w4a16:",
      "        b_zp_num = 8",
      "    if not has_zp and use_int8_w8a16:",
      "        b_zp_num = 128",
      "    elif has_zp and use_int4_w4a16:",
      "        b_zp_shifter = (offs_bn[None, :] % 2) * 4",
      "",
      "    # -----------------------------------------------------------",
      "    # Iterate to compute a block of the C matrix.",
      "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block",
      "    # of fp32 values for higher accuracy.",
      "    # `accumulator` will be converted back to fp16 after the loop.",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        # Load the next block of A and B, generate a mask by checking the",
      "        # K dimension.",
      "",
      "        if not block_k_diviable:",
      "            k_mask = offs_k[:, None] < K - k * BLOCK_SIZE_K",
      "            k_other = 0.0",
      "        else:",
      "            k_mask = None",
      "            k_other = None",
      "",
      "        a = tl.load(a_ptrs,",
      "                    mask=token_mask[:, None] &",
      "                    (offs_k[None, :] < K - k * BLOCK_SIZE_K),",
      "                    other=0.0)",
      "        b = tl.load(b_ptrs)",
      "        if use_int4_w4a16:",
      "            b = (b >> b_shifter) & 0xF",
      "",
      "        b_scale_ptrs = b_scale_ptr + off_experts * stride_bse + \\",
      "            offs_bn[None, :] * stride_bsn + \\",
      "            ((offs_k[:, None] + BLOCK_SIZE_K * k) // group_size) * \\",
      "                stride_bsk",
      "        b_scale = tl.load(b_scale_ptrs, mask=k_mask, other=k_other)",
      "        b_scale = b_scale.to(tl.float32)",
      "",
      "        if has_zp and use_int4_w4a16:",
      "            offs_k_true = (offs_k[:, None] + BLOCK_SIZE_K * k) // group_size",
      "            b_zp_ptrs = b_zp_ptr + off_experts * stride_bze + \\",
      "                (offs_bn[None, :] // 2) * stride_bzn + \\",
      "                offs_k_true * stride_bzk",
      "            b_zp = tl.load(b_zp_ptrs, mask=k_mask, other=k_other)",
      "            b_zp = ((b_zp >> b_zp_shifter) & 0xF)",
      "            b_zp = b_zp.to(tl.float32)",
      "        elif has_zp and use_int8_w8a16:",
      "            offs_k_true = (offs_k[:, None] + BLOCK_SIZE_K * k) // group_size",
      "            b_zp_ptrs = b_zp_ptr + off_experts * stride_bze + \\",
      "                offs_bn[None, :] * stride_bzn + \\",
      "                offs_k_true * stride_bzk",
      "            b_zp = tl.load(b_zp_ptrs, mask=k_mask, other=k_other)",
      "            b_zp = b_zp.to(tl.float32)",
      "",
      "        # We accumulate along the K dimension.",
      "        if has_zp:",
      "            b = ((b.to(tl.float32) - b_zp) * b_scale).to(compute_type)",
      "        else:",
      "            b = ((b.to(tl.float32) - b_zp_num) * b_scale).to(compute_type)",
      "        accumulator = tl.dot(a, b, acc=accumulator)",
      "",
      "        # Advance the ptrs to the next K block.",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        if use_int4_w4a16:",
      "            b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk",
      "        else:",
      "            b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    if MUL_ROUTED_WEIGHT:",
      "        moe_weight = tl.load(topk_weights_ptr + offs_token,",
      "                             mask=token_mask,",
      "                             other=0)",
      "        accumulator = accumulator * moe_weight[:, None]",
      "",
      "    accumulator = accumulator.to(compute_type)",
      "    # -----------------------------------------------------------",
      "    # Write back the block of the output",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[",
      "        None, :]",
      "    c_mask = token_mask[:, None] & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, accumulator, mask=c_mask)",
      "",
      "",
      "@triton.jit",
      "def fused_moe_kernel(",
      "    # Pointers to matrices",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    b_bias_ptr,",
      "    a_scale_ptr,",
      "    b_scale_ptr,",
      "    topk_weights_ptr,",
      "    sorted_token_ids_ptr,",
      "    expert_ids_ptr,",
      "    num_tokens_post_padded_ptr,",
      "    # Matrix dimensions",
      "    N,",
      "    K,",
      "    EM,",
      "    num_valid_tokens,",
      "    # The stride variables represent how much to increase the ptr by when",
      "    # moving by 1 element in a particular dimension. E.g. `stride_am` is",
      "    # how much to increase `a_ptr` by to get the element one row down",
      "    # (A has M rows).",
      "    stride_am,",
      "    stride_ak,",
      "    stride_be,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    stride_asm,",
      "    stride_ask,",
      "    stride_bse,",
      "    stride_bsk,",
      "    stride_bsn,",
      "    stride_bbe,  # bias expert stride",
      "    stride_bbn,  # bias N stride",
      "    # Block size for block-wise quantization",
      "    group_n: tl.constexpr,",
      "    group_k: tl.constexpr,",
      "    # Meta-parameters",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    MUL_ROUTED_WEIGHT: tl.constexpr,",
      "    top_k: tl.constexpr,",
      "    compute_type: tl.constexpr,",
      "    use_fp8_w8a8: tl.constexpr,",
      "    use_int8_w8a8: tl.constexpr,",
      "    use_int8_w8a16: tl.constexpr,",
      "    per_channel_quant: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "):",
      "    \"\"\"",
      "    Implements the fused computation for a Mixture of Experts (MOE) using",
      "    token and expert matrices.",
      "",
      "    Key Parameters:",
      "    - A: The input tensor representing tokens with shape (*, K), where '*' can",
      "        be any shape representing batches and K is the feature dimension of",
      "        each token.",
      "    - B: The stacked MOE weight tensor with shape (E, N, K), where E is",
      "        the number of experts, K is the input feature dimension, and N is",
      "        the output feature dimension.",
      "    - C: The output cache tensor with shape (M, topk, N), where M is the",
      "        total number of tokens post padding, topk is the number of times",
      "        each token is repeated, and N is the output feature dimension.",
      "    - sorted_token_ids: A tensor containing the sorted indices of tokens,",
      "        repeated topk times and arranged by the expert index they are",
      "        assigned to.",
      "    - expert_ids: A tensor containing the indices of the expert for each",
      "        block. It determines which expert matrix from B should be used for",
      "        each block in A.",
      "    This kernel performs the multiplication of a token by its corresponding",
      "    expert matrix as determined by `expert_ids`. The sorting of",
      "    `sorted_token_ids` by expert index and padding ensures divisibility by",
      "    BLOCK_SIZE_M, which is necessary to maintain consistency in block matrix",
      "    multiplication across different blocks processed by the same expert.",
      "    \"\"\"",
      "    # -----------------------------------------------------------",
      "    # Map program ids `pid` to the block of C it should compute.",
      "    # This is done in a grouped ordering to promote L2 data reuse.",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    # ----------------------------------------------------------",
      "    # Create pointers for the first blocks of A and B.",
      "    # We will advance this pointer as we move in the K direction",
      "    # and accumulate",
      "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers",
      "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers",
      "    num_tokens_post_padded = tl.load(num_tokens_post_padded_ptr)",
      "    if pid_m * BLOCK_SIZE_M >= num_tokens_post_padded:",
      "        return",
      "    offs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(",
      "        tl.int64)",
      "    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)",
      "    token_mask = offs_token < num_valid_tokens",
      "",
      "    off_experts = tl.load(expert_ids_ptr + pid_m).to(tl.int64)",
      "    if off_experts == -1:",
      "        # -----------------------------------------------------------",
      "        # Write back zeros to the output when the expert is not",
      "        # in the current expert parallel rank.",
      "        write_zeros_to_output(c_ptr, stride_cm, stride_cn, pid_n, N,",
      "                              offs_token, token_mask, BLOCK_SIZE_M,",
      "                              BLOCK_SIZE_N, compute_type)",
      "        return",
      "",
      "    offs_bn = (pid_n * BLOCK_SIZE_N +",
      "               tl.arange(0, BLOCK_SIZE_N).to(tl.int64)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_token[:, None] // top_k * stride_am +",
      "                      offs_k[None, :] * stride_ak)",
      "",
      "    b_ptrs = b_ptr + off_experts * stride_be + (offs_k[:, None] * stride_bk +",
      "                                                offs_bn[None, :] * stride_bn)",
      "    if use_int8_w8a16:",
      "        b_scale_ptrs = b_scale_ptr + off_experts * stride_bse + offs_bn[",
      "            None, :] * stride_bsn",
      "        b_scale = tl.load(b_scale_ptrs)",
      "",
      "    if use_fp8_w8a8 or use_int8_w8a8:",
      "        # block-wise",
      "        if group_k > 0 and group_n > 0:",
      "            a_scale_ptrs = a_scale_ptr + (offs_token // top_k) * stride_asm",
      "            offs_bsn = offs_bn // group_n",
      "            b_scale_ptrs = (b_scale_ptr + off_experts * stride_bse +",
      "                            offs_bsn * stride_bsn)",
      "        # channel-wise",
      "        elif per_channel_quant:",
      "            b_scale_ptrs = b_scale_ptr + off_experts * stride_bse + offs_bn[",
      "                None, :] * stride_bsn",
      "            b_scale = tl.load(b_scale_ptrs)",
      "            # Load per-token scale for activations",
      "            a_scale_ptrs = a_scale_ptr + (offs_token // top_k) * stride_asm",
      "            a_scale = tl.load(a_scale_ptrs, mask=token_mask, other=0.0)[:,",
      "                                                                        None]",
      "        # tensor-wise",
      "        else:",
      "            a_scale = tl.load(a_scale_ptr)",
      "            b_scale = tl.load(b_scale_ptr + off_experts)",
      "    if HAS_BIAS:",
      "        # bias shape: [num_experts, N]",
      "        bias_ptrs = b_bias_ptr + off_experts * stride_bbe + offs_bn * stride_bbn",
      "        bias = tl.load(bias_ptrs, mask=(offs_bn < N), other=0.0)",
      "    # -----------------------------------------------------------",
      "    # Iterate to compute a block of the C matrix.",
      "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block",
      "    # of fp32 values for higher accuracy.",
      "    # `accumulator` will be converted back to fp16 after the loop.",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        # Load the next block of A and B, generate a mask by checking the",
      "        # K dimension.",
      "        a = tl.load(a_ptrs,",
      "                    mask=token_mask[:, None] &",
      "                    (offs_k[None, :] < K - k * BLOCK_SIZE_K),",
      "                    other=0.0)",
      "        b = tl.load(b_ptrs,",
      "                    mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,",
      "                    other=0.0)",
      "        # We accumulate along the K dimension.",
      "        if use_int8_w8a16:",
      "            accumulator = tl.dot(a, b.to(compute_type), acc=accumulator)",
      "        elif use_fp8_w8a8 or use_int8_w8a8:",
      "            if group_k > 0 and group_n > 0:",
      "                k_start = k * BLOCK_SIZE_K",
      "                offs_ks = k_start // group_k",
      "                a_scale = tl.load(a_scale_ptrs + offs_ks * stride_ask,",
      "                                  mask=token_mask,",
      "                                  other=0.0)",
      "                b_scale = tl.load(b_scale_ptrs + offs_ks * stride_bsk)",
      "",
      "                accumulator += tl.dot(a, b) * a_scale[:,",
      "                                                      None] * b_scale[None, :]",
      "            else:",
      "                if use_fp8_w8a8:",
      "                    # acc used to enable fp8_fast_accum",
      "                    accumulator = tl.dot(a, b, acc=accumulator)",
      "                else:",
      "                    accumulator += tl.dot(a, b)",
      "        else:",
      "            accumulator += tl.dot(a, b)",
      "        # Advance the ptrs to the next K block.",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "    if HAS_BIAS:",
      "        accumulator = accumulator + bias[None, :]",
      "    if MUL_ROUTED_WEIGHT:",
      "        moe_weight = tl.load(topk_weights_ptr + offs_token,",
      "                             mask=token_mask,",
      "                             other=0)",
      "        accumulator = accumulator * moe_weight[:, None]",
      "    if use_int8_w8a16:",
      "        accumulator = (accumulator * b_scale).to(compute_type)",
      "    elif use_fp8_w8a8 or use_int8_w8a8:",
      "        if group_k > 0 and group_n > 0:",
      "            accumulator = accumulator.to(compute_type)",
      "        else:",
      "            accumulator = (accumulator * a_scale * b_scale).to(compute_type)",
      "    else:",
      "        accumulator = accumulator.to(compute_type)",
      "",
      "    # -----------------------------------------------------------",
      "    # Write back the block of the output",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[",
      "        None, :]",
      "    c_mask = token_mask[:, None] & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, accumulator, mask=c_mask)",
      "",
      "",
      "def invoke_fused_moe_kernel(A: torch.Tensor,",
      "                            B: torch.Tensor,",
      "                            C: torch.Tensor,",
      "                            A_scale: Optional[torch.Tensor],",
      "                            B_scale: Optional[torch.Tensor],",
      "                            B_zp: Optional[torch.Tensor],",
      "                            topk_weights: Optional[torch.Tensor],",
      "                            sorted_token_ids: torch.Tensor,",
      "                            expert_ids: torch.Tensor,",
      "                            num_tokens_post_padded: torch.Tensor,",
      "                            mul_routed_weight: bool,",
      "                            top_k: int,",
      "                            config: dict[str, Any],",
      "                            compute_type: tl.dtype,",
      "                            use_fp8_w8a8: bool,",
      "                            use_int8_w8a8: bool,",
      "                            use_int8_w8a16: bool,",
      "                            use_int4_w4a16: bool,",
      "                            per_channel_quant: bool,",
      "                            block_shape: Optional[list[int]] = None,",
      "                            B_bias: Optional[torch.Tensor] = None) -> None:",
      "    assert topk_weights is not None or not mul_routed_weight",
      "    assert topk_weights is None or topk_weights.stride(1) == 1",
      "    assert sorted_token_ids.stride(0) == 1",
      "",
      "    if use_fp8_w8a8 or use_int8_w8a8:",
      "        assert B_scale is not None",
      "        assert (block_shape is None",
      "                or triton.cdiv(B.size(-2), block_shape[0]) == B_scale.size(-2))",
      "        assert (block_shape is None",
      "                or triton.cdiv(B.size(-1), block_shape[1]) == B_scale.size(-1))",
      "",
      "    elif use_int8_w8a16 or use_int4_w4a16:",
      "        assert B_scale is not None",
      "        assert block_shape is None or block_shape[0] == 0",
      "    else:",
      "        assert A_scale is None",
      "        assert B_scale is None",
      "",
      "    M = A.size(0)",
      "    num_tokens = M * top_k",
      "",
      "    EM = sorted_token_ids.size(0)",
      "    if A.size(0) < config[\"BLOCK_SIZE_M\"]:",
      "        # optimize for small batch_size.",
      "        # We assume that top_ids of each token is unique,",
      "        # so num_valid_experts <= batch_size <= BLOCK_SIZE_M,",
      "        # and we can skip some invalid blocks.",
      "        EM = min(sorted_token_ids.size(0),",
      "                 A.size(0) * top_k * config['BLOCK_SIZE_M'])",
      "    grid = lambda META: (triton.cdiv(EM, META['BLOCK_SIZE_M']) * triton.cdiv(",
      "        B.size(1), META['BLOCK_SIZE_N']), )",
      "    HAS_BIAS = B_bias is not None",
      "    if (use_int8_w8a16 or use_int4_w4a16) and \\",
      "            block_shape is not None and block_shape[1] > 0:",
      "        assert B_scale is not None and B_scale.ndim == 3",
      "        assert B_zp is None or B_zp.ndim == 3",
      "",
      "        use_moe_wna16_cuda = should_moe_wna16_use_cuda(",
      "            num_valid_tokens=num_tokens,",
      "            group_size=block_shape[1],",
      "            num_experts=B.size(0),",
      "            bit=4 if use_int4_w4a16 else 8)",
      "        config = config.copy()",
      "        config.update(",
      "            get_moe_wna16_block_config(config=config,",
      "                                       use_moe_wna16_cuda=use_moe_wna16_cuda,",
      "                                       num_valid_tokens=num_tokens,",
      "                                       size_k=A.size(1),",
      "                                       size_n=B.size(1),",
      "                                       num_experts=B.size(1),",
      "                                       group_size=block_shape[1],",
      "                                       real_top_k=top_k,",
      "                                       block_size_m=config[\"BLOCK_SIZE_M\"]))",
      "",
      "        if use_moe_wna16_cuda:",
      "            bit = 4 if use_int4_w4a16 else 8",
      "            ops.moe_wna16_gemm(A, C, B, B_scale, B_zp,",
      "                               topk_weights if mul_routed_weight else None,",
      "                               sorted_token_ids, expert_ids,",
      "                               num_tokens_post_padded, top_k,",
      "                               config[\"BLOCK_SIZE_M\"], config[\"BLOCK_SIZE_N\"],",
      "                               config[\"BLOCK_SIZE_K\"], bit)",
      "            return",
      "",
      "        fused_moe_kernel_gptq_awq[grid](",
      "            A,",
      "            B,",
      "            C,",
      "            B_scale,",
      "            B_zp,",
      "            topk_weights,",
      "            sorted_token_ids,",
      "            expert_ids,",
      "            num_tokens_post_padded,",
      "            B.size(1),",
      "            A.size(1),",
      "            EM,",
      "            num_tokens,",
      "            A.stride(0),",
      "            A.stride(1),",
      "            B.stride(0),",
      "            B.stride(2),",
      "            B.stride(1),",
      "            C.stride(1),",
      "            C.stride(2),",
      "            B_scale.stride(0),",
      "            B_scale.stride(2),",
      "            B_scale.stride(1),",
      "            B_zp.stride(0) if B_zp is not None else 0,",
      "            B_zp.stride(2) if B_zp is not None else 0,",
      "            B_zp.stride(1) if B_zp is not None else 0,",
      "            block_k_diviable=A.size(1) % config[\"BLOCK_SIZE_K\"] == 0,",
      "            group_size=block_shape[1],",
      "            MUL_ROUTED_WEIGHT=mul_routed_weight,",
      "            top_k=top_k,",
      "            compute_type=compute_type,",
      "            has_zp=B_zp is not None,",
      "            use_int4_w4a16=use_int4_w4a16,",
      "            use_int8_w8a16=use_int8_w8a16,",
      "            **config,",
      "        )",
      "    else:",
      "        config = config.copy()",
      "        BLOCK_SIZE_K = config.pop(\"BLOCK_SIZE_K\")",
      "        if block_shape is not None:",
      "            BLOCK_SIZE_K = min(BLOCK_SIZE_K, min(block_shape[0],",
      "                                                 block_shape[1]))",
      "        fused_moe_kernel[grid](",
      "            A,",
      "            B,",
      "            C,",
      "            B_bias,",
      "            A_scale,",
      "            B_scale,",
      "            topk_weights,",
      "            sorted_token_ids,",
      "            expert_ids,",
      "            num_tokens_post_padded,",
      "            B.size(1),",
      "            B.size(2),",
      "            EM,",
      "            num_tokens,",
      "            A.stride(0),",
      "            A.stride(1),",
      "            B.stride(0),",
      "            B.stride(2),",
      "            B.stride(1),",
      "            C.stride(1),",
      "            C.stride(2),",
      "            A_scale.stride(0)",
      "            if A_scale is not None and A_scale.ndim == 2 else 0,",
      "            A_scale.stride(1)",
      "            if A_scale is not None and A_scale.ndim == 2 else 0,",
      "            B_scale.stride(0)",
      "            if B_scale is not None and B_scale.ndim >= 2 else 0,",
      "            B_scale.stride(2)",
      "            if B_scale is not None and B_scale.ndim == 3 else 0,",
      "            B_scale.stride(1)",
      "            if B_scale is not None and B_scale.ndim >= 2 else 0,",
      "            B_bias.stride(0) if B_bias is not None else 0,",
      "            B_bias.stride(1) if B_bias is not None else 0,",
      "            0 if block_shape is None else block_shape[0],",
      "            0 if block_shape is None else block_shape[1],",
      "            MUL_ROUTED_WEIGHT=mul_routed_weight,",
      "            top_k=top_k,",
      "            compute_type=compute_type,",
      "            use_fp8_w8a8=use_fp8_w8a8,",
      "            use_int8_w8a8=use_int8_w8a8,",
      "            use_int8_w8a16=use_int8_w8a16,",
      "            per_channel_quant=per_channel_quant,",
      "            HAS_BIAS=HAS_BIAS,",
      "            BLOCK_SIZE_K=BLOCK_SIZE_K,",
      "            **config,",
      "        )",
      "",
      "",
      "# Adapted from: https://github.com/sgl-project/sglang/pull/2628",
      "def get_config_file_name(E: int,",
      "                         N: int,",
      "                         dtype: Optional[str],",
      "                         block_shape: Optional[list[int]] = None) -> str:",
      "    device_name = current_platform.get_device_name().replace(\" \", \"_\")",
      "    dtype_selector = \"\" if not dtype else f\",dtype={dtype}\"",
      "    block_shape_selector = (\"\" if not block_shape or not all(block_shape) else",
      "                            f\",block_shape={block_shape}\").replace(\" \", \"\")",
      "    return f\"E={E},N={N},device_name={device_name}{dtype_selector}{block_shape_selector}.json\"  # noqa: E501",
      "",
      "",
      "# Adapted from: https://github.com/sgl-project/sglang/pull/2628",
      "@functools.lru_cache",
      "def get_moe_configs(",
      "    E: int,",
      "    N: int,",
      "    dtype: Optional[str],",
      "    block_n: Optional[int] = None,",
      "    block_k: Optional[int] = None,",
      ") -> Optional[dict[int, Any]]:",
      "    \"\"\"",
      "    Return optimized configurations for the fused MoE kernel.",
      "",
      "    The return value will be a dictionary that maps an irregular grid of",
      "    batch sizes to configurations of the fused_moe kernel. To evaluate the",
      "    kernel on a given batch size bs, the closest batch size in the grid should",
      "    be picked and the associated configuration chosen to invoke the kernel.",
      "    \"\"\"",
      "",
      "    # First look up if an optimized configuration is available in the configs",
      "    # directory",
      "    block_shape = [block_n, block_k] if block_n and block_k else None",
      "    json_file_name = get_config_file_name(E, N, dtype, block_shape)",
      "",
      "    config_file_paths = []",
      "",
      "    # note that we prioritize user defined config",
      "    user_defined_config_folder = envs.VLLM_TUNED_CONFIG_FOLDER",
      "    if user_defined_config_folder is not None:",
      "        user_defined_config_file_path = os.path.join(",
      "            user_defined_config_folder, json_file_name)",
      "        config_file_paths.append(user_defined_config_file_path)",
      "",
      "    default_config_file_path = os.path.join(",
      "        os.path.dirname(os.path.realpath(__file__)), \"configs\", json_file_name)",
      "    config_file_paths.append(default_config_file_path)",
      "",
      "    for config_file_path in config_file_paths:",
      "        if os.path.exists(config_file_path):",
      "            with open(config_file_path) as f:",
      "                logger.info(\"Using configuration from %s for MoE layer.\",",
      "                            config_file_path)",
      "                # If a configuration has been found, return it",
      "                return {int(key): val for key, val in json.load(f).items()}",
      "",
      "    # If no optimized configuration is available, we will use the default",
      "    # configuration",
      "    logger.warning(",
      "        (\"Using default MoE config. Performance might be sub-optimal! \"",
      "         \"Config file not found at %s\"), config_file_paths)",
      "    return None",
      "",
      "",
      "def get_moe_wna16_block_config(config: dict[str,",
      "                                            int], use_moe_wna16_cuda: bool,",
      "                               num_valid_tokens: int, size_k: int, size_n: int,",
      "                               num_experts: int, group_size: int,",
      "                               real_top_k: int, block_size_m: int):",
      "    if \"BLOCK_SIZE_N\" in config and \"BLOCK_SIZE_K\" in config:",
      "        # optimal block config is set",
      "        return {}",
      "    if not use_moe_wna16_cuda:",
      "        # triton moe wna16 kernel",
      "        if num_valid_tokens // real_top_k == 1:",
      "            # if bs=1, use a smaller BLOCK_SIZE_N",
      "            return {\"BLOCK_SIZE_N\": 32, \"BLOCK_SIZE_K\": 64}",
      "        else:",
      "            return {\"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 32}",
      "    else:",
      "        # cuda moe wna16 kernel",
      "        # set default block_size 128, and increase them when num_blocks",
      "        # is too large.",
      "        block_size_n = 128",
      "        block_size_k = 128",
      "        if block_size_k <= group_size:",
      "            block_size_k = group_size",
      "",
      "        num_n_blocks = size_k // block_size_k",
      "        num_k_blocks = size_n // block_size_k",
      "        num_m_blocks = (num_valid_tokens + block_size_m - 1) / block_size_m + \\",
      "            num_experts",
      "        if num_valid_tokens // real_top_k <= block_size_m:",
      "            num_m_blocks = min(num_m_blocks, num_valid_tokens)",
      "        num_blocks = num_m_blocks * num_n_blocks * num_k_blocks",
      "",
      "        if size_k % 256 == 0 and num_blocks >= 256 and \\",
      "                block_size_k < 256:",
      "            block_size_k = 256",
      "            num_blocks = num_blocks // (256 // block_size_k)",
      "",
      "        if num_m_blocks <= 16 and size_k % (block_size_k * 2) == 0 and \\",
      "                size_k % (block_size_k * 2) == 0 and block_size_k <= 512 and \\",
      "                num_blocks >= 512:",
      "            block_size_k = block_size_k * 2",
      "            num_blocks = num_blocks // 2",
      "",
      "        if num_blocks > 1024:",
      "            block_size_n = 256",
      "            num_n_blocks = num_n_blocks // 2",
      "            num_blocks = num_blocks // 2",
      "",
      "        if size_n <= 1024 and num_blocks >= 1024:",
      "            # The kernel performance got much better with BLOCK_SIZE_N=1024",
      "            # when num_blocks is large, event when N is small.",
      "            # Not sure why, maybe it force the CUDA SM process only one block",
      "            # at the same time.",
      "            block_size_n = 1024",
      "",
      "        return {\"BLOCK_SIZE_N\": block_size_n, \"BLOCK_SIZE_K\": block_size_k}",
      "",
      "",
      "def should_moe_wna16_use_cuda(num_valid_tokens: int, group_size: int,",
      "                              num_experts: int, bit: int):",
      "    return current_platform.is_cuda() and bit == 4 and \\",
      "        group_size in [32, 64, 128] and num_valid_tokens / num_experts <= 6",
      "",
      "",
      "def get_default_config(",
      "    M: int,",
      "    E: int,",
      "    N: int,",
      "    K: int,",
      "    topk: int,",
      "    dtype: Optional[str],",
      "    block_shape: Optional[list[int]] = None,",
      ") -> dict[str, int]:",
      "    if dtype == \"fp8_w8a8\" and block_shape is not None:",
      "        # Block-wise quant: BLOCK_SIZE_N must be divisible by block_shape[0]",
      "        # BLOCK_SIZE_K must be divisible by block_shape[1]",
      "        # num_stages=3 can cause triton.runtime.errors.OutOfResources",
      "        # on ROCm, set it to 2 instead.",
      "        config = {",
      "            \"BLOCK_SIZE_M\": 64,",
      "            \"BLOCK_SIZE_N\": block_shape[0],",
      "            \"BLOCK_SIZE_K\": block_shape[1],",
      "            \"GROUP_SIZE_M\": 32,",
      "            \"num_warps\": 4,",
      "            \"num_stages\": 3 if not current_platform.is_rocm() else 2,",
      "        }",
      "    elif dtype in [\"int4_w4a16\", \"int8_w8a16\"] and block_shape is not None:",
      "        # moe wna16 kernels",
      "        # only set BLOCK_SIZE_M",
      "        # BLOCK_SIZE_N and BLOCK_SIZE_K would be set later",
      "        bit = 4 if dtype == \"int4_w4a16\" else 8",
      "        use_moe_wna16_cuda = should_moe_wna16_use_cuda(M * topk,",
      "                                                       block_shape[1], E, bit)",
      "        if use_moe_wna16_cuda:",
      "            config = {\"BLOCK_SIZE_M\": min(16, M)}",
      "        elif M <= 20:",
      "            config = {\"BLOCK_SIZE_M\": 16, \"GROUP_SIZE_M\": 1}",
      "        elif M <= 40:",
      "            config = {\"BLOCK_SIZE_M\": 32, \"GROUP_SIZE_M\": 1}",
      "        else:",
      "            config = {\"BLOCK_SIZE_M\": 64, \"GROUP_SIZE_M\": 1}",
      "    elif M <= E:",
      "        config = {",
      "            \"BLOCK_SIZE_M\": 16,",
      "            \"BLOCK_SIZE_N\": 32,",
      "            \"BLOCK_SIZE_K\": 64,",
      "            \"GROUP_SIZE_M\": 1,",
      "        }",
      "    else:",
      "        config = {",
      "            \"BLOCK_SIZE_M\": 64,",
      "            \"BLOCK_SIZE_N\": 64,",
      "            \"BLOCK_SIZE_K\": 32,",
      "            \"GROUP_SIZE_M\": 8,",
      "        }",
      "    return config",
      "",
      "",
      "def try_get_optimal_moe_config(",
      "    w1_shape: tuple[int, ...],",
      "    w2_shape: tuple[int, ...],",
      "    top_k: int,",
      "    dtype: Optional[str],",
      "    M: int,",
      "    block_shape: Optional[list[int]] = None,",
      ") -> dict[str, int]:",
      "    from vllm.model_executor.layers.fused_moe import get_config",
      "    override_config = get_config()",
      "    if override_config:",
      "        config = override_config",
      "    else:",
      "        # First try to load optimal config from the file",
      "        E, _, N = w2_shape",
      "        if dtype == \"int4_w4a16\":",
      "            N = N * 2",
      "        block_n = block_shape[0] if block_shape else 0",
      "        block_k = block_shape[1] if block_shape else 0",
      "        configs = get_moe_configs(E, N, dtype, block_n, block_k)",
      "",
      "        if configs:",
      "            # If an optimal configuration map has been found, look up the",
      "            # optimal config",
      "            config = configs[min(configs.keys(), key=lambda x: abs(x - M))]",
      "        else:",
      "            # Else use the default config",
      "            config = get_default_config(M, E, N, w1_shape[2], top_k, dtype,",
      "                                        block_shape)",
      "    return config",
      "",
      "",
      "def vllm_topk_softmax(topk_weights: torch.Tensor, topk_indices: torch.Tensor,",
      "                      token_expert_indices: torch.Tensor,",
      "                      gating_output: torch.Tensor,",
      "                      renormalize: bool) -> tuple[torch.Tensor, ...]:",
      "    ops.topk_softmax(",
      "        topk_weights,",
      "        topk_indices,",
      "        token_expert_indices,",
      "        gating_output,",
      "    )",
      "    if renormalize:",
      "        topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)",
      "",
      "    return topk_weights, topk_indices",
      "",
      "",
      "def dispatch_topk_func() -> Callable[..., tuple[torch.Tensor, ...]]:",
      "    if is_rocm_aiter_moe_enabled():",
      "        from .rocm_aiter_fused_moe import rocm_aiter_topk_softmax",
      "        return rocm_aiter_topk_softmax",
      "    return vllm_topk_softmax",
      "",
      "",
      "def fused_topk(",
      "    hidden_states: torch.Tensor,",
      "    gating_output: torch.Tensor,",
      "    topk: int,",
      "    renormalize: bool,",
      "    indices_type: Optional[torch.dtype] = None,",
      ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:",
      "    assert hidden_states.size(0) == gating_output.size(0), (",
      "        \"Number of tokens mismatch\")",
      "",
      "    M, _ = hidden_states.size()",
      "",
      "    topk_weights = torch.empty(M,",
      "                               topk,",
      "                               dtype=torch.float32,",
      "                               device=hidden_states.device)",
      "    topk_ids = torch.empty(",
      "        M,",
      "        topk,",
      "        dtype=torch.int32 if indices_type is None else indices_type,",
      "        device=hidden_states.device)",
      "    token_expert_indices = torch.empty(M,",
      "                                       topk,",
      "                                       dtype=torch.int32,",
      "                                       device=hidden_states.device)",
      "",
      "    gating_output_float = gating_output.float()  # TODO(woosuk): Optimize this.",
      "",
      "    topk_func = dispatch_topk_func()",
      "    topk_weights, topk_ids = topk_func(topk_weights, topk_ids,",
      "                                       token_expert_indices,",
      "                                       gating_output_float, renormalize)",
      "",
      "    return topk_weights, topk_ids, token_expert_indices",
      "",
      "",
      "# This is used by the Deepseek-V2 and Deepseek-V3 model",
      "@torch.compile(dynamic=True, backend=current_platform.simple_compile_backend)",
      "def grouped_topk(",
      "    hidden_states: torch.Tensor,",
      "    gating_output: torch.Tensor,",
      "    topk: int,",
      "    renormalize: bool,",
      "    num_expert_group: int = 0,",
      "    topk_group: int = 0,",
      "    scoring_func: str = \"softmax\",",
      "    routed_scaling_factor: float = 1.0,",
      "    e_score_correction_bias: Optional[torch.Tensor] = None,",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    if envs.VLLM_USE_FUSED_MOE_GROUPED_TOPK and \\",
      "            current_platform.is_cuda() and \\",
      "            num_expert_group <= 32 and topk <= 32 and \\",
      "            e_score_correction_bias is not None:",
      "        return fused_grouped_topk(",
      "            hidden_states=hidden_states,",
      "            gating_output=gating_output,",
      "            topk=topk,",
      "            renormalize=renormalize,",
      "            e_score_correction_bias=e_score_correction_bias,",
      "            num_expert_group=num_expert_group,",
      "            topk_group=topk_group,",
      "            scoring_func=scoring_func,",
      "            routed_scaling_factor=routed_scaling_factor)",
      "",
      "    assert hidden_states.size(0) == gating_output.size(0), (",
      "        \"Number of tokens mismatch\")",
      "",
      "    if scoring_func == \"softmax\":",
      "        scores = torch.softmax(gating_output, dim=-1)",
      "    elif scoring_func == \"sigmoid\":",
      "        scores = gating_output.sigmoid()",
      "    else:",
      "        raise ValueError(f\"Unsupported scoring function: {scoring_func}\")",
      "",
      "    num_token = scores.size(0)",
      "    if e_score_correction_bias is not None:",
      "        # Store original scores before applying correction bias. We use biased",
      "        # scores for expert selection but original scores for routing weights",
      "        original_scores = scores",
      "        scores = scores + e_score_correction_bias.unsqueeze(0)",
      "        group_scores = (scores.view(num_token, num_expert_group,",
      "                                    -1).topk(2, dim=-1)[0].sum(dim=-1))",
      "    else:",
      "        group_scores = scores.view(num_token, num_expert_group,",
      "                                   -1).max(dim=-1).values  # [n, n_group]",
      "    group_idx = torch.topk(group_scores, k=topk_group, dim=-1,",
      "                           sorted=False)[1]  # [n, top_k_group]",
      "    group_mask = torch.zeros_like(group_scores)  # [n, n_group]",
      "    group_mask.scatter_(1, group_idx, 1)  # [n, n_group]",
      "    score_mask = group_mask.unsqueeze(-1).expand(",
      "        num_token, num_expert_group,",
      "        scores.size(-1) // num_expert_group).reshape(num_token, -1)  # [n, e]",
      "    tmp_scores = scores.masked_fill(~score_mask.bool(),",
      "                                    float(\"-inf\"))  # [n, e]",
      "",
      "    if e_score_correction_bias is not None:",
      "        topk_ids = torch.topk(tmp_scores, k=topk, dim=-1, sorted=False)[1]",
      "        # Use original unbiased scores for the routing weights",
      "        topk_weights = original_scores.gather(1, topk_ids)",
      "    else:",
      "        topk_weights, topk_ids = torch.topk(tmp_scores,",
      "                                            k=topk,",
      "                                            dim=-1,",
      "                                            sorted=False)",
      "",
      "    if renormalize:",
      "        topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)",
      "",
      "    if routed_scaling_factor != 1.0:",
      "        topk_weights = topk_weights * routed_scaling_factor",
      "    return topk_weights.to(torch.float32), topk_ids.to(torch.int32)",
      "",
      "",
      "def fused_grouped_topk(",
      "    hidden_states: torch.Tensor,",
      "    gating_output: torch.Tensor,",
      "    topk: int,",
      "    renormalize: bool,",
      "    e_score_correction_bias: torch.Tensor,",
      "    num_expert_group: int = 0,",
      "    topk_group: int = 0,",
      "    scoring_func: str = \"softmax\",",
      "    routed_scaling_factor: float = 1.0,",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    assert hidden_states.size(0) == gating_output.size(0), (",
      "        \"Number of tokens mismatch\")",
      "",
      "    if scoring_func == \"softmax\":",
      "        scores = torch.softmax(gating_output, dim=-1)",
      "    elif scoring_func == \"sigmoid\":",
      "        scores = gating_output.sigmoid()",
      "    else:",
      "        raise ValueError(f\"Unsupported scoring function: {scoring_func}\")",
      "",
      "    scores_with_bias = scores + e_score_correction_bias.unsqueeze(0)",
      "    topk_values, topk_indices = ops.grouped_topk(",
      "        scores, scores_with_bias.to(scores.dtype), num_expert_group,",
      "        topk_group, topk, renormalize, routed_scaling_factor)",
      "    return topk_values.to(torch.float32), topk_indices.to(torch.int32)",
      "",
      "",
      "def get_config_dtype_str(",
      "        dtype: torch.dtype,",
      "        use_int4_w4a16: Optional[bool] = False,",
      "        use_int8_w8a16: Optional[bool] = False,",
      "        use_fp8_w8a8: Optional[bool] = False,",
      "        use_mxfp4_w4a4: Optional[bool] = False) -> Optional[str]:",
      "    if use_fp8_w8a8:",
      "        return \"fp8_w8a8\"",
      "    elif use_int8_w8a16:",
      "        return \"int8_w8a16\"",
      "    elif use_int4_w4a16:",
      "        return \"int4_w4a16\"",
      "    elif use_mxfp4_w4a4:",
      "        return \"mxfp4_w4a4\"",
      "    elif dtype == torch.float:",
      "        # avoiding cases where kernel fails when float32 MoE",
      "        # use fp16/bfloat16 configs",
      "        return \"float32\"",
      "    return None",
      "",
      "",
      "def inplace_fused_experts(",
      "        hidden_states: torch.Tensor,",
      "        w1: torch.Tensor,",
      "        w2: torch.Tensor,",
      "        topk_weights: torch.Tensor,",
      "        topk_ids: torch.Tensor,",
      "        activation: str = \"silu\",",
      "        is_act_and_mul: bool = True,",
      "        apply_router_weight_on_input: bool = False,",
      "        use_fp8_w8a8: bool = False,",
      "        use_int8_w8a8: bool = False,",
      "        use_int8_w8a16: bool = False,",
      "        use_int4_w4a16: bool = False,",
      "        use_mxfp4_w4a4: bool = False,",
      "        per_channel_quant: bool = False,",
      "        global_num_experts: int = -1,",
      "        expert_map: Optional[torch.Tensor] = None,",
      "        w1_scale: Optional[torch.Tensor] = None,",
      "        w2_scale: Optional[torch.Tensor] = None,",
      "        w1_zp: Optional[torch.Tensor] = None,",
      "        w2_zp: Optional[torch.Tensor] = None,",
      "        a1_scale: Optional[torch.Tensor] = None,",
      "        a2_scale: Optional[torch.Tensor] = None,",
      "        block_shape: Optional[List[int]] = None,  #noqa: UP006",
      "        w1_bias: Optional[torch.Tensor] = None,",
      "        w2_bias: Optional[torch.Tensor] = None) -> None:",
      "    fused_experts_impl(hidden_states, w1, w2, topk_weights, topk_ids, True,",
      "                       activation, is_act_and_mul,",
      "                       apply_router_weight_on_input, use_fp8_w8a8,",
      "                       use_int8_w8a8, use_int8_w8a16, use_int4_w4a16,",
      "                       use_mxfp4_w4a4, per_channel_quant, global_num_experts,",
      "                       expert_map, w1_scale, w2_scale, w1_zp, w2_zp, a1_scale,",
      "                       a2_scale, block_shape, w1_bias, w2_bias)",
      "",
      "",
      "def inplace_fused_experts_fake(hidden_states: torch.Tensor,",
      "                               w1: torch.Tensor,",
      "                               w2: torch.Tensor,",
      "                               topk_weights: torch.Tensor,",
      "                               topk_ids: torch.Tensor,",
      "                               activation: str = \"silu\",",
      "                               is_act_and_mul: bool = True,",
      "                               apply_router_weight_on_input: bool = False,",
      "                               use_fp8_w8a8: bool = False,",
      "                               use_int8_w8a8: bool = False,",
      "                               use_int8_w8a16: bool = False,",
      "                               use_int4_w4a16: bool = False,",
      "                               use_mxfp4_w4a4: bool = False,",
      "                               per_channel_quant: bool = False,",
      "                               global_num_experts: int = -1,",
      "                               expert_map: Optional[torch.Tensor] = None,",
      "                               w1_scale: Optional[torch.Tensor] = None,",
      "                               w2_scale: Optional[torch.Tensor] = None,",
      "                               w1_zp: Optional[torch.Tensor] = None,",
      "                               w2_zp: Optional[torch.Tensor] = None,",
      "                               a1_scale: Optional[torch.Tensor] = None,",
      "                               a2_scale: Optional[torch.Tensor] = None,",
      "                               block_shape: Optional[list[int]] = None,",
      "                               w1_bias: Optional[torch.Tensor] = None,",
      "                               w2_bias: Optional[torch.Tensor] = None) -> None:",
      "    pass",
      "",
      "",
      "direct_register_custom_op(",
      "    op_name=\"inplace_fused_experts\",",
      "    op_func=inplace_fused_experts,",
      "    mutates_args=[\"hidden_states\"],",
      "    fake_impl=inplace_fused_experts_fake,",
      "    tags=(() if is_torch_equal_or_newer(\"2.7.0\") else",
      "          (torch.Tag.needs_fixed_stride_order, )),",
      ")",
      "",
      "",
      "def flashinfer_fused_moe_blockscale_fp8(",
      "        routing_logits: torch.Tensor,",
      "        routing_bias: torch.Tensor,",
      "        x: torch.Tensor,",
      "        w13_weight: torch.Tensor,",
      "        w13_weight_scale_inv: torch.Tensor,",
      "        w2_weight: torch.Tensor,",
      "        w2_weight_scale_inv: torch.Tensor,",
      "        global_num_experts: int,",
      "        top_k: int,",
      "        num_expert_group: int,",
      "        topk_group: int,",
      "        intermediate_size: int,",
      "        expert_offset: int,",
      "        local_num_experts: int,",
      "        block_shape: List[int],  #noqa: UP006",
      "        routed_scaling: float = 1.0) -> torch.Tensor:",
      "    from vllm.utils.flashinfer import flashinfer_trtllm_fp8_block_scale_moe",
      "    assert top_k <= global_num_experts",
      "    assert top_k <= 8",
      "    assert topk_group <= 4",
      "    assert global_num_experts > num_expert_group",
      "    assert global_num_experts % num_expert_group == 0",
      "    assert global_num_experts % 4 == 0",
      "    assert top_k < (topk_group * global_num_experts / num_expert_group)",
      "    assert block_shape == [128, 128]",
      "",
      "    a_q, a_sf = per_token_group_quant_fp8(x, block_shape[1])",
      "    # NOTE: scales of hidden states have to be transposed!",
      "    a_sf_t = a_sf.t().contiguous()",
      "    return flashinfer_trtllm_fp8_block_scale_moe(",
      "        routing_logits=routing_logits,",
      "        routing_bias=routing_bias,",
      "        hidden_states=a_q,",
      "        hidden_states_scale=a_sf_t,",
      "        gemm1_weights=w13_weight,",
      "        gemm1_weights_scale=w13_weight_scale_inv,",
      "        gemm2_weights=w2_weight,",
      "        gemm2_weights_scale=w2_weight_scale_inv,",
      "        num_experts=global_num_experts,",
      "        top_k=top_k,",
      "        n_group=num_expert_group,",
      "        topk_group=topk_group,",
      "        intermediate_size=intermediate_size,",
      "        local_expert_offset=expert_offset,",
      "        local_num_experts=local_num_experts,",
      "        routed_scaling_factor=routed_scaling,",
      "        tile_tokens_dim=calculate_tile_tokens_dim(x.shape[0], top_k,",
      "                                                  global_num_experts),",
      "        routing_method_type=2,  # DeepSeek-styled routing method",
      "        use_shuffled_weight=False,",
      "    )",
      "",
      "",
      "def flashinfer_fused_moe_blockscale_fp8_fake(",
      "        routing_logits: torch.Tensor,",
      "        routing_bias: torch.Tensor,",
      "        x: torch.Tensor,",
      "        w13_weight: torch.Tensor,",
      "        w13_weight_scale_inv: torch.Tensor,",
      "        w2_weight: torch.Tensor,",
      "        w2_weight_scale_inv: torch.Tensor,",
      "        global_num_experts: int,",
      "        top_k: int,",
      "        num_expert_group: int,",
      "        topk_group: int,",
      "        intermediate_size: int,",
      "        expert_offset: int,",
      "        local_num_experts: int,",
      "        block_shape: list[int],",
      "        routed_scaling: float = 1.0) -> torch.Tensor:",
      "    return torch.empty_like(x)",
      "",
      "",
      "direct_register_custom_op(",
      "    op_name=\"flashinfer_fused_moe_blockscale_fp8\",",
      "    op_func=flashinfer_fused_moe_blockscale_fp8,",
      "    mutates_args=[],",
      "    fake_impl=flashinfer_fused_moe_blockscale_fp8_fake,",
      "    tags=(torch.Tag.needs_fixed_stride_order, ),",
      ")",
      "",
      "",
      "def flashinfer_fused_moe_per_tensor_scale_fp8(",
      "        routing_logits: torch.Tensor,",
      "        routing_bias: Optional[torch.Tensor],",
      "        hidden_states: torch.Tensor,",
      "        input_scale: torch.Tensor,",
      "        gemm1_weights: torch.Tensor,",
      "        gemm2_weights: torch.Tensor,",
      "        output1_scales_scalar: torch.Tensor,",
      "        output1_scales_gate_scalar: torch.Tensor,",
      "        output2_scales_scalar: torch.Tensor,",
      "        num_experts: int,",
      "        top_k: int,",
      "        num_expert_group: Optional[int],",
      "        topk_group: Optional[int],",
      "        intermediate_size: int,",
      "        local_expert_offset: int,",
      "        local_num_experts: int,",
      "        use_routing_scales_on_input: bool,",
      "        routing_method_type: int,",
      "        routed_scaling_factor: float = 1.0) -> torch.Tensor:",
      "    num_expert_group = num_expert_group if num_expert_group is not None else 0",
      "    topk_group = topk_group if topk_group is not None else 0",
      "",
      "    quant_hidden_states, _ = moe_kernel_quantize_input(",
      "        hidden_states,",
      "        input_scale,",
      "        quant_dtype=torch.float8_e4m3fn,",
      "        per_act_token_quant=False)",
      "",
      "    from vllm.utils.flashinfer import (",
      "        flashinfer_trtllm_fp8_per_tensor_scale_moe)",
      "    return flashinfer_trtllm_fp8_per_tensor_scale_moe(",
      "        routing_logits=routing_logits,",
      "        routing_bias=routing_bias,",
      "        hidden_states=quant_hidden_states,",
      "        gemm1_weights=gemm1_weights,",
      "        output1_scales_scalar=output1_scales_scalar,",
      "        output1_scales_gate_scalar=output1_scales_gate_scalar,",
      "        gemm2_weights=gemm2_weights,",
      "        output2_scales_scalar=output2_scales_scalar,",
      "        num_experts=num_experts,",
      "        top_k=top_k,",
      "        n_group=num_expert_group,",
      "        topk_group=topk_group,",
      "        intermediate_size=intermediate_size,",
      "        local_expert_offset=local_expert_offset,",
      "        local_num_experts=local_num_experts,",
      "        routed_scaling_factor=routed_scaling_factor,",
      "        use_routing_scales_on_input=use_routing_scales_on_input,",
      "        tile_tokens_dim=calculate_tile_tokens_dim(hidden_states.shape[0],",
      "                                                  top_k, num_experts),",
      "        routing_method_type=routing_method_type)",
      "",
      "",
      "def flashinfer_fused_moe_per_tensor_scale_fp8_fake(",
      "        routing_logits: torch.Tensor,",
      "        routing_bias: Optional[torch.Tensor],",
      "        hidden_states: torch.Tensor,",
      "        input_scale: torch.Tensor,",
      "        gemm1_weights: torch.Tensor,",
      "        gemm2_weights: torch.Tensor,",
      "        output1_scales_scalar: torch.Tensor,",
      "        output1_scales_gate_scalar: torch.Tensor,",
      "        output2_scales_scalar: torch.Tensor,",
      "        num_experts: int,",
      "        top_k: int,",
      "        num_expert_group: Optional[int],",
      "        topk_group: Optional[int],",
      "        intermediate_size: int,",
      "        local_expert_offset: int,",
      "        local_num_experts: int,",
      "        use_routing_scales_on_input: bool,",
      "        routing_method_type: int,",
      "        routed_scaling_factor: float = 1.0) -> torch.Tensor:",
      "    pass",
      "",
      "",
      "direct_register_custom_op(",
      "    op_name=\"flashinfer_fused_moe_per_tensor_scale_fp8\",",
      "    op_func=flashinfer_fused_moe_per_tensor_scale_fp8,",
      "    mutates_args=[\"hidden_states\"],",
      "    fake_impl=flashinfer_fused_moe_per_tensor_scale_fp8_fake,",
      "    tags=(torch.Tag.needs_fixed_stride_order, ),",
      ")",
      "",
      "",
      "def outplace_fused_experts(",
      "    hidden_states: torch.Tensor,",
      "    w1: torch.Tensor,",
      "    w2: torch.Tensor,",
      "    topk_weights: torch.Tensor,",
      "    topk_ids: torch.Tensor,",
      "    activation: str = \"silu\",",
      "    is_act_and_mul: bool = True,",
      "    apply_router_weight_on_input: bool = False,",
      "    use_fp8_w8a8: bool = False,",
      "    use_int8_w8a8: bool = False,",
      "    use_int8_w8a16: bool = False,",
      "    use_int4_w4a16: bool = False,",
      "    use_mxfp4_w4a4: bool = False,",
      "    per_channel_quant: bool = False,",
      "    global_num_experts: int = -1,",
      "    expert_map: Optional[torch.Tensor] = None,",
      "    w1_scale: Optional[torch.Tensor] = None,",
      "    w2_scale: Optional[torch.Tensor] = None,",
      "    w1_zp: Optional[torch.Tensor] = None,",
      "    w2_zp: Optional[torch.Tensor] = None,",
      "    a1_scale: Optional[torch.Tensor] = None,",
      "    a2_scale: Optional[torch.Tensor] = None,",
      "    block_shape: Optional[List[int]] = None,  #noqa: UP006",
      "    w1_bias: Optional[torch.Tensor] = None,",
      "    w2_bias: Optional[torch.Tensor] = None,",
      ") -> torch.Tensor:",
      "    return fused_experts_impl(",
      "        hidden_states, w1, w2, topk_weights, topk_ids, False, activation,",
      "        is_act_and_mul, apply_router_weight_on_input, use_fp8_w8a8,",
      "        use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, use_mxfp4_w4a4,",
      "        per_channel_quant, global_num_experts, expert_map, w1_scale, w2_scale,",
      "        w1_zp, w2_zp, a1_scale, a2_scale, block_shape, w1_bias, w2_bias)",
      "",
      "",
      "def outplace_fused_experts_fake(",
      "        hidden_states: torch.Tensor,",
      "        w1: torch.Tensor,",
      "        w2: torch.Tensor,",
      "        topk_weights: torch.Tensor,",
      "        topk_ids: torch.Tensor,",
      "        activation: str = \"silu\",",
      "        is_act_and_mul: bool = True,",
      "        use_fp8_w8a8: bool = False,",
      "        use_int8_w8a8: bool = False,",
      "        use_int8_w8a16: bool = False,",
      "        use_int4_w4a16: bool = False,",
      "        use_mxfp4_w4a4: bool = False,",
      "        per_channel_quant: bool = False,",
      "        global_num_experts: int = -1,",
      "        expert_map: Optional[torch.Tensor] = None,",
      "        w1_scale: Optional[torch.Tensor] = None,",
      "        w2_scale: Optional[torch.Tensor] = None,",
      "        w1_zp: Optional[torch.Tensor] = None,",
      "        w2_zp: Optional[torch.Tensor] = None,",
      "        a1_scale: Optional[torch.Tensor] = None,",
      "        a2_scale: Optional[torch.Tensor] = None,",
      "        block_shape: Optional[list[int]] = None,",
      "        w1_bias: Optional[torch.Tensor] = None,",
      "        w2_bias: Optional[torch.Tensor] = None) -> torch.Tensor:",
      "    return torch.empty_like(hidden_states)",
      "",
      "",
      "direct_register_custom_op(",
      "    op_name=\"outplace_fused_experts\",",
      "    op_func=outplace_fused_experts,",
      "    mutates_args=[],",
      "    fake_impl=outplace_fused_experts_fake,",
      "    tags=(() if is_torch_equal_or_newer(\"2.7.0\") else",
      "          (torch.Tag.needs_fixed_stride_order, )),",
      ")",
      "",
      "",
      "def torch_vllm_inplace_fused_experts(**kwargs) -> torch.Tensor:",
      "    torch.ops.vllm.inplace_fused_experts(**kwargs)",
      "    hidden_states = kwargs['hidden_states']",
      "    return hidden_states",
      "",
      "",
      "def torch_vllm_outplace_fused_experts(**kwargs) -> torch.Tensor:",
      "    return torch.ops.vllm.outplace_fused_experts(**kwargs)",
      "",
      "",
      "def dispatch_fused_experts_func(inplace: bool) -> Callable[..., torch.Tensor]:",
      "    if inplace:",
      "        return torch_vllm_inplace_fused_experts",
      "    return torch_vllm_outplace_fused_experts",
      "",
      "",
      "# TODO (bnell): replace this with modular op.  Can get rid of inplace/outplace",
      "# torch ops.",
      "def fused_experts(hidden_states: torch.Tensor,",
      "                  w1: torch.Tensor,",
      "                  w2: torch.Tensor,",
      "                  topk_weights: torch.Tensor,",
      "                  topk_ids: torch.Tensor,",
      "                  inplace: bool = False,",
      "                  activation: str = \"silu\",",
      "                  is_act_and_mul: bool = True,",
      "                  apply_router_weight_on_input: bool = False,",
      "                  use_fp8_w8a8: bool = False,",
      "                  use_int8_w8a8: bool = False,",
      "                  use_int8_w8a16: bool = False,",
      "                  use_int4_w4a16: bool = False,",
      "                  use_mxfp4_w4a4: bool = False,",
      "                  per_channel_quant: bool = False,",
      "                  global_num_experts: int = -1,",
      "                  expert_map: Optional[torch.Tensor] = None,",
      "                  w1_scale: Optional[torch.Tensor] = None,",
      "                  w2_scale: Optional[torch.Tensor] = None,",
      "                  w1_zp: Optional[torch.Tensor] = None,",
      "                  w2_zp: Optional[torch.Tensor] = None,",
      "                  a1_scale: Optional[torch.Tensor] = None,",
      "                  a2_scale: Optional[torch.Tensor] = None,",
      "                  block_shape: Optional[list[int]] = None,",
      "                  allow_deep_gemm: bool = False,",
      "                  allow_cutlass_block_scaled_grouped_gemm: bool = False,",
      "                  w1_bias: Optional[torch.Tensor] = None,",
      "                  w2_bias: Optional[torch.Tensor] = None) -> torch.Tensor:",
      "    # For now, disable DeepGemm for small N (<= 512) until better",
      "    # permute/unpermute ops are available.",
      "    # However, on B200, we use DeepGemm for all cases because they only support",
      "    # E8M0 scale, which means we requantize the weight and input to the specific",
      "    # scale. Fallen back to cutlass or triton for some cases would cause",
      "    # accuracy issue.",
      "    if (allow_deep_gemm and use_fp8_w8a8 and",
      "        (is_deep_gemm_e8m0_used() or _valid_deep_gemm(hidden_states, w1, w2))):",
      "        assert apply_router_weight_on_input is False",
      "        assert is_act_and_mul, (",
      "            \"DeepGemm only supports is_act_and_mul=True for now.\")",
      "        return deep_gemm_moe_fp8(",
      "            hidden_states=hidden_states,",
      "            w1=w1,",
      "            w2=w2,",
      "            topk_weights=topk_weights,",
      "            topk_ids=topk_ids,",
      "            inplace=inplace,",
      "            activation=activation,",
      "            global_num_experts=global_num_experts,",
      "            expert_map=expert_map,",
      "            w1_scale=w1_scale,",
      "            w2_scale=w2_scale,",
      "            a1_scale=a1_scale,",
      "            a2_scale=a2_scale,",
      "            apply_router_weight_on_input=apply_router_weight_on_input,",
      "        )",
      "    elif (allow_cutlass_block_scaled_grouped_gemm and use_fp8_w8a8",
      "          and _valid_cutlass_block_scaled_grouped_gemm(",
      "              w1, w2, inplace, activation, apply_router_weight_on_input,",
      "              expert_map)):",
      "        return run_cutlass_block_scaled_fused_experts(",
      "            a=hidden_states,",
      "            w1=w1,",
      "            w2=w2,",
      "            w1_scale=w1_scale,",
      "            w2_scale=w2_scale,",
      "            topk_weights=topk_weights,",
      "            topk_ids=topk_ids)",
      "    else:",
      "        return dispatch_fused_experts_func(inplace)(",
      "            hidden_states=hidden_states,",
      "            w1=w1,",
      "            w2=w2,",
      "            topk_weights=topk_weights,",
      "            topk_ids=topk_ids,",
      "            activation=activation,",
      "            is_act_and_mul=is_act_and_mul,",
      "            apply_router_weight_on_input=apply_router_weight_on_input,",
      "            use_fp8_w8a8=use_fp8_w8a8,",
      "            use_int8_w8a8=use_int8_w8a8,",
      "            use_int8_w8a16=use_int8_w8a16,",
      "            use_int4_w4a16=use_int4_w4a16,",
      "            use_mxfp4_w4a4=use_mxfp4_w4a4,",
      "            per_channel_quant=per_channel_quant,",
      "            global_num_experts=global_num_experts,",
      "            expert_map=expert_map,",
      "            w1_scale=w1_scale,",
      "            w2_scale=w2_scale,",
      "            w1_zp=w1_zp,",
      "            w2_zp=w2_zp,",
      "            a1_scale=a1_scale,",
      "            a2_scale=a2_scale,",
      "            block_shape=block_shape,",
      "            w1_bias=w1_bias,",
      "            w2_bias=w2_bias,",
      "        )",
      "",
      "",
      "def fused_experts_impl(",
      "    hidden_states: torch.Tensor,",
      "    w1: torch.Tensor,",
      "    w2: torch.Tensor,",
      "    topk_weights: torch.Tensor,",
      "    topk_ids: torch.Tensor,",
      "    inplace: bool = False,",
      "    activation: str = \"silu\",",
      "    is_act_and_mul: bool = True,",
      "    apply_router_weight_on_input: bool = False,",
      "    use_fp8_w8a8: bool = False,",
      "    use_int8_w8a8: bool = False,",
      "    use_int8_w8a16: bool = False,",
      "    use_int4_w4a16: bool = False,",
      "    use_mxfp4_w4a4: bool = False,",
      "    per_channel_quant: bool = False,",
      "    global_num_experts: int = -1,",
      "    expert_map: Optional[torch.Tensor] = None,",
      "    w1_scale: Optional[torch.Tensor] = None,",
      "    w2_scale: Optional[torch.Tensor] = None,",
      "    w1_zp: Optional[torch.Tensor] = None,",
      "    w2_zp: Optional[torch.Tensor] = None,",
      "    a1_scale: Optional[torch.Tensor] = None,",
      "    a2_scale: Optional[torch.Tensor] = None,",
      "    block_shape: Optional[list[int]] = None,",
      "    w1_bias: Optional[torch.Tensor] = None,",
      "    w2_bias: Optional[torch.Tensor] = None,",
      ") -> torch.Tensor:",
      "    # Check constraints.",
      "    if use_int4_w4a16:",
      "        assert hidden_states.size(1) // 2 == w1.size(2), (",
      "            \"Hidden size mismatch\")",
      "    elif use_mxfp4_w4a4:",
      "        # 16bit activation and fp4x2 packed weight",
      "        assert hidden_states.size(1) // 2 == w1.size(2), \"hidden size mismatch\"",
      "    else:",
      "        assert hidden_states.size(1) == w1.size(2), (",
      "            f\"Hidden size mismatch {hidden_states.size(1)} != {w1.size(2)}\")",
      "",
      "    assert topk_weights.size() == topk_ids.size(), \"topk shape mismatch\"",
      "    assert hidden_states.is_contiguous(), \"Hidden_states must be contiguous\"",
      "    assert w1.stride(-1) == 1, \"Stride of last dimension must be 1\"",
      "    assert w2.stride(-1) == 1, \"Stride of last dimension must be 1\"",
      "    assert hidden_states.dtype in [",
      "        torch.float32, torch.float16, torch.bfloat16",
      "    ]",
      "",
      "    num_tokens = hidden_states.size(0)",
      "    E, N, _ = w1.size()",
      "    K = w2.size(1)",
      "    if global_num_experts == -1:",
      "        global_num_experts = E",
      "    top_k_num = topk_ids.size(1)",
      "    # We execute the fused_moe kernel in chunks to circumvent this issue:",
      "    # https://github.com/vllm-project/vllm/issues/5938",
      "    CHUNK_SIZE = envs.VLLM_FUSED_MOE_CHUNK_SIZE",
      "    M = min(num_tokens, CHUNK_SIZE)",
      "    config_dtype = get_config_dtype_str(use_fp8_w8a8=use_fp8_w8a8,",
      "                                        use_int8_w8a16=use_int8_w8a16,",
      "                                        use_int4_w4a16=use_int4_w4a16,",
      "                                        use_mxfp4_w4a4=use_mxfp4_w4a4,",
      "                                        dtype=hidden_states.dtype)",
      "",
      "    qtype = get_config_quant_dtype(use_fp8_w8a8=use_fp8_w8a8,",
      "                                   use_int8_w8a8=use_int8_w8a8,",
      "                                   use_int8_w8a16=use_int8_w8a16,",
      "                                   use_int4_w4a16=use_int4_w4a16,",
      "                                   use_mxfp4_w4a4=use_mxfp4_w4a4)",
      "",
      "    get_config_func = functools.partial(",
      "        try_get_optimal_moe_config,",
      "        w1.size(),",
      "        w2.size(),",
      "        top_k_num,",
      "        config_dtype,",
      "        block_shape=block_shape,",
      "    )",
      "",
      "    config = get_config_func(M)",
      "",
      "    # We can reuse the memory between these because by the time we need",
      "    # cache3, we're done with cache1",
      "    cache13 = torch.empty(M * top_k_num * max(N, K),",
      "                          device=hidden_states.device,",
      "                          dtype=hidden_states.dtype)",
      "    intermediate_cache1 = cache13[:M * top_k_num * N].view(M, top_k_num, N)",
      "    intermediate_cache3 = cache13[:M * top_k_num * K].view(M, top_k_num, K)",
      "",
      "    # This needs separate memory since it's used concurrently with cache1",
      "    intermediate_cache2 = torch.empty((M * top_k_num, N // 2),",
      "                                      device=hidden_states.device,",
      "                                      dtype=hidden_states.dtype)",
      "",
      "    if hidden_states.dtype == torch.bfloat16:",
      "        compute_type = tl.bfloat16",
      "    elif hidden_states.dtype == torch.float16:",
      "        compute_type = tl.float16",
      "    elif hidden_states.dtype == torch.float32:",
      "        compute_type = tl.float32",
      "    else:",
      "        raise ValueError(f\"Unsupported compute_type: {hidden_states.dtype}\")",
      "",
      "    if inplace:",
      "        out_hidden_states = hidden_states",
      "    else:",
      "        out_hidden_states = torch.empty_like(hidden_states)",
      "",
      "    if use_mxfp4_w4a4:",
      "        # Weight has to be dequantized for mxfp4 emulation.",
      "        w1 = dequant_mxfp4(w1, w1_scale, hidden_states.dtype)",
      "        w1_scale = None",
      "        w2 = dequant_mxfp4(w2, w2_scale, hidden_states.dtype)",
      "        w2_scale = None",
      "",
      "    for chunk in range((num_tokens // CHUNK_SIZE) + 1):",
      "        begin_chunk_idx, end_chunk_idx = (chunk * CHUNK_SIZE,",
      "                                          min((chunk + 1) * CHUNK_SIZE,",
      "                                              num_tokens))",
      "        curr_hidden_states = hidden_states[begin_chunk_idx:end_chunk_idx]",
      "        tokens_in_chunk, _ = curr_hidden_states.size()",
      "",
      "        if tokens_in_chunk == 0:",
      "            break",
      "",
      "        if tokens_in_chunk < CHUNK_SIZE and chunk > 0:",
      "            # Adjust the intermediate cache size and config for the last",
      "            # chunk. Note that in most cases we only have one chunk",
      "            # so the cache size and config are already set correctly and",
      "            # do not need to be adjusted.",
      "            intermediate_cache1 = intermediate_cache1[:tokens_in_chunk]",
      "            intermediate_cache2 = intermediate_cache2[:tokens_in_chunk *",
      "                                                      topk_ids.size(1)]",
      "            intermediate_cache3 = intermediate_cache3[:tokens_in_chunk]",
      "            config = get_config_func(tokens_in_chunk)",
      "",
      "        curr_topk_ids = topk_ids[begin_chunk_idx:end_chunk_idx]",
      "        curr_topk_weights = topk_weights[begin_chunk_idx:end_chunk_idx]",
      "        qcurr_hidden_states, a1q_scale = moe_kernel_quantize_input(",
      "            A=curr_hidden_states,",
      "            A_scale=a1_scale,",
      "            quant_dtype=qtype,",
      "            per_act_token_quant=per_channel_quant,",
      "            block_shape=block_shape)",
      "",
      "        sorted_token_ids, expert_ids, num_tokens_post_padded = (",
      "            moe_align_block_size(curr_topk_ids, config['BLOCK_SIZE_M'],",
      "                                 global_num_experts, expert_map))",
      "",
      "        invoke_fused_moe_kernel(qcurr_hidden_states,",
      "                                w1,",
      "                                intermediate_cache1,",
      "                                a1q_scale,",
      "                                w1_scale,",
      "                                w1_zp,",
      "                                curr_topk_weights,",
      "                                sorted_token_ids,",
      "                                expert_ids,",
      "                                num_tokens_post_padded,",
      "                                apply_router_weight_on_input,",
      "                                top_k_num,",
      "                                config,",
      "                                compute_type=compute_type,",
      "                                use_fp8_w8a8=use_fp8_w8a8,",
      "                                use_int8_w8a8=use_int8_w8a8,",
      "                                use_int8_w8a16=use_int8_w8a16,",
      "                                use_int4_w4a16=use_int4_w4a16,",
      "                                per_channel_quant=per_channel_quant,",
      "                                block_shape=block_shape,",
      "                                B_bias=w1_bias)",
      "",
      "        # Activation function with multiplication",
      "        if activation == \"silu\" and is_act_and_mul:",
      "            torch.ops._C.silu_and_mul(intermediate_cache2,",
      "                                      intermediate_cache1.view(-1, N))",
      "        elif activation == \"gelu\" and is_act_and_mul:",
      "            torch.ops._C.gelu_and_mul(intermediate_cache2,",
      "                                      intermediate_cache1.view(-1, N))",
      "        elif activation == \"swigluoai\" and is_act_and_mul:",
      "            # alpha = 1.702, limit = 7.0",
      "            torch.ops._C.swigluoai_and_mul(intermediate_cache2,",
      "                                           intermediate_cache1.view(-1, N))",
      "        # Activation function without multiplication",
      "        elif activation == \"silu\":",
      "            intermediate_cache2 = F.silu(intermediate_cache1.view(-1, N))",
      "        elif activation == \"gelu\":",
      "            intermediate_cache2 = F.gelu(intermediate_cache1.view(-1, N))",
      "",
      "        else:",
      "            raise ValueError(f\"Unsupported FusedMoe activation: {activation}, \"",
      "                             f\"with is_act_and_mul={is_act_and_mul}.\")",
      "",
      "        qintermediate_cache2, a2q_scale = moe_kernel_quantize_input(",
      "            A=intermediate_cache2,",
      "            A_scale=a2_scale,",
      "            quant_dtype=qtype,",
      "            per_act_token_quant=per_channel_quant,",
      "            block_shape=block_shape)",
      "",
      "        invoke_fused_moe_kernel(qintermediate_cache2,",
      "                                w2,",
      "                                intermediate_cache3,",
      "                                a2q_scale,",
      "                                w2_scale,",
      "                                w2_zp,",
      "                                curr_topk_weights,",
      "                                sorted_token_ids,",
      "                                expert_ids,",
      "                                num_tokens_post_padded,",
      "                                not apply_router_weight_on_input,",
      "                                1,",
      "                                config,",
      "                                compute_type=compute_type,",
      "                                use_fp8_w8a8=use_fp8_w8a8,",
      "                                use_int8_w8a8=use_int8_w8a8,",
      "                                use_int8_w8a16=use_int8_w8a16,",
      "                                use_int4_w4a16=use_int4_w4a16,",
      "                                per_channel_quant=per_channel_quant,",
      "                                block_shape=block_shape,",
      "                                B_bias=w2_bias)",
      "",
      "        ops.moe_sum(intermediate_cache3.view(*intermediate_cache3.size()),",
      "                    out_hidden_states[begin_chunk_idx:end_chunk_idx])",
      "",
      "    return out_hidden_states",
      "",
      "",
      "def fused_moe(",
      "    hidden_states: torch.Tensor,",
      "    w1: torch.Tensor,",
      "    w2: torch.Tensor,",
      "    gating_output: torch.Tensor,",
      "    topk: int,",
      "    renormalize: bool,",
      "    inplace: bool = False,",
      "    activation: str = \"silu\",",
      "    is_act_and_mul: bool = True,",
      "    use_grouped_topk: bool = False,",
      "    num_expert_group: Optional[int] = None,",
      "    topk_group: Optional[int] = None,",
      "    custom_routing_function: Optional[Callable] = None,",
      "    use_fp8_w8a8: bool = False,",
      "    use_int8_w8a8: bool = False,",
      "    use_int8_w8a16: bool = False,",
      "    use_int4_w4a16: bool = False,",
      "    use_mxfp4_w4a4: bool = False,",
      "    per_channel_quant: bool = False,",
      "    global_num_experts: int = -1,",
      "    expert_map: Optional[torch.Tensor] = None,",
      "    w1_scale: Optional[torch.Tensor] = None,",
      "    w2_scale: Optional[torch.Tensor] = None,",
      "    w1_zp: Optional[torch.Tensor] = None,",
      "    w2_zp: Optional[torch.Tensor] = None,",
      "    a1_scale: Optional[torch.Tensor] = None,",
      "    a2_scale: Optional[torch.Tensor] = None,",
      "    block_shape: Optional[list[int]] = None,",
      "    w1_bias: Optional[torch.Tensor] = None,",
      "    w2_bias: Optional[torch.Tensor] = None,",
      ") -> torch.Tensor:",
      "    \"\"\"",
      "    This function computes a Mixture of Experts (MoE) layer using two sets of",
      "    weights, w1 and w2, and top-k gating mechanism.",
      "",
      "    Parameters:",
      "    - hidden_states (torch.Tensor): The input tensor to the MoE layer.",
      "    - w1 (torch.Tensor): The first set of expert weights.",
      "    - w2 (torch.Tensor): The second set of expert weights.",
      "    - gating_output (torch.Tensor): The output of the gating operation",
      "        (before softmax).",
      "    - topk (int): The number of top-k experts to select.",
      "    - renormalize (bool): If True, renormalize the top-k weights to sum to 1.",
      "    - inplace (bool): If True, perform the operation in-place.",
      "        Defaults to False.",
      "    - activation (str): The activation function to apply after the first",
      "        MoE layer.",
      "    - is_act_and_mul (bool): If True, use activation-and-mul function for",
      "        activation (self-gated activation), otherwise use activation function",
      "        for activation (ungated activation).",
      "    - num_expert_group: Optional[int]: additional parameter for grouped_topk",
      "    - topk_group: Optional[int]: additional parameter for grouped_topk",
      "    - use_grouped_topk: If True, use grouped_topk instead of fused_topk",
      "        note: Deepseekv2 model uses grouped_topk",
      "    - use_fp8_w8a8 (bool): If True, use fp8 arithmetic to compute the inner",
      "        products for w1 and w2. Defaults to False.",
      "    - use_int8_w8a8 (bool): If True, use int8 arithmetic to compute the inner",
      "        products for w1 and w2. Defaults to False.",
      "    - use_int8_w8a16 (bool): If True, use matmul of int8 weight and bf16/fp16",
      "        activation to compute the inner products for w1 and w2.",
      "        Defaults to False.",
      "    - use_int4_w4a16 (bool): If True, use matmul of int4 weight and bf16/fp16",
      "        activation to compute the inner products for w1 and w2.",
      "        Defaults to False.",
      "    - use_mxfp4_w4a4 (bool): If True, use matmul of OCP MXFP4 weight and",
      "        OCP MXFP4 activation to compute the inner products for w1 and w2.",
      "        Defaults to False.",
      "    - global_num_experts (int): The total number of experts in the global",
      "        expert space.",
      "    - expert_map (Optional[torch.Tensor]):  A tensor mapping expert indices",
      "        from the global expert space to the local expert space of the expert",
      "        parallel shard.",
      "    - w1_scale (Optional[torch.Tensor]): Optional scale to be used for",
      "        w1.",
      "    - w2_scale (Optional[torch.Tensor]): Optional scale to be used for",
      "        w2.",
      "    - a1_scale (Optional[torch.Tensor]): Optional scale to be used for",
      "        a1.",
      "    - a2_scale (Optional[torch.Tensor]): Optional scale to be used for",
      "        a2.",
      "    - block_shape: (Optional[list[int]]): Optional block size for block-wise",
      "        quantization.",
      "",
      "    Returns:",
      "    - torch.Tensor: The output tensor after applying the MoE layer.",
      "    \"\"\"",
      "    if not is_act_and_mul:",
      "        assert inplace is False, (",
      "            \"is_act_and_mul=False is not supported with inplace=True\")",
      "",
      "    if use_grouped_topk:",
      "        assert num_expert_group is not None and topk_group is not None",
      "        topk_weights, topk_ids = grouped_topk(hidden_states, gating_output,",
      "                                              topk, renormalize,",
      "                                              num_expert_group, topk_group)",
      "    elif custom_routing_function is None:",
      "        topk_weights, topk_ids, token_expert_indices = fused_topk(",
      "            hidden_states, gating_output, topk, renormalize)",
      "    else:",
      "        topk_weights, topk_ids = custom_routing_function(",
      "            hidden_states, gating_output, topk, renormalize)",
      "",
      "    return fused_experts(hidden_states,",
      "                         w1,",
      "                         w2,",
      "                         topk_weights,",
      "                         topk_ids,",
      "                         inplace=inplace,",
      "                         activation=activation,",
      "                         is_act_and_mul=is_act_and_mul,",
      "                         use_fp8_w8a8=use_fp8_w8a8,",
      "                         use_int8_w8a8=use_int8_w8a8,",
      "                         use_int8_w8a16=use_int8_w8a16,",
      "                         use_int4_w4a16=use_int4_w4a16,",
      "                         use_mxfp4_w4a4=use_mxfp4_w4a4,",
      "                         per_channel_quant=per_channel_quant,",
      "                         global_num_experts=global_num_experts,",
      "                         expert_map=expert_map,",
      "                         w1_scale=w1_scale,",
      "                         w2_scale=w2_scale,",
      "                         w1_zp=w1_zp,",
      "                         w2_zp=w2_zp,",
      "                         a1_scale=a1_scale,",
      "                         a2_scale=a2_scale,",
      "                         block_shape=block_shape,",
      "                         w1_bias=w1_bias,",
      "                         w2_bias=w2_bias)",
      "",
      "",
      "class TritonExperts(mk.FusedMoEPermuteExpertsUnpermute):",
      "",
      "    def __init__(",
      "        self,",
      "        use_fp8_w8a8: bool = False,",
      "        use_int8_w8a8: bool = False,",
      "        use_int8_w8a16: bool = False,",
      "        use_int4_w4a16: bool = False,",
      "        use_mxfp4_w4a4: bool = False,",
      "        per_act_token_quant: bool = False,",
      "        block_shape: Optional[list[int]] = None,",
      "    ):",
      "        super().__init__(",
      "            FusedMoEQuantConfig.make(",
      "                use_fp8_w8a8=use_fp8_w8a8,",
      "                use_int8_w8a8=use_int8_w8a8,",
      "                use_int8_w8a16=use_int8_w8a16,",
      "                use_int4_w4a16=use_int4_w4a16,",
      "                use_mxfp4_w4a4=use_mxfp4_w4a4,",
      "                per_act_token_quant=per_act_token_quant,",
      "                block_shape=block_shape,",
      "            ))",
      "",
      "        self.use_fp8_w8a8 = use_fp8_w8a8",
      "        self.use_int4_w4a16 = use_int4_w4a16",
      "        self.use_int8_w8a8 = use_int8_w8a8",
      "        self.use_int8_w8a16 = use_int8_w8a16",
      "        self.use_mxfp4_w4a4 = use_mxfp4_w4a4",
      "",
      "    @property",
      "    def activation_formats(",
      "        self",
      "    ) -> tuple[mk.FusedMoEActivationFormat, mk.FusedMoEActivationFormat]:",
      "        return (mk.FusedMoEActivationFormat.Standard,",
      "                mk.FusedMoEActivationFormat.Standard)",
      "",
      "    def supports_chunking(self) -> bool:",
      "        return True",
      "",
      "    def supports_expert_map(self) -> bool:",
      "        return True",
      "",
      "    def finalize_weight_and_reduce_impl(self) -> mk.TopKWeightAndReduce:",
      "        return TopKWeightAndReduceNoOP()",
      "",
      "    def workspace_shapes(",
      "        self,",
      "        a: torch.Tensor,",
      "        aq: torch.Tensor,",
      "        M: int,",
      "        N: int,",
      "        K: int,",
      "        topk: int,",
      "        global_num_experts: int,",
      "        local_num_experts: int,",
      "        expert_tokens_meta: Optional[mk.ExpertTokensMetadata],",
      "    ) -> tuple[tuple[int, ...], tuple[int, ...], tuple[int, ...], torch.dtype]:",
      "        workspace1 = (M, topk, max(N // 2, K))",
      "        workspace2 = (M, topk, max(N, K))",
      "        output = (M, K)",
      "        return (workspace1, workspace2, output, a.dtype)",
      "",
      "    def apply(",
      "        self,",
      "        output: torch.Tensor,",
      "        hidden_states: torch.Tensor,",
      "        w1: torch.Tensor,",
      "        w2: torch.Tensor,",
      "        topk_weights: torch.Tensor,",
      "        topk_ids: torch.Tensor,",
      "        activation: str,",
      "        global_num_experts: int,",
      "        expert_map: Optional[torch.Tensor],",
      "        w1_scale: Optional[torch.Tensor],",
      "        w2_scale: Optional[torch.Tensor],",
      "        w1_zp: Optional[torch.Tensor],",
      "        w2_zp: Optional[torch.Tensor],",
      "        a1q_scale: Optional[torch.Tensor],",
      "        a2_scale: Optional[torch.Tensor],",
      "        workspace13: torch.Tensor,",
      "        workspace2: torch.Tensor,",
      "        expert_tokens_meta: Optional[mk.ExpertTokensMetadata],",
      "        apply_router_weight_on_input: bool,",
      "    ):",
      "        # Check constraints.",
      "        if self.use_int4_w4a16:",
      "            assert hidden_states.size(-1) // 2 == w1.size(2), (",
      "                \"Hidden size mismatch\")",
      "        else:",
      "            assert hidden_states.size(-1) == w1.size(2), \\",
      "                (f\"Hidden size mismatch {hidden_states.size(-1)} \"",
      "                 f\"!= {w1.size(2)}\")",
      "",
      "        assert hidden_states.is_contiguous(",
      "        ), \"Hidden_states must be contiguous\"",
      "        assert hidden_states.dim() == 2",
      "        assert w1.stride(-1) == 1, \"Stride of last dimension must be 1\"",
      "        assert w2.stride(-1) == 1, \"Stride of last dimension must be 1\"",
      "        assert hidden_states.dtype in [",
      "            torch.float32, torch.float16, torch.bfloat16, torch.float8_e4m3fn",
      "        ]",
      "",
      "        E, num_tokens, N, K, top_k_num = mk._moe_problem_size(",
      "            hidden_states, w1, w2, topk_ids)",
      "",
      "        if global_num_experts == -1:",
      "            global_num_experts = E",
      "",
      "        config_dtype = get_config_dtype_str(use_fp8_w8a8=self.use_fp8_w8a8,",
      "                                            use_int8_w8a16=self.use_int8_w8a16,",
      "                                            use_int4_w4a16=self.use_int4_w4a16,",
      "                                            use_mxfp4_w4a4=self.use_mxfp4_w4a4,",
      "                                            dtype=hidden_states.dtype)",
      "",
      "        config = try_get_optimal_moe_config(",
      "            w1.size(),",
      "            w2.size(),",
      "            top_k_num,",
      "            config_dtype,",
      "            num_tokens,",
      "            block_shape=self.block_shape,",
      "        )",
      "",
      "        if hidden_states.dtype == torch.bfloat16:",
      "            compute_type = tl.bfloat16",
      "        elif hidden_states.dtype == torch.float16:",
      "            compute_type = tl.float16",
      "        elif hidden_states.dtype == torch.float32:",
      "            compute_type = tl.float32",
      "        elif hidden_states.dtype == torch.float8_e4m3fn:",
      "            compute_type = tl.bfloat16",
      "        else:",
      "            raise ValueError(",
      "                f\"Unsupported compute_type: {hidden_states.dtype}\")",
      "",
      "        # Note that the output tensor might be in workspace1",
      "        intermediate_cache1 = _resize_cache(workspace2,",
      "                                            (num_tokens, top_k_num, N))",
      "        intermediate_cache2 = _resize_cache(workspace13,",
      "                                            (num_tokens * top_k_num, N // 2))",
      "        intermediate_cache3 = _resize_cache(workspace2,",
      "                                            (num_tokens, top_k_num, K))",
      "",
      "        sorted_token_ids, expert_ids, num_tokens_post_padded = (",
      "            moe_align_block_size(topk_ids, config['BLOCK_SIZE_M'],",
      "                                 global_num_experts, expert_map))",
      "",
      "        invoke_fused_moe_kernel(",
      "            hidden_states,",
      "            w1,",
      "            intermediate_cache1,",
      "            a1q_scale,",
      "            w1_scale,",
      "            w1_zp,",
      "            None,  # topk_weights",
      "            sorted_token_ids,",
      "            expert_ids,",
      "            num_tokens_post_padded,",
      "            False,  # mul_routed_weights",
      "            top_k_num,",
      "            config,",
      "            compute_type=compute_type,",
      "            use_fp8_w8a8=self.use_fp8_w8a8,",
      "            use_int8_w8a8=self.use_int8_w8a8,",
      "            use_int8_w8a16=self.use_int8_w8a16,",
      "            use_int4_w4a16=self.use_int4_w4a16,",
      "            per_channel_quant=self.per_act_token_quant,",
      "            block_shape=self.block_shape,",
      "            B_bias=None  # TODO support B_bias",
      "        )",
      "",
      "        self.activation(activation, intermediate_cache2,",
      "                        intermediate_cache1.view(-1, N))",
      "",
      "        a2q_scale: Optional[torch.Tensor] = None",
      "",
      "        qintermediate_cache2, a2q_scale = moe_kernel_quantize_input(",
      "            intermediate_cache2, a2_scale, self.quant_dtype,",
      "            self.per_act_token_quant, self.block_shape)",
      "",
      "        invoke_fused_moe_kernel(",
      "            qintermediate_cache2,",
      "            w2,",
      "            intermediate_cache3,",
      "            a2q_scale,",
      "            w2_scale,",
      "            w2_zp,",
      "            topk_weights,",
      "            sorted_token_ids,",
      "            expert_ids,",
      "            num_tokens_post_padded,",
      "            not apply_router_weight_on_input,",
      "            1,",
      "            config,",
      "            compute_type=compute_type,",
      "            use_fp8_w8a8=self.use_fp8_w8a8,",
      "            use_int8_w8a8=self.use_int8_w8a8,",
      "            use_int8_w8a16=self.use_int8_w8a16,",
      "            use_int4_w4a16=self.use_int4_w4a16,",
      "            per_channel_quant=self.per_act_token_quant,",
      "            block_shape=self.block_shape,",
      "            B_bias=None  # TODO support B_bias",
      "        )",
      "",
      "        ops.moe_sum(intermediate_cache3, output)",
      "",
      "",
      "def modular_triton_fused_moe(",
      "    use_fp8_w8a8: bool,",
      "    use_int8_w8a8: bool,",
      "    use_int8_w8a16: bool,",
      "    use_int4_w4a16: bool,",
      "    use_mxfp4_w4a4: bool,",
      "    per_act_token_quant: bool,",
      "    block_shape: Optional[list[int]] = None,",
      ") -> mk.FusedMoEModularKernel:",
      "    return mk.FusedMoEModularKernel(",
      "        MoEPrepareAndFinalizeNoEP(),",
      "        TritonExperts(",
      "            use_fp8_w8a8=use_fp8_w8a8,",
      "            use_int8_w8a8=use_int8_w8a8,",
      "            use_int8_w8a16=use_int8_w8a16,",
      "            use_int4_w4a16=use_int4_w4a16,",
      "            use_mxfp4_w4a4=use_mxfp4_w4a4,",
      "            per_act_token_quant=per_act_token_quant,",
      "            block_shape=block_shape,",
      "        ),",
      "    )"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/fused_moe/fused_batched_moe.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "\"\"\"Fused batched MoE kernel.\"\"\"",
      "from typing import Optional",
      "",
      "import torch",
      "",
      "import vllm.model_executor.layers.fused_moe.modular_kernel as mk",
      "from vllm.model_executor.layers.fused_moe.config import FusedMoEQuantConfig",
      "from vllm.model_executor.layers.fused_moe.fused_moe import (",
      "    get_config_dtype_str, try_get_optimal_moe_config)",
      "from vllm.model_executor.layers.fused_moe.topk_weight_and_reduce import (",
      "    TopKWeightAndReduceDelegate, TopKWeightAndReduceNaiveBatched)",
      "from vllm.model_executor.layers.fused_moe.utils import (",
      "    _resize_cache, moe_kernel_quantize_input, normalize_batched_scales_shape,",
      "    normalize_scales_shape)",
      "from vllm.model_executor.layers.quantization.utils.quant_utils import (",
      "    group_broadcast)",
      "from vllm.triton_utils import tl, triton",
      "",
      "",
      "@triton.jit",
      "def moe_mmk(",
      "    a_ptrs,",
      "    b_ptrs,",
      "    K,",
      "    expert_id,",
      "    a_scale_ptr,",
      "    b_scale_ptr,",
      "    # The stride variables represent how much to increase the ptr by when",
      "    # moving by 1 element in a particular dimension. E.g. `stride_am` is",
      "    # how much to increase `a_ptr` by to get the element one row down",
      "    # (A has M rows).",
      "    stride_ak: tl.int64,",
      "    stride_bk: tl.int64,",
      "    stride_ase: tl.int64,",
      "    stride_asm: tl.int64,",
      "    stride_ask: tl.int64,",
      "    stride_bse: tl.int64,",
      "    stride_bsk: tl.int64,",
      "    stride_bsn: tl.int64,",
      "    # Offsets and masks",
      "    offs_m,",
      "    offs_n,",
      "    offs_bn,",
      "    mask_m,",
      "    # Block size for block-wise quantization",
      "    group_n: tl.constexpr,",
      "    group_k: tl.constexpr,",
      "    # Meta-parameters",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    compute_type: tl.constexpr,",
      "    use_w8a8: tl.constexpr,",
      "    use_w8a16: tl.constexpr,",
      "    per_act_token_quant: tl.constexpr,",
      "):",
      "",
      "    offs_k = tl.arange(0, BLOCK_K)",
      "",
      "    if use_w8a16:",
      "        b_scale_ptrs = b_scale_ptr + expert_id * stride_bse + offs_n[",
      "            None, :] * stride_bsn",
      "        b_scale = tl.load(b_scale_ptrs)",
      "",
      "    if use_w8a8:",
      "        # block-wise",
      "        if group_k > 0 and group_n > 0:",
      "            a_scale_ptrs = a_scale_ptr + offs_m * stride_asm",
      "            offs_bsn = offs_bn // group_n",
      "            b_scale_ptrs = b_scale_ptr + offs_bsn * stride_bsn",
      "",
      "        # per act token",
      "        elif per_act_token_quant:",
      "            # Load per-token scale for activations",
      "            a_scale_ptrs = a_scale_ptr + offs_m * stride_asm",
      "            a_scale = tl.load(a_scale_ptrs, mask=mask_m, other=0.0)[:, None]",
      "",
      "            b_scale_ptrs = b_scale_ptr + offs_bn[None, :] * stride_bsn",
      "            b_scale = tl.load(b_scale_ptrs)",
      "",
      "        # tensor-wise",
      "        else:",
      "            a_scale = tl.load(a_scale_ptr)",
      "            b_scale = tl.load(b_scale_ptr)",
      "",
      "    # -----------------------------------------------------------",
      "    # Iterate to compute a block of the C matrix.",
      "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block",
      "    # of fp32 values for higher accuracy.",
      "    # `accumulator` will be converted back to fp16 after the loop.",
      "    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_K)):",
      "        # Load the next block of A and B, generate a mask by checking the",
      "        # K dimension.",
      "        a = tl.load(a_ptrs,",
      "                    mask=mask_m[:, None] & (offs_k[None, :] < K - k * BLOCK_K),",
      "                    other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_K, other=0.0)",
      "        # We accumulate along the K dimension.",
      "        if use_w8a16:",
      "            accumulator = tl.dot(a, b.to(compute_type), acc=accumulator)",
      "        elif use_w8a8:",
      "            if group_k > 0 and group_n > 0:",
      "                k_start = k * BLOCK_K",
      "                offs_ks = k_start // group_k",
      "                a_scale = tl.load(a_scale_ptrs + offs_ks * stride_ask,",
      "                                  mask=mask_m,",
      "                                  other=0.0)",
      "                b_scale = tl.load(b_scale_ptrs + offs_ks * stride_bsk)",
      "",
      "                accumulator += tl.dot(a, b) * a_scale[:,",
      "                                                      None] * b_scale[None, :]",
      "            else:",
      "                # acc used to enable fp8_fast_accum",
      "                accumulator = tl.dot(a, b, acc=accumulator)",
      "        else:",
      "            accumulator += tl.dot(a, b)",
      "",
      "        # Advance the ptrs to the next K block.",
      "        a_ptrs += BLOCK_K * stride_ak",
      "        b_ptrs += BLOCK_K * stride_bk",
      "",
      "    if use_w8a16:",
      "        accumulator = (accumulator * b_scale).to(compute_type)",
      "    elif use_w8a8:",
      "        if group_k > 0 and group_n > 0:",
      "            accumulator = accumulator.to(compute_type)",
      "        else:",
      "            accumulator = (accumulator * a_scale * b_scale).to(compute_type)",
      "    else:",
      "        accumulator = accumulator.to(compute_type)",
      "",
      "    return accumulator",
      "",
      "",
      "@triton.jit",
      "def expert_triton_kernel(",
      "    a_ptr,  #[max_tokens, K]",
      "    b_ptr,  #[K, N]",
      "    c_ptr,  #[max_tokens, N]",
      "    expert_id,",
      "    compute_type: tl.constexpr,",
      "    # Dimensions",
      "    M,",
      "    N,",
      "    K,",
      "    # Quantization data",
      "    a_scale_ptr,",
      "    b_scale_ptr,",
      "    b_zp_ptr,",
      "    # strides",
      "    stride_am: tl.int64,",
      "    stride_ak: tl.int64,",
      "    stride_bk: tl.int64,",
      "    stride_bn: tl.int64,",
      "    stride_cm: tl.int64,",
      "    stride_cn: tl.int64,",
      "    stride_ase: tl.int64,",
      "    stride_asm: tl.int64,",
      "    stride_ask: tl.int64,",
      "    stride_bse: tl.int64,",
      "    stride_bsk: tl.int64,",
      "    stride_bsn: tl.int64,",
      "    # offsets",
      "    offs_bn,",
      "    # Blockwise quantization data",
      "    group_n,",
      "    group_k,",
      "    # Quantization schemes",
      "    use_fp8_w8a8: tl.constexpr,",
      "    use_int8_w8a16: tl.constexpr,",
      "    per_act_token_quant: tl.constexpr,",
      "    # Kernel config",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "):",
      "",
      "    offs_m = tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N) % N",
      "    offs_k = tl.arange(0, BLOCK_K)",
      "    mask_m = offs_m < M",
      "",
      "    # Make grids of a + b pointers",
      "    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak",
      "    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn",
      "",
      "    accumulator = moe_mmk(",
      "        a_ptrs,",
      "        b_ptrs,",
      "        K,",
      "        expert_id,",
      "        a_scale_ptr,",
      "        b_scale_ptr,",
      "        # The stride variables represent how much to increase the ptr by when",
      "        # moving by 1 element in a particular dimension. E.g. `stride_am` is",
      "        # how much to increase `a_ptr` by to get the element one row down",
      "        # (A has M rows).",
      "        stride_ak,",
      "        stride_bk,",
      "        stride_ase,",
      "        stride_asm,",
      "        stride_ask,",
      "        stride_bse,",
      "        stride_bsk,",
      "        stride_bsn,",
      "        # Offsets and masks",
      "        offs_m,",
      "        offs_n,",
      "        offs_bn,",
      "        mask_m,",
      "        # Block size for block-wise quantization",
      "        group_n,",
      "        group_k,",
      "        # Meta-parameters",
      "        BLOCK_M,",
      "        BLOCK_N,",
      "        BLOCK_K,",
      "        compute_type,",
      "        use_fp8_w8a8,",
      "        use_int8_w8a16,",
      "        per_act_token_quant)",
      "",
      "    # store in C",
      "    offs_cn = tl.arange(0, BLOCK_N)",
      "    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_cn[None, :] * stride_cn",
      "    c_mask = mask_m[:, None] & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, accumulator, mask=c_mask)",
      "",
      "",
      "@triton.jit",
      "def batched_triton_kernel(",
      "    a_ptr,  # [E, max_num_tokens, K]",
      "    b_ptr,  # [E, K, N]",
      "    c_ptr,  # [E, max_num_tokens, N]",
      "    expert_num_tokens,  # [E]",
      "    compute_type: tl.constexpr,",
      "    # Dimensions",
      "    max_num_tokens,",
      "    K,",
      "    N,",
      "    # Quantization data",
      "    a_scale_ptr,",
      "    b_scale_ptr,",
      "    b_zp_ptr,",
      "    # The stride variables represent how much to increase the ptr by when",
      "    # moving by 1 element in a particular dimension. E.g. `stride_am` is",
      "    # how much to increase `a_ptr` by to get the element one row down",
      "    # (A has M rows).",
      "    stride_ae: tl.int64,",
      "    stride_am: tl.int64,",
      "    stride_ak: tl.int64,",
      "    stride_be: tl.int64,",
      "    stride_bk: tl.int64,",
      "    stride_bn: tl.int64,",
      "    stride_ce: tl.int64,",
      "    stride_cm: tl.int64,",
      "    stride_cn: tl.int64,",
      "    stride_ase: tl.int64,",
      "    stride_asm: tl.int64,",
      "    stride_ask: tl.int64,",
      "    stride_bse: tl.int64,",
      "    stride_bsk: tl.int64,",
      "    stride_bsn: tl.int64,",
      "    # Blockwise quantization data",
      "    group_n: tl.constexpr,",
      "    group_k: tl.constexpr,",
      "    # Quantization schemes",
      "    use_fp8_w8a8: tl.constexpr,",
      "    use_int8_w8a16: tl.constexpr,",
      "    per_act_token_quant: tl.constexpr,",
      "    # Kernel config",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "):",
      "    expert_id = tl.program_id(axis=0)",
      "    e_num_tokens = tl.load(expert_num_tokens + expert_id)",
      "    if e_num_tokens == 0:",
      "        # Early exit",
      "        return",
      "",
      "    # axis 1 is M_blocks * N_blocks",
      "    pid_mn = tl.program_id(axis=1)",
      "    #num_pid_m = tl.cdiv(max_num_tokens, BLOCK_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_N)",
      "    pid_m = pid_mn // num_pid_n",
      "    pid_n = pid_mn % num_pid_n",
      "",
      "    cta_m_start = pid_m * BLOCK_M",
      "    cta_n_start = pid_n * BLOCK_N",
      "    if cta_m_start >= e_num_tokens:",
      "        # Early exit",
      "        return",
      "",
      "    cta_m_size = min(BLOCK_M, e_num_tokens - cta_m_start)",
      "    cta_n_size = min(BLOCK_N, N - cta_n_start)",
      "",
      "    a_ptr = a_ptr + expert_id * stride_ae + cta_m_start * stride_am",
      "    b_ptr = b_ptr + expert_id * stride_be + cta_n_start * stride_bn",
      "    c_ptr = (c_ptr + expert_id * stride_ce + cta_m_start * stride_cm +",
      "             cta_n_start * stride_cn)",
      "",
      "    offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N).to(tl.int64)) % N",
      "",
      "    if use_fp8_w8a8:",
      "        a_scale_ptr = a_scale_ptr + expert_id * stride_ase",
      "        b_scale_ptr = b_scale_ptr + expert_id * stride_bse",
      "",
      "        # block-wise",
      "        if group_k > 0 and group_n > 0 or per_act_token_quant:",
      "            a_scale_ptr = a_scale_ptr + cta_m_start * stride_asm",
      "",
      "    expert_triton_kernel(",
      "        a_ptr,",
      "        b_ptr,",
      "        c_ptr,",
      "        expert_id,",
      "        compute_type,",
      "        cta_m_size,  # M",
      "        cta_n_size,  # N",
      "        K,  # K",
      "        a_scale_ptr,",
      "        b_scale_ptr,",
      "        b_zp_ptr,",
      "        # Strides",
      "        stride_am,",
      "        stride_ak,",
      "        stride_bk,",
      "        stride_bn,",
      "        stride_cm,",
      "        stride_cn,",
      "        stride_ase,",
      "        stride_asm,",
      "        stride_ask,",
      "        stride_bse,",
      "        stride_bsk,",
      "        stride_bsn,",
      "        # offsets",
      "        offs_bn,",
      "        # Blockwise quantization data",
      "        group_n,",
      "        group_k,",
      "        # Quantization schemes",
      "        use_fp8_w8a8,",
      "        use_int8_w8a16,",
      "        per_act_token_quant,",
      "        # Kernel config",
      "        BLOCK_M,",
      "        BLOCK_N,",
      "        BLOCK_K)",
      "",
      "",
      "def invoke_moe_batched_triton_kernel(",
      "        A: torch.Tensor,  # [E, max_tokens, K]",
      "        B: torch.Tensor,  # [E, K, N]",
      "        C: torch.Tensor,  # [E, max_tokens, N]",
      "        expert_num_tokens: torch.Tensor,  # [E]",
      "        compute_type: tl.dtype,",
      "        # Quantization data",
      "        A_scale: Optional[torch.Tensor],",
      "        B_scale: Optional[torch.Tensor],",
      "        B_zp: torch.Tensor,",
      "        # Quantization schemes",
      "        use_fp8_w8a8: bool,",
      "        use_int8_w8a16: bool,",
      "        use_int4_w4a16: bool,",
      "        config: dict[str, int],",
      "        per_act_token_quant: bool,",
      "        block_shape: Optional[list[int]] = None):",
      "",
      "    assert not use_int4_w4a16",
      "    max_num_tokens = A.size(1)",
      "    K = A.size(2)",
      "    N = C.size(2)",
      "",
      "    BLOCK_M = config['BLOCK_SIZE_M']",
      "    BLOCK_N = config['BLOCK_SIZE_N']",
      "    BLOCK_K = config['BLOCK_SIZE_K']",
      "",
      "    grid = (expert_num_tokens.size(0), triton.cdiv(max_num_tokens, BLOCK_M) *",
      "            triton.cdiv(B.size(1), BLOCK_N))",
      "",
      "    A_scale = normalize_batched_scales_shape(A_scale,",
      "                                             expert_num_tokens.shape[0])",
      "",
      "    if B_scale is not None and B_scale.ndim == 1:",
      "        assert B_scale.numel() == expert_num_tokens.shape[0]",
      "        B_scale = B_scale.view(-1, 1, 1)",
      "",
      "    assert A_scale is None or A_scale.ndim == 3, (",
      "        f\"{0 if A_scale is None else A_scale.shape}\")",
      "    assert B_scale is None or B_scale.ndim == 1 or B_scale.ndim == 3, (",
      "        f\"{0 if B_scale is None else B_scale.shape}\")",
      "",
      "    if B_scale is not None:",
      "        if B_scale.ndim == 1:",
      "            stride_bse = 1",
      "            stride_bsk = 0",
      "            stride_bsn = 0",
      "        else:",
      "            stride_bse = B_scale.stride(0)",
      "            stride_bsk = B_scale.stride(2)",
      "            stride_bsn = B_scale.stride(1)",
      "",
      "    else:",
      "        stride_bse = 0",
      "        stride_bsk = 0",
      "        stride_bsn = 0",
      "",
      "    if A_scale is not None:",
      "        stride_ase = A_scale.stride(0)",
      "        stride_asm = A_scale.stride(1)",
      "        stride_ask = A_scale.stride(2)",
      "    else:",
      "        stride_ase = 0",
      "        stride_asm = 0",
      "        stride_ask = 0",
      "",
      "    batched_triton_kernel[grid](",
      "        A,",
      "        B,",
      "        C,",
      "        expert_num_tokens,",
      "        compute_type,",
      "        # Dimensions",
      "        max_num_tokens,",
      "        K,",
      "        N,",
      "        # Quantization data",
      "        A_scale,",
      "        B_scale,",
      "        B_zp,",
      "        # Strides",
      "        A.stride(0),",
      "        A.stride(1),",
      "        A.stride(2),",
      "        B.stride(0),",
      "        B.stride(2),",
      "        B.stride(1),",
      "        C.stride(0),",
      "        C.stride(1),",
      "        C.stride(2),",
      "        stride_ase,",
      "        stride_asm,",
      "        stride_ask,",
      "        stride_bse,",
      "        stride_bsk,",
      "        stride_bsn,",
      "        # Blockwise quantization data",
      "        0 if block_shape is None else block_shape[0],",
      "        0 if block_shape is None else block_shape[1],",
      "        # Quantization schemes",
      "        use_fp8_w8a8,",
      "        use_int8_w8a16,",
      "        per_act_token_quant,",
      "        # Kernel config",
      "        BLOCK_M=BLOCK_M,",
      "        BLOCK_N=BLOCK_N,",
      "        BLOCK_K=BLOCK_K)",
      "",
      "",
      "class BatchedPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):",
      "    \"\"\"",
      "    A reference prepare/finalize class that reorganizes the tokens into",
      "    expert batched format, i.e. E x max_num_tokens x K.  This is the format",
      "    that the PPLX dispatch/combine kernels use.",
      "    \"\"\"",
      "",
      "    def __init__(",
      "        self,",
      "        max_num_tokens: int,",
      "        num_local_experts: int,",
      "        num_dispatchers: int,",
      "        rank: int,",
      "    ):",
      "        super().__init__()",
      "        self.max_num_tokens = max_num_tokens",
      "        self.num_local_experts = num_local_experts",
      "        self.rank = rank",
      "        self.num_dispatchers_ = num_dispatchers",
      "",
      "    @property",
      "    def activation_format(self) -> mk.FusedMoEActivationFormat:",
      "        return mk.FusedMoEActivationFormat.BatchedExperts",
      "",
      "    def max_num_tokens_per_rank(self) -> Optional[int]:",
      "        return self.max_num_tokens",
      "",
      "    def topk_indices_dtype(self) -> Optional[torch.dtype]:",
      "        return None",
      "",
      "    def num_dispatchers(self) -> int:",
      "        return self.num_dispatchers_",
      "",
      "    def prepare(",
      "        self,",
      "        a1: torch.Tensor,",
      "        a1_scale: Optional[torch.Tensor],",
      "        a2_scale: Optional[torch.Tensor],",
      "        topk_weights: torch.Tensor,",
      "        topk_ids: torch.Tensor,",
      "        num_experts: int,",
      "        expert_map: Optional[torch.Tensor],",
      "        apply_router_weight_on_input: bool,",
      "        quant_config: FusedMoEQuantConfig,",
      "    ) -> mk.PrepareResultType:",
      "        assert a1.dim() == 2",
      "        assert topk_ids.dim() == 2",
      "        assert topk_ids.size(0) == a1.size(0)",
      "",
      "        if apply_router_weight_on_input:",
      "            topk = topk_ids.size(1)",
      "            # TODO: this only works for topK=1, will need to update for topK>1",
      "            assert topk == 1, \\",
      "                \"apply_router_weight_on_input is only implemented for topk=1\"",
      "            a1.mul_(topk_weights.to(a1.dtype))",
      "",
      "        num_tokens, hidden_dim = a1.size()",
      "        topk = topk_ids.size(1)",
      "",
      "        tokens_per_expert = torch.zeros(num_experts,",
      "                                        dtype=torch.int,",
      "                                        device=a1.device)",
      "",
      "        num_local_experts = self.num_local_experts",
      "",
      "        if quant_config.quant_dtype is None:",
      "            b_type = a1.dtype",
      "        else:",
      "            b_type = quant_config.quant_dtype",
      "",
      "        b_a1 = torch.zeros(",
      "            (num_local_experts, self.max_num_tokens, hidden_dim),",
      "            dtype=b_type,",
      "            device=a1.device)",
      "",
      "        if quant_config.is_quantized:",
      "            scale_shape = quant_config.batched_scale_shape(",
      "                num_local_experts, self.max_num_tokens, hidden_dim)",
      "",
      "            b_a1_scale = torch.empty(scale_shape,",
      "                                     dtype=torch.float32,",
      "                                     device=a1.device)",
      "        else:",
      "            assert a1_scale is None",
      "            b_a1_scale = None",
      "",
      "        first_expert = num_local_experts * self.rank",
      "        last_expert = first_expert + num_local_experts",
      "",
      "        a1_scale = normalize_scales_shape(a1_scale)",
      "        a2_scale = normalize_scales_shape(a2_scale)",
      "",
      "        for expert_id in range(first_expert, last_expert):",
      "            topks = torch.any(topk_ids == expert_id, dim=1).flatten()",
      "            rows = torch.count_nonzero(topks.flatten())",
      "            if rows == 0:",
      "                continue",
      "            idx = expert_id - first_expert",
      "            tokens_per_expert[idx] = rows",
      "            rhs = a1[:topks.numel()][topks]",
      "            if quant_config.quant_dtype is not None:",
      "                if a1_scale is not None:",
      "                    if quant_config.is_per_act_token:",
      "                        rhs_a1_scale = a1_scale[:topks.numel()][topks]",
      "                    else:",
      "                        rhs_a1_scale = a1_scale",
      "                else:",
      "                    rhs_a1_scale = None",
      "                b_a1[idx, :rows, :], b_s = moe_kernel_quantize_input(",
      "                    rhs,",
      "                    rhs_a1_scale,",
      "                    quant_config.quant_dtype,",
      "                    quant_config.per_act_token_quant,",
      "                    quant_config.block_shape,",
      "                )",
      "                assert b_s is not None",
      "                if quant_config.is_per_act_token:",
      "                    b_a1_scale[idx, :rows] = b_s[:rows]",
      "                else:",
      "                    b_a1_scale[idx, :b_s.shape[0]] = b_s",
      "            else:",
      "                b_a1[idx, :rows, :] = rhs",
      "",
      "        assert b_a1_scale is None or b_a1_scale.ndim == 3",
      "",
      "        expert_tokens_meta = mk.ExpertTokensMetadata(",
      "            expert_num_tokens=tokens_per_expert, expert_num_tokens_cpu=None)",
      "",
      "        return b_a1, b_a1_scale, expert_tokens_meta, None, None",
      "",
      "    def finalize(",
      "        self,",
      "        output: torch.Tensor,",
      "        fused_expert_output: torch.Tensor,",
      "        topk_weights: torch.Tensor,",
      "        topk_ids: torch.Tensor,",
      "        apply_router_weight_on_input: bool,",
      "        weight_and_reduce_impl: mk.TopKWeightAndReduce,",
      "    ) -> None:",
      "        if isinstance(weight_and_reduce_impl, TopKWeightAndReduceDelegate):",
      "            weight_and_reduce_impl = TopKWeightAndReduceNaiveBatched(self.rank)",
      "        weight_and_reduce_impl.apply(",
      "            output=output,",
      "            fused_expert_output=fused_expert_output,",
      "            topk_weights=topk_weights,",
      "            topk_ids=topk_ids,",
      "            apply_router_weight_on_input=apply_router_weight_on_input,",
      "        )",
      "",
      "",
      "class NaiveBatchedExperts(mk.FusedMoEPermuteExpertsUnpermute):",
      "    \"\"\"",
      "    A reference MoE expert class that operates on expert batched format,",
      "    i.e. E x max_num_tokens x K.  This is the format that the pplx",
      "    dispatch/combine kernels use.",
      "    \"\"\"",
      "",
      "    def __init__(",
      "        self,",
      "        max_num_tokens: int,",
      "        num_dispatchers: int,",
      "        use_fp8_w8a8: bool = False,",
      "        use_int8_w8a8: bool = False,",
      "        use_int8_w8a16: bool = False,",
      "        use_int4_w4a16: bool = False,",
      "        use_mxfp4_w4a4: bool = False,",
      "        block_shape: Optional[list[int]] = None,",
      "        per_act_token_quant: bool = False,",
      "    ):",
      "        super().__init__(",
      "            FusedMoEQuantConfig.make(",
      "                use_fp8_w8a8=use_fp8_w8a8,",
      "                use_int8_w8a8=use_int8_w8a8,",
      "                use_int8_w8a16=use_int8_w8a16,",
      "                use_int4_w4a16=use_int4_w4a16,",
      "                use_mxfp4_w4a4=use_mxfp4_w4a4,",
      "                per_act_token_quant=per_act_token_quant,",
      "                block_shape=block_shape,",
      "            ))",
      "        assert not use_int8_w8a8, \"NYI\"",
      "        assert not use_int8_w8a16, \"NYI\"",
      "        assert not use_int4_w4a16, \"NYI\"",
      "        assert not use_mxfp4_w4a4, \"NYI\"",
      "        self.max_num_tokens = max_num_tokens",
      "        self.num_dispatchers = num_dispatchers",
      "",
      "    @property",
      "    def activation_formats(",
      "        self",
      "    ) -> tuple[mk.FusedMoEActivationFormat, mk.FusedMoEActivationFormat]:",
      "        return (mk.FusedMoEActivationFormat.BatchedExperts,",
      "                mk.FusedMoEActivationFormat.BatchedExperts)",
      "",
      "    def supports_chunking(self) -> bool:",
      "        return False",
      "",
      "    def supports_expert_map(self) -> bool:",
      "        return False",
      "",
      "    def finalize_weight_and_reduce_impl(self) -> mk.TopKWeightAndReduce:",
      "        # Let PrepareAndFinalize::finalize() decide the impl.",
      "        return TopKWeightAndReduceDelegate()",
      "",
      "    def workspace_shapes(",
      "        self,",
      "        a: torch.Tensor,",
      "        aq: torch.Tensor,",
      "        M: int,",
      "        N: int,",
      "        K: int,",
      "        topk: int,",
      "        global_num_experts: int,",
      "        local_num_experts: int,",
      "        expert_tokens_meta: Optional[mk.ExpertTokensMetadata],",
      "    ) -> tuple[tuple[int, ...], tuple[int, ...], tuple[int, ...], torch.dtype]:",
      "        assert a.dim() == 2",
      "        num_dp = self.num_dispatchers",
      "        num_experts = local_num_experts",
      "        workspace13 = (num_experts, self.max_num_tokens * num_dp, K)",
      "        workspace2 = (self.max_num_tokens * num_dp, N)",
      "        output = workspace13",
      "        return (workspace13, workspace2, output, a.dtype)",
      "",
      "    def dequant(self, t: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:",
      "        assert self.quant_config.is_quantized",
      "        f32 = torch.float32",
      "        if (self.quant_config.is_per_act_token",
      "                or self.quant_config.is_per_tensor):",
      "            return t.to(f32) * scale",
      "        else:",
      "            return t.to(f32) * group_broadcast(scale, t.shape)",
      "",
      "    def apply(",
      "        self,",
      "        output: torch.Tensor,",
      "        hidden_states: torch.Tensor,",
      "        w1: torch.Tensor,",
      "        w2: torch.Tensor,",
      "        topk_weights: torch.Tensor,",
      "        topk_ids: torch.Tensor,",
      "        activation: str,",
      "        global_num_experts: int,",
      "        expert_map: Optional[torch.Tensor],",
      "        w1_scale: Optional[torch.Tensor],",
      "        w2_scale: Optional[torch.Tensor],",
      "        w1_zp: Optional[torch.Tensor],",
      "        w2_zp: Optional[torch.Tensor],",
      "        a1q_scale: Optional[torch.Tensor],",
      "        a2_scale: Optional[torch.Tensor],",
      "        workspace13: torch.Tensor,",
      "        workspace2: torch.Tensor,",
      "        expert_tokens_meta: Optional[mk.ExpertTokensMetadata],",
      "        apply_router_weight_on_input: bool,",
      "    ):",
      "        assert hidden_states.dim() == 3",
      "        assert expert_tokens_meta is not None",
      "        expert_num_tokens = expert_tokens_meta.expert_num_tokens",
      "",
      "        num_local_experts = w1.size(0)",
      "        assert num_local_experts == w1.size(0), (",
      "            f\"{num_local_experts} == {w1.size(0)}\")",
      "",
      "        N = w1.size(1) // 2",
      "",
      "        for expert in range(num_local_experts):",
      "            # Indexing expert_num_tokens doesn't work w/cudagraphs or inductor",
      "            if (torch.compiler.is_compiling()",
      "                    or torch.cuda.is_current_stream_capturing()):",
      "                num = hidden_states.shape[1]",
      "            else:",
      "                num = int(expert_num_tokens[expert].item())",
      "",
      "            if num == 0:",
      "                continue",
      "",
      "            tmp = _resize_cache(workspace2, (num, N))",
      "",
      "            if self.quant_config.is_quantized:",
      "                assert a1q_scale is not None and w1_scale is not None",
      "                input = self.dequant(hidden_states[expert, :, :],",
      "                                     a1q_scale[expert])",
      "                w1_dq = self.dequant(w1[expert], w1_scale[expert])",
      "                input = input[:num] @ w1_dq.transpose(0, 1)",
      "            else:",
      "                input = hidden_states[expert, :num, :] @ w1[expert].transpose(",
      "                    0, 1)",
      "",
      "            self.activation(activation, tmp, input.to(tmp.dtype))",
      "",
      "            if self.quant_config.is_quantized:",
      "                assert w2_scale is not None",
      "                w2_dq = self.dequant(w2[expert], w2_scale[expert])",
      "            else:",
      "                w2_dq = w2[expert]",
      "",
      "            output[expert, :num, :] = tmp @ w2_dq.transpose(0, 1).to(tmp.dtype)",
      "",
      "",
      "def batched_moe_kernel_quantize_input(",
      "    A: torch.Tensor,",
      "    A_scale: Optional[torch.Tensor],",
      "    num_tokens: int,",
      "    E: int,",
      "    N: int,",
      "    expert_num_tokens: torch.Tensor,",
      "    qtype: Optional[torch.dtype],",
      "    per_act_token_quant: bool,",
      "    block_shape: Optional[list[int]] = None,",
      ") -> tuple[torch.Tensor, Optional[torch.Tensor]]:",
      "    if (torch.compiler.is_compiling()",
      "            or torch.cuda.is_current_stream_capturing()):",
      "        # Note: this does a bunch of extra work because expert_num_tokens is",
      "        # ignored but it does support torch.compile + cudagraphs.",
      "        hidden_dim = A.size(-1)",
      "        assert A_scale is None or A_scale.ndim <= 2, (",
      "            f\"{A_scale.shape if A_scale is not None else None}\")",
      "        A_q, A_q_scale = moe_kernel_quantize_input(A.view(-1,",
      "                                                          hidden_dim), A_scale,",
      "                                                   qtype, per_act_token_quant,",
      "                                                   block_shape)",
      "        A_q = A_q.view(E, -1, hidden_dim)",
      "        A_q_scale = normalize_batched_scales_shape(A_q_scale, E)",
      "",
      "        return A_q, A_q_scale",
      "    elif qtype is None:",
      "        return A, normalize_batched_scales_shape(A_scale, E)",
      "    else:",
      "        A_q = torch.empty_like(A, dtype=qtype)",
      "",
      "        if per_act_token_quant:",
      "            assert block_shape is None",
      "            scale_shape = (E, num_tokens, 1)",
      "        elif block_shape is not None:",
      "            _, block_k = block_shape",
      "            k_tiles = (A.shape[-1] + block_k - 1) // block_k",
      "            scale_shape = (E, num_tokens, k_tiles)",
      "        else:",
      "            scale_shape = (E, 1, 1)",
      "",
      "        A_q_scale = torch.zeros(scale_shape,",
      "                                dtype=torch.float32,",
      "                                device=A.device)",
      "",
      "        num_experts = expert_num_tokens.numel()",
      "",
      "        A_scale = normalize_batched_scales_shape(A_scale, num_experts)",
      "",
      "        for e in range(E):",
      "            num_tokens = int(expert_num_tokens[e].item())",
      "            if num_tokens > 0:",
      "                if A_scale is not None:",
      "                    scales = A_scale[e, :min(num_tokens, A_scale.shape[1])]",
      "                else:",
      "                    scales = None",
      "                A_q[e, :num_tokens], tmp_scale = moe_kernel_quantize_input(",
      "                    A[e, :num_tokens],",
      "                    scales,",
      "                    qtype,",
      "                    per_act_token_quant,",
      "                    block_shape,",
      "                )",
      "                assert tmp_scale is not None",
      "                A_q_scale[e, :tmp_scale.shape[0]] = tmp_scale",
      "",
      "        return A_q, A_q_scale",
      "",
      "",
      "class BatchedTritonExperts(mk.FusedMoEPermuteExpertsUnpermute):",
      "    \"\"\"",
      "    A Triton based MoE expert class that operates on expert batched format,",
      "    i.e. E x max_num_tokens x K.  This is the format that the pplx",
      "    dispatch/combine kernels use.",
      "    \"\"\"",
      "",
      "    def __init__(",
      "        self,",
      "        max_num_tokens: int,",
      "        num_dispatchers: int,",
      "        use_fp8_w8a8: bool = False,",
      "        use_int8_w8a8: bool = False,",
      "        use_int8_w8a16: bool = False,",
      "        use_int4_w4a16: bool = False,",
      "        use_mxfp4_w4a4: bool = False,",
      "        per_act_token_quant: bool = False,",
      "        block_shape: Optional[list[int]] = None,",
      "    ):",
      "        super().__init__(",
      "            FusedMoEQuantConfig.make(",
      "                use_fp8_w8a8=use_fp8_w8a8,",
      "                use_int8_w8a8=use_int8_w8a8,",
      "                use_int8_w8a16=use_int8_w8a16,",
      "                use_int4_w4a16=use_int4_w4a16,",
      "                use_mxfp4_w4a4=use_mxfp4_w4a4,",
      "                per_act_token_quant=per_act_token_quant,",
      "                block_shape=block_shape,",
      "            ))",
      "        assert not use_int8_w8a8, \"NYI\"",
      "        assert not use_int8_w8a16, \"NYI\"",
      "        assert not use_int4_w4a16, \"NYI\"",
      "        assert not use_mxfp4_w4a4, \"NYI\"",
      "        assert max_num_tokens > 0",
      "        assert num_dispatchers > 0",
      "        self.use_fp8_w8a8 = use_fp8_w8a8",
      "        self.use_int8_w8a8 = use_int8_w8a8",
      "        self.use_int4_w4a16 = use_int4_w4a16",
      "        self.use_int8_w8a16 = use_int8_w8a16",
      "        self.use_mxfp4_w4a4 = use_mxfp4_w4a4",
      "        self.max_num_tokens = max_num_tokens",
      "        self.num_dispatchers = num_dispatchers",
      "",
      "    @property",
      "    def activation_formats(",
      "        self",
      "    ) -> tuple[mk.FusedMoEActivationFormat, mk.FusedMoEActivationFormat]:",
      "        return (mk.FusedMoEActivationFormat.BatchedExperts,",
      "                mk.FusedMoEActivationFormat.BatchedExperts)",
      "",
      "    def supports_chunking(self) -> bool:",
      "        return False",
      "",
      "    def supports_expert_map(self) -> bool:",
      "        return False",
      "",
      "    def finalize_weight_and_reduce_impl(self) -> mk.TopKWeightAndReduce:",
      "        # Let PrepareAndFinalize::finalize() decide the impl.",
      "        return TopKWeightAndReduceDelegate()",
      "",
      "    def workspace_shapes(",
      "        self,",
      "        a: torch.Tensor,",
      "        aq: torch.Tensor,",
      "        M: int,",
      "        N: int,",
      "        K: int,",
      "        topk: int,",
      "        global_num_experts: int,",
      "        local_num_experts: int,",
      "        expert_tokens_meta: Optional[mk.ExpertTokensMetadata],",
      "    ) -> tuple[tuple[int, ...], tuple[int, ...], tuple[int, ...], torch.dtype]:",
      "        assert a.dim() == 2",
      "        num_dp = self.num_dispatchers",
      "        num_experts = local_num_experts",
      "        max_num_tokens = self.max_num_tokens",
      "        workspace13 = (num_experts, max_num_tokens * num_dp, max(K, N))",
      "        workspace2 = (num_experts, max_num_tokens * num_dp, (N // 2))",
      "        output = (num_experts, max_num_tokens * num_dp, K)",
      "        return (workspace13, workspace2, output, a.dtype)",
      "",
      "    def apply(",
      "        self,",
      "        output: torch.Tensor,",
      "        hidden_states: torch.Tensor,",
      "        w1: torch.Tensor,",
      "        w2: torch.Tensor,",
      "        topk_weights: torch.Tensor,",
      "        topk_ids: torch.Tensor,",
      "        activation: str,",
      "        global_num_experts: int,",
      "        expert_map: Optional[torch.Tensor],",
      "        w1_scale: Optional[torch.Tensor],",
      "        w2_scale: Optional[torch.Tensor],",
      "        w1_zp: Optional[torch.Tensor],",
      "        w2_zp: Optional[torch.Tensor],",
      "        a1q_scale: Optional[torch.Tensor],",
      "        a2_scale: Optional[torch.Tensor],",
      "        workspace13: torch.Tensor,",
      "        workspace2: torch.Tensor,",
      "        expert_tokens_meta: Optional[mk.ExpertTokensMetadata],",
      "        apply_router_weight_on_input: bool,",
      "    ):",
      "        # Check constraints.",
      "        if self.use_int4_w4a16:",
      "            assert hidden_states.size(-1) // 2 == w1.size(2), (",
      "                \"Hidden size mismatch\")",
      "        else:",
      "            assert hidden_states.size(-1) == w1.size(2), (",
      "                f\"Hidden size mismatch {hidden_states.size(-1)} \"",
      "                f\"!= {w1.size(2)}\")",
      "",
      "        assert hidden_states.is_contiguous(",
      "        ), \"Hidden_states must be contiguous\"",
      "        assert w1.stride(-1) == 1, \"Stride of last dimension must be 1\"",
      "        assert w2.stride(-1) == 1, \"Stride of last dimension must be 1\"",
      "        assert hidden_states.dtype in [",
      "            torch.float32, torch.float16, torch.bfloat16, torch.float8_e4m3fn",
      "        ]",
      "        assert expert_tokens_meta is not None",
      "",
      "        expert_num_tokens = expert_tokens_meta.expert_num_tokens",
      "",
      "        E, max_num_tokens, N, K, top_k_num = mk._moe_problem_size(",
      "            hidden_states, w1, w2, topk_ids)",
      "",
      "        assert w1.size(0) == E",
      "        assert w2.size(0) == E",
      "",
      "        config_dtype = get_config_dtype_str(use_fp8_w8a8=self.use_fp8_w8a8,",
      "                                            use_int8_w8a16=self.use_int8_w8a16,",
      "                                            use_int4_w4a16=self.use_int4_w4a16,",
      "                                            use_mxfp4_w4a4=self.use_mxfp4_w4a4,",
      "                                            dtype=hidden_states.dtype)",
      "",
      "        config = try_get_optimal_moe_config(",
      "            w1.size(),",
      "            w2.size(),",
      "            top_k_num,",
      "            config_dtype,",
      "            max_num_tokens,",
      "            block_shape=self.block_shape,",
      "        )",
      "",
      "        if hidden_states.dtype == torch.bfloat16:",
      "            compute_type = tl.bfloat16",
      "        elif hidden_states.dtype == torch.float16:",
      "            compute_type = tl.float16",
      "        elif hidden_states.dtype == torch.float32:",
      "            compute_type = tl.float32",
      "        elif hidden_states.dtype == torch.float8_e4m3fn:",
      "            compute_type = tl.bfloat16",
      "        else:",
      "            raise ValueError(",
      "                f\"Unsupported compute_type: {hidden_states.dtype}\")",
      "",
      "        # We can reuse the memory between these because by the time we need",
      "        # cache3, we're done with cache1",
      "        intermediate_cache1 = _resize_cache(workspace13,",
      "                                            (E, max_num_tokens, N))",
      "        intermediate_cache2 = _resize_cache(workspace2,",
      "                                            (E, max_num_tokens, N // 2))",
      "",
      "        if self.use_fp8_w8a8:",
      "            intermediate_cache1.fill_(0)",
      "",
      "        a1q_scale = normalize_batched_scales_shape(a1q_scale, E)",
      "",
      "        # MM1",
      "        invoke_moe_batched_triton_kernel(",
      "            A=hidden_states,",
      "            B=w1,",
      "            C=intermediate_cache1,",
      "            expert_num_tokens=expert_num_tokens,",
      "            compute_type=compute_type,",
      "            A_scale=a1q_scale,",
      "            B_scale=w1_scale,",
      "            B_zp=w1_zp,",
      "            use_fp8_w8a8=self.use_fp8_w8a8,",
      "            use_int8_w8a16=self.use_int8_w8a16,",
      "            use_int4_w4a16=self.use_int4_w4a16,",
      "            config=config,",
      "            per_act_token_quant=self.per_act_token_quant,",
      "            block_shape=self.block_shape)",
      "",
      "        intermediate_cache2.fill_(0)",
      "",
      "        # TODO (bnell): use triton utility from batched deep gemm.",
      "        self.activation(activation, intermediate_cache2.view(-1, N // 2),",
      "                        intermediate_cache1.view(-1, N))",
      "",
      "        qintermediate_cache2, a2q_scale = batched_moe_kernel_quantize_input(",
      "            intermediate_cache2, a2_scale, max_num_tokens, E, N,",
      "            expert_num_tokens, self.quant_dtype, self.per_act_token_quant,",
      "            self.block_shape)",
      "",
      "        invoke_moe_batched_triton_kernel(",
      "            A=qintermediate_cache2,",
      "            B=w2,",
      "            C=output,",
      "            expert_num_tokens=expert_num_tokens,",
      "            compute_type=compute_type,",
      "            A_scale=a2q_scale,",
      "            B_scale=w2_scale,",
      "            B_zp=w2_zp,",
      "            use_fp8_w8a8=self.use_fp8_w8a8,",
      "            use_int8_w8a16=self.use_int8_w8a16,",
      "            use_int4_w4a16=self.use_int4_w4a16,",
      "            config=config,",
      "            per_act_token_quant=self.per_act_token_quant,",
      "            block_shape=self.block_shape)"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/fused_moe/deep_gemm_utils.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "\"\"\"",
      "Taken from https://github.com/ModelTC/LightLLM/blob/8ed97c74c18f11505b048b1ba00ba5c0cef8bff6/lightllm/common/fused_moe/deepep_scatter_gather.py",
      "and updated to fit vllm needs and terminology.",
      "\"\"\"",
      "",
      "import functools",
      "from typing import Optional",
      "",
      "import torch",
      "",
      "import vllm.model_executor.layers.fused_moe.modular_kernel as mk",
      "from vllm.model_executor.layers.fused_moe.utils import count_expert_num_tokens",
      "from vllm.triton_utils import tl, triton",
      "from vllm.utils import round_up",
      "",
      "",
      "@functools.cache",
      "def deep_gemm_block_shape() -> list[int]:",
      "    # Lazy import to avoid CUDA initialization problems.",
      "    import deep_gemm as dg",
      "    block = dg.get_m_alignment_for_contiguous_layout()",
      "    return [block, block]",
      "",
      "",
      "def expert_num_tokens_round_up_and_sum(expert_num_tokens: torch.Tensor,",
      "                                       alignment: int) -> int:",
      "    # Round up each element in expert_num_tokens to the nearest multiple of",
      "    # alignment.",
      "    ent = (expert_num_tokens.to(torch.int64) +",
      "           (alignment - 1)) // alignment * alignment",
      "    return torch.sum(ent).item()",
      "",
      "",
      "def compute_aligned_M(M: int, num_topk: int, local_num_experts: int,",
      "                      alignment: int,",
      "                      expert_tokens_meta: Optional[mk.ExpertTokensMetadata]):",
      "",
      "    if ((expert_tokens_meta is not None)",
      "            and (expert_tokens_meta.expert_num_tokens_cpu is not None)):",
      "        return expert_num_tokens_round_up_and_sum(",
      "            expert_tokens_meta.expert_num_tokens_cpu, alignment=alignment)",
      "",
      "    # expert_num_tokens information is not available on the cpu.",
      "    # compute the max required size.",
      "    M_sum = (M * num_topk) + local_num_experts * (alignment - 1)",
      "    M_sum = round_up(M_sum, alignment)",
      "    return M_sum",
      "",
      "",
      "@triton.jit",
      "def apply_expert_map(expert_id, expert_map):",
      "    if expert_id != -1:",
      "        expert_id = tl.load(expert_map + expert_id).to(expert_id.dtype)",
      "    return expert_id",
      "",
      "",
      "@triton.jit",
      "def round_up_128(x: int) -> int:",
      "    y = 128",
      "    return ((x + y - 1) // y) * y",
      "",
      "",
      "@triton.jit",
      "def _fwd_kernel_ep_scatter_1(",
      "    num_recv_tokens_per_expert,",
      "    expert_start_loc,",
      "    m_indices,",
      "    num_experts: tl.constexpr,",
      "    BLOCK_E: tl.constexpr,",
      "    BLOCK_EXPERT_NUM: tl.constexpr,",
      "):",
      "    cur_expert = tl.program_id(0)",
      "",
      "    offset_cumsum = tl.arange(0, BLOCK_EXPERT_NUM)",
      "    tokens_per_expert = tl.load(num_recv_tokens_per_expert + offset_cumsum,",
      "                                mask=offset_cumsum < num_experts,",
      "                                other=0)",
      "    tokens_per_expert = round_up_128(tokens_per_expert)",
      "    cumsum = tl.cumsum(tokens_per_expert) - tokens_per_expert",
      "    tl.store(expert_start_loc + offset_cumsum,",
      "             cumsum,",
      "             mask=offset_cumsum < num_experts)",
      "",
      "    cur_expert_start = tl.load(expert_start_loc + cur_expert)",
      "    cur_expert_token_num = tl.load(num_recv_tokens_per_expert + cur_expert)",
      "",
      "    m_indices_start_ptr = m_indices + cur_expert_start",
      "    off_expert = tl.arange(0, BLOCK_E)",
      "",
      "    for start_m in tl.range(0, cur_expert_token_num, BLOCK_E, num_stages=4):",
      "        tl.store(",
      "            m_indices_start_ptr + start_m + off_expert,",
      "            cur_expert,",
      "        )",
      "",
      "",
      "@triton.jit",
      "def _fwd_kernel_ep_scatter_2(",
      "    total_token_num,",
      "    expert_start_loc,",
      "    recv_x,",
      "    recv_x_stride0,",
      "    recv_x_stride1,",
      "    recv_x_scale,",
      "    recv_x_scale_stride0,",
      "    recv_x_scale_stride1,",
      "    recv_topk,",
      "    recv_topk_stride0,",
      "    recv_topk_stride1,",
      "    output_tensor,",
      "    output_tensor_stride0,",
      "    output_tensor_stride1,",
      "    output_tensor_scale,",
      "    output_tensor_scale_stride0,",
      "    output_tensor_scale_stride1,",
      "    output_index,",
      "    output_index_stride0,",
      "    output_index_stride1,",
      "    topk_num: tl.constexpr,",
      "    expert_map,",
      "    HAS_EXPERT_MAP: tl.constexpr,",
      "    HIDDEN_SIZE: tl.constexpr,",
      "    HIDDEN_SIZE_PAD: tl.constexpr,",
      "    SCALE_HIDDEN_SIZE: tl.constexpr,",
      "    SCALE_HIDDEN_SIZE_PAD: tl.constexpr,",
      "):",
      "    start_token_id = tl.program_id(0)",
      "    grid_num = tl.num_programs(0)",
      "",
      "    offset_in = tl.arange(0, HIDDEN_SIZE_PAD)",
      "    mask = offset_in < HIDDEN_SIZE",
      "",
      "    offset_in_s = tl.arange(0, SCALE_HIDDEN_SIZE_PAD)",
      "    mask_s = offset_in_s < SCALE_HIDDEN_SIZE",
      "",
      "    for token_id in range(start_token_id, total_token_num, grid_num):",
      "        to_copy = tl.load(recv_x + token_id * recv_x_stride0 + offset_in,",
      "                          mask=mask)",
      "        to_copy_s = tl.load(recv_x_scale + token_id * recv_x_scale_stride0 +",
      "                            offset_in_s,",
      "                            mask=mask_s)",
      "",
      "        for topk_index in tl.range(0, topk_num, 1, num_stages=4):",
      "            expert_id = tl.load(recv_topk + token_id * recv_topk_stride0 +",
      "                                topk_index)",
      "",
      "            if HAS_EXPERT_MAP:",
      "                expert_id = apply_expert_map(expert_id, expert_map)",
      "",
      "            if expert_id >= 0:",
      "                dest_token_index = tl.atomic_add(expert_start_loc + expert_id,",
      "                                                 1)",
      "                tl.store(",
      "                    output_index + token_id * output_index_stride0 +",
      "                    topk_index, dest_token_index)",
      "                output_tensor_ptr = (output_tensor +",
      "                                     dest_token_index * output_tensor_stride0)",
      "                output_tensor_scale_ptr = (",
      "                    output_tensor_scale +",
      "                    dest_token_index * output_tensor_scale_stride0)",
      "                tl.store(output_tensor_ptr + offset_in, to_copy, mask=mask)",
      "                tl.store(output_tensor_scale_ptr + offset_in_s,",
      "                         to_copy_s,",
      "                         mask=mask_s)",
      "",
      "",
      "@torch.no_grad()",
      "def ep_scatter(",
      "    recv_x: torch.Tensor,",
      "    recv_x_scale: torch.Tensor,",
      "    recv_topk: torch.Tensor,",
      "    num_recv_tokens_per_expert: torch.Tensor,",
      "    expert_map: Optional[torch.Tensor],",
      "    expert_start_loc: torch.Tensor,",
      "    output_tensor: torch.Tensor,",
      "    output_tensor_scale: torch.Tensor,",
      "    m_indices: torch.Tensor,",
      "    output_index: torch.Tensor,",
      "):",
      "    BLOCK_E = 128  # token num of per expert is aligned to 128",
      "    BLOCK_D = 128  # block size of quantization",
      "    num_warps = 8",
      "    num_experts = num_recv_tokens_per_expert.shape[0]",
      "    hidden_size = recv_x.shape[1]",
      "    # grid = (triton.cdiv(hidden_size, BLOCK_D), num_experts)",
      "    grid = num_experts",
      "",
      "    assert m_indices.shape[0] % BLOCK_E == 0",
      "",
      "    _fwd_kernel_ep_scatter_1[(grid, )](",
      "        num_recv_tokens_per_expert,",
      "        expert_start_loc,",
      "        m_indices,",
      "        num_experts=num_experts,",
      "        num_warps=num_warps,",
      "        BLOCK_E=BLOCK_E,",
      "        BLOCK_EXPERT_NUM=triton.next_power_of_2(num_experts),",
      "    )",
      "",
      "    grid = min(recv_topk.shape[0], 1024 * 8)",
      "",
      "    _fwd_kernel_ep_scatter_2[(grid, )](",
      "        recv_topk.shape[0],",
      "        expert_start_loc,",
      "        recv_x,",
      "        recv_x.stride(0),",
      "        recv_x.stride(1),",
      "        recv_x_scale,",
      "        recv_x_scale.stride(0),",
      "        recv_x_scale.stride(1),",
      "        recv_topk,",
      "        recv_topk.stride(0),",
      "        recv_topk.stride(1),",
      "        output_tensor,",
      "        output_tensor.stride(0),",
      "        output_tensor.stride(1),",
      "        output_tensor_scale,",
      "        output_tensor_scale.stride(0),",
      "        output_tensor_scale.stride(1),",
      "        output_index,",
      "        output_index.stride(0),",
      "        output_index.stride(1),",
      "        topk_num=recv_topk.shape[1],",
      "        expert_map=expert_map,",
      "        HAS_EXPERT_MAP=expert_map is not None,",
      "        num_warps=num_warps,",
      "        HIDDEN_SIZE=hidden_size,",
      "        HIDDEN_SIZE_PAD=triton.next_power_of_2(hidden_size),",
      "        SCALE_HIDDEN_SIZE=hidden_size // BLOCK_D,",
      "        SCALE_HIDDEN_SIZE_PAD=triton.next_power_of_2(hidden_size // BLOCK_D),",
      "    )",
      "    return",
      "",
      "",
      "@triton.jit",
      "def _fwd_kernel_ep_gather(",
      "    total_token_num,",
      "    input_tensor,",
      "    input_tensor_stride0,",
      "    input_tensor_stride1,",
      "    recv_topk_ids,",
      "    recv_topk_ids_stride0,",
      "    recv_topk_ids_stride1,",
      "    recv_topk_weight,",
      "    recv_topk_weight_stride0,",
      "    recv_topk_weight_stride1,",
      "    input_index,",
      "    input_index_stride0,",
      "    input_index_stride1,",
      "    output_tensor,",
      "    output_tensor_stride0,",
      "    output_tensor_stride1,",
      "    topk_num: tl.constexpr,",
      "    expert_map,",
      "    HAS_EXPERT_MAP: tl.constexpr,",
      "    BLOCK_D: tl.constexpr,",
      "):",
      "    cur_block = tl.program_id(0)",
      "    start_cur_token = tl.program_id(1)",
      "    grid_num = tl.num_programs(1)",
      "",
      "    for cur_token in range(start_cur_token, total_token_num, grid_num):",
      "        off_d = tl.arange(0, BLOCK_D)",
      "        accumulator = tl.zeros([BLOCK_D], dtype=tl.float32)",
      "        for topk_index in range(0, topk_num):",
      "            expert_id = tl.load(recv_topk_ids +",
      "                                cur_token * recv_topk_ids_stride0 + topk_index)",
      "",
      "            if HAS_EXPERT_MAP:",
      "                expert_id = apply_expert_map(expert_id, expert_map)",
      "",
      "            if expert_id >= 0:",
      "                source_token_index = tl.load(input_index +",
      "                                             cur_token * input_index_stride0 +",
      "                                             topk_index)",
      "                acc_weight = tl.load(recv_topk_weight +",
      "                                     cur_token * recv_topk_weight_stride0 +",
      "                                     topk_index)",
      "                tmp = tl.load(input_tensor +",
      "                              source_token_index * input_tensor_stride0 +",
      "                              cur_block * BLOCK_D + off_d)",
      "                accumulator += tmp.to(tl.float32) * acc_weight",
      "",
      "        tl.store(",
      "            output_tensor + cur_token * output_tensor_stride0 +",
      "            cur_block * BLOCK_D + off_d,",
      "            accumulator.to(output_tensor.dtype.element_ty),",
      "        )",
      "",
      "",
      "@torch.no_grad()",
      "def ep_gather(",
      "    input_tensor: torch.Tensor,",
      "    recv_topk_ids: torch.Tensor,",
      "    recv_topk_weight: torch.Tensor,",
      "    input_index: torch.Tensor,",
      "    expert_map: Optional[torch.Tensor],",
      "    output_tensor: torch.Tensor,",
      "):",
      "    num_warps = 2",
      "    num_tokens = output_tensor.shape[0]",
      "    hidden_size = input_tensor.shape[1]",
      "    BLOCK_D = min(hidden_size, 1024)",
      "    assert hidden_size % BLOCK_D == 0",
      "    grid = (triton.cdiv(hidden_size, BLOCK_D), min(num_tokens, 1024))",
      "",
      "    _fwd_kernel_ep_gather[grid](",
      "        num_tokens,",
      "        input_tensor,",
      "        input_tensor.stride(0),",
      "        input_tensor.stride(1),",
      "        recv_topk_ids,",
      "        recv_topk_ids.stride(0),",
      "        recv_topk_ids.stride(1),",
      "        recv_topk_weight,",
      "        recv_topk_weight.stride(0),",
      "        recv_topk_weight.stride(1),",
      "        input_index,",
      "        input_index.stride(0),",
      "        input_index.stride(1),",
      "        output_tensor,",
      "        output_tensor.stride(0),",
      "        output_tensor.stride(1),",
      "        topk_num=recv_topk_ids.shape[1],",
      "        expert_map=expert_map,",
      "        HAS_EXPERT_MAP=expert_map is not None,",
      "        num_warps=num_warps,",
      "        BLOCK_D=BLOCK_D,",
      "    )",
      "    return",
      "",
      "",
      "def deepgemm_moe_permute(aq: torch.Tensor,",
      "                         aq_scale: torch.Tensor,",
      "                         topk_ids: torch.Tensor,",
      "                         local_num_experts: int,",
      "                         expert_map: Optional[torch.Tensor],",
      "                         expert_tokens_meta: Optional[mk.ExpertTokensMetadata],",
      "                         aq_out: Optional[torch.Tensor] = None):",
      "",
      "    assert aq.ndim == 2",
      "    assert topk_ids.dtype.is_signed, (",
      "        \"The kernel uses -1 to represent invalid topk_ids\")",
      "    H = aq.size(1)",
      "    device = aq.device",
      "",
      "    block_m = deep_gemm_block_shape()[0]",
      "    block_k = deep_gemm_block_shape()[1]",
      "",
      "    M_sum = compute_aligned_M(M=topk_ids.size(0),",
      "                              num_topk=topk_ids.size(1),",
      "                              local_num_experts=local_num_experts,",
      "                              alignment=block_m,",
      "                              expert_tokens_meta=expert_tokens_meta)",
      "",
      "    expert_start_loc = torch.empty((local_num_experts),",
      "                                   device=device,",
      "                                   dtype=torch.int32)",
      "",
      "    assert aq_out is None or aq_out.shape == (M_sum, H)",
      "    if aq_out is None:",
      "        aq_out = torch.empty((M_sum, H), device=device, dtype=aq.dtype)",
      "",
      "    aq_scale_out = torch.empty((M_sum, H // block_k),",
      "                               device=device,",
      "                               dtype=torch.float32)",
      "",
      "    maybe_has_empty_blocks = ((expert_tokens_meta is None)",
      "                              or (expert_tokens_meta.expert_num_tokens_cpu",
      "                                  is None))",
      "    expert_ids_init = torch.zeros if maybe_has_empty_blocks else torch.empty",
      "",
      "    expert_ids = expert_ids_init((M_sum), device=device, dtype=torch.int32)",
      "    inv_perm = torch.empty(topk_ids.shape, device=device, dtype=torch.int32)",
      "",
      "    expert_num_tokens = None",
      "    if expert_tokens_meta is not None:",
      "        expert_num_tokens = expert_tokens_meta.expert_num_tokens",
      "    else:",
      "        expert_num_tokens = count_expert_num_tokens(topk_ids,",
      "                                                    local_num_experts,",
      "                                                    expert_map)",
      "",
      "    ep_scatter(recv_x=aq,",
      "               recv_x_scale=aq_scale,",
      "               recv_topk=topk_ids,",
      "               num_recv_tokens_per_expert=expert_num_tokens,",
      "               expert_start_loc=expert_start_loc,",
      "               expert_map=expert_map,",
      "               output_tensor=aq_out,",
      "               output_tensor_scale=aq_scale_out,",
      "               m_indices=expert_ids,",
      "               output_index=inv_perm)",
      "",
      "    return aq_out, aq_scale_out, expert_ids, inv_perm",
      "",
      "",
      "def deepgemm_unpermute_and_reduce(",
      "        a: torch.Tensor,  # Grouped gemm output",
      "        topk_ids: torch.Tensor,",
      "        topk_weights: torch.Tensor,",
      "        inv_perm: torch.Tensor,",
      "        expert_map: Optional[torch.Tensor],",
      "        output: torch.Tensor):",
      "",
      "    return ep_gather(input_tensor=a,",
      "                     recv_topk_ids=topk_ids,",
      "                     recv_topk_weight=topk_weights,",
      "                     input_index=inv_perm,",
      "                     expert_map=expert_map,",
      "                     output_tensor=output)"
    ]
  },
  {
    "type": "triton",
    "path": "vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "from typing import Optional",
      "",
      "import torch",
      "",
      "import vllm.model_executor.layers.fused_moe.modular_kernel as mk",
      "from vllm.logger import init_logger",
      "from vllm.model_executor.layers.fused_moe.config import FusedMoEQuantConfig",
      "from vllm.model_executor.layers.fused_moe.topk_weight_and_reduce import (",
      "    TopKWeightAndReduceDelegate)",
      "from vllm.model_executor.layers.fused_moe.utils import _resize_cache",
      "from vllm.triton_utils import tl, triton",
      "from vllm.utils.deep_gemm import (fp8_m_grouped_gemm_nt_masked,",
      "                                  is_deep_gemm_e8m0_used)",
      "",
      "logger = init_logger(__name__)",
      "",
      "",
      "@triton.jit",
      "def _silu_mul_fp8_quant_deep_gemm(",
      "    # Pointers ------------------------------------------------------------",
      "    input_ptr,  # 16-bit activations (E, T, 2*H)",
      "    y_q_ptr,  # fp8 quantized activations (E, T, H)",
      "    y_s_ptr,  # 16-bit scales (E, T, G)",
      "    counts_ptr,  # int32 num tokens per expert (E)",
      "",
      "    # Sizes ---------------------------------------------------------------",
      "    H: tl.constexpr,  # hidden dimension (per output)",
      "    GROUP_SIZE: tl.constexpr,  # elements per group (usually 128)",
      "",
      "    # Strides for input (elements) ---------------------------------------",
      "    stride_i_e,",
      "    stride_i_t,",
      "    stride_i_h,",
      "",
      "    # Strides for y_q (elements) -----------------------------------------",
      "    stride_yq_e,",
      "    stride_yq_t,",
      "    stride_yq_h,",
      "",
      "    # Strides for y_s (elements) -----------------------------------------",
      "    stride_ys_e,",
      "    stride_ys_t,",
      "    stride_ys_g,",
      "",
      "    # Stride for counts (elements)",
      "    stride_counts_e,",
      "",
      "    # Numeric params ------------------------------------------------------",
      "    eps: tl.constexpr,",
      "    fp8_min: tl.constexpr,",
      "    fp8_max: tl.constexpr,",
      "    use_ue8m0: tl.constexpr,",
      "",
      "    # Meta ---------------------------------------------------------------",
      "    BLOCK: tl.constexpr,",
      "    NUM_STAGES: tl.constexpr,",
      "):",
      "    G = H // GROUP_SIZE",
      "",
      "    # map program id -> (e, g)",
      "    pid = tl.program_id(0)",
      "    e = pid // G",
      "    g = pid % G",
      "",
      "    e = e.to(tl.int64)",
      "    g = g.to(tl.int64)",
      "",
      "    # number of valid tokens for this expert",
      "    n_tokens = tl.load(counts_ptr + e * stride_counts_e).to(tl.int64)",
      "",
      "    cols = tl.arange(0, BLOCK).to(tl.int64)",
      "    mask = cols < BLOCK",
      "",
      "    base_input_offset = e * stride_i_e + g * GROUP_SIZE * stride_i_h",
      "    base_gate_offset = base_input_offset + cols * stride_i_h",
      "    base_up_offset = base_input_offset + H * stride_i_h + cols * stride_i_h",
      "    base_yq_offset = (e * stride_yq_e + g * GROUP_SIZE * stride_yq_h +",
      "                      cols * stride_yq_h)",
      "    base_ys_offset = e * stride_ys_e + g * stride_ys_g",
      "",
      "    for t in tl.range(0, n_tokens, num_stages=NUM_STAGES):",
      "        gate = tl.load(input_ptr + base_gate_offset + t * stride_i_t,",
      "                       mask=mask,",
      "                       other=0.0).to(tl.float32)",
      "        up = tl.load(input_ptr + base_up_offset + t * stride_i_t,",
      "                     mask=mask,",
      "                     other=0.0)",
      "",
      "        gate = gate * (1.0 / (1.0 + tl.exp(-gate)))",
      "        y = gate * up",
      "",
      "        y_s = tl.maximum(tl.max(tl.abs(y)), eps) / fp8_max",
      "        if use_ue8m0:",
      "            y_s = tl.exp2(tl.ceil(tl.log2(y_s)))",
      "",
      "        y_q = tl.clamp(y / y_s, fp8_min, fp8_max).to(y_q_ptr.dtype.element_ty)",
      "",
      "        tl.store(y_q_ptr + base_yq_offset + t * stride_yq_t, y_q, mask=mask)",
      "        tl.store(y_s_ptr + base_ys_offset + t * stride_ys_t, y_s)",
      "",
      "",
      "def silu_mul_fp8_quant_deep_gemm(",
      "    y: torch.Tensor,  # (E, T, 2*H)",
      "    tokens_per_expert: torch.Tensor,  # (E,) number of valid tokens per expert",
      "    group_size: int = 128,",
      "    eps: float = 1e-10,",
      ") -> tuple[torch.Tensor, torch.Tensor]:",
      "    \"\"\"Quantize silu(y[..., :H]) * y[..., H:] to FP8 with group per-token scales",
      "",
      "    y has shape (E, T, 2*H). The first half of the last dimension is ",
      "    silu-activated, multiplied by the second half, then quantized into FP8.",
      "",
      "    Returns `(y_q, y_s)` where",
      "    * `y_q`: FP8 tensor, shape (E, T, H), same layout as y[..., :H]",
      "    * `y_s`: FP32 tensor, shape (E, T, H // group_size), strides (T*G, 1, T)",
      "    \"\"\"",
      "    assert y.ndim == 3, \"y must be (E, T, 2*H)\"",
      "    E, T, H2 = y.shape",
      "    assert H2 % 2 == 0, \"last dim of y must be even (2*H)\"",
      "    H = H2 // 2",
      "    G = H // group_size",
      "    assert H % group_size == 0, \"H must be divisible by group_size\"",
      "    assert tokens_per_expert.ndim == 1 and tokens_per_expert.shape[0] == E, \\",
      "        \"tokens_per_expert must be shape (E,)\"",
      "    tokens_per_expert = tokens_per_expert.to(device=y.device,",
      "                                             dtype=torch.int32)",
      "",
      "    # allocate outputs",
      "    fp8_dtype = torch.float8_e4m3fn",
      "    y_q = torch.empty((E, T, H), dtype=fp8_dtype, device=y.device)",
      "",
      "    # strides (elements)",
      "    stride_i_e, stride_i_t, stride_i_h = y.stride()",
      "    stride_yq_e, stride_yq_t, stride_yq_h = y_q.stride()",
      "",
      "    # desired scale strides (elements): (T*G, 1, T)",
      "    stride_ys_e = T * G",
      "    stride_ys_t = 1",
      "    stride_ys_g = T",
      "    y_s = torch.empty_strided((E, T, G),",
      "                              (stride_ys_e, stride_ys_t, stride_ys_g),",
      "                              dtype=torch.float32,",
      "                              device=y.device)",
      "",
      "    stride_cnt_e = tokens_per_expert.stride()[0]",
      "",
      "    # Static grid over experts and H-groups.",
      "    # A loop inside the kernel handles the token dim",
      "    grid = (E * G, )",
      "",
      "    f_info = torch.finfo(fp8_dtype)",
      "    fp8_max = f_info.max",
      "    fp8_min = f_info.min",
      "",
      "    _silu_mul_fp8_quant_deep_gemm[grid](",
      "        y,",
      "        y_q,",
      "        y_s,",
      "        tokens_per_expert,",
      "        H,",
      "        group_size,",
      "        stride_i_e,",
      "        stride_i_t,",
      "        stride_i_h,",
      "        stride_yq_e,",
      "        stride_yq_t,",
      "        stride_yq_h,",
      "        stride_ys_e,",
      "        stride_ys_t,",
      "        stride_ys_g,",
      "        stride_cnt_e,",
      "        eps,",
      "        fp8_min,",
      "        fp8_max,",
      "        is_deep_gemm_e8m0_used(),",
      "        BLOCK=group_size,",
      "        NUM_STAGES=4,",
      "        num_warps=1,",
      "    )",
      "",
      "    return y_q, y_s",
      "",
      "",
      "class BatchedDeepGemmExperts(mk.FusedMoEPermuteExpertsUnpermute):",
      "",
      "    # The Deep Gemm kernels only support block size of 128",
      "    DEEPGEMM_BLOCK_SHAPE: list[int] = [128, 128]",
      "",
      "    def __init__(self,",
      "                 max_num_tokens: int,",
      "                 num_dispatchers: int,",
      "                 block_shape: list[int],",
      "                 per_act_token_quant=False):",
      "        \"\"\"",
      "        max_num_tokens: Maximum number of tokens from a DP Rank",
      "        num_dispatchers: The number of DP dispatchers.",
      "        block_shape: Block quantization block shape.",
      "        per_act_token_quant: Per activation token quantization flag.",
      "        \"\"\"",
      "        super().__init__(",
      "            FusedMoEQuantConfig(",
      "                quant_dtype=torch.float8_e4m3fn,",
      "                per_act_token_quant=per_act_token_quant,",
      "                block_shape=block_shape,",
      "            ))",
      "        assert self.block_shape == self.DEEPGEMM_BLOCK_SHAPE",
      "        self.max_num_tokens = max_num_tokens",
      "        self.num_dispatchers = num_dispatchers",
      "",
      "    @property",
      "    def activation_formats(",
      "        self",
      "    ) -> tuple[mk.FusedMoEActivationFormat, mk.FusedMoEActivationFormat]:",
      "        return (mk.FusedMoEActivationFormat.BatchedExperts,",
      "                mk.FusedMoEActivationFormat.BatchedExperts)",
      "",
      "    def supports_chunking(self) -> bool:",
      "        return False",
      "",
      "    def supports_expert_map(self) -> bool:",
      "        return False",
      "",
      "    def finalize_weight_and_reduce_impl(self) -> mk.TopKWeightAndReduce:",
      "        # Let PrepareAndFinalize::finalize() decide the impl.",
      "        return TopKWeightAndReduceDelegate()",
      "",
      "    def workspace_shapes(",
      "        self,",
      "        a: torch.Tensor,",
      "        aq: torch.Tensor,",
      "        M: int,",
      "        N: int,",
      "        K: int,",
      "        topk: int,",
      "        global_num_experts: int,",
      "        local_num_experts: int,",
      "        expert_tokens_metadata: Optional[mk.ExpertTokensMetadata],",
      "    ) -> tuple[tuple[int, ...], tuple[int, ...], tuple[int, ...], torch.dtype]:",
      "        assert a.dim() == 2",
      "        # FIXME (varun): We should be able to dispatch only from the leader",
      "        # DP ranks in the case of TP > 1. At the moment, all the Ranks",
      "        # end up sending their tokens. This needs to be fixed.",
      "        num_dispatchers = self.num_dispatchers",
      "        num_experts = local_num_experts",
      "        max_num_tokens = a.size(",
      "            0) if self.max_num_tokens is None else self.max_num_tokens",
      "        workspace13 = (num_experts, max_num_tokens * num_dispatchers,",
      "                       max(K, N))",
      "        workspace2 = (num_experts, max_num_tokens * num_dispatchers, (N // 2))",
      "        output = (num_experts, max_num_tokens * num_dispatchers, K)",
      "        return (workspace13, workspace2, output, a.dtype)",
      "",
      "    def apply(",
      "        self,",
      "        output: torch.Tensor,",
      "        hidden_states: torch.Tensor,",
      "        w1: torch.Tensor,",
      "        w2: torch.Tensor,",
      "        topk_weights: torch.Tensor,",
      "        topk_ids: torch.Tensor,",
      "        activation: str,",
      "        global_num_experts: int,",
      "        expert_map: Optional[torch.Tensor],",
      "        w1_scale: Optional[torch.Tensor],",
      "        w2_scale: Optional[torch.Tensor],",
      "        w1_zp: Optional[torch.Tensor],",
      "        w2_zp: Optional[torch.Tensor],",
      "        a1q_scale: Optional[torch.Tensor],",
      "        a2_scale: Optional[torch.Tensor],",
      "        workspace13: torch.Tensor,",
      "        workspace2: torch.Tensor,",
      "        expert_tokens_meta: Optional[mk.ExpertTokensMetadata],",
      "        apply_router_weight_on_input: bool,",
      "    ):",
      "        assert expert_tokens_meta is not None",
      "        expert_num_tokens = expert_tokens_meta.expert_num_tokens",
      "",
      "        assert hidden_states.ndim == 3",
      "        assert self.block_shape is not None",
      "",
      "        a1q = hidden_states",
      "        _, N, K = w1.size()",
      "",
      "        assert w2.size(1) == K",
      "",
      "        E, max_num_tokens, N, K, top_k_num = mk._moe_problem_size(",
      "            hidden_states, w1, w2, topk_ids)",
      "",
      "        workspace1 = _resize_cache(workspace13, (E, max_num_tokens, N))",
      "",
      "        # (from deepgemm docs) : A value hint (which is a value on CPU)",
      "        # for the M expectation of each batch, correctly setting this value",
      "        # may lead to better performance.",
      "        expected_m = max_num_tokens",
      "        fp8_m_grouped_gemm_nt_masked((a1q, a1q_scale), (w1, w1_scale),",
      "                                     workspace1, expert_num_tokens, expected_m)",
      "",
      "        a2q, a2q_scale = silu_mul_fp8_quant_deep_gemm(workspace1,",
      "                                                      expert_num_tokens)",
      "",
      "        fp8_m_grouped_gemm_nt_masked((a2q, a2q_scale), (w2, w2_scale), output,",
      "                                     expert_num_tokens, expected_m)"
    ]
  },
  {
    "type": "triton",
    "path": "tests/test_triton_utils.py",
    "source": [
      "# SPDX-License-Identifier: Apache-2.0",
      "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project",
      "",
      "import sys",
      "import types",
      "from unittest import mock",
      "",
      "from vllm.triton_utils.importing import (TritonLanguagePlaceholder,",
      "                                         TritonPlaceholder)",
      "",
      "",
      "def test_triton_placeholder_is_module():",
      "    triton = TritonPlaceholder()",
      "    assert isinstance(triton, types.ModuleType)",
      "    assert triton.__name__ == \"triton\"",
      "",
      "",
      "def test_triton_language_placeholder_is_module():",
      "    triton_language = TritonLanguagePlaceholder()",
      "    assert isinstance(triton_language, types.ModuleType)",
      "    assert triton_language.__name__ == \"triton.language\"",
      "",
      "",
      "def test_triton_placeholder_decorators():",
      "    triton = TritonPlaceholder()",
      "",
      "    @triton.jit",
      "    def foo(x):",
      "        return x",
      "",
      "    @triton.autotune",
      "    def bar(x):",
      "        return x",
      "",
      "    @triton.heuristics",
      "    def baz(x):",
      "        return x",
      "",
      "    assert foo(1) == 1",
      "    assert bar(2) == 2",
      "    assert baz(3) == 3",
      "",
      "",
      "def test_triton_placeholder_decorators_with_args():",
      "    triton = TritonPlaceholder()",
      "",
      "    @triton.jit(debug=True)",
      "    def foo(x):",
      "        return x",
      "",
      "    @triton.autotune(configs=[], key=\"x\")",
      "    def bar(x):",
      "        return x",
      "",
      "    @triton.heuristics(",
      "        {\"BLOCK_SIZE\": lambda args: 128 if args[\"x\"] > 1024 else 64})",
      "    def baz(x):",
      "        return x",
      "",
      "    assert foo(1) == 1",
      "    assert bar(2) == 2",
      "    assert baz(3) == 3",
      "",
      "",
      "def test_triton_placeholder_language():",
      "    lang = TritonLanguagePlaceholder()",
      "    assert isinstance(lang, types.ModuleType)",
      "    assert lang.__name__ == \"triton.language\"",
      "    assert lang.constexpr is None",
      "    assert lang.dtype is None",
      "    assert lang.int64 is None",
      "",
      "",
      "def test_triton_placeholder_language_from_parent():",
      "    triton = TritonPlaceholder()",
      "    lang = triton.language",
      "    assert isinstance(lang, TritonLanguagePlaceholder)",
      "",
      "",
      "def test_no_triton_fallback():",
      "    # clear existing triton modules",
      "    sys.modules.pop(\"triton\", None)",
      "    sys.modules.pop(\"triton.language\", None)",
      "    sys.modules.pop(\"vllm.triton_utils\", None)",
      "    sys.modules.pop(\"vllm.triton_utils.importing\", None)",
      "",
      "    # mock triton not being installed",
      "    with mock.patch.dict(sys.modules, {\"triton\": None}):",
      "        from vllm.triton_utils import HAS_TRITON, tl, triton",
      "        assert HAS_TRITON is False",
      "        assert triton.__class__.__name__ == \"TritonPlaceholder\"",
      "        assert triton.language.__class__.__name__ == \"TritonLanguagePlaceholder\"",
      "        assert tl.__class__.__name__ == \"TritonLanguagePlaceholder\""
    ]
  }
]